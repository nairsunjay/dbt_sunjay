

============================== 04:50:32.589806 | 1d90fe44-dcf0-463a-be01-15ef1c8374e8 ==============================
[0m04:50:32.589806 [info ] [MainThread]: Running with dbt=1.6.17
[0m04:50:32.590483 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/workspaces/dbt_interview/transformation/logs', 'debug': 'False', 'profiles_dir': '/workspaces/dbt_interview/transformation', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt build', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m04:50:32.779020 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m04:50:32.779880 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 3 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m04:50:32.780595 [debug] [MainThread]: Command `dbt build` failed at 04:50:32.780470 after 0.21 seconds
[0m04:50:32.781057 [debug] [MainThread]: Flushing usage events
[0m04:51:41.994142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f31796623d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f317991fa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3179608ed0>]}


============================== 04:51:41.996798 | e328dc54-c758-4d42-a042-4dbcb6114912 ==============================
[0m04:51:41.996798 [info ] [MainThread]: Running with dbt=1.6.17
[0m04:51:41.997453 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/workspaces/dbt_interview/transformation', 'version_check': 'True', 'debug': 'False', 'log_path': '/workspaces/dbt_interview/transformation/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m04:51:42.025814 [error] [MainThread]: Encountered an error:
Runtime Error
  at path []: Additional properties are not allowed ('config' was unexpected)

Error encountered in /workspaces/dbt_interview/transformation/dbt_project.yml
[0m04:51:42.026554 [debug] [MainThread]: Command `dbt deps` failed at 04:51:42.026432 after 0.04 seconds
[0m04:51:42.026978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f317934e0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3179347590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f317df31a90>]}
[0m04:51:42.027455 [debug] [MainThread]: Flushing usage events
[0m04:52:07.448252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7aba3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7769390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7a3f690>]}


============================== 04:52:07.450857 | 5b036ffc-3e60-4c54-bf2e-d049e72b8b7a ==============================
[0m04:52:07.450857 [info ] [MainThread]: Running with dbt=1.6.17
[0m04:52:07.451487 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/workspaces/dbt_interview/transformation', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/workspaces/dbt_interview/transformation/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt deps', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:52:07.495493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5b036ffc-3e60-4c54-bf2e-d049e72b8b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea74baed0>]}
[0m04:52:07.497190 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-lnnf05nj'
[0m04:52:07.497738 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m04:52:07.647205 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m04:52:07.647906 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json
[0m04:52:07.717532 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json 200
[0m04:52:07.720140 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m04:52:07.789779 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m04:52:07.795391 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m04:52:07.865049 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m04:52:07.883051 [info ] [MainThread]: Installing dbt-labs/codegen
[0m04:52:08.120758 [info ] [MainThread]: Installed from version 0.11.0
[0m04:52:08.121290 [info ] [MainThread]: Updated version available: 0.12.1
[0m04:52:08.121818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5b036ffc-3e60-4c54-bf2e-d049e72b8b7a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea77c6850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea74b9dd0>]}
[0m04:52:08.122294 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m04:52:08.388774 [info ] [MainThread]: Installed from version 1.2.0
[0m04:52:08.389270 [info ] [MainThread]: Up to date!
[0m04:52:08.389732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5b036ffc-3e60-4c54-bf2e-d049e72b8b7a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7a752d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7a74110>]}
[0m04:52:08.390167 [info ] [MainThread]: Installing calogica/dbt_date
[0m04:52:08.591973 [info ] [MainThread]: Installed from version 0.9.2
[0m04:52:08.592604 [info ] [MainThread]: Updated version available: 0.10.1
[0m04:52:08.593074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5b036ffc-3e60-4c54-bf2e-d049e72b8b7a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea794f790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea74ba9d0>]}
[0m04:52:08.593519 [info ] [MainThread]: 
[0m04:52:08.593941 [info ] [MainThread]: Updates available for packages: ['dbt-labs/codegen', 'calogica/dbt_date']                 
Update your versions in packages.yml, then run dbt deps
[0m04:52:08.594986 [debug] [MainThread]: Command `dbt deps` succeeded at 04:52:08.594869 after 1.16 seconds
[0m04:52:08.595381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7aab190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7ec2550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdeac035690>]}
[0m04:52:08.595898 [debug] [MainThread]: Flushing usage events
[0m04:52:12.676279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b6fb0250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33bbb35710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b753f550>]}


============================== 04:52:12.678873 | ca662291-5622-4965-bcf1-6dd532afcfe9 ==============================
[0m04:52:12.678873 [info ] [MainThread]: Running with dbt=1.6.17
[0m04:52:12.679501 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/workspaces/dbt_interview/transformation', 'log_path': '/workspaces/dbt_interview/transformation/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m04:52:12.786174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b7249dd0>]}
[0m04:52:12.804360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b6fb3090>]}
[0m04:52:12.805018 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m04:52:12.832039 [debug] [MainThread]: checksum: 5068fe71dce20d17d52958fa9cd93ea8f20705f4dc29725ea3e5886531e90d9e, vars: {}, profile: , target: , version: 1.6.17
[0m04:52:12.832844 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m04:52:12.833314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b6ff4810>]}
[0m04:52:14.379046 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m04:52:14.385150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33abde3b90>]}
[0m04:52:14.403560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9c34750>]}
[0m04:52:14.404054 [info ] [MainThread]: Found 14 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m04:52:14.406830 [info ] [MainThread]: 
[0m04:52:14.407675 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m04:52:14.409480 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m04:52:14.422743 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m04:52:14.423147 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m04:52:14.423520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:52:15.607834 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m04:52:15.609415 [debug] [ThreadPool]: On list_dbt: Close
[0m04:52:15.615038 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m04:52:15.615530 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m04:52:15.616001 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:15.625396 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.626746 [debug] [ThreadPool]: On list_dbt: Close
[0m04:52:15.629509 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m04:52:15.630264 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m04:52:15.635155 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m04:52:15.635543 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m04:52:15.635914 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:15.644312 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.644698 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m04:52:15.645119 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m04:52:15.646452 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.647383 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m04:52:15.647770 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m04:52:15.648193 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m04:52:15.649012 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.649415 [debug] [ThreadPool]: On create_dbt_main: Close
[0m04:52:15.652486 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now create_dbt_staging)
[0m04:52:15.653307 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m04:52:15.655763 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m04:52:15.656178 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m04:52:15.656540 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:15.664929 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.665318 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m04:52:15.665688 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m04:52:15.666315 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.667222 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m04:52:15.667608 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m04:52:15.667975 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m04:52:15.668625 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.669026 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m04:52:15.673260 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_main)
[0m04:52:15.680093 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m04:52:15.680481 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m04:52:15.680863 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:15.689643 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.690033 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m04:52:15.690417 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m04:52:15.717200 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.718456 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m04:52:15.719374 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m04:52:15.719750 [debug] [ThreadPool]: On list_dbt_main: Close
[0m04:52:15.723045 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m04:52:15.725502 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m04:52:15.725890 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m04:52:15.726248 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:15.734821 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.735197 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m04:52:15.735584 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m04:52:15.759912 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.761510 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m04:52:15.762004 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m04:52:15.762440 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m04:52:15.767609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9b742d0>]}
[0m04:52:15.768100 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:15.768506 [debug] [MainThread]: On master: BEGIN
[0m04:52:15.768865 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:52:15.778117 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m04:52:15.778509 [debug] [MainThread]: On master: COMMIT
[0m04:52:15.778875 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:15.779242 [debug] [MainThread]: On master: COMMIT
[0m04:52:15.779854 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m04:52:15.780251 [debug] [MainThread]: On master: Close
[0m04:52:15.782516 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:52:15.782993 [info ] [MainThread]: 
[0m04:52:15.790679 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m04:52:15.791255 [info ] [Thread-1 (]: 1 of 15 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m04:52:15.792142 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.elastic_dbt_interview.stg_salesforce__account'
[0m04:52:15.792635 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m04:52:15.801907 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.803618 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 04:52:15.793010 => 04:52:15.803399
[0m04:52:15.804061 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m04:52:15.840151 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.841877 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.842385 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m04:52:15.842803 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m04:52:15.853929 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.854375 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.854871 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m04:52:15.856302 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.863417 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.863864 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m04:52:15.864718 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.868197 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.868653 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m04:52:15.869786 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.888132 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m04:52:15.888586 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.889008 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m04:52:15.894034 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.899532 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.899972 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m04:52:15.903717 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.905308 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 04:52:15.804519 => 04:52:15.905115
[0m04:52:15.905744 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m04:52:15.929563 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b3d21a90>]}
[0m04:52:15.930211 [info ] [Thread-1 (]: 1 of 15 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.14s]
[0m04:52:15.930910 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m04:52:15.931425 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m04:52:15.932045 [info ] [Thread-1 (]: 2 of 15 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m04:52:15.932858 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m04:52:15.933319 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m04:52:15.938111 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.938852 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 04:52:15.933664 => 04:52:15.938603
[0m04:52:15.939305 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m04:52:15.943897 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.944642 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.945078 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m04:52:15.945523 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:15.954618 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.955081 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.955550 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m04:52:15.956777 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.960266 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.960701 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m04:52:15.961513 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.964800 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.965250 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m04:52:15.966171 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.967999 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m04:52:15.968434 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.968852 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m04:52:15.972613 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.975594 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.976037 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m04:52:15.980639 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.982069 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 04:52:15.939620 => 04:52:15.981878
[0m04:52:15.982512 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m04:52:16.001623 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9b72310>]}
[0m04:52:16.002301 [info ] [Thread-1 (]: 2 of 15 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.07s]
[0m04:52:16.003018 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m04:52:16.003551 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m04:52:16.004185 [info ] [Thread-1 (]: 3 of 15 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m04:52:16.005009 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m04:52:16.005495 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m04:52:16.008856 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.009526 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 04:52:16.005812 => 04:52:16.009318
[0m04:52:16.009948 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m04:52:16.014793 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.015402 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.015908 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m04:52:16.016337 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.025334 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.025767 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.026218 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m04:52:16.027394 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.030819 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.031278 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m04:52:16.032069 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.035359 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.035819 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m04:52:16.036588 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.038403 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m04:52:16.038830 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.039247 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m04:52:16.043802 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.046743 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.047202 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m04:52:16.050754 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.052174 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 04:52:16.010258 => 04:52:16.051987
[0m04:52:16.052662 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m04:52:16.072675 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b6fc2c10>]}
[0m04:52:16.073300 [info ] [Thread-1 (]: 3 of 15 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.07s]
[0m04:52:16.073987 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m04:52:16.074500 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m04:52:16.075124 [info ] [Thread-1 (]: 4 of 15 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m04:52:16.075949 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m04:52:16.076387 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m04:52:16.081440 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.082110 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 04:52:16.076703 => 04:52:16.081900
[0m04:52:16.082585 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m04:52:16.087439 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.088034 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.088500 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m04:52:16.088918 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.097938 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.098389 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.098838 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m04:52:16.099718 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.103318 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.103761 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m04:52:16.104534 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.107794 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.108233 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m04:52:16.109032 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.110844 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m04:52:16.111299 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.111716 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m04:52:16.116410 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.119367 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.119834 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m04:52:16.123414 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.124994 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 04:52:16.082899 => 04:52:16.124779
[0m04:52:16.125436 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m04:52:16.144940 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9b6aad0>]}
[0m04:52:16.145571 [info ] [Thread-1 (]: 4 of 15 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.07s]
[0m04:52:16.146301 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m04:52:16.146811 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m04:52:16.147440 [info ] [Thread-1 (]: 5 of 15 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m04:52:16.148250 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m04:52:16.148684 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m04:52:16.152013 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.152750 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 04:52:16.148995 => 04:52:16.152513
[0m04:52:16.153179 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m04:52:16.157666 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.158260 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.158694 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m04:52:16.159102 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.167564 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.168001 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.168522 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m04:52:16.169815 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.173426 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.173863 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m04:52:16.174673 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.178066 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.178536 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m04:52:16.179325 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.181162 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m04:52:16.181638 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.182060 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m04:52:16.185922 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.190640 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.191090 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m04:52:16.195769 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.197198 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 04:52:16.153487 => 04:52:16.197007
[0m04:52:16.197681 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m04:52:16.219898 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9b7fed0>]}
[0m04:52:16.220513 [info ] [Thread-1 (]: 5 of 15 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.07s]
[0m04:52:16.221215 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m04:52:16.221728 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m04:52:16.222409 [info ] [Thread-1 (]: 6 of 15 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m04:52:16.223201 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m04:52:16.223622 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m04:52:16.227209 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.227899 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 04:52:16.223987 => 04:52:16.227616
[0m04:52:16.228325 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m04:52:16.233427 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.234030 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.234454 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m04:52:16.234903 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.244430 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.244869 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.245339 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m04:52:16.246656 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.250120 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.250562 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m04:52:16.251380 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.254638 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.255081 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m04:52:16.255915 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.257778 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m04:52:16.258305 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.258723 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m04:52:16.263613 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.266452 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.266886 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m04:52:16.270462 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.271977 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 04:52:16.228638 => 04:52:16.271721
[0m04:52:16.272522 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m04:52:16.291137 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a99f8bd0>]}
[0m04:52:16.291751 [info ] [Thread-1 (]: 6 of 15 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.07s]
[0m04:52:16.292516 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m04:52:16.293063 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m04:52:16.293711 [info ] [Thread-1 (]: 7 of 15 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m04:52:16.294487 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m04:52:16.294949 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m04:52:16.298266 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.298938 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 04:52:16.295264 => 04:52:16.298726
[0m04:52:16.299359 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m04:52:16.304145 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.304751 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.305181 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m04:52:16.305602 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.314169 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.314628 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.315112 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m04:52:16.316319 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.319769 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.320270 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m04:52:16.321040 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.324299 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.324741 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m04:52:16.325525 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.327283 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m04:52:16.327721 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.328159 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m04:52:16.332695 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.337486 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.337922 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m04:52:16.341465 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.343082 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 04:52:16.299690 => 04:52:16.342891
[0m04:52:16.343545 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m04:52:16.364640 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9ab4210>]}
[0m04:52:16.365237 [info ] [Thread-1 (]: 7 of 15 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.07s]
[0m04:52:16.365938 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m04:52:16.366455 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m04:52:16.367092 [info ] [Thread-1 (]: 8 of 15 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m04:52:16.367894 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m04:52:16.368347 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m04:52:16.371514 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.372181 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 04:52:16.368671 => 04:52:16.371972
[0m04:52:16.372689 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m04:52:16.377315 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.377965 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.378392 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m04:52:16.378808 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.387765 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.388257 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.388723 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m04:52:16.389701 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.393246 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.393687 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m04:52:16.394452 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.397853 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.398305 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m04:52:16.399070 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.400894 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m04:52:16.401338 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.401781 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m04:52:16.405666 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.408534 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.408976 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m04:52:16.413532 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.414972 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 04:52:16.373002 => 04:52:16.414782
[0m04:52:16.415398 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m04:52:16.433948 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9a7ed10>]}
[0m04:52:16.434571 [info ] [Thread-1 (]: 8 of 15 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.07s]
[0m04:52:16.435302 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m04:52:16.435815 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m04:52:16.436470 [info ] [Thread-1 (]: 9 of 15 START sql view model staging.stg_salesforce__pricebook_entry ........... [RUN]
[0m04:52:16.437253 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m04:52:16.437690 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m04:52:16.440828 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.441586 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 04:52:16.438009 => 04:52:16.441293
[0m04:52:16.442055 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m04:52:16.446913 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.447472 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.447893 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m04:52:16.448312 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.456816 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.457269 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.457708 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m04:52:16.458666 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.462171 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.462674 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m04:52:16.463453 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.468463 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.468903 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m04:52:16.469704 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.471517 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m04:52:16.471968 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.472407 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m04:52:16.476210 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.479082 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.479541 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m04:52:16.484269 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.485671 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 04:52:16.442432 => 04:52:16.485481
[0m04:52:16.486106 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m04:52:16.507829 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9ad37d0>]}
[0m04:52:16.508488 [info ] [Thread-1 (]: 9 of 15 OK created sql view model staging.stg_salesforce__pricebook_entry ...... [[32mOK[0m in 0.07s]
[0m04:52:16.509178 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m04:52:16.509679 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m04:52:16.510283 [info ] [Thread-1 (]: 10 of 15 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m04:52:16.511079 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m04:52:16.511518 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m04:52:16.514813 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.515546 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 04:52:16.511834 => 04:52:16.515328
[0m04:52:16.515977 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m04:52:16.520574 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.521187 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.521608 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m04:52:16.522054 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.531343 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.531776 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.532274 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m04:52:16.533312 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.536709 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.537145 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m04:52:16.537908 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.541258 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.541740 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m04:52:16.542530 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.544298 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m04:52:16.544727 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.545180 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m04:52:16.549866 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.552917 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.553394 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m04:52:16.556987 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.558489 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 04:52:16.516299 => 04:52:16.558304
[0m04:52:16.558918 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m04:52:16.580154 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9a619d0>]}
[0m04:52:16.580755 [info ] [Thread-1 (]: 10 of 15 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.07s]
[0m04:52:16.581496 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m04:52:16.582021 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m04:52:16.582610 [info ] [Thread-1 (]: 11 of 15 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m04:52:16.583392 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m04:52:16.583824 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m04:52:16.587016 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.587751 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 04:52:16.584190 => 04:52:16.587471
[0m04:52:16.588182 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m04:52:16.592840 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.593453 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.593876 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m04:52:16.594305 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.602888 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.603354 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.603803 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m04:52:16.604730 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.608201 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.608644 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m04:52:16.609425 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.614710 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.615150 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m04:52:16.615974 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.617739 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m04:52:16.618210 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.618639 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m04:52:16.622371 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.625178 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.625664 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m04:52:16.630206 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.631677 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 04:52:16.588492 => 04:52:16.631490
[0m04:52:16.632101 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m04:52:16.652804 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9a59050>]}
[0m04:52:16.653440 [info ] [Thread-1 (]: 11 of 15 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.07s]
[0m04:52:16.654135 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m04:52:16.654637 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m04:52:16.655266 [info ] [Thread-1 (]: 12 of 15 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m04:52:16.656029 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m04:52:16.656458 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m04:52:16.659687 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.660419 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 04:52:16.656776 => 04:52:16.660151
[0m04:52:16.660903 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m04:52:16.665809 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.666434 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.666858 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m04:52:16.667298 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.676430 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.676879 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.677321 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m04:52:16.678313 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.681876 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.682403 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m04:52:16.683185 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.686512 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.686966 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m04:52:16.687808 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.689694 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m04:52:16.690132 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.690561 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m04:52:16.695395 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.698325 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.698761 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m04:52:16.702427 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.703902 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 04:52:16.661223 => 04:52:16.703709
[0m04:52:16.704384 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m04:52:16.723990 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a990b110>]}
[0m04:52:16.724619 [info ] [Thread-1 (]: 12 of 15 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.07s]
[0m04:52:16.725352 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m04:52:16.725875 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m04:52:16.726497 [info ] [Thread-1 (]: 13 of 15 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m04:52:16.727295 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m04:52:16.727739 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m04:52:16.731325 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.732006 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 04:52:16.728091 => 04:52:16.731799
[0m04:52:16.732462 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m04:52:16.737124 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.737752 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.738175 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m04:52:16.738586 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.747861 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.748352 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.748867 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m04:52:16.750356 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.756135 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.756591 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m04:52:16.757409 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.760673 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.761124 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m04:52:16.761896 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.763788 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m04:52:16.764244 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.764663 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m04:52:16.769463 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.772403 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.772851 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m04:52:16.776488 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.778032 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 04:52:16.732774 => 04:52:16.777843
[0m04:52:16.778460 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m04:52:16.802584 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9a0e290>]}
[0m04:52:16.803230 [info ] [Thread-1 (]: 13 of 15 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.08s]
[0m04:52:16.803926 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m04:52:16.804438 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m04:52:16.805058 [info ] [Thread-1 (]: 14 of 15 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m04:52:16.805859 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m04:52:16.806393 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m04:52:16.809732 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.810383 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 04:52:16.806747 => 04:52:16.810167
[0m04:52:16.810844 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m04:52:16.815484 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.816088 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.816512 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m04:52:16.816931 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.826533 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.826983 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.827432 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m04:52:16.828454 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.832100 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.832616 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m04:52:16.833410 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.836813 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.837252 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m04:52:16.838118 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.839886 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m04:52:16.840321 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.840738 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m04:52:16.845398 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.848307 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.848743 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m04:52:16.852365 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.853841 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 04:52:16.811159 => 04:52:16.853637
[0m04:52:16.854311 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m04:52:16.873657 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9935710>]}
[0m04:52:16.874279 [info ] [Thread-1 (]: 14 of 15 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.07s]
[0m04:52:16.874976 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m04:52:16.875537 [debug] [Thread-1 (]: Began running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m04:52:16.876176 [info ] [Thread-1 (]: 15 of 15 START seed file main.dbt_project_evaluator_exceptions ................. [RUN]
[0m04:52:16.876975 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now seed.elastic_dbt_interview.dbt_project_evaluator_exceptions)
[0m04:52:16.877454 [debug] [Thread-1 (]: Began compiling node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m04:52:16.877979 [debug] [Thread-1 (]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (compile): 04:52:16.877771 => 04:52:16.877773
[0m04:52:16.878415 [debug] [Thread-1 (]: Began executing node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m04:52:16.904355 [debug] [Thread-1 (]: Using duckdb connection "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"
[0m04:52:16.904805 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: BEGIN
[0m04:52:16.905263 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.914055 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.914508 [debug] [Thread-1 (]: Using duckdb connection "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"
[0m04:52:16.914939 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"} */

    create table "dbt"."main"."dbt_project_evaluator_exceptions" ("fct_name" text,"column_name" text,"id_to_exclude" text,"comment" text)
  
[0m04:52:16.915755 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.929961 [debug] [Thread-1 (]: Using duckdb connection "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"
[0m04:52:16.930414 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: 
          COPY "dbt"."main"."dbt_project_evaluator_exceptions" FROM '/workspaces/dbt_interview/transformation/seeds/dbt_project_evaluator_exceptions.csv' (FORMAT CSV, HEADER TRUE)
        ...
[0m04:52:16.934540 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.940480 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"
[0m04:52:16.946246 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: COMMIT
[0m04:52:16.946714 [debug] [Thread-1 (]: Using duckdb connection "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"
[0m04:52:16.947149 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: COMMIT
[0m04:52:16.951838 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.952786 [debug] [Thread-1 (]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (execute): 04:52:16.878726 => 04:52:16.952597
[0m04:52:16.953221 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: Close
[0m04:52:16.975466 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b3c87810>]}
[0m04:52:16.976073 [info ] [Thread-1 (]: 15 of 15 OK loaded seed file main.dbt_project_evaluator_exceptions ............. [[32mINSERT 2[0m in 0.10s]
[0m04:52:16.976843 [debug] [Thread-1 (]: Finished running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m04:52:16.978786 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:16.979178 [debug] [MainThread]: On master: BEGIN
[0m04:52:16.979543 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m04:52:16.988646 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m04:52:16.989042 [debug] [MainThread]: On master: COMMIT
[0m04:52:16.989436 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:16.989809 [debug] [MainThread]: On master: COMMIT
[0m04:52:16.990441 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m04:52:16.990870 [debug] [MainThread]: On master: Close
[0m04:52:16.993359 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:52:16.993726 [debug] [MainThread]: Connection 'list_dbt_staging' was properly closed.
[0m04:52:16.994082 [debug] [MainThread]: Connection 'seed.elastic_dbt_interview.dbt_project_evaluator_exceptions' was properly closed.
[0m04:52:16.994520 [info ] [MainThread]: 
[0m04:52:16.995096 [info ] [MainThread]: Finished running 14 view models, 1 seed in 0 hours 0 minutes and 2.59 seconds (2.59s).
[0m04:52:16.997954 [debug] [MainThread]: Command end result
[0m04:52:17.012491 [info ] [MainThread]: 
[0m04:52:17.012984 [info ] [MainThread]: [32mCompleted successfully[0m
[0m04:52:17.013453 [info ] [MainThread]: 
[0m04:52:17.013912 [info ] [MainThread]: Done. PASS=15 WARN=0 ERROR=0 SKIP=0 TOTAL=15
[0m04:52:17.014600 [debug] [MainThread]: Command `dbt build` succeeded at 04:52:17.014490 after 4.35 seconds
[0m04:52:17.015022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b744ee50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b753f6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b7ae3950>]}
[0m04:52:17.015455 [debug] [MainThread]: Flushing usage events
[0m03:15:51.095218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13b74dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13ebaad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13b75050>]}


============================== 03:15:51.099332 | 4ab9fbc2-941e-4162-8d34-9c144dfe19e2 ==============================
[0m03:15:51.099332 [info ] [MainThread]: Running with dbt=1.6.17
[0m03:15:51.100140 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/workspaces/dbt_interview/transformation/logs', 'profiles_dir': '/workspaces/dbt_interview/transformation', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m03:15:51.286310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc138dbc50>]}
[0m03:15:51.304893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13d5b690>]}
[0m03:15:51.305800 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m03:15:51.333500 [debug] [MainThread]: checksum: 5068fe71dce20d17d52958fa9cd93ea8f20705f4dc29725ea3e5886531e90d9e, vars: {}, profile: , target: , version: 1.6.17
[0m03:15:51.415710 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m03:15:51.416211 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m03:15:51.417493 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m03:15:51.424534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13f4d110>]}
[0m03:15:51.444201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc08742e50>]}
[0m03:15:51.445091 [info ] [MainThread]: Found 14 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m03:15:51.445659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13d45550>]}
[0m03:15:51.448531 [info ] [MainThread]: 
[0m03:15:51.449441 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m03:15:51.451285 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m03:15:51.465504 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m03:15:51.465950 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m03:15:51.466317 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:15:51.491481 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.493009 [debug] [ThreadPool]: On list_dbt: Close
[0m03:15:51.496709 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m03:15:51.497517 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m03:15:51.502621 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m03:15:51.503005 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m03:15:51.503357 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:15:51.513325 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.513901 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m03:15:51.514506 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m03:15:51.516107 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.517123 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m03:15:51.517550 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m03:15:51.517916 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m03:15:51.518829 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.519259 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m03:15:51.523746 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_main)
[0m03:15:51.530309 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m03:15:51.530688 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m03:15:51.531034 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:15:51.540956 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.541419 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m03:15:51.541959 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m03:15:51.569043 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.573946 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m03:15:51.574986 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m03:15:51.575415 [debug] [ThreadPool]: On list_dbt_main: Close
[0m03:15:51.579277 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m03:15:51.582102 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m03:15:51.582470 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m03:15:51.582832 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:15:51.592038 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.592426 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m03:15:51.592789 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m03:15:51.618044 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.619746 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m03:15:51.620275 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m03:15:51.620619 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m03:15:51.626184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc1481f590>]}
[0m03:15:51.626698 [debug] [MainThread]: Using duckdb connection "master"
[0m03:15:51.627073 [debug] [MainThread]: On master: BEGIN
[0m03:15:51.627459 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:15:51.637843 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m03:15:51.638235 [debug] [MainThread]: On master: COMMIT
[0m03:15:51.638593 [debug] [MainThread]: Using duckdb connection "master"
[0m03:15:51.638986 [debug] [MainThread]: On master: COMMIT
[0m03:15:51.639597 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m03:15:51.639973 [debug] [MainThread]: On master: Close
[0m03:15:51.642611 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:15:51.643053 [info ] [MainThread]: 
[0m03:15:51.646022 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m03:15:51.646552 [info ] [Thread-1 (]: 1 of 14 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m03:15:51.647718 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.elastic_dbt_interview.stg_salesforce__account'
[0m03:15:51.648146 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m03:15:51.658585 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.659307 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 03:15:51.648446 => 03:15:51.659089
[0m03:15:51.659753 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m03:15:51.697589 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.698295 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.698713 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m03:15:51.699139 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m03:15:51.713642 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.714093 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.714629 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m03:15:51.716109 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.723958 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.724395 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m03:15:51.725276 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.728986 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.730572 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m03:15:51.732335 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.754310 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m03:15:51.754797 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.755256 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m03:15:51.760345 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.766547 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.767005 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m03:15:51.772594 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.774162 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 03:15:51.660046 => 03:15:51.773962
[0m03:15:51.774621 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m03:15:51.798531 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc087604d0>]}
[0m03:15:51.799236 [info ] [Thread-1 (]: 1 of 14 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.15s]
[0m03:15:51.799943 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m03:15:51.800449 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m03:15:51.801172 [info ] [Thread-1 (]: 2 of 14 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m03:15:51.802130 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m03:15:51.802616 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m03:15:51.806789 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.807789 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 03:15:51.803025 => 03:15:51.807350
[0m03:15:51.808267 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m03:15:51.814570 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.815416 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.815859 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m03:15:51.816259 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:51.826658 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.827119 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.827760 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m03:15:51.829308 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.833603 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.834030 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m03:15:51.834866 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.840572 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.841206 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m03:15:51.842017 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.843960 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m03:15:51.844461 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.844925 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m03:15:51.850048 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.853162 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.853666 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m03:15:51.857566 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.859744 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 03:15:51.808593 => 03:15:51.859387
[0m03:15:51.860398 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m03:15:51.888979 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc0832c4d0>]}
[0m03:15:51.889764 [info ] [Thread-1 (]: 2 of 14 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.09s]
[0m03:15:51.890528 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m03:15:51.891098 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m03:15:51.891733 [info ] [Thread-1 (]: 3 of 14 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m03:15:51.892606 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m03:15:51.893081 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m03:15:51.897389 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.898221 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 03:15:51.893471 => 03:15:51.897976
[0m03:15:51.898640 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m03:15:51.903954 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.904603 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.905027 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m03:15:51.905437 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:51.918827 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.919315 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.919767 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m03:15:51.920971 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.925235 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.925680 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m03:15:51.926563 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.930310 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.930755 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m03:15:51.931518 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.933379 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m03:15:51.933820 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.934220 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m03:15:51.938021 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.941349 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.941778 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m03:15:51.946515 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.948057 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 03:15:51.898936 => 03:15:51.947852
[0m03:15:51.948498 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m03:15:51.969300 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc138ff490>]}
[0m03:15:51.969968 [info ] [Thread-1 (]: 3 of 14 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.08s]
[0m03:15:51.970695 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m03:15:51.971236 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m03:15:51.971897 [info ] [Thread-1 (]: 4 of 14 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m03:15:51.972955 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m03:15:51.973517 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m03:15:51.977199 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:51.977999 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 03:15:51.973834 => 03:15:51.977781
[0m03:15:51.978425 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m03:15:51.983568 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:51.984248 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:51.984661 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m03:15:51.985056 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:51.994956 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.995410 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:51.995840 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m03:15:51.996799 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.000713 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:52.001170 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m03:15:52.001952 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.049308 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:52.049818 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m03:15:52.050837 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.052881 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m03:15:52.053324 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:52.053747 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m03:15:52.057598 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.060561 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:52.061033 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m03:15:52.065794 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.067276 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 03:15:51.978785 => 03:15:52.067042
[0m03:15:52.067778 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m03:15:52.089056 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc15569ad0>]}
[0m03:15:52.089689 [info ] [Thread-1 (]: 4 of 14 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.12s]
[0m03:15:52.090391 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m03:15:52.090907 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m03:15:52.091547 [info ] [Thread-1 (]: 5 of 14 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m03:15:52.092308 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m03:15:52.092748 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m03:15:52.096361 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.097116 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 03:15:52.093045 => 03:15:52.096837
[0m03:15:52.097578 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m03:15:52.102718 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.103301 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.103720 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m03:15:52.104117 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.116323 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.117054 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.117572 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m03:15:52.119168 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.122936 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.123382 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m03:15:52.124221 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.127748 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.128174 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m03:15:52.128951 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.130894 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m03:15:52.131307 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.131703 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m03:15:52.136548 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.139489 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.139952 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m03:15:52.143715 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.145454 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 03:15:52.097921 => 03:15:52.145167
[0m03:15:52.145884 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m03:15:52.168709 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc086f8990>]}
[0m03:15:52.169380 [info ] [Thread-1 (]: 5 of 14 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.08s]
[0m03:15:52.170084 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m03:15:52.170594 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m03:15:52.171254 [info ] [Thread-1 (]: 6 of 14 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m03:15:52.172036 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m03:15:52.172478 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m03:15:52.176280 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.176980 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 03:15:52.172776 => 03:15:52.176766
[0m03:15:52.177473 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m03:15:52.183671 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.184333 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.184821 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m03:15:52.185302 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.196016 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.196443 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.196924 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m03:15:52.198333 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.204992 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.205569 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m03:15:52.206413 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.209925 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.210348 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m03:15:52.211315 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.213162 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m03:15:52.213582 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.214005 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m03:15:52.218941 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.222125 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.222666 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m03:15:52.226372 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.227897 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 03:15:52.177962 => 03:15:52.227682
[0m03:15:52.228388 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m03:15:52.251328 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13bed310>]}
[0m03:15:52.252024 [info ] [Thread-1 (]: 6 of 14 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.08s]
[0m03:15:52.252819 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m03:15:52.253333 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m03:15:52.254039 [info ] [Thread-1 (]: 7 of 14 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m03:15:52.254832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m03:15:52.255256 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m03:15:52.259385 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.260091 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 03:15:52.255573 => 03:15:52.259849
[0m03:15:52.260504 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m03:15:52.266521 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.267494 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.268240 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m03:15:52.268941 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.283823 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.284351 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.284927 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m03:15:52.286226 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.290758 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.291265 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m03:15:52.292149 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.296335 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.296831 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m03:15:52.297625 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.299644 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m03:15:52.300094 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.300500 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m03:15:52.304493 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.307584 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.308014 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m03:15:52.312671 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.314178 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 03:15:52.260878 => 03:15:52.313948
[0m03:15:52.314615 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m03:15:52.336861 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc139351d0>]}
[0m03:15:52.337583 [info ] [Thread-1 (]: 7 of 14 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.08s]
[0m03:15:52.338322 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m03:15:52.338932 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m03:15:52.339573 [info ] [Thread-1 (]: 8 of 14 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m03:15:52.340393 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m03:15:52.340827 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m03:15:52.344433 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.345186 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 03:15:52.341155 => 03:15:52.344886
[0m03:15:52.345654 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m03:15:52.350699 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.351324 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.351732 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m03:15:52.352132 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.363054 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.363533 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.363979 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m03:15:52.365003 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.371655 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.372088 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m03:15:52.372939 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.376464 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.376898 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m03:15:52.377937 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.379965 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m03:15:52.380385 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.380818 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m03:15:52.385579 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.388521 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.388950 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m03:15:52.392677 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.394615 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 03:15:52.345949 => 03:15:52.394296
[0m03:15:52.395228 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m03:15:52.418910 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc083b1610>]}
[0m03:15:52.419589 [info ] [Thread-1 (]: 8 of 14 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.08s]
[0m03:15:52.420286 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m03:15:52.420870 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m03:15:52.421519 [info ] [Thread-1 (]: 9 of 14 START sql view model staging.stg_salesforce__pricebook_entry ........... [RUN]
[0m03:15:52.422318 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m03:15:52.422748 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m03:15:52.426381 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.427038 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 03:15:52.423051 => 03:15:52.426824
[0m03:15:52.427539 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m03:15:52.432992 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.433698 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.434140 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m03:15:52.434551 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.445042 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.445488 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.445939 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m03:15:52.446900 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.450742 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.451171 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m03:15:52.451972 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.456152 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.456862 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m03:15:52.457745 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.459631 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m03:15:52.460108 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.460513 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m03:15:52.465606 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.468570 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.469031 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m03:15:52.472879 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.474427 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 03:15:52.427868 => 03:15:52.474225
[0m03:15:52.474889 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m03:15:52.498870 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc0862c750>]}
[0m03:15:52.499787 [info ] [Thread-1 (]: 9 of 14 OK created sql view model staging.stg_salesforce__pricebook_entry ...... [[32mOK[0m in 0.08s]
[0m03:15:52.500524 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m03:15:52.501184 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m03:15:52.501846 [info ] [Thread-1 (]: 10 of 14 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m03:15:52.502708 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m03:15:52.503144 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m03:15:52.506758 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.507694 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 03:15:52.503450 => 03:15:52.507373
[0m03:15:52.508216 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m03:15:52.516602 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.517184 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.517648 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m03:15:52.518047 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.528393 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.528824 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.529260 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m03:15:52.530329 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.534884 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.535322 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m03:15:52.536124 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.539697 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.540185 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m03:15:52.541064 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.542950 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m03:15:52.543391 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.543813 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m03:15:52.548297 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.551426 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.551906 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m03:15:52.556747 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.558317 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 03:15:52.508513 => 03:15:52.558086
[0m03:15:52.558778 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m03:15:52.580347 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc08748810>]}
[0m03:15:52.580983 [info ] [Thread-1 (]: 10 of 14 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.08s]
[0m03:15:52.581667 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m03:15:52.582204 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m03:15:52.582924 [info ] [Thread-1 (]: 11 of 14 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m03:15:52.584021 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m03:15:52.584544 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m03:15:52.588914 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.589754 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 03:15:52.584956 => 03:15:52.589471
[0m03:15:52.590265 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m03:15:52.595387 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.596018 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.596422 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m03:15:52.596893 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.607086 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.607544 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.608011 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m03:15:52.608998 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.612998 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.613431 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m03:15:52.614219 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.617740 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.618197 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m03:15:52.619006 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.621610 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m03:15:52.622117 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.622678 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m03:15:52.626653 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.629824 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.630276 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m03:15:52.634933 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.636455 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 03:15:52.590571 => 03:15:52.636255
[0m03:15:52.636952 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m03:15:52.658930 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13ef0650>]}
[0m03:15:52.659609 [info ] [Thread-1 (]: 11 of 14 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.08s]
[0m03:15:52.660318 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m03:15:52.660827 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m03:15:52.661561 [info ] [Thread-1 (]: 12 of 14 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m03:15:52.662390 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m03:15:52.662847 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m03:15:52.666378 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.667076 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 03:15:52.663168 => 03:15:52.666858
[0m03:15:52.667566 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m03:15:52.675067 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.675672 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.676139 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m03:15:52.676580 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.686560 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.686978 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.687404 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m03:15:52.688861 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.692762 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.693202 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m03:15:52.693991 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.697438 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.697943 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m03:15:52.698704 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.700574 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m03:15:52.700993 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.701439 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m03:15:52.706267 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.709488 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.709925 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m03:15:52.713696 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.715856 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 03:15:52.667877 => 03:15:52.715597
[0m03:15:52.716443 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m03:15:52.738066 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc0850b150>]}
[0m03:15:52.738701 [info ] [Thread-1 (]: 12 of 14 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.08s]
[0m03:15:52.739484 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m03:15:52.740002 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m03:15:52.740624 [info ] [Thread-1 (]: 13 of 14 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m03:15:52.741444 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m03:15:52.741867 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m03:15:52.745754 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.746463 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 03:15:52.742164 => 03:15:52.746217
[0m03:15:52.746972 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m03:15:52.752050 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.752734 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.753250 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m03:15:52.753726 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.764014 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.764493 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.764972 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m03:15:52.766499 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.770346 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.770792 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m03:15:52.771578 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.774917 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.775342 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m03:15:52.776209 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.778335 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m03:15:52.778785 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.779652 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m03:15:52.785638 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.788768 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.789204 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m03:15:52.792898 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.794398 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 03:15:52.747270 => 03:15:52.794169
[0m03:15:52.794998 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m03:15:52.826946 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc084f4390>]}
[0m03:15:52.827801 [info ] [Thread-1 (]: 13 of 14 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.09s]
[0m03:15:52.828576 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m03:15:52.829291 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m03:15:52.830058 [info ] [Thread-1 (]: 14 of 14 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m03:15:52.831281 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m03:15:52.831888 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m03:15:52.835928 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.836674 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 03:15:52.832191 => 03:15:52.836439
[0m03:15:52.837124 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m03:15:52.845007 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.845585 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.846123 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m03:15:52.846527 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.856879 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.857456 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.858044 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m03:15:52.859291 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.863428 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.863880 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m03:15:52.864673 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.868126 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.868604 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m03:15:52.869476 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.871726 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m03:15:52.872268 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.872813 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m03:15:52.876734 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.880610 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.881064 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m03:15:52.885762 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.887333 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 03:15:52.837434 => 03:15:52.887125
[0m03:15:52.887809 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m03:15:52.910149 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc08359050>]}
[0m03:15:52.911249 [info ] [Thread-1 (]: 14 of 14 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.08s]
[0m03:15:52.912318 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m03:15:52.914451 [debug] [MainThread]: Using duckdb connection "master"
[0m03:15:52.914826 [debug] [MainThread]: On master: BEGIN
[0m03:15:52.915204 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m03:15:52.925278 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m03:15:52.925666 [debug] [MainThread]: On master: COMMIT
[0m03:15:52.926026 [debug] [MainThread]: Using duckdb connection "master"
[0m03:15:52.926419 [debug] [MainThread]: On master: COMMIT
[0m03:15:52.927198 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m03:15:52.927686 [debug] [MainThread]: On master: Close
[0m03:15:52.930780 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:15:52.931129 [debug] [MainThread]: Connection 'list_dbt_staging' was properly closed.
[0m03:15:52.931464 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.stg_salesforce__user_role' was properly closed.
[0m03:15:52.931947 [info ] [MainThread]: 
[0m03:15:52.932404 [info ] [MainThread]: Finished running 14 view models in 0 hours 0 minutes and 1.48 seconds (1.48s).
[0m03:15:52.934819 [debug] [MainThread]: Command end result
[0m03:15:52.951930 [info ] [MainThread]: 
[0m03:15:52.952480 [info ] [MainThread]: [32mCompleted successfully[0m
[0m03:15:52.952952 [info ] [MainThread]: 
[0m03:15:52.953419 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m03:15:52.954102 [debug] [MainThread]: Command `dbt run` succeeded at 03:15:52.953991 after 1.87 seconds
[0m03:15:52.954509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc15385110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc1448f090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc18489650>]}
[0m03:15:52.954979 [debug] [MainThread]: Flushing usage events


============================== 17:40:00.944557 | 9dd7b6f5-5b7c-4f38-94e2-ce83778e330e ==============================
[0m17:40:00.944557 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:40:00.947698 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'False'}
[0m17:40:00.947952 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:40:01.107348 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:40:01.129394 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:40:01.138183 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m17:40:01.911145 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.elastic_dbt_interview.dim_opportunity' (models/dimensions/dim_opportunity.sql) depends on a node named 'stg_opportunity' which was not found
[0m17:40:01.911669 [debug] [MainThread]: Command `dbt run` failed at 17:40:01.911606 after 0.99 seconds
[0m17:40:01.911878 [debug] [MainThread]: Flushing usage events


============================== 17:40:36.304666 | d1bed3e9-a3d4-41ab-8f0d-2303aa609775 ==============================
[0m17:40:36.304666 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:40:36.308190 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt compile', 'send_anonymous_usage_stats': 'False'}
[0m17:40:36.308495 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:40:36.407523 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:40:36.428440 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:40:36.437429 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m17:40:37.206107 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.elastic_dbt_interview.dim_opportunity' (models/dimensions/dim_opportunity.sql) depends on a node named 'stg_opportunity' which was not found
[0m17:40:37.206607 [debug] [MainThread]: Command `dbt compile` failed at 17:40:37.206545 after 0.92 seconds
[0m17:40:37.206807 [debug] [MainThread]: Flushing usage events


============================== 17:41:01.960693 | c03a14de-c8d5-4ee7-acbc-9b95b8bcf0e6 ==============================
[0m17:41:01.960693 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:41:01.963845 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt compile', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m17:41:01.964095 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:41:02.045501 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:41:02.091448 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:41:02.100062 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m17:41:02.888593 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m17:41:02.905056 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m17:41:02.906984 [info ] [MainThread]: 
[0m17:41:02.907597 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:41:02.909065 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt_main'
[0m17:41:02.915362 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:41:02.915607 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m17:41:02.915769 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:41:02.932050 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:02.932365 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:41:02.932555 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m17:41:02.958375 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:02.963053 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m17:41:02.963720 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m17:41:02.963903 [debug] [ThreadPool]: On list_dbt_main: Close
[0m17:41:02.966397 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m17:41:02.968588 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:41:02.968770 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m17:41:02.968926 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:02.974707 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:02.974961 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:41:02.975134 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m17:41:02.993450 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:02.994502 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m17:41:02.994738 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m17:41:02.994898 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m17:41:02.998854 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:41:02.999279 [info ] [MainThread]: 
[0m17:41:03.002667 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:03.003085 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_campaign_performance)
[0m17:41:03.003290 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:03.009382 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:41:03.011409 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 17:41:03.003428 => 17:41:03.011219
[0m17:41:03.011686 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:03.011940 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 17:41:03.011824 => 17:41:03.011831
[0m17:41:03.012451 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:03.012687 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m17:41:03.013001 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_case_history)
[0m17:41:03.013197 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m17:41:03.015451 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m17:41:03.016117 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 17:41:03.013324 => 17:41:03.016018
[0m17:41:03.016316 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m17:41:03.016531 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 17:41:03.016450 => 17:41:03.016454
[0m17:41:03.016957 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m17:41:03.017168 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m17:41:03.017491 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_product_sales)
[0m17:41:03.017681 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m17:41:03.020294 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m17:41:03.021162 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 17:41:03.017810 => 17:41:03.021068
[0m17:41:03.021363 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m17:41:03.021577 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 17:41:03.021492 => 17:41:03.021496
[0m17:41:03.021992 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m17:41:03.022204 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:03.022535 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m17:41:03.022728 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:03.024531 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:03.025374 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 17:41:03.022864 => 17:41:03.025283
[0m17:41:03.025564 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:03.025772 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 17:41:03.025692 => 17:41:03.025695
[0m17:41:03.026182 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:03.026442 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:03.026864 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m17:41:03.027086 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:03.028882 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:03.029443 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 17:41:03.027228 => 17:41:03.029342
[0m17:41:03.029635 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:03.029847 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 17:41:03.029762 => 17:41:03.029766
[0m17:41:03.030266 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:03.030473 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:03.030819 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m17:41:03.031007 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:03.032736 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:03.033286 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 17:41:03.031145 => 17:41:03.033193
[0m17:41:03.033475 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:03.033694 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 17:41:03.033603 => 17:41:03.033606
[0m17:41:03.034101 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:03.034316 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:03.034658 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m17:41:03.034846 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:03.036508 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:03.037003 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 17:41:03.034975 => 17:41:03.036914
[0m17:41:03.037194 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:03.037433 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 17:41:03.037350 => 17:41:03.037353
[0m17:41:03.037868 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:03.038076 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:03.038409 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m17:41:03.038593 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:03.040357 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:03.040741 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 17:41:03.038728 => 17:41:03.040637
[0m17:41:03.040930 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:03.041144 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 17:41:03.041061 => 17:41:03.041066
[0m17:41:03.041546 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:03.041753 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:03.042088 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m17:41:03.042273 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:03.044757 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:03.045207 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 17:41:03.042396 => 17:41:03.045095
[0m17:41:03.045406 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:03.045626 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 17:41:03.045539 => 17:41:03.045543
[0m17:41:03.046047 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:03.046261 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:03.046632 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m17:41:03.046821 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:03.048521 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:03.048944 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 17:41:03.046944 => 17:41:03.048838
[0m17:41:03.049142 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:03.049351 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 17:41:03.049269 => 17:41:03.049272
[0m17:41:03.049759 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:03.049963 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:03.050443 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m17:41:03.050718 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:03.052703 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:03.053240 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 17:41:03.050863 => 17:41:03.053139
[0m17:41:03.053436 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:03.053658 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 17:41:03.053568 => 17:41:03.053573
[0m17:41:03.054129 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:03.054351 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:03.054804 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m17:41:03.055016 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:03.056785 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:03.057183 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 17:41:03.055148 => 17:41:03.057092
[0m17:41:03.057370 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:03.057581 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 17:41:03.057498 => 17:41:03.057501
[0m17:41:03.057997 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:03.058210 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:03.058669 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m17:41:03.058910 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:03.060885 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:03.061371 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 17:41:03.059051 => 17:41:03.061262
[0m17:41:03.061593 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:03.061816 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 17:41:03.061731 => 17:41:03.061736
[0m17:41:03.062257 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:03.062471 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:03.062827 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m17:41:03.063014 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:03.064840 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:03.065498 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 17:41:03.063141 => 17:41:03.065375
[0m17:41:03.065729 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:03.065938 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 17:41:03.065855 => 17:41:03.065859
[0m17:41:03.066368 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:03.066594 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:03.067021 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m17:41:03.067217 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:03.069717 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:03.070188 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 17:41:03.067347 => 17:41:03.070089
[0m17:41:03.070383 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:03.070595 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 17:41:03.070508 => 17:41:03.070512
[0m17:41:03.071008 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:03.071214 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:03.071572 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m17:41:03.071757 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:03.097725 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:03.098433 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 17:41:03.071881 => 17:41:03.098325
[0m17:41:03.098639 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:03.098856 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 17:41:03.098772 => 17:41:03.098776
[0m17:41:03.099316 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:03.099534 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:03.099845 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m17:41:03.100028 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:03.101720 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:03.102141 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 17:41:03.100156 => 17:41:03.102046
[0m17:41:03.102335 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:03.102543 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 17:41:03.102463 => 17:41:03.102466
[0m17:41:03.102946 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:03.103152 [debug] [Thread-1  ]: Began running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m17:41:03.103486 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now seed.elastic_dbt_interview.dbt_project_evaluator_exceptions)
[0m17:41:03.103671 [debug] [Thread-1  ]: Began compiling node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m17:41:03.104867 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (compile): 17:41:03.103797 => 17:41:03.104774
[0m17:41:03.105053 [debug] [Thread-1  ]: Began executing node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m17:41:03.105262 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (execute): 17:41:03.105179 => 17:41:03.105182
[0m17:41:03.105664 [debug] [Thread-1  ]: Finished running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m17:41:03.105869 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m17:41:03.106180 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly seed.elastic_dbt_interview.dbt_project_evaluator_exceptions, now model.elastic_dbt_interview.dim_account)
[0m17:41:03.106366 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m17:41:03.108091 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m17:41:03.108978 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 17:41:03.106490 => 17:41:03.108879
[0m17:41:03.109175 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m17:41:03.109395 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 17:41:03.109311 => 17:41:03.109314
[0m17:41:03.109809 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m17:41:03.110012 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m17:41:03.110338 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m17:41:03.110517 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m17:41:03.112387 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m17:41:03.112776 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 17:41:03.110650 => 17:41:03.112685
[0m17:41:03.112963 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m17:41:03.113174 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 17:41:03.113096 => 17:41:03.113100
[0m17:41:03.113581 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m17:41:03.113790 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m17:41:03.114108 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.fact_case)
[0m17:41:03.114291 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m17:41:03.116744 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m17:41:03.117136 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 17:41:03.114415 => 17:41:03.117047
[0m17:41:03.117324 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m17:41:03.117537 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 17:41:03.117456 => 17:41:03.117460
[0m17:41:03.117957 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m17:41:03.118158 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m17:41:03.118465 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.dim_case_status)
[0m17:41:03.118646 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m17:41:03.120465 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m17:41:03.120867 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 17:41:03.118771 => 17:41:03.120778
[0m17:41:03.121054 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m17:41:03.121282 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 17:41:03.121193 => 17:41:03.121197
[0m17:41:03.121699 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m17:41:03.121904 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m17:41:03.122220 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m17:41:03.122402 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m17:41:03.124147 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m17:41:03.124528 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 17:41:03.122541 => 17:41:03.124440
[0m17:41:03.124714 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m17:41:03.124924 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 17:41:03.124845 => 17:41:03.124849
[0m17:41:03.125336 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m17:41:03.125554 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m17:41:03.125877 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m17:41:03.126057 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m17:41:03.127771 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m17:41:03.128125 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 17:41:03.126184 => 17:41:03.128040
[0m17:41:03.128318 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m17:41:03.128528 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 17:41:03.128448 => 17:41:03.128451
[0m17:41:03.128929 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m17:41:03.129132 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m17:41:03.129443 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m17:41:03.129627 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m17:41:03.131585 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:41:03.131972 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 17:41:03.129756 => 17:41:03.131882
[0m17:41:03.132163 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m17:41:03.132373 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 17:41:03.132294 => 17:41:03.132298
[0m17:41:03.132772 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m17:41:03.132976 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:03.133302 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m17:41:03.133509 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:03.135321 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:03.135705 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 17:41:03.133644 => 17:41:03.135619
[0m17:41:03.135891 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:03.136101 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 17:41:03.136022 => 17:41:03.136026
[0m17:41:03.136503 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:03.136702 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m17:41:03.137016 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.fact_opportunity)
[0m17:41:03.137215 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m17:41:03.139795 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m17:41:03.140177 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 17:41:03.137352 => 17:41:03.140084
[0m17:41:03.140373 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m17:41:03.140589 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 17:41:03.140508 => 17:41:03.140511
[0m17:41:03.140994 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m17:41:03.141197 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:03.141509 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_opportunity_history)
[0m17:41:03.141696 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:03.143486 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:41:03.144202 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 17:41:03.141824 => 17:41:03.144111
[0m17:41:03.144391 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:03.144603 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 17:41:03.144521 => 17:41:03.144524
[0m17:41:03.145010 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:03.145211 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m17:41:03.145583 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity_history, now model.elastic_dbt_interview.dim_pricebook)
[0m17:41:03.145811 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m17:41:03.147715 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m17:41:03.148146 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 17:41:03.145967 => 17:41:03.148046
[0m17:41:03.148353 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m17:41:03.148593 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 17:41:03.148503 => 17:41:03.148507
[0m17:41:03.149015 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m17:41:03.149221 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m17:41:03.149547 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m17:41:03.149755 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m17:41:03.151581 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m17:41:03.151969 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 17:41:03.149890 => 17:41:03.151881
[0m17:41:03.152160 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m17:41:03.152377 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 17:41:03.152298 => 17:41:03.152301
[0m17:41:03.152783 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m17:41:03.152993 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m17:41:03.153311 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_solution)
[0m17:41:03.153500 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m17:41:03.155292 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m17:41:03.155679 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 17:41:03.153627 => 17:41:03.155584
[0m17:41:03.155868 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m17:41:03.156078 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 17:41:03.155999 => 17:41:03.156002
[0m17:41:03.156480 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m17:41:03.156682 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m17:41:03.156994 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m17:41:03.157176 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m17:41:03.158879 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m17:41:03.159588 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 17:41:03.157303 => 17:41:03.159497
[0m17:41:03.159775 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m17:41:03.159983 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 17:41:03.159905 => 17:41:03.159908
[0m17:41:03.160385 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m17:41:03.160583 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user_role
[0m17:41:03.160924 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.dim_user_role)
[0m17:41:03.161120 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user_role
[0m17:41:03.163704 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user_role"
[0m17:41:03.164098 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (compile): 17:41:03.161256 => 17:41:03.164007
[0m17:41:03.164287 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user_role
[0m17:41:03.164500 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (execute): 17:41:03.164419 => 17:41:03.164423
[0m17:41:03.164906 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user_role
[0m17:41:03.165412 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:41:03.165595 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user_role' was properly closed.
[0m17:41:03.167711 [debug] [MainThread]: Command end result
[0m17:41:03.175288 [debug] [MainThread]: Command `dbt compile` succeeded at 17:41:03.175196 after 1.23 seconds
[0m17:41:03.175516 [debug] [MainThread]: Flushing usage events


============================== 17:41:40.371182 | 1bba16fb-169c-4153-9d6e-bcd945922b41 ==============================
[0m17:41:40.371182 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:41:40.374670 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m17:41:40.374934 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:41:40.467451 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:41:40.487406 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:41:40.539415 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:41:40.539702 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:41:40.540666 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m17:41:40.555371 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m17:41:40.557188 [info ] [MainThread]: 
[0m17:41:40.557655 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:41:40.559125 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m17:41:40.566317 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:41:40.566575 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:41:40.566747 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:41:40.578813 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.579892 [debug] [ThreadPool]: On list_dbt: Close
[0m17:41:40.583061 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:41:40.583383 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:41:40.583623 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:40.589686 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.590490 [debug] [ThreadPool]: On list_dbt: Close
[0m17:41:40.592458 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m17:41:40.592901 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m17:41:40.595759 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:41:40.595963 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m17:41:40.596124 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:40.601688 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.601946 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:41:40.602115 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m17:41:40.602644 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.603189 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m17:41:40.603358 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:41:40.603510 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m17:41:40.603720 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.603874 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m17:41:40.605514 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now create_dbt_main)
[0m17:41:40.605924 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m17:41:40.607446 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:41:40.607623 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m17:41:40.607773 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:40.612590 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.612823 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:41:40.612984 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m17:41:40.613219 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.613696 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:41:40.613861 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:41:40.614016 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:41:40.614234 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.614394 [debug] [ThreadPool]: On create_dbt_main: Close
[0m17:41:40.617114 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m17:41:40.620380 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:41:40.620569 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m17:41:40.620720 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:40.626083 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.626347 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:41:40.626527 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m17:41:40.644484 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.645453 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m17:41:40.646124 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m17:41:40.646294 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m17:41:40.648439 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m17:41:40.651724 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:41:40.651935 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m17:41:40.652093 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:40.657795 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.658083 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:41:40.658268 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m17:41:40.677729 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.682444 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m17:41:40.682800 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m17:41:40.682988 [debug] [ThreadPool]: On list_dbt_main: Close
[0m17:41:40.687111 [debug] [MainThread]: Using duckdb connection "master"
[0m17:41:40.687388 [debug] [MainThread]: On master: BEGIN
[0m17:41:40.687557 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:41:40.693598 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:41:40.693911 [debug] [MainThread]: On master: COMMIT
[0m17:41:40.694094 [debug] [MainThread]: Using duckdb connection "master"
[0m17:41:40.694261 [debug] [MainThread]: On master: COMMIT
[0m17:41:40.694477 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:41:40.694645 [debug] [MainThread]: On master: Close
[0m17:41:40.696121 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:41:40.696431 [info ] [MainThread]: 
[0m17:41:40.698571 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:40.698957 [info ] [Thread-1  ]: 1 of 32 START sql table model main.fact_campaign_performance ................... [RUN]
[0m17:41:40.699520 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_campaign_performance)
[0m17:41:40.699792 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:40.706394 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:41:40.707659 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 17:41:40.699955 => 17:41:40.707354
[0m17:41:40.707944 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:40.727740 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:41:40.728696 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:41:40.728939 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: BEGIN
[0m17:41:40.729127 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.734613 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.734898 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:41:40.735113 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */

  
    
    

    create  table
      "dbt"."main"."fact_campaign_performance__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."campaign"  -- Hypothetical source for campaign performance metrics
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS campaign_performance_id,  -- Surrogate Key
    campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
    leads_generated,
    opportunities_created,
    revenue_generated,
    expenses,
    createddate AS performance_created_at
FROM source
    );
  
  
[0m17:41:40.739632 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 17:41:40.708109 => 17:41:40.739498
[0m17:41:40.739886 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: ROLLBACK
[0m17:41:40.743628 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_campaign_performance'
[0m17:41:40.743834 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: Close
[0m17:41:40.745549 [debug] [Thread-1  ]: Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m17:41:40.745970 [error] [Thread-1  ]: 1 of 32 ERROR creating sql table model main.fact_campaign_performance .......... [[31mERROR[0m in 0.05s]
[0m17:41:40.746327 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:40.746569 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m17:41:40.746865 [info ] [Thread-1  ]: 2 of 32 START sql table model main.fact_case_history ........................... [RUN]
[0m17:41:40.747226 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_case_history)
[0m17:41:40.747423 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m17:41:40.749603 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.750188 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 17:41:40.747556 => 17:41:40.750091
[0m17:41:40.750384 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m17:41:40.752907 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.753421 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.753619 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: BEGIN
[0m17:41:40.753796 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.759233 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.759496 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.759700 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */

  
    
    

    create  table
      "dbt"."main"."fact_case_history__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY systemmodstamp) AS case_history_id,  -- Surrogate Key
    caseid AS case_fk,                                               -- Foreign Key to fact_case
    status,
    systemmodstamp AS history_date
FROM source
    );
  
  
[0m17:41:40.764770 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.769723 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.770028 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
alter table "dbt"."main"."fact_case_history" rename to "fact_case_history__dbt_backup"
[0m17:41:40.770790 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.772608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.772815 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
alter table "dbt"."main"."fact_case_history__dbt_tmp" rename to "fact_case_history"
[0m17:41:40.773122 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.783692 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: COMMIT
[0m17:41:40.783920 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.784108 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: COMMIT
[0m17:41:40.785029 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.788006 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.788232 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
drop table if exists "dbt"."main"."fact_case_history__dbt_backup" cascade
[0m17:41:40.788617 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.789363 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 17:41:40.750518 => 17:41:40.789270
[0m17:41:40.789569 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: Close
[0m17:41:40.807525 [info ] [Thread-1  ]: 2 of 32 OK created sql table model main.fact_case_history ...................... [[32mOK[0m in 0.06s]
[0m17:41:40.807930 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m17:41:40.808172 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m17:41:40.808444 [info ] [Thread-1  ]: 3 of 32 START sql table model main.fact_product_sales .......................... [RUN]
[0m17:41:40.808803 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_product_sales)
[0m17:41:40.808996 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m17:41:40.811101 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m17:41:40.811545 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 17:41:40.809127 => 17:41:40.811445
[0m17:41:40.811744 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m17:41:40.814201 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_product_sales"
[0m17:41:40.815169 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m17:41:40.815418 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: BEGIN
[0m17:41:40.815594 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.820840 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.821110 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m17:41:40.821325 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_product_sales"} */

  
    
    

    create  table
      "dbt"."main"."fact_product_sales__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."pricebook_entry"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS product_sales_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
    product2id AS product_fk,                                      -- Foreign Key to dim_product
    pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
    unitprice AS unit_price,
    isactive AS is_active,
    createddate AS product_sales_created_at,
    lastmodifieddate AS product_sales_last_modified_date
FROM source
    );
  
  
[0m17:41:40.821978 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 17:41:40.811875 => 17:41:40.821873
[0m17:41:40.822193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: ROLLBACK
[0m17:41:40.822768 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_product_sales'
[0m17:41:40.822948 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: Close
[0m17:41:40.824919 [debug] [Thread-1  ]: Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:40.825350 [error] [Thread-1  ]: 3 of 32 ERROR creating sql table model main.fact_product_sales ................. [[31mERROR[0m in 0.02s]
[0m17:41:40.825680 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m17:41:40.825901 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:40.826289 [info ] [Thread-1  ]: 4 of 32 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m17:41:40.826840 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m17:41:40.827067 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:40.855451 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.856138 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 17:41:40.827234 => 17:41:40.856032
[0m17:41:40.856483 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:40.867581 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.868325 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.868557 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m17:41:40.868762 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.874019 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.874295 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.874533 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m17:41:40.875354 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.877649 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.877893 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m17:41:40.878531 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.880636 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.880887 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m17:41:40.881220 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.882142 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m17:41:40.882337 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.882527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m17:41:40.883114 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.884801 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.885024 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m17:41:40.885380 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.886113 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 17:41:40.856702 => 17:41:40.886021
[0m17:41:40.886326 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m17:41:40.894013 [info ] [Thread-1  ]: 4 of 32 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.07s]
[0m17:41:40.894405 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:40.894679 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:40.895012 [info ] [Thread-1  ]: 5 of 32 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m17:41:40.895386 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m17:41:40.895588 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:40.898339 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.898870 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 17:41:40.895718 => 17:41:40.898764
[0m17:41:40.899063 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:40.901751 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.902206 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.902410 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m17:41:40.902592 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.907978 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.908242 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.908474 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m17:41:40.909264 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.911434 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.911662 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m17:41:40.911968 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.913765 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.913994 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m17:41:40.914327 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.915365 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m17:41:40.915665 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.915854 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m17:41:40.916604 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.918367 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.918576 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m17:41:40.919033 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.919804 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 17:41:40.899196 => 17:41:40.919706
[0m17:41:40.920037 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m17:41:40.927886 [info ] [Thread-1  ]: 5 of 32 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.03s]
[0m17:41:40.928316 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:40.928560 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:40.928897 [info ] [Thread-1  ]: 6 of 32 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m17:41:40.929301 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m17:41:40.929504 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:40.931551 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.932075 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 17:41:40.929637 => 17:41:40.931978
[0m17:41:40.932269 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:40.935822 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.936351 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.936548 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m17:41:40.936730 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.942012 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.942257 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.942478 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m17:41:40.943167 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.945270 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.945488 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m17:41:40.945783 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.947460 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.947653 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m17:41:40.947930 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.948760 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m17:41:40.948950 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.949125 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m17:41:40.949969 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.951478 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.951675 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m17:41:40.952113 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.953173 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 17:41:40.932397 => 17:41:40.953071
[0m17:41:40.953430 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m17:41:40.961562 [info ] [Thread-1  ]: 6 of 32 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.03s]
[0m17:41:40.961983 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:40.962239 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:40.962570 [info ] [Thread-1  ]: 7 of 32 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m17:41:40.962939 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m17:41:40.963137 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:40.965304 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.965783 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 17:41:40.963266 => 17:41:40.965683
[0m17:41:40.965983 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:40.969359 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.969833 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.970036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m17:41:40.970230 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.975637 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.975873 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.976070 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m17:41:40.976507 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.978608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.978831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m17:41:40.979128 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.980809 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.981004 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m17:41:40.981311 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.982148 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m17:41:40.982343 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.982528 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m17:41:40.983425 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.985391 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.985664 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m17:41:40.986136 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.987026 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 17:41:40.966113 => 17:41:40.986930
[0m17:41:40.987246 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m17:41:40.994980 [info ] [Thread-1  ]: 7 of 32 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.03s]
[0m17:41:40.995369 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:40.995632 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:40.995944 [info ] [Thread-1  ]: 8 of 32 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m17:41:40.996378 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m17:41:40.996574 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:40.998780 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:40.999311 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 17:41:40.996705 => 17:41:40.999214
[0m17:41:40.999516 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:41.002262 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.002832 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.003014 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m17:41:41.003183 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.008520 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.008796 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.009036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m17:41:41.009823 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.012682 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.012911 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m17:41:41.013222 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.014910 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.015110 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m17:41:41.015442 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.016428 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m17:41:41.016730 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.016927 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m17:41:41.017591 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.019495 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.019766 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m17:41:41.020216 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.021029 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 17:41:40.999644 => 17:41:41.020933
[0m17:41:41.021242 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m17:41:41.029017 [info ] [Thread-1  ]: 8 of 32 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.03s]
[0m17:41:41.029401 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:41.029663 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:41.030019 [info ] [Thread-1  ]: 9 of 32 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m17:41:41.030415 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m17:41:41.030712 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:41.032878 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.033391 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 17:41:41.030860 => 17:41:41.033289
[0m17:41:41.033594 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:41.036261 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.036672 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.036871 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m17:41:41.037049 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.042587 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.042864 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.043100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m17:41:41.043902 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.046832 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.047059 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m17:41:41.047395 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.049136 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.049343 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m17:41:41.049647 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.050496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m17:41:41.050814 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.051058 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m17:41:41.051706 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.053497 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.053704 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m17:41:41.054112 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.054951 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 17:41:41.033733 => 17:41:41.054851
[0m17:41:41.055172 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m17:41:41.062939 [info ] [Thread-1  ]: 9 of 32 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.03s]
[0m17:41:41.063308 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:41.063548 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:41.063872 [info ] [Thread-1  ]: 10 of 32 START sql view model staging.stg_salesforce__opportunity .............. [RUN]
[0m17:41:41.064334 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m17:41:41.064571 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:41.066612 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.067102 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 17:41:41.064715 => 17:41:41.066998
[0m17:41:41.067297 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:41.069765 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.070152 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.070339 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m17:41:41.070510 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.075972 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.076275 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.076514 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m17:41:41.077259 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.079671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.080033 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m17:41:41.080441 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.083204 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.083433 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m17:41:41.083750 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.084763 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m17:41:41.085046 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.085245 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m17:41:41.085865 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.087656 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.087879 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m17:41:41.088265 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.089093 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 17:41:41.067426 => 17:41:41.088992
[0m17:41:41.089311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m17:41:41.096993 [info ] [Thread-1  ]: 10 of 32 OK created sql view model staging.stg_salesforce__opportunity ......... [[32mOK[0m in 0.03s]
[0m17:41:41.097409 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:41.097682 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:41.098149 [info ] [Thread-1  ]: 11 of 32 START sql view model staging.stg_salesforce__opportunity_history ...... [RUN]
[0m17:41:41.098570 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m17:41:41.098781 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:41.100823 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.101315 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 17:41:41.098919 => 17:41:41.101213
[0m17:41:41.101518 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:41.104188 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.104692 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.104890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m17:41:41.105072 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.110408 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.110666 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.110878 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m17:41:41.111457 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.113619 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.113838 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m17:41:41.114141 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.115826 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.116024 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m17:41:41.116317 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.117155 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m17:41:41.117353 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.117530 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m17:41:41.118125 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.120864 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.121092 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m17:41:41.121532 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.122393 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 17:41:41.101649 => 17:41:41.122297
[0m17:41:41.122637 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m17:41:41.130446 [info ] [Thread-1  ]: 11 of 32 OK created sql view model staging.stg_salesforce__opportunity_history . [[32mOK[0m in 0.03s]
[0m17:41:41.130903 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:41.131138 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:41.131510 [info ] [Thread-1  ]: 12 of 32 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m17:41:41.131958 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m17:41:41.132172 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:41.134176 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.134654 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 17:41:41.132318 => 17:41:41.134554
[0m17:41:41.134881 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:41.137492 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.137934 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.138119 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m17:41:41.138295 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.143747 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.144068 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.144295 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m17:41:41.144777 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.147190 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.147462 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m17:41:41.147823 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.149671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.149890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m17:41:41.150182 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.151028 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m17:41:41.151217 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.151394 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m17:41:41.152123 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.154767 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.154988 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m17:41:41.155393 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.156197 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 17:41:41.135014 => 17:41:41.156094
[0m17:41:41.156411 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m17:41:41.164454 [info ] [Thread-1  ]: 12 of 32 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.03s]
[0m17:41:41.164879 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:41.165124 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:41.165441 [info ] [Thread-1  ]: 13 of 32 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m17:41:41.165813 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m17:41:41.166007 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:41.168078 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.168597 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 17:41:41.166142 => 17:41:41.168490
[0m17:41:41.168803 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:41.171483 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.171970 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.172167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m17:41:41.172354 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.177755 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.178063 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.178288 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m17:41:41.178828 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.181301 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.181581 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m17:41:41.181940 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.183714 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.183913 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m17:41:41.184205 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.185055 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m17:41:41.185243 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.185415 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m17:41:41.186187 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.188013 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.188228 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m17:41:41.188598 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.189454 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 17:41:41.168934 => 17:41:41.189358
[0m17:41:41.189677 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m17:41:41.196938 [info ] [Thread-1  ]: 13 of 32 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.03s]
[0m17:41:41.197297 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:41.197524 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:41.197947 [info ] [Thread-1  ]: 14 of 32 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m17:41:41.198416 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m17:41:41.198624 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:41.201411 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.201913 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 17:41:41.198764 => 17:41:41.201804
[0m17:41:41.202119 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:41.204803 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.205868 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.206110 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m17:41:41.206315 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.211565 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.211831 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.212036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m17:41:41.212660 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.214831 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.215041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m17:41:41.215356 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.217481 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.217751 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m17:41:41.218152 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.219086 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m17:41:41.219285 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.219459 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m17:41:41.219989 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.221528 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.221732 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m17:41:41.222174 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.222981 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 17:41:41.202251 => 17:41:41.222876
[0m17:41:41.223203 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m17:41:41.230916 [info ] [Thread-1  ]: 14 of 32 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.03s]
[0m17:41:41.231263 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:41.231485 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:41.231837 [info ] [Thread-1  ]: 15 of 32 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m17:41:41.232310 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m17:41:41.232518 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:41.235371 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.235873 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 17:41:41.232654 => 17:41:41.235771
[0m17:41:41.236076 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:41.238713 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.239217 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.239413 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m17:41:41.239588 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.245054 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.245353 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.245571 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m17:41:41.246196 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.248349 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.248592 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m17:41:41.248908 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.250636 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.250835 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m17:41:41.251138 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.252116 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m17:41:41.252402 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.252592 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m17:41:41.253197 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.254958 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.255160 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m17:41:41.255525 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.256392 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 17:41:41.236204 => 17:41:41.256300
[0m17:41:41.256606 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m17:41:41.264301 [info ] [Thread-1  ]: 15 of 32 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.03s]
[0m17:41:41.264689 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:41.264944 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:41.265254 [info ] [Thread-1  ]: 16 of 32 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m17:41:41.265627 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m17:41:41.265827 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:41.267925 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.268390 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 17:41:41.265960 => 17:41:41.268294
[0m17:41:41.268589 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:41.272103 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.272565 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.272760 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m17:41:41.272938 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.278398 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.278726 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.278998 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m17:41:41.280031 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.282323 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.282540 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m17:41:41.282853 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.284684 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.284916 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m17:41:41.285236 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.286156 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m17:41:41.286346 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.286521 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m17:41:41.287083 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.288624 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.288820 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m17:41:41.289159 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.289853 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 17:41:41.268717 => 17:41:41.289762
[0m17:41:41.290055 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m17:41:41.297211 [info ] [Thread-1  ]: 16 of 32 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.03s]
[0m17:41:41.297597 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:41.297844 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:41.298153 [info ] [Thread-1  ]: 17 of 32 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m17:41:41.298505 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m17:41:41.298697 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:41.300610 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.301069 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 17:41:41.298828 => 17:41:41.300973
[0m17:41:41.301259 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:41.304467 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.304960 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.305260 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m17:41:41.305482 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.310894 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.311176 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.311392 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m17:41:41.311913 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.314120 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.314332 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m17:41:41.314630 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.316777 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.317055 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m17:41:41.317454 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.318371 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m17:41:41.318585 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.318764 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m17:41:41.319295 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.320837 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.321054 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m17:41:41.321408 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.322265 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 17:41:41.301388 => 17:41:41.322169
[0m17:41:41.322494 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m17:41:41.329745 [info ] [Thread-1  ]: 17 of 32 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.03s]
[0m17:41:41.330077 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:41.330309 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m17:41:41.330731 [info ] [Thread-1  ]: 18 of 32 START sql table model main.dim_account ................................ [RUN]
[0m17:41:41.331256 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m17:41:41.331463 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m17:41:41.333559 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m17:41:41.334023 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 17:41:41.331605 => 17:41:41.333919
[0m17:41:41.334217 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m17:41:41.336849 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m17:41:41.337892 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m17:41:41.338167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m17:41:41.338356 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.343790 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.344073 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m17:41:41.344273 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."main"."dim_account__dbt_tmp"
  
    as (
      

WITH base AS (
    SELECT
        id AS account_id,
        name AS account_name,
        industry,
        type AS account_type,
        billing_city,
        billing_state,
        billing_country,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__account"
)
SELECT * FROM base;
    );
  
  
[0m17:41:41.345130 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 17:41:41.334354 => 17:41:41.344962
[0m17:41:41.345404 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: ROLLBACK
[0m17:41:41.346036 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_account'
[0m17:41:41.346219 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m17:41:41.348055 [debug] [Thread-1  ]: Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.348484 [error] [Thread-1  ]: 18 of 32 ERROR creating sql table model main.dim_account ....................... [[31mERROR[0m in 0.02s]
[0m17:41:41.348816 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m17:41:41.349058 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m17:41:41.349339 [info ] [Thread-1  ]: 19 of 32 START sql table model main.dim_campaign ............................... [RUN]
[0m17:41:41.349700 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m17:41:41.349889 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m17:41:41.352835 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m17:41:41.353404 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 17:41:41.350027 => 17:41:41.353268
[0m17:41:41.353623 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m17:41:41.356153 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m17:41:41.356584 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:41:41.356773 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m17:41:41.356947 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.362148 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.362581 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:41:41.362811 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."main"."dim_campaign__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__campaign"
)

SELECT
    id AS campaign_id,                       -- Surrogate Key
    name AS campaign_name,
    type AS campaign_type,
    status AS campaign_status,
    startdate AS campaign_start_date,
    enddate AS campaign_end_date,
    createddate AS campaign_created_date,
    lastmodifieddate AS campaign_last_modified_date
FROM source
    );
  
  
[0m17:41:41.363977 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 17:41:41.353750 => 17:41:41.363873
[0m17:41:41.364193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: ROLLBACK
[0m17:41:41.364754 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_campaign'
[0m17:41:41.364934 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m17:41:41.366900 [debug] [Thread-1  ]: Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.ownerid"
  LINE 19:     id AS campaign_id,                       -- Surrogate Key
      name AS campaign_name,
      type AS campaign_type,
      status AS campaign_status,
      startdate AS campaign_start_date,
      enddate AS campaign_end_date,
      createddate AS campaign_created_date,
      lastmodifieddate AS campaign_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.367382 [error] [Thread-1  ]: 19 of 32 ERROR creating sql table model main.dim_campaign ...................... [[31mERROR[0m in 0.02s]
[0m17:41:41.367734 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m17:41:41.367978 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m17:41:41.368254 [info ] [Thread-1  ]: 20 of 32 START sql table model main.fact_case .................................. [RUN]
[0m17:41:41.368620 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.fact_case)
[0m17:41:41.368811 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m17:41:41.371111 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m17:41:41.371636 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 17:41:41.368947 => 17:41:41.371532
[0m17:41:41.371838 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m17:41:41.374141 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m17:41:41.374536 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m17:41:41.374731 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m17:41:41.375036 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.380302 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.380567 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m17:41:41.380783 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."main"."fact_case__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__case"
)

SELECT
    id AS case_id,                           -- Surrogate Key
    accountid AS account_fk,                 -- Foreign Key to dim_account
    contactid AS contact_fk,                 -- Foreign Key to dim_contact
    ownerid AS user_fk,                      -- Foreign Key to dim_user
    status AS status_name,
    priority,
    origin,
    subject,
    createddate AS case_created_at,
    lastmodifieddate AS case_last_modified_date
FROM source
    );
  
  
[0m17:41:41.381805 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 17:41:41.371974 => 17:41:41.381707
[0m17:41:41.382016 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m17:41:41.382564 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m17:41:41.382755 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m17:41:41.384723 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.case_id", "source.assetid", "source.ownerid"
  LINE 19:     id AS case_id,                           -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      contactid AS contact_fk,                 -- Foreign Key to dim_contact
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      status AS status_name,
      priority,
      origin,
      subject,
      createddate AS case_created_at,
      lastmodifieddate AS case_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.385199 [error] [Thread-1  ]: 20 of 32 ERROR creating sql table model main.fact_case ......................... [[31mERROR[0m in 0.02s]
[0m17:41:41.385551 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m17:41:41.385842 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m17:41:41.386248 [info ] [Thread-1  ]: 21 of 32 START sql table model main.dim_case_status ............................ [RUN]
[0m17:41:41.386669 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.dim_case_status)
[0m17:41:41.386868 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m17:41:41.389028 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.389525 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 17:41:41.387002 => 17:41:41.389428
[0m17:41:41.389720 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m17:41:41.392885 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.393357 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.393546 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m17:41:41.393724 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.399077 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.399398 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.399613 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."main"."dim_case_status__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        status AS status_name,
        status AS status_description  -- Adjust as needed, typically a description field should be separate
    FROM "dbt"."staging"."stg_salesforce__case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY status_name) AS status_id,  -- Surrogate Key
    status_name,
    status_description
FROM source
    );
  
  
[0m17:41:41.402087 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.404357 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.404630 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m17:41:41.404975 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.405885 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m17:41:41.406077 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.406252 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m17:41:41.406812 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.408649 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.408884 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."main"."dim_case_status__dbt_backup" cascade
[0m17:41:41.409214 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.410000 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 17:41:41.389847 => 17:41:41.409902
[0m17:41:41.410212 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m17:41:41.418449 [info ] [Thread-1  ]: 21 of 32 OK created sql table model main.dim_case_status ....................... [[32mOK[0m in 0.03s]
[0m17:41:41.418852 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m17:41:41.419145 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m17:41:41.419443 [info ] [Thread-1  ]: 22 of 32 START sql table model main.dim_contact ................................ [RUN]
[0m17:41:41.419835 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m17:41:41.420038 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m17:41:41.422160 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m17:41:41.422654 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 17:41:41.420179 => 17:41:41.422554
[0m17:41:41.422869 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m17:41:41.425421 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m17:41:41.425891 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m17:41:41.426077 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m17:41:41.426250 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.431731 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.432010 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m17:41:41.432212 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

WITH base AS (
    SELECT
        id AS contact_id,
        account_id,
        first_name,
        last_name,
        email,
        phone,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__contact"
)
SELECT * FROM base;
    );
  
  
[0m17:41:41.432642 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 17:41:41.423005 => 17:41:41.432547
[0m17:41:41.432839 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m17:41:41.433364 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m17:41:41.433547 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m17:41:41.435467 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.435940 [error] [Thread-1  ]: 22 of 32 ERROR creating sql table model main.dim_contact ....................... [[31mERROR[0m in 0.02s]
[0m17:41:41.436290 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m17:41:41.436525 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m17:41:41.436910 [info ] [Thread-1  ]: 23 of 32 START sql view model main.dim_lead .................................... [RUN]
[0m17:41:41.437368 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m17:41:41.437583 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m17:41:41.439717 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m17:41:41.440263 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 17:41:41.437730 => 17:41:41.440157
[0m17:41:41.440459 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m17:41:41.443945 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m17:41:41.444568 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m17:41:41.444921 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m17:41:41.445139 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.450232 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.450493 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m17:41:41.450699 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
  create view "dbt"."main"."dim_lead__dbt_tmp" as (
    

WITH base AS (
    SELECT
        id AS lead_id,
        first_name,
        last_name,
        company,
        email,
        phone,
        status,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__lead"
)

SELECT * FROM base;
  );

[0m17:41:41.451116 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 17:41:41.440589 => 17:41:41.451022
[0m17:41:41.451317 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: ROLLBACK
[0m17:41:41.451841 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_lead'
[0m17:41:41.452022 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m17:41:41.453704 [debug] [Thread-1  ]: Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.454074 [error] [Thread-1  ]: 23 of 32 ERROR creating sql view model main.dim_lead ........................... [[31mERROR[0m in 0.02s]
[0m17:41:41.454369 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m17:41:41.454590 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m17:41:41.454969 [info ] [Thread-1  ]: 24 of 32 START sql table model main.dim_opportunity ............................ [RUN]
[0m17:41:41.455437 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m17:41:41.455645 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m17:41:41.458020 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:41:41.458545 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 17:41:41.455785 => 17:41:41.458433
[0m17:41:41.458752 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m17:41:41.461307 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:41:41.461867 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:41:41.462196 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m17:41:41.462380 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.467718 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.467997 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:41:41.468276 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity__dbt_tmp"
  
    as (
      



renamed AS (

    SELECT
        /* IDs */
        id AS opportunity_id,
        accountid AS account_fk,
        ownerid AS user_fk,
        contactid AS primary_contact_fk,
        campaignid AS campaign_fk,
        pricebook2id AS pricebook_fk,
        contractid AS contract_fk,
        primarypartneraccountid AS primary_partner_account_fk,
        createdbyid AS created_by_fk,
        lastmodifiedbyid AS last_modified_by_fk,
        lastamountchangedhistoryid AS last_amount_change_history_fk,
        lastclosedatechangedhistoryid AS last_close_date_change_history_fk,

        /* Metrics */
        amount,
        probability,
        expectedrevenue AS expected_revenue,
        totalopportunityquantity AS total_opportunity_quantity,

        /* Dimensions */
        name AS opportunity_name,
        description,
        stagename AS stage_name,
        stagesortorder AS stage_sort_order,
        type AS opportunity_type,
        nextstep AS next_step,
        leadsource AS lead_source,
        isclosed AS is_closed,
        iswon AS is_won,
        forecastcategory AS forecast_category,
        forecastcategoryname AS forecast_category_name,
        hasopportunitylineitem AS has_opportunity_line_item,
        deliveryinstallationstatus__c AS delivery_installation_status,
        trackingnumber__c AS tracking_number,
        ordernumber__c AS order_number,
        currentgenerators__c AS current_generators,
        maincompetitors__c AS main_competitors,

        /* Dates */
        closedate AS close_date,
        createddate AS created_at,
        lastmodifieddate AS last_modified_date,
        systemmodstamp AS system_mod_stamp,
        lastactivitydate AS last_activity_date,
        laststagechangedate AS last_stage_change_date,
        fiscalyear AS fiscal_year,
        fiscalquarter AS fiscal_quarter

    FROM "dbt"."staging"."stg_salesforce__opportunity"

)

SELECT
    /* IDs */
    opportunity_id,
    account_fk,
    user_fk,
    primary_contact_fk,
    campaign_fk,
    pricebook_fk,
    contract_fk,
    primary_partner_account_fk,
    created_by_fk,
    last_modified_by_fk,
    last_amount_change_history_fk,
    last_close_date_change_history_fk,

    /* Metrics */
    amount,
    probability,
    expected_revenue,
    total_opportunity_quantity,

    /* Dimensions */
    opportunity_name,
    description,
    stage_name,
    stage_sort_order,
    opportunity_type,
    next_step,
    lead_source,
    is_closed,
    is_won,
    forecast_category,
    forecast_category_name,
    has_opportunity_line_item,
    delivery_installation_status,
    tracking_number,
    order_number,
    current_generators,
    main_competitors,

    /* Dates */
    close_date,
    created_at,
    last_modified_date,
    system_mod_stamp,
    last_activity_date,
    last_stage_change_date,
    fiscal_year,
    fiscal_quarter

FROM renamed
    );
  
  
[0m17:41:41.468769 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 17:41:41.458885 => 17:41:41.468677
[0m17:41:41.468967 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: ROLLBACK
[0m17:41:41.469473 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_opportunity'
[0m17:41:41.469650 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m17:41:41.471661 [debug] [Thread-1  ]: Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Parser Error: syntax error at or near "renamed"
[0m17:41:41.472124 [error] [Thread-1  ]: 24 of 32 ERROR creating sql table model main.dim_opportunity ................... [[31mERROR[0m in 0.02s]
[0m17:41:41.472463 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m17:41:41.472714 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:41.473032 [info ] [Thread-1  ]: 25 of 32 START sql table model main.dim_opportunity_stage ...................... [RUN]
[0m17:41:41.473469 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m17:41:41.473710 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:41.475958 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.476458 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 17:41:41.473855 => 17:41:41.476356
[0m17:41:41.476653 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:41.479260 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.479773 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.479980 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m17:41:41.480169 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.485485 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.485805 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.486007 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        stagename AS stage_name,
        stagesortorder AS stage_sort_order
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY stage_sort_order) AS stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
FROM source
    );
  
  
[0m17:41:41.489490 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.492461 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.492698 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m17:41:41.493055 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.521544 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.521895 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m17:41:41.522340 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.523454 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m17:41:41.523657 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.523831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m17:41:41.524474 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.526104 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.526339 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."main"."dim_opportunity_stage__dbt_backup" cascade
[0m17:41:41.526739 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.527460 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 17:41:41.476783 => 17:41:41.527367
[0m17:41:41.527663 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m17:41:41.536726 [info ] [Thread-1  ]: 25 of 32 OK created sql table model main.dim_opportunity_stage ................. [[32mOK[0m in 0.06s]
[0m17:41:41.537101 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:41.537320 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m17:41:41.537668 [info ] [Thread-1  ]: 26 of 32 START sql table model main.fact_opportunity ........................... [RUN]
[0m17:41:41.538156 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.fact_opportunity)
[0m17:41:41.538371 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m17:41:41.540579 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m17:41:41.541051 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 17:41:41.538511 => 17:41:41.540952
[0m17:41:41.541256 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m17:41:41.543867 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m17:41:41.544319 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m17:41:41.544514 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m17:41:41.544697 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.550035 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.550301 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m17:41:41.550538 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."fact_opportunity__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    id AS opportunity_id,                    -- Surrogate Key
    accountid AS account_fk,                 -- Foreign Key to dim_account
    ownerid AS user_fk,                      -- Foreign Key to dim_user
    campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
    pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
    stagename AS stage_name,                 -- Captures the current stage
    stagesortorder AS stage_sort_order,
    amount,
    probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,
    closedate AS close_date,
    createddate AS opportunity_created_at,
    lastmodifieddate AS opportunity_last_modified_date,
    isclosed AS is_closed,
    iswon AS is_won
FROM source
    );
  
  
[0m17:41:41.551656 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 17:41:41.541390 => 17:41:41.551556
[0m17:41:41.551861 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m17:41:41.552449 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m17:41:41.552632 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m17:41:41.554654 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.iswon", "source.ownerid"
  LINE 19:     id AS opportunity_id,                    -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
      pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
      stagename AS stage_name,                 -- Captures the current stage
      stagesortorder AS stage_sort_order,
      amount,
      probability,
      expectedrevenue AS expected_revenue,
      totalopportunityquantity AS total_opportunity_quantity,
      closedate AS close_date,
      createddate AS opportunity_created_at,
      lastmodifieddate AS opportunity_last_modified_date,
      isclosed AS is_closed,
      iswon AS is_won
  FROM source
      );
    
    ...
               ^
[0m17:41:41.555141 [error] [Thread-1  ]: 26 of 32 ERROR creating sql table model main.fact_opportunity .................. [[31mERROR[0m in 0.02s]
[0m17:41:41.555506 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m17:41:41.555754 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:41.556071 [info ] [Thread-1  ]: 27 of 32 START sql table model main.fact_opportunity_history ................... [RUN]
[0m17:41:41.556497 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_opportunity_history)
[0m17:41:41.556713 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:41.558848 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:41:41.559360 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 17:41:41.556847 => 17:41:41.559255
[0m17:41:41.559557 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:41.562868 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:41:41.563703 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:41:41.563984 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m17:41:41.564174 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.569382 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.569659 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:41:41.569866 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."main"."fact_opportunity_history__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__opportunity_history"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY systemmodstamp) AS history_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                            -- Foreign Key to fact_opportunity
    stage_fk AS stage_fk,                                       -- Foreign Key to dim_opportunity_stage
    amount,
    probability,
    closedate AS close_date,
    systemmodstamp AS history_date
FROM source
    );
  
  
[0m17:41:41.571273 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 17:41:41.559693 => 17:41:41.571112
[0m17:41:41.571543 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m17:41:41.572101 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m17:41:41.572291 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m17:41:41.574305 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Column "stage_fk" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m17:41:41.574718 [error] [Thread-1  ]: 27 of 32 ERROR creating sql table model main.fact_opportunity_history .......... [[31mERROR[0m in 0.02s]
[0m17:41:41.575055 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:41.575294 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m17:41:41.575570 [info ] [Thread-1  ]: 28 of 32 START sql table model main.dim_pricebook .............................. [RUN]
[0m17:41:41.575937 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity_history, now model.elastic_dbt_interview.dim_pricebook)
[0m17:41:41.576130 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m17:41:41.578266 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m17:41:41.578757 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 17:41:41.576262 => 17:41:41.578656
[0m17:41:41.578952 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m17:41:41.581443 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m17:41:41.581922 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m17:41:41.582119 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m17:41:41.582304 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.587640 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.587958 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m17:41:41.588157 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."main"."dim_pricebook__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        pricebook2id AS pricebook_id,        -- Surrogate Key
        name AS pricebook_name,
        isactive AS is_active,
        description,
        createddate AS pricebook_created_date,
        lastmodifieddate AS pricebook_last_modified_date
    FROM "dbt"."staging"."stg_salesforce__pricebook_entry"
)

SELECT * FROM source
    );
  
  
[0m17:41:41.588912 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 17:41:41.579077 => 17:41:41.588813
[0m17:41:41.589117 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: ROLLBACK
[0m17:41:41.589649 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_pricebook'
[0m17:41:41.589881 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m17:41:41.591748 [debug] [Thread-1  ]: Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m17:41:41.592178 [error] [Thread-1  ]: 28 of 32 ERROR creating sql table model main.dim_pricebook ..................... [[31mERROR[0m in 0.02s]
[0m17:41:41.592505 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m17:41:41.592732 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m17:41:41.593067 [info ] [Thread-1  ]: 29 of 32 START sql table model main.dim_product ................................ [RUN]
[0m17:41:41.593512 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m17:41:41.593722 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m17:41:41.595958 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m17:41:41.596443 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 17:41:41.593861 => 17:41:41.596340
[0m17:41:41.596641 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m17:41:41.599300 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m17:41:41.600174 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m17:41:41.600486 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m17:41:41.600700 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.606217 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.606448 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m17:41:41.606643 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."main"."dim_product__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__product_2"
)

SELECT
    id AS product_id,                        -- Surrogate Key
    name AS product_name,
    productcode AS product_code,
    description,
    isactive AS is_active,
    createddate AS product_created_date,
    lastmodifieddate AS product_last_modified_date
FROM source
    );
  
  
[0m17:41:41.607511 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 17:41:41.596773 => 17:41:41.607406
[0m17:41:41.607721 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: ROLLBACK
[0m17:41:41.608248 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_product'
[0m17:41:41.608432 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m17:41:41.610173 [debug] [Thread-1  ]: Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS product_id,                        -- Surrogate Key
      name AS product_name,
      productcode AS product_code,
      description,
      isactive AS is_active,
      createddate AS product_created_date,
      lastmodifieddate AS product_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.610524 [error] [Thread-1  ]: 29 of 32 ERROR creating sql table model main.dim_product ....................... [[31mERROR[0m in 0.02s]
[0m17:41:41.610832 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m17:41:41.611046 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m17:41:41.611454 [info ] [Thread-1  ]: 30 of 32 START sql table model main.dim_solution ............................... [RUN]
[0m17:41:41.611956 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_solution)
[0m17:41:41.612169 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m17:41:41.614359 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m17:41:41.614911 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 17:41:41.612306 => 17:41:41.614770
[0m17:41:41.615111 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m17:41:41.618500 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m17:41:41.618989 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m17:41:41.619175 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m17:41:41.619352 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.624512 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.624773 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m17:41:41.625115 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."main"."dim_solution__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__solution"
)

SELECT
    id AS solution_id,                       -- Surrogate Key
    solutionname AS solution_name,
    status,
    description,
    createddate AS solution_created_date,
    lastmodifieddate AS solution_last_modified_date
FROM source
    );
  
  
[0m17:41:41.626037 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 17:41:41.615248 => 17:41:41.625924
[0m17:41:41.626256 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: ROLLBACK
[0m17:41:41.626819 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_solution'
[0m17:41:41.627029 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m17:41:41.628889 [debug] [Thread-1  ]: Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.caseid", "source.ownerid"
  LINE 19:     id AS solution_id,                       -- Surrogate Key
      solutionname AS solution_name,
      status,
      description,
      createddate AS solution_created_date,
      lastmodifieddate AS solution_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.629284 [error] [Thread-1  ]: 30 of 32 ERROR creating sql table model main.dim_solution ...................... [[31mERROR[0m in 0.02s]
[0m17:41:41.629620 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m17:41:41.629872 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m17:41:41.630139 [info ] [Thread-1  ]: 31 of 32 START sql table model main.dim_user ................................... [RUN]
[0m17:41:41.630500 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m17:41:41.630690 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m17:41:41.632819 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m17:41:41.633288 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 17:41:41.630824 => 17:41:41.633194
[0m17:41:41.633479 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m17:41:41.635795 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m17:41:41.636358 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m17:41:41.636574 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m17:41:41.636888 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.642197 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.642473 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m17:41:41.642669 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

WITH base AS (
    SELECT
        id AS user_id,
        username,
        first_name,
        last_name,
        email,
        role_id,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__user"
)
SELECT * FROM base;
    );
  
  
[0m17:41:41.643087 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 17:41:41.633607 => 17:41:41.642992
[0m17:41:41.643287 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m17:41:41.643812 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m17:41:41.643989 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m17:41:41.645994 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.646432 [error] [Thread-1  ]: 31 of 32 ERROR creating sql table model main.dim_user .......................... [[31mERROR[0m in 0.02s]
[0m17:41:41.646764 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m17:41:41.647006 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user_role
[0m17:41:41.647288 [info ] [Thread-1  ]: 32 of 32 START sql table model main.dim_user_role .............................. [RUN]
[0m17:41:41.647654 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.dim_user_role)
[0m17:41:41.647856 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user_role
[0m17:41:41.650123 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user_role"
[0m17:41:41.650684 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (compile): 17:41:41.647986 => 17:41:41.650578
[0m17:41:41.650880 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user_role
[0m17:41:41.653247 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user_role"
[0m17:41:41.653633 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m17:41:41.653940 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: BEGIN
[0m17:41:41.654190 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.659467 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.659778 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m17:41:41.659981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user_role"} */

  
    
    

    create  table
      "dbt"."main"."dim_user_role__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__user_role"
)

SELECT
    id AS role_id,                           -- Surrogate Key
    name AS role_name,
    roledescription AS role_description
FROM source
    );
  
  
[0m17:41:41.660868 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (execute): 17:41:41.651013 => 17:41:41.660763
[0m17:41:41.661085 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: ROLLBACK
[0m17:41:41.661633 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user_role'
[0m17:41:41.661815 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: Close
[0m17:41:41.663680 [debug] [Thread-1  ]: Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS role_id,                           -- Surrogate Key
      name AS role_name,
      roledescription AS role_description
  FROM source
      );
    
    ...
               ^
[0m17:41:41.664057 [error] [Thread-1  ]: 32 of 32 ERROR creating sql table model main.dim_user_role ..................... [[31mERROR[0m in 0.02s]
[0m17:41:41.664382 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user_role
[0m17:41:41.665195 [debug] [MainThread]: Using duckdb connection "master"
[0m17:41:41.665369 [debug] [MainThread]: On master: BEGIN
[0m17:41:41.665517 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:41:41.670770 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:41:41.671061 [debug] [MainThread]: On master: COMMIT
[0m17:41:41.671245 [debug] [MainThread]: Using duckdb connection "master"
[0m17:41:41.671398 [debug] [MainThread]: On master: COMMIT
[0m17:41:41.671604 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:41:41.671762 [debug] [MainThread]: On master: Close
[0m17:41:41.673405 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:41:41.673646 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user_role' was properly closed.
[0m17:41:41.673895 [info ] [MainThread]: 
[0m17:41:41.674078 [info ] [MainThread]: Finished running 17 table models, 15 view models in 0 hours 0 minutes and 1.12 seconds (1.12s).
[0m17:41:41.676322 [debug] [MainThread]: Command end result
[0m17:41:41.685459 [info ] [MainThread]: 
[0m17:41:41.685765 [info ] [MainThread]: [31mCompleted with 15 errors and 0 warnings:[0m
[0m17:41:41.686017 [info ] [MainThread]: 
[0m17:41:41.686283 [error] [MainThread]:   Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m17:41:41.686532 [info ] [MainThread]: 
[0m17:41:41.686732 [error] [MainThread]:   Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.686918 [info ] [MainThread]: 
[0m17:41:41.687067 [error] [MainThread]:   Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.687217 [info ] [MainThread]: 
[0m17:41:41.687377 [error] [MainThread]:   Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.ownerid"
  LINE 19:     id AS campaign_id,                       -- Surrogate Key
      name AS campaign_name,
      type AS campaign_type,
      status AS campaign_status,
      startdate AS campaign_start_date,
      enddate AS campaign_end_date,
      createddate AS campaign_created_date,
      lastmodifieddate AS campaign_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.687537 [info ] [MainThread]: 
[0m17:41:41.687701 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.case_id", "source.assetid", "source.ownerid"
  LINE 19:     id AS case_id,                           -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      contactid AS contact_fk,                 -- Foreign Key to dim_contact
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      status AS status_name,
      priority,
      origin,
      subject,
      createddate AS case_created_at,
      lastmodifieddate AS case_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.687867 [info ] [MainThread]: 
[0m17:41:41.688012 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.688154 [info ] [MainThread]: 
[0m17:41:41.688297 [error] [MainThread]:   Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.688444 [info ] [MainThread]: 
[0m17:41:41.688586 [error] [MainThread]:   Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Parser Error: syntax error at or near "renamed"
[0m17:41:41.688729 [info ] [MainThread]: 
[0m17:41:41.688900 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.iswon", "source.ownerid"
  LINE 19:     id AS opportunity_id,                    -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
      pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
      stagename AS stage_name,                 -- Captures the current stage
      stagesortorder AS stage_sort_order,
      amount,
      probability,
      expectedrevenue AS expected_revenue,
      totalopportunityquantity AS total_opportunity_quantity,
      closedate AS close_date,
      createddate AS opportunity_created_at,
      lastmodifieddate AS opportunity_last_modified_date,
      isclosed AS is_closed,
      iswon AS is_won
  FROM source
      );
    
    ...
               ^
[0m17:41:41.689204 [info ] [MainThread]: 
[0m17:41:41.689415 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Column "stage_fk" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m17:41:41.689586 [info ] [MainThread]: 
[0m17:41:41.689774 [error] [MainThread]:   Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m17:41:41.689942 [info ] [MainThread]: 
[0m17:41:41.690110 [error] [MainThread]:   Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS product_id,                        -- Surrogate Key
      name AS product_name,
      productcode AS product_code,
      description,
      isactive AS is_active,
      createddate AS product_created_date,
      lastmodifieddate AS product_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.690283 [info ] [MainThread]: 
[0m17:41:41.690449 [error] [MainThread]:   Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.caseid", "source.ownerid"
  LINE 19:     id AS solution_id,                       -- Surrogate Key
      solutionname AS solution_name,
      status,
      description,
      createddate AS solution_created_date,
      lastmodifieddate AS solution_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.690615 [info ] [MainThread]: 
[0m17:41:41.690806 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.690986 [info ] [MainThread]: 
[0m17:41:41.691151 [error] [MainThread]:   Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS role_id,                           -- Surrogate Key
      name AS role_name,
      roledescription AS role_description
  FROM source
      );
    
    ...
               ^
[0m17:41:41.691335 [info ] [MainThread]: 
[0m17:41:41.691523 [info ] [MainThread]: Done. PASS=17 WARN=0 ERROR=15 SKIP=0 TOTAL=32
[0m17:41:41.691912 [debug] [MainThread]: Command `dbt run` failed at 17:41:41.691850 after 1.34 seconds
[0m17:41:41.692132 [debug] [MainThread]: Flushing usage events


============================== 17:47:47.440212 | 1c681bd4-4916-4c64-8bef-2e58d3a55d8e ==============================
[0m17:47:47.440212 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:47:47.443229 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m17:47:47.443500 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:47:47.527915 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:47:47.548562 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:47:47.597658 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 5 files changed.
[0m17:47:47.598108 [debug] [MainThread]: Partial parsing: deleted file: elastic_dbt_interview://models/mart/mart_sales_performance.sql
[0m17:47:47.598396 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_campaign.sql
[0m17:47:47.598640 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_solution.sql
[0m17:47:47.598848 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m17:47:47.599055 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user_role.sql
[0m17:47:47.599263 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_account.sql
[0m17:47:47.635862 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m17:47:47.651586 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m17:47:47.653456 [info ] [MainThread]: 
[0m17:47:47.653938 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:47:47.655242 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m17:47:47.662357 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:47:47.662655 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:47:47.662835 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:47:47.674036 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.675019 [debug] [ThreadPool]: On list_dbt: Close
[0m17:47:47.678216 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:47:47.678494 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:47:47.678711 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:47:47.685475 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.686238 [debug] [ThreadPool]: On list_dbt: Close
[0m17:47:47.688136 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m17:47:47.688696 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m17:47:47.692395 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:47:47.692633 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m17:47:47.692806 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:47:47.699221 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.699464 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:47:47.699632 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m17:47:47.699869 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.700361 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:47:47.700535 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:47:47.700690 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:47:47.700964 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.701223 [debug] [ThreadPool]: On create_dbt_main: Close
[0m17:47:47.703207 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now create_dbt_staging)
[0m17:47:47.703849 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m17:47:47.706537 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:47:47.706842 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m17:47:47.707044 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:47:47.713260 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.713501 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:47:47.713867 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m17:47:47.714254 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.714947 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m17:47:47.715175 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:47:47.715370 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m17:47:47.715717 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.715962 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m17:47:47.719464 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_main)
[0m17:47:47.723896 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:47:47.724172 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m17:47:47.724398 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:47:47.731085 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.731262 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:47:47.731426 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m17:47:47.747501 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.751610 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m17:47:47.752324 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m17:47:47.752583 [debug] [ThreadPool]: On list_dbt_main: Close
[0m17:47:47.754968 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m17:47:47.757534 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:47:47.757767 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m17:47:47.757936 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:47:47.763698 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.763948 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:47:47.764157 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m17:47:47.782986 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.783985 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m17:47:47.784222 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m17:47:47.784381 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m17:47:47.787530 [debug] [MainThread]: Using duckdb connection "master"
[0m17:47:47.787795 [debug] [MainThread]: On master: BEGIN
[0m17:47:47.787972 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:47:47.793558 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:47:47.793832 [debug] [MainThread]: On master: COMMIT
[0m17:47:47.794008 [debug] [MainThread]: Using duckdb connection "master"
[0m17:47:47.794170 [debug] [MainThread]: On master: COMMIT
[0m17:47:47.794385 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:47:47.794546 [debug] [MainThread]: On master: Close
[0m17:47:47.796076 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:47:47.796327 [info ] [MainThread]: 
[0m17:47:47.798099 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:47:47.798463 [info ] [Thread-1  ]: 1 of 32 START sql table model main.fact_campaign_performance ................... [RUN]
[0m17:47:47.798991 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_campaign_performance)
[0m17:47:47.799212 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m17:47:47.804799 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:47:47.805936 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 17:47:47.799357 => 17:47:47.805804
[0m17:47:47.806173 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m17:47:47.825810 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:47:47.826556 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:47:47.826770 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: BEGIN
[0m17:47:47.826962 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:47.832358 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.832617 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:47:47.832831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */

  
    
    

    create  table
      "dbt"."main"."fact_campaign_performance__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."campaign"  -- Hypothetical source for campaign performance metrics
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS campaign_performance_id,  -- Surrogate Key
    campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
    leads_generated,
    opportunities_created,
    revenue_generated,
    expenses,
    createddate AS performance_created_at
FROM source
    );
  
  
[0m17:47:47.833888 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 17:47:47.806328 => 17:47:47.833788
[0m17:47:47.834128 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: ROLLBACK
[0m17:47:47.838017 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_campaign_performance'
[0m17:47:47.838314 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: Close
[0m17:47:47.840262 [debug] [Thread-1  ]: Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m17:47:47.840692 [error] [Thread-1  ]: 1 of 32 ERROR creating sql table model main.fact_campaign_performance .......... [[31mERROR[0m in 0.04s]
[0m17:47:47.841073 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:47:47.841330 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m17:47:47.841623 [info ] [Thread-1  ]: 2 of 32 START sql table model main.fact_case_history ........................... [RUN]
[0m17:47:47.842023 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_case_history)
[0m17:47:47.842234 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m17:47:47.870193 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.871229 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 17:47:47.842392 => 17:47:47.871053
[0m17:47:47.871504 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m17:47:47.874212 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.874705 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.874921 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: BEGIN
[0m17:47:47.875111 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:47.880698 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.880983 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.881185 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */

  
    
    

    create  table
      "dbt"."main"."fact_case_history__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY systemmodstamp) AS case_history_id,  -- Surrogate Key
    caseid AS case_fk,                                               -- Foreign Key to fact_case
    status,
    systemmodstamp AS history_date
FROM source
    );
  
  
[0m17:47:47.882431 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.886388 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.886631 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
alter table "dbt"."main"."fact_case_history" rename to "fact_case_history__dbt_backup"
[0m17:47:47.887004 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.888812 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.889039 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
alter table "dbt"."main"."fact_case_history__dbt_tmp" rename to "fact_case_history"
[0m17:47:47.889342 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.899885 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: COMMIT
[0m17:47:47.900127 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.900320 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: COMMIT
[0m17:47:47.901000 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.903886 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.904105 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
drop table if exists "dbt"."main"."fact_case_history__dbt_backup" cascade
[0m17:47:47.904499 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.905307 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 17:47:47.871652 => 17:47:47.905211
[0m17:47:47.905536 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: Close
[0m17:47:47.922397 [info ] [Thread-1  ]: 2 of 32 OK created sql table model main.fact_case_history ...................... [[32mOK[0m in 0.08s]
[0m17:47:47.922813 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m17:47:47.923054 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m17:47:47.923351 [info ] [Thread-1  ]: 3 of 32 START sql table model main.fact_product_sales .......................... [RUN]
[0m17:47:47.923772 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_product_sales)
[0m17:47:47.923989 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m17:47:47.926917 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m17:47:47.927417 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 17:47:47.924124 => 17:47:47.927313
[0m17:47:47.927623 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m17:47:47.930388 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_product_sales"
[0m17:47:47.939978 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m17:47:47.940338 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: BEGIN
[0m17:47:47.940542 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:47.946266 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.946529 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m17:47:47.946747 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_product_sales"} */

  
    
    

    create  table
      "dbt"."main"."fact_product_sales__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."pricebook_entry"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS product_sales_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
    product2id AS product_fk,                                      -- Foreign Key to dim_product
    pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
    unitprice AS unit_price,
    isactive AS is_active,
    createddate AS product_sales_created_at,
    lastmodifieddate AS product_sales_last_modified_date
FROM source
    );
  
  
[0m17:47:47.947431 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 17:47:47.927758 => 17:47:47.947331
[0m17:47:47.947633 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: ROLLBACK
[0m17:47:47.948190 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_product_sales'
[0m17:47:47.948377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: Close
[0m17:47:47.950459 [debug] [Thread-1  ]: Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:47.950962 [error] [Thread-1  ]: 3 of 32 ERROR creating sql table model main.fact_product_sales ................. [[31mERROR[0m in 0.03s]
[0m17:47:47.951307 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m17:47:47.951544 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:47:47.951769 [info ] [Thread-1  ]: 4 of 32 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m17:47:47.952208 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m17:47:47.952439 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m17:47:47.954606 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.955412 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 17:47:47.952588 => 17:47:47.955301
[0m17:47:47.955631 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m17:47:47.966168 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.967008 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.967301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m17:47:47.967515 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:47.973501 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.973831 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.974084 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m17:47:47.974944 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.977315 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.977558 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m17:47:47.977906 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.980702 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.980973 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m17:47:47.981346 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.982212 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m17:47:47.982417 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.982592 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m17:47:47.983352 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.985190 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.985420 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m17:47:47.985815 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.986586 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 17:47:47.955791 => 17:47:47.986491
[0m17:47:47.986801 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m17:47:47.994739 [info ] [Thread-1  ]: 4 of 32 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.04s]
[0m17:47:47.995119 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:47:47.995370 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:47:47.995734 [info ] [Thread-1  ]: 5 of 32 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m17:47:47.996138 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m17:47:47.996355 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:47:47.998523 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:47.999093 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 17:47:47.996500 => 17:47:47.998988
[0m17:47:47.999299 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:47:48.002074 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.002541 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.002730 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m17:47:48.002907 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.008387 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.008714 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.009001 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m17:47:48.009927 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.012146 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.012398 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m17:47:48.012747 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.015193 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.015409 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m17:47:48.015730 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.016936 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m17:47:48.017228 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.017431 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m17:47:48.018154 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.019742 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.019972 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m17:47:48.020389 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.021308 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 17:47:47.999427 => 17:47:48.021209
[0m17:47:48.021567 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m17:47:48.029333 [info ] [Thread-1  ]: 5 of 32 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.03s]
[0m17:47:48.029774 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:47:48.030033 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:47:48.030382 [info ] [Thread-1  ]: 6 of 32 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m17:47:48.030788 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m17:47:48.030981 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m17:47:48.033097 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.033621 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 17:47:48.031119 => 17:47:48.033522
[0m17:47:48.033831 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m17:47:48.036545 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.036981 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.037173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m17:47:48.037347 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.042884 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.043099 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.043324 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m17:47:48.044002 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.046044 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.046266 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m17:47:48.046596 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.048288 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.048490 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m17:47:48.048777 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.049680 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m17:47:48.049871 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.050041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m17:47:48.050708 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.052944 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.053218 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m17:47:48.053665 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.054674 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 17:47:48.034072 => 17:47:48.054571
[0m17:47:48.054932 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m17:47:48.062988 [info ] [Thread-1  ]: 6 of 32 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.03s]
[0m17:47:48.063380 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:47:48.063614 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:47:48.064016 [info ] [Thread-1  ]: 7 of 32 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m17:47:48.064505 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m17:47:48.064724 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:47:48.066720 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.067573 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 17:47:48.064866 => 17:47:48.067458
[0m17:47:48.067791 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:47:48.070441 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.070917 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.071119 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m17:47:48.071293 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.076555 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.076833 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.077035 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m17:47:48.077461 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.079747 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.080089 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m17:47:48.080498 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.082455 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.082696 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m17:47:48.083013 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.083887 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m17:47:48.084079 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.084252 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m17:47:48.084861 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.086733 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.086958 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m17:47:48.087368 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.088955 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 17:47:48.067922 => 17:47:48.088864
[0m17:47:48.089169 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m17:47:48.097056 [info ] [Thread-1  ]: 7 of 32 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.03s]
[0m17:47:48.097466 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:47:48.097707 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:47:48.098023 [info ] [Thread-1  ]: 8 of 32 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m17:47:48.098379 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m17:47:48.098573 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:47:48.100561 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.101091 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 17:47:48.098700 => 17:47:48.100976
[0m17:47:48.101285 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:47:48.103948 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.104392 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.104582 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m17:47:48.104757 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.110423 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.110696 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.110930 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m17:47:48.111706 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.113871 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.114079 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m17:47:48.114378 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.116081 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.116282 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m17:47:48.116565 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.117390 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m17:47:48.117579 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.117748 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m17:47:48.118476 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.120464 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.120715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m17:47:48.121140 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.122077 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 17:47:48.101411 => 17:47:48.121976
[0m17:47:48.122301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m17:47:48.130227 [info ] [Thread-1  ]: 8 of 32 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.03s]
[0m17:47:48.130645 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:47:48.130891 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:47:48.131226 [info ] [Thread-1  ]: 9 of 32 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m17:47:48.131583 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m17:47:48.131781 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:47:48.134567 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.135037 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 17:47:48.131918 => 17:47:48.134942
[0m17:47:48.135233 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:47:48.137921 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.138383 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.138577 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m17:47:48.138759 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.143996 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.144259 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.144498 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m17:47:48.145296 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.147468 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.147676 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m17:47:48.147975 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.149685 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.149879 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m17:47:48.150162 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.151053 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m17:47:48.151236 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.151404 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m17:47:48.152040 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.153971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.154189 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m17:47:48.154624 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.155448 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 17:47:48.135360 => 17:47:48.155345
[0m17:47:48.155660 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m17:47:48.163248 [info ] [Thread-1  ]: 9 of 32 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.03s]
[0m17:47:48.163630 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:47:48.163882 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:47:48.164210 [info ] [Thread-1  ]: 10 of 32 START sql view model staging.stg_salesforce__opportunity .............. [RUN]
[0m17:47:48.164561 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m17:47:48.164757 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:47:48.166806 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.167309 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 17:47:48.164886 => 17:47:48.167210
[0m17:47:48.167509 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:47:48.171041 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.171486 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.171680 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m17:47:48.171864 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.177126 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.177373 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.177599 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m17:47:48.178305 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.180388 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.180598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m17:47:48.180893 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.182594 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.182803 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m17:47:48.183089 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.183918 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m17:47:48.184121 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.184292 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m17:47:48.184946 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.186881 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.187102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m17:47:48.187542 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.188383 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 17:47:48.167642 => 17:47:48.188273
[0m17:47:48.188620 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m17:47:48.196384 [info ] [Thread-1  ]: 10 of 32 OK created sql view model staging.stg_salesforce__opportunity ......... [[32mOK[0m in 0.03s]
[0m17:47:48.196767 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:47:48.196992 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:47:48.197425 [info ] [Thread-1  ]: 11 of 32 START sql view model staging.stg_salesforce__opportunity_history ...... [RUN]
[0m17:47:48.197893 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m17:47:48.198122 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:47:48.200123 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.200589 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 17:47:48.198258 => 17:47:48.200491
[0m17:47:48.200795 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:47:48.204057 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.204486 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.204677 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m17:47:48.204851 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.210328 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.210587 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.210802 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m17:47:48.211437 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.213903 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.214137 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m17:47:48.214517 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.216632 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.216875 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m17:47:48.217261 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.218219 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m17:47:48.218442 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.218635 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m17:47:48.219259 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.220951 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.221188 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m17:47:48.221555 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.222291 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 17:47:48.200932 => 17:47:48.222196
[0m17:47:48.222504 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m17:47:48.230236 [info ] [Thread-1  ]: 11 of 32 OK created sql view model staging.stg_salesforce__opportunity_history . [[32mOK[0m in 0.03s]
[0m17:47:48.230624 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:47:48.230878 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:47:48.231213 [info ] [Thread-1  ]: 12 of 32 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m17:47:48.231573 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m17:47:48.231763 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:47:48.233777 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.234297 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 17:47:48.231898 => 17:47:48.234172
[0m17:47:48.234520 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:47:48.237426 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.237953 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.238174 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m17:47:48.238362 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.244059 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.244322 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.244529 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m17:47:48.244977 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.248108 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.248346 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m17:47:48.248664 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.250378 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.250580 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m17:47:48.250880 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.252069 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m17:47:48.252281 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.252454 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m17:47:48.253138 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.254855 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.255062 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m17:47:48.255487 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.256639 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 17:47:48.234658 => 17:47:48.256520
[0m17:47:48.256906 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m17:47:48.264510 [info ] [Thread-1  ]: 12 of 32 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.03s]
[0m17:47:48.264935 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:47:48.265190 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:47:48.265526 [info ] [Thread-1  ]: 13 of 32 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m17:47:48.265888 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m17:47:48.266082 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:47:48.268278 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.268816 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 17:47:48.266214 => 17:47:48.268717
[0m17:47:48.269011 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:47:48.271627 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.272009 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.272193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m17:47:48.272367 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.277782 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.278069 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.278280 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m17:47:48.278812 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.281654 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.281869 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m17:47:48.282205 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.283906 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.284101 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m17:47:48.284488 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.285460 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m17:47:48.285812 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.286034 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m17:47:48.286659 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.288585 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.288854 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m17:47:48.289321 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.290212 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 17:47:48.269141 => 17:47:48.290111
[0m17:47:48.290443 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m17:47:48.298229 [info ] [Thread-1  ]: 13 of 32 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.03s]
[0m17:47:48.298639 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:47:48.298887 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:47:48.299243 [info ] [Thread-1  ]: 14 of 32 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m17:47:48.299639 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m17:47:48.299846 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:47:48.301956 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.302418 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 17:47:48.299998 => 17:47:48.302320
[0m17:47:48.302616 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:47:48.305419 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.305903 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.306095 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m17:47:48.306271 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.311508 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.311804 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.312015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m17:47:48.312481 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.314651 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.314865 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m17:47:48.315170 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.317577 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.317777 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m17:47:48.318077 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.318942 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m17:47:48.319140 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.319471 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m17:47:48.320126 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.322017 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.322346 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m17:47:48.322784 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.323667 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 17:47:48.302745 => 17:47:48.323564
[0m17:47:48.323890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m17:47:48.331951 [info ] [Thread-1  ]: 14 of 32 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.03s]
[0m17:47:48.332358 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:47:48.332603 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:47:48.332875 [info ] [Thread-1  ]: 15 of 32 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m17:47:48.333239 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m17:47:48.333429 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:47:48.335504 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.335988 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 17:47:48.333560 => 17:47:48.335888
[0m17:47:48.336192 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:47:48.338833 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.339282 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.339467 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m17:47:48.339645 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.344949 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.345227 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.345436 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m17:47:48.345933 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.348153 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.348378 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m17:47:48.348719 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.351078 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.351296 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m17:47:48.351620 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.352467 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m17:47:48.352663 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.352835 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m17:47:48.353599 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.355551 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.355860 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m17:47:48.356319 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.357153 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 17:47:48.336320 => 17:47:48.357052
[0m17:47:48.357368 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m17:47:48.365238 [info ] [Thread-1  ]: 15 of 32 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.03s]
[0m17:47:48.365603 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:47:48.365823 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:47:48.366235 [info ] [Thread-1  ]: 16 of 32 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m17:47:48.366716 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m17:47:48.366925 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m17:47:48.369144 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.369638 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 17:47:48.367066 => 17:47:48.369534
[0m17:47:48.369838 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m17:47:48.372492 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.372978 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.373179 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m17:47:48.373356 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.378594 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.378864 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.379115 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m17:47:48.380097 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.382390 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.382614 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m17:47:48.382927 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.384653 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.384857 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m17:47:48.385185 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.386089 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m17:47:48.386463 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.386696 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m17:47:48.387384 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.389854 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.417313 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m17:47:48.417857 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.418706 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 17:47:48.369971 => 17:47:48.418612
[0m17:47:48.418924 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m17:47:48.426422 [info ] [Thread-1  ]: 16 of 32 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.06s]
[0m17:47:48.426798 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:47:48.427044 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:47:48.427360 [info ] [Thread-1  ]: 17 of 32 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m17:47:48.427715 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m17:47:48.427919 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:47:48.429960 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.430473 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 17:47:48.428050 => 17:47:48.430379
[0m17:47:48.430665 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:47:48.433281 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.433798 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.434029 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m17:47:48.434245 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.439610 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.439858 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.440061 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m17:47:48.440575 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.442880 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.443136 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m17:47:48.443509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.445277 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.445478 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m17:47:48.445781 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.446667 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m17:47:48.446853 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.447021 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m17:47:48.447635 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.449671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.449913 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m17:47:48.450395 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.451219 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 17:47:48.430796 => 17:47:48.451122
[0m17:47:48.451427 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m17:47:48.459650 [info ] [Thread-1  ]: 17 of 32 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.03s]
[0m17:47:48.461344 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:47:48.461684 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m17:47:48.462335 [info ] [Thread-1  ]: 18 of 32 START sql table model main.dim_account ................................ [RUN]
[0m17:47:48.462862 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m17:47:48.463122 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m17:47:48.466616 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m17:47:48.467669 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 17:47:48.463274 => 17:47:48.467538
[0m17:47:48.467943 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m17:47:48.471188 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m17:47:48.471944 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m17:47:48.472238 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m17:47:48.472448 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.480931 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.481375 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m17:47:48.481610 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."main"."dim_account__dbt_tmp"
  
    as (
      


select
    id as account_id,
    name as account_name,
    industry,
    type as account_type,
    billing_city,
    billing_state,
    billing_country,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__account"
    );
  
  
[0m17:47:48.483143 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 17:47:48.468554 => 17:47:48.483021
[0m17:47:48.483379 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: ROLLBACK
[0m17:47:48.483969 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_account'
[0m17:47:48.484188 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m17:47:48.486317 [debug] [Thread-1  ]: Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__account.sic", "stg_salesforce__account.site", "stg_salesforce__account.fax", "stg_salesforce__account.sicdesc", "stg_salesforce__account.ownerid"
  LINE 15:     id as account_id,
               ^
[0m17:47:48.486756 [error] [Thread-1  ]: 18 of 32 ERROR creating sql table model main.dim_account ....................... [[31mERROR[0m in 0.02s]
[0m17:47:48.487094 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m17:47:48.487330 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m17:47:48.487750 [info ] [Thread-1  ]: 19 of 32 START sql table model main.dim_campaign ............................... [RUN]
[0m17:47:48.488292 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m17:47:48.488529 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m17:47:48.490900 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.491469 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 17:47:48.488685 => 17:47:48.491355
[0m17:47:48.491692 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m17:47:48.494663 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.495286 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.495513 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m17:47:48.495707 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.501706 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.502048 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.502279 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."main"."dim_campaign__dbt_tmp"
  
    as (
      


select
    campaign_id,
    name as campaign_name,
    type as campaign_type,
    status as campaign_status,
    startdate as campaign_start_date,
    enddate as campaign_end_date,
    createddate as campaign_created_date,
    lastmodifieddate as campaign_last_modified_date
from "dbt"."staging"."stg_salesforce__campaign"
    );
  
  
[0m17:47:48.504161 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.506485 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.506733 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."main"."dim_campaign" rename to "dim_campaign__dbt_backup"
[0m17:47:48.507130 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.509971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.510224 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."main"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m17:47:48.510550 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.511533 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m17:47:48.511724 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.511894 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m17:47:48.512601 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.514527 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.514754 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."main"."dim_campaign__dbt_backup" cascade
[0m17:47:48.515233 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.516104 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 17:47:48.491836 => 17:47:48.516006
[0m17:47:48.516319 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m17:47:48.530297 [info ] [Thread-1  ]: 19 of 32 OK created sql table model main.dim_campaign .......................... [[32mOK[0m in 0.04s]
[0m17:47:48.530668 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m17:47:48.530888 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m17:47:48.531255 [info ] [Thread-1  ]: 20 of 32 START sql table model main.fact_case .................................. [RUN]
[0m17:47:48.531735 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.fact_case)
[0m17:47:48.531950 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m17:47:48.534253 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m17:47:48.534754 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 17:47:48.532083 => 17:47:48.534640
[0m17:47:48.534975 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m17:47:48.537618 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m17:47:48.538122 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m17:47:48.538311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m17:47:48.538477 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.543956 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.544219 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m17:47:48.544425 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."main"."fact_case__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__case"
)

SELECT
    id AS case_id,                           -- Surrogate Key
    accountid AS account_fk,                 -- Foreign Key to dim_account
    contactid AS contact_fk,                 -- Foreign Key to dim_contact
    ownerid AS user_fk,                      -- Foreign Key to dim_user
    status AS status_name,
    priority,
    origin,
    subject,
    createddate AS case_created_at,
    lastmodifieddate AS case_last_modified_date
FROM source
    );
  
  
[0m17:47:48.545475 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 17:47:48.535122 => 17:47:48.545376
[0m17:47:48.545694 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m17:47:48.546249 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m17:47:48.546429 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m17:47:48.548478 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.case_id", "source.assetid", "source.ownerid"
  LINE 19:     id AS case_id,                           -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      contactid AS contact_fk,                 -- Foreign Key to dim_contact
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      status AS status_name,
      priority,
      origin,
      subject,
      createddate AS case_created_at,
      lastmodifieddate AS case_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:48.548951 [error] [Thread-1  ]: 20 of 32 ERROR creating sql table model main.fact_case ......................... [[31mERROR[0m in 0.02s]
[0m17:47:48.549311 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m17:47:48.549556 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m17:47:48.549832 [info ] [Thread-1  ]: 21 of 32 START sql table model main.dim_case_status ............................ [RUN]
[0m17:47:48.550193 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.dim_case_status)
[0m17:47:48.550388 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m17:47:48.552682 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.553570 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 17:47:48.550519 => 17:47:48.553421
[0m17:47:48.553827 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m17:47:48.556501 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.557364 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.557628 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m17:47:48.557823 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.563213 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.563492 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.563699 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."main"."dim_case_status__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        status AS status_name,
        status AS status_description  -- Adjust as needed, typically a description field should be separate
    FROM "dbt"."staging"."stg_salesforce__case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY status_name) AS status_id,  -- Surrogate Key
    status_name,
    status_description
FROM source
    );
  
  
[0m17:47:48.565029 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.568184 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.568449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m17:47:48.568845 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.570707 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.570927 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m17:47:48.571218 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.572167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m17:47:48.572360 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.572535 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m17:47:48.573143 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.575021 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.575270 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."main"."dim_case_status__dbt_backup" cascade
[0m17:47:48.575695 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.576509 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 17:47:48.553970 => 17:47:48.576414
[0m17:47:48.576737 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m17:47:48.585174 [info ] [Thread-1  ]: 21 of 32 OK created sql table model main.dim_case_status ....................... [[32mOK[0m in 0.03s]
[0m17:47:48.585597 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m17:47:48.585845 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m17:47:48.586129 [info ] [Thread-1  ]: 22 of 32 START sql table model main.dim_contact ................................ [RUN]
[0m17:47:48.586507 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m17:47:48.586708 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m17:47:48.588805 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m17:47:48.589268 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 17:47:48.586844 => 17:47:48.589172
[0m17:47:48.589463 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m17:47:48.591973 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m17:47:48.592379 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m17:47:48.592601 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m17:47:48.592846 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.598337 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.598641 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m17:47:48.598849 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

WITH base AS (
    SELECT
        id AS contact_id,
        account_id,
        first_name,
        last_name,
        email,
        phone,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__contact"
)
SELECT * FROM base;
    );
  
  
[0m17:47:48.599309 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 17:47:48.589591 => 17:47:48.599211
[0m17:47:48.599512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m17:47:48.600049 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m17:47:48.600229 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m17:47:48.602171 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.602578 [error] [Thread-1  ]: 22 of 32 ERROR creating sql table model main.dim_contact ....................... [[31mERROR[0m in 0.02s]
[0m17:47:48.602897 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m17:47:48.603135 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m17:47:48.603410 [info ] [Thread-1  ]: 23 of 32 START sql view model main.dim_lead .................................... [RUN]
[0m17:47:48.603777 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m17:47:48.603962 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m17:47:48.606012 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m17:47:48.606500 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 17:47:48.604093 => 17:47:48.606408
[0m17:47:48.606683 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m17:47:48.610281 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m17:47:48.610779 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m17:47:48.610967 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m17:47:48.611141 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.616591 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.616891 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m17:47:48.617100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
  create view "dbt"."main"."dim_lead__dbt_tmp" as (
    

WITH base AS (
    SELECT
        id AS lead_id,
        first_name,
        last_name,
        company,
        email,
        phone,
        status,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__lead"
)

SELECT * FROM base;
  );

[0m17:47:48.617547 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 17:47:48.606808 => 17:47:48.617423
[0m17:47:48.617775 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: ROLLBACK
[0m17:47:48.618379 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_lead'
[0m17:47:48.618615 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m17:47:48.620490 [debug] [Thread-1  ]: Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.620926 [error] [Thread-1  ]: 23 of 32 ERROR creating sql view model main.dim_lead ........................... [[31mERROR[0m in 0.02s]
[0m17:47:48.621251 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m17:47:48.621490 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m17:47:48.621773 [info ] [Thread-1  ]: 24 of 32 START sql table model main.dim_opportunity ............................ [RUN]
[0m17:47:48.622134 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m17:47:48.622328 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m17:47:48.624657 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:47:48.625193 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 17:47:48.622457 => 17:47:48.625089
[0m17:47:48.625398 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m17:47:48.627978 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:47:48.628614 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:47:48.628892 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m17:47:48.629163 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.634924 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.635153 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:47:48.635424 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity__dbt_tmp"
  
    as (
      



renamed AS (

    SELECT
        /* IDs */
        id AS opportunity_id,
        accountid AS account_fk,
        ownerid AS user_fk,
        contactid AS primary_contact_fk,
        campaignid AS campaign_fk,
        pricebook2id AS pricebook_fk,
        contractid AS contract_fk,
        primarypartneraccountid AS primary_partner_account_fk,
        createdbyid AS created_by_fk,
        lastmodifiedbyid AS last_modified_by_fk,
        lastamountchangedhistoryid AS last_amount_change_history_fk,
        lastclosedatechangedhistoryid AS last_close_date_change_history_fk,

        /* Metrics */
        amount,
        probability,
        expectedrevenue AS expected_revenue,
        totalopportunityquantity AS total_opportunity_quantity,

        /* Dimensions */
        name AS opportunity_name,
        description,
        stagename AS stage_name,
        stagesortorder AS stage_sort_order,
        type AS opportunity_type,
        nextstep AS next_step,
        leadsource AS lead_source,
        isclosed AS is_closed,
        iswon AS is_won,
        forecastcategory AS forecast_category,
        forecastcategoryname AS forecast_category_name,
        hasopportunitylineitem AS has_opportunity_line_item,
        deliveryinstallationstatus__c AS delivery_installation_status,
        trackingnumber__c AS tracking_number,
        ordernumber__c AS order_number,
        currentgenerators__c AS current_generators,
        maincompetitors__c AS main_competitors,

        /* Dates */
        closedate AS close_date,
        createddate AS created_at,
        lastmodifieddate AS last_modified_date,
        systemmodstamp AS system_mod_stamp,
        lastactivitydate AS last_activity_date,
        laststagechangedate AS last_stage_change_date,
        fiscalyear AS fiscal_year,
        fiscalquarter AS fiscal_quarter

    FROM "dbt"."staging"."stg_salesforce__opportunity"

)

SELECT
    /* IDs */
    opportunity_id,
    account_fk,
    user_fk,
    primary_contact_fk,
    campaign_fk,
    pricebook_fk,
    contract_fk,
    primary_partner_account_fk,
    created_by_fk,
    last_modified_by_fk,
    last_amount_change_history_fk,
    last_close_date_change_history_fk,

    /* Metrics */
    amount,
    probability,
    expected_revenue,
    total_opportunity_quantity,

    /* Dimensions */
    opportunity_name,
    description,
    stage_name,
    stage_sort_order,
    opportunity_type,
    next_step,
    lead_source,
    is_closed,
    is_won,
    forecast_category,
    forecast_category_name,
    has_opportunity_line_item,
    delivery_installation_status,
    tracking_number,
    order_number,
    current_generators,
    main_competitors,

    /* Dates */
    close_date,
    created_at,
    last_modified_date,
    system_mod_stamp,
    last_activity_date,
    last_stage_change_date,
    fiscal_year,
    fiscal_quarter

FROM renamed
    );
  
  
[0m17:47:48.635906 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 17:47:48.625537 => 17:47:48.635810
[0m17:47:48.636101 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: ROLLBACK
[0m17:47:48.636602 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_opportunity'
[0m17:47:48.636798 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m17:47:48.638693 [debug] [Thread-1  ]: Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Parser Error: syntax error at or near "renamed"
[0m17:47:48.639172 [error] [Thread-1  ]: 24 of 32 ERROR creating sql table model main.dim_opportunity ................... [[31mERROR[0m in 0.02s]
[0m17:47:48.639511 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m17:47:48.639755 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:47:48.640036 [info ] [Thread-1  ]: 25 of 32 START sql table model main.dim_opportunity_stage ...................... [RUN]
[0m17:47:48.640401 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m17:47:48.640601 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:47:48.642848 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.643372 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 17:47:48.640728 => 17:47:48.643267
[0m17:47:48.643577 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:47:48.646186 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.646614 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.646799 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m17:47:48.646971 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.652173 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.652441 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.652641 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        stagename AS stage_name,
        stagesortorder AS stage_sort_order
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY stage_sort_order) AS stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
FROM source
    );
  
  
[0m17:47:48.654942 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.657989 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.658241 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m17:47:48.658635 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.660585 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.660821 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m17:47:48.661152 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.662153 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m17:47:48.662346 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.662517 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m17:47:48.663142 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.664733 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.664935 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."main"."dim_opportunity_stage__dbt_backup" cascade
[0m17:47:48.665309 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.666006 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 17:47:48.643719 => 17:47:48.665908
[0m17:47:48.666223 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m17:47:48.674653 [info ] [Thread-1  ]: 25 of 32 OK created sql table model main.dim_opportunity_stage ................. [[32mOK[0m in 0.03s]
[0m17:47:48.675045 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:47:48.675292 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m17:47:48.675560 [info ] [Thread-1  ]: 26 of 32 START sql table model main.fact_opportunity ........................... [RUN]
[0m17:47:48.675913 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.fact_opportunity)
[0m17:47:48.676112 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m17:47:48.678241 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m17:47:48.678708 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 17:47:48.676249 => 17:47:48.678612
[0m17:47:48.678909 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m17:47:48.681272 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m17:47:48.681639 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m17:47:48.681824 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m17:47:48.682001 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.687525 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.687812 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m17:47:48.688025 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."fact_opportunity__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    id AS opportunity_id,                    -- Surrogate Key
    accountid AS account_fk,                 -- Foreign Key to dim_account
    ownerid AS user_fk,                      -- Foreign Key to dim_user
    campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
    pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
    stagename AS stage_name,                 -- Captures the current stage
    stagesortorder AS stage_sort_order,
    amount,
    probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,
    closedate AS close_date,
    createddate AS opportunity_created_at,
    lastmodifieddate AS opportunity_last_modified_date,
    isclosed AS is_closed,
    iswon AS is_won
FROM source
    );
  
  
[0m17:47:48.689218 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 17:47:48.679043 => 17:47:48.689109
[0m17:47:48.689435 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m17:47:48.690014 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m17:47:48.690195 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m17:47:48.692293 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.iswon", "source.ownerid"
  LINE 19:     id AS opportunity_id,                    -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
      pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
      stagename AS stage_name,                 -- Captures the current stage
      stagesortorder AS stage_sort_order,
      amount,
      probability,
      expectedrevenue AS expected_revenue,
      totalopportunityquantity AS total_opportunity_quantity,
      closedate AS close_date,
      createddate AS opportunity_created_at,
      lastmodifieddate AS opportunity_last_modified_date,
      isclosed AS is_closed,
      iswon AS is_won
  FROM source
      );
    
    ...
               ^
[0m17:47:48.692712 [error] [Thread-1  ]: 26 of 32 ERROR creating sql table model main.fact_opportunity .................. [[31mERROR[0m in 0.02s]
[0m17:47:48.693080 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m17:47:48.693323 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:47:48.693621 [info ] [Thread-1  ]: 27 of 32 START sql table model main.fact_opportunity_history ................... [RUN]
[0m17:47:48.693991 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_opportunity_history)
[0m17:47:48.694190 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m17:47:48.696290 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:47:48.696757 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 17:47:48.694319 => 17:47:48.696659
[0m17:47:48.696947 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m17:47:48.700583 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:47:48.701243 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:47:48.701453 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m17:47:48.701631 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.707097 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.707401 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:47:48.707644 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."main"."fact_opportunity_history__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__opportunity_history"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY systemmodstamp) AS history_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                            -- Foreign Key to fact_opportunity
    stage_fk AS stage_fk,                                       -- Foreign Key to dim_opportunity_stage
    amount,
    probability,
    closedate AS close_date,
    systemmodstamp AS history_date
FROM source
    );
  
  
[0m17:47:48.708533 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 17:47:48.697074 => 17:47:48.708421
[0m17:47:48.708768 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m17:47:48.709390 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m17:47:48.709623 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m17:47:48.711432 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Column "stage_fk" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m17:47:48.711844 [error] [Thread-1  ]: 27 of 32 ERROR creating sql table model main.fact_opportunity_history .......... [[31mERROR[0m in 0.02s]
[0m17:47:48.712184 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:47:48.712415 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m17:47:48.712719 [info ] [Thread-1  ]: 28 of 32 START sql table model main.dim_pricebook .............................. [RUN]
[0m17:47:48.713115 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity_history, now model.elastic_dbt_interview.dim_pricebook)
[0m17:47:48.713340 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m17:47:48.715509 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m17:47:48.715984 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 17:47:48.713477 => 17:47:48.715884
[0m17:47:48.716173 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m17:47:48.718814 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m17:47:48.719265 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m17:47:48.719491 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m17:47:48.719676 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.725067 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.725346 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m17:47:48.725546 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."main"."dim_pricebook__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        pricebook2id AS pricebook_id,        -- Surrogate Key
        name AS pricebook_name,
        isactive AS is_active,
        description,
        createddate AS pricebook_created_date,
        lastmodifieddate AS pricebook_last_modified_date
    FROM "dbt"."staging"."stg_salesforce__pricebook_entry"
)

SELECT * FROM source
    );
  
  
[0m17:47:48.726283 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 17:47:48.716303 => 17:47:48.726187
[0m17:47:48.726481 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: ROLLBACK
[0m17:47:48.726999 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_pricebook'
[0m17:47:48.727178 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m17:47:48.728905 [debug] [Thread-1  ]: Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m17:47:48.729275 [error] [Thread-1  ]: 28 of 32 ERROR creating sql table model main.dim_pricebook ..................... [[31mERROR[0m in 0.02s]
[0m17:47:48.729585 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m17:47:48.729807 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m17:47:48.730185 [info ] [Thread-1  ]: 29 of 32 START sql table model main.dim_product ................................ [RUN]
[0m17:47:48.730668 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m17:47:48.730896 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m17:47:48.733233 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m17:47:48.733780 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 17:47:48.731042 => 17:47:48.733671
[0m17:47:48.733979 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m17:47:48.736634 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m17:47:48.737123 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m17:47:48.737307 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m17:47:48.737477 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.742875 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.743189 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m17:47:48.743404 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."main"."dim_product__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__product_2"
)

SELECT
    id AS product_id,                        -- Surrogate Key
    name AS product_name,
    productcode AS product_code,
    description,
    isactive AS is_active,
    createddate AS product_created_date,
    lastmodifieddate AS product_last_modified_date
FROM source
    );
  
  
[0m17:47:48.744379 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 17:47:48.734116 => 17:47:48.744152
[0m17:47:48.744597 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: ROLLBACK
[0m17:47:48.745181 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_product'
[0m17:47:48.745368 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m17:47:48.747307 [debug] [Thread-1  ]: Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS product_id,                        -- Surrogate Key
      name AS product_name,
      productcode AS product_code,
      description,
      isactive AS is_active,
      createddate AS product_created_date,
      lastmodifieddate AS product_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:48.747757 [error] [Thread-1  ]: 29 of 32 ERROR creating sql table model main.dim_product ....................... [[31mERROR[0m in 0.02s]
[0m17:47:48.748253 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m17:47:48.748522 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m17:47:48.748868 [info ] [Thread-1  ]: 30 of 32 START sql table model main.dim_solution ............................... [RUN]
[0m17:47:48.749365 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_solution)
[0m17:47:48.749599 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m17:47:48.752447 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m17:47:48.752924 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 17:47:48.749735 => 17:47:48.752830
[0m17:47:48.753114 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m17:47:48.755657 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m17:47:48.756077 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m17:47:48.756262 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m17:47:48.756431 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.761798 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.762126 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m17:47:48.762338 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."main"."dim_solution__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__solution"
)

SELECT
    solution_id,                       -- Surrogate Key
    solutionname AS solution_name,
    status,
    description,
    createddate AS solution_created_date,
    lastmodifieddate AS solution_last_modified_date
FROM source
    );
  
  
[0m17:47:48.763154 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 17:47:48.753240 => 17:47:48.763051
[0m17:47:48.763374 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: ROLLBACK
[0m17:47:48.763975 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_solution'
[0m17:47:48.764214 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m17:47:48.766045 [debug] [Thread-1  ]: Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "description" not found in FROM clause!
  Candidate bindings: "source.ownerid"
  LINE 22:     description,
               ^
[0m17:47:48.766412 [error] [Thread-1  ]: 30 of 32 ERROR creating sql table model main.dim_solution ...................... [[31mERROR[0m in 0.02s]
[0m17:47:48.766719 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m17:47:48.766933 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m17:47:48.767305 [info ] [Thread-1  ]: 31 of 32 START sql table model main.dim_user ................................... [RUN]
[0m17:47:48.767810 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m17:47:48.768019 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m17:47:48.770169 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m17:47:48.770685 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 17:47:48.768154 => 17:47:48.770587
[0m17:47:48.770887 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m17:47:48.773525 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m17:47:48.774050 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m17:47:48.774260 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m17:47:48.774436 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.779664 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.779919 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m17:47:48.780111 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

WITH base AS (
    SELECT
        user_id,
        username,
        first_name,
        last_name,
        email,
        role_id,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__user"
)
SELECT * FROM base;
    );
  
  
[0m17:47:48.780523 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 17:47:48.771018 => 17:47:48.780429
[0m17:47:48.780727 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m17:47:48.781251 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m17:47:48.781428 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m17:47:48.783361 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.783800 [error] [Thread-1  ]: 31 of 32 ERROR creating sql table model main.dim_user .......................... [[31mERROR[0m in 0.02s]
[0m17:47:48.784129 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m17:47:48.784361 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user_role
[0m17:47:48.784588 [info ] [Thread-1  ]: 32 of 32 START sql table model main.dim_user_role .............................. [RUN]
[0m17:47:48.784945 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.dim_user_role)
[0m17:47:48.785135 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user_role
[0m17:47:48.787322 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user_role"
[0m17:47:48.787809 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (compile): 17:47:48.785267 => 17:47:48.787711
[0m17:47:48.788002 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user_role
[0m17:47:48.791659 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user_role"
[0m17:47:48.792308 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m17:47:48.792521 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: BEGIN
[0m17:47:48.792703 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.798095 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.798340 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m17:47:48.798528 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user_role"} */

  
    
    

    create  table
      "dbt"."main"."dim_user_role__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__user_role"
)

SELECT
    user_role_id,                           -- Surrogate Key
    name AS role_name,
    roledescription AS role_description
FROM source
    );
  
  
[0m17:47:48.799321 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (execute): 17:47:48.788129 => 17:47:48.799227
[0m17:47:48.799529 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: ROLLBACK
[0m17:47:48.800045 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user_role'
[0m17:47:48.800221 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: Close
[0m17:47:48.802145 [debug] [Thread-1  ]: Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "roledescription" not found in FROM clause!
  Candidate bindings: "source.rollupdescription"
  LINE 21:     roledescription AS role_description
               ^
[0m17:47:48.802600 [error] [Thread-1  ]: 32 of 32 ERROR creating sql table model main.dim_user_role ..................... [[31mERROR[0m in 0.02s]
[0m17:47:48.802928 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user_role
[0m17:47:48.803743 [debug] [MainThread]: Using duckdb connection "master"
[0m17:47:48.803916 [debug] [MainThread]: On master: BEGIN
[0m17:47:48.804062 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:47:48.809335 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:47:48.809564 [debug] [MainThread]: On master: COMMIT
[0m17:47:48.809718 [debug] [MainThread]: Using duckdb connection "master"
[0m17:47:48.809863 [debug] [MainThread]: On master: COMMIT
[0m17:47:48.810054 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:47:48.810206 [debug] [MainThread]: On master: Close
[0m17:47:48.811827 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:47:48.812119 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user_role' was properly closed.
[0m17:47:48.812375 [info ] [MainThread]: 
[0m17:47:48.812578 [info ] [MainThread]: Finished running 17 table models, 15 view models in 0 hours 0 minutes and 1.16 seconds (1.16s).
[0m17:47:48.815083 [debug] [MainThread]: Command end result
[0m17:47:48.823408 [info ] [MainThread]: 
[0m17:47:48.823666 [info ] [MainThread]: [31mCompleted with 14 errors and 0 warnings:[0m
[0m17:47:48.823825 [info ] [MainThread]: 
[0m17:47:48.824014 [error] [MainThread]:   Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m17:47:48.824190 [info ] [MainThread]: 
[0m17:47:48.824366 [error] [MainThread]:   Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:48.824537 [info ] [MainThread]: 
[0m17:47:48.824699 [error] [MainThread]:   Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__account.sic", "stg_salesforce__account.site", "stg_salesforce__account.fax", "stg_salesforce__account.sicdesc", "stg_salesforce__account.ownerid"
  LINE 15:     id as account_id,
               ^
[0m17:47:48.824858 [info ] [MainThread]: 
[0m17:47:48.825025 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.case_id", "source.assetid", "source.ownerid"
  LINE 19:     id AS case_id,                           -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      contactid AS contact_fk,                 -- Foreign Key to dim_contact
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      status AS status_name,
      priority,
      origin,
      subject,
      createddate AS case_created_at,
      lastmodifieddate AS case_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:48.825193 [info ] [MainThread]: 
[0m17:47:48.825342 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.825488 [info ] [MainThread]: 
[0m17:47:48.825630 [error] [MainThread]:   Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.825777 [info ] [MainThread]: 
[0m17:47:48.825921 [error] [MainThread]:   Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Parser Error: syntax error at or near "renamed"
[0m17:47:48.826064 [info ] [MainThread]: 
[0m17:47:48.826246 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.iswon", "source.ownerid"
  LINE 19:     id AS opportunity_id,                    -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
      pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
      stagename AS stage_name,                 -- Captures the current stage
      stagesortorder AS stage_sort_order,
      amount,
      probability,
      expectedrevenue AS expected_revenue,
      totalopportunityquantity AS total_opportunity_quantity,
      closedate AS close_date,
      createddate AS opportunity_created_at,
      lastmodifieddate AS opportunity_last_modified_date,
      isclosed AS is_closed,
      iswon AS is_won
  FROM source
      );
    
    ...
               ^
[0m17:47:48.826596 [info ] [MainThread]: 
[0m17:47:48.826805 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Column "stage_fk" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m17:47:48.826978 [info ] [MainThread]: 
[0m17:47:48.827144 [error] [MainThread]:   Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m17:47:48.827314 [info ] [MainThread]: 
[0m17:47:48.827495 [error] [MainThread]:   Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS product_id,                        -- Surrogate Key
      name AS product_name,
      productcode AS product_code,
      description,
      isactive AS is_active,
      createddate AS product_created_date,
      lastmodifieddate AS product_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:48.827673 [info ] [MainThread]: 
[0m17:47:48.827833 [error] [MainThread]:   Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "description" not found in FROM clause!
  Candidate bindings: "source.ownerid"
  LINE 22:     description,
               ^
[0m17:47:48.827993 [info ] [MainThread]: 
[0m17:47:48.828217 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.828387 [info ] [MainThread]: 
[0m17:47:48.828545 [error] [MainThread]:   Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "roledescription" not found in FROM clause!
  Candidate bindings: "source.rollupdescription"
  LINE 21:     roledescription AS role_description
               ^
[0m17:47:48.828722 [info ] [MainThread]: 
[0m17:47:48.828888 [info ] [MainThread]: Done. PASS=18 WARN=0 ERROR=14 SKIP=0 TOTAL=32
[0m17:47:48.829249 [debug] [MainThread]: Command `dbt run` failed at 17:47:48.829192 after 1.41 seconds
[0m17:47:48.829449 [debug] [MainThread]: Flushing usage events


============================== 17:53:51.126907 | 1a8587ac-c80a-4d03-bad6-ab49176e9ccb ==============================
[0m17:53:51.126907 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:53:51.130259 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity.sql', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m17:53:51.130538 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:53:51.215267 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:53:51.235770 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:53:51.285298 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 12 files changed.
[0m17:53:51.285757 [debug] [MainThread]: Partial parsing: added file: elastic_dbt_interview://models/dimensions/dim_date.sql
[0m17:53:51.286014 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_account.sql
[0m17:53:51.286232 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m17:53:51.286445 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_lead.sql
[0m17:53:51.286656 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_case_history.sql
[0m17:53:51.286866 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m17:53:51.287079 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m17:53:51.287290 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m17:53:51.287508 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_case.sql
[0m17:53:51.287719 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_product.sql
[0m17:53:51.287921 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_solution.sql
[0m17:53:51.288124 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_campaign.sql
[0m17:53:51.288346 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_opportunity.sql
[0m17:53:51.338805 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m17:53:51.354967 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m17:53:51.356455 [info ] [MainThread]: 
[0m17:53:51.356920 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:53:51.357499 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m17:53:51.364913 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:53:51.365209 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:53:51.365391 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:53:51.376655 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.377565 [debug] [ThreadPool]: On list_dbt: Close
[0m17:53:51.379343 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m17:53:51.379830 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m17:53:51.382696 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:53:51.382907 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m17:53:51.383067 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:53:51.388624 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.388925 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:53:51.389087 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m17:53:51.389499 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.390006 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:53:51.390187 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:53:51.390336 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:53:51.390563 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.390718 [debug] [ThreadPool]: On create_dbt_main: Close
[0m17:53:51.393606 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m17:53:51.397229 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:53:51.397453 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m17:53:51.397616 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:53:51.402991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.403269 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:53:51.403443 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m17:53:51.421194 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.422256 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m17:53:51.422926 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m17:53:51.423112 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m17:53:51.425034 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m17:53:51.427417 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:53:51.427589 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m17:53:51.427733 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:53:51.432680 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.432920 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:53:51.433088 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m17:53:51.448345 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.451992 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m17:53:51.452323 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m17:53:51.452492 [debug] [ThreadPool]: On list_dbt_main: Close
[0m17:53:51.455167 [debug] [MainThread]: Using duckdb connection "master"
[0m17:53:51.455373 [debug] [MainThread]: On master: BEGIN
[0m17:53:51.455537 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:53:51.460686 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:53:51.460939 [debug] [MainThread]: On master: COMMIT
[0m17:53:51.461099 [debug] [MainThread]: Using duckdb connection "master"
[0m17:53:51.461248 [debug] [MainThread]: On master: COMMIT
[0m17:53:51.461449 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:53:51.461608 [debug] [MainThread]: On master: Close
[0m17:53:51.462866 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:53:51.463078 [info ] [MainThread]: 
[0m17:53:51.464624 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m17:53:51.464918 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_opportunity .............................. [RUN]
[0m17:53:51.465427 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_opportunity)
[0m17:53:51.465629 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m17:53:51.471317 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:53:51.472202 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 17:53:51.465769 => 17:53:51.472088
[0m17:53:51.472403 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m17:53:51.491264 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:53:51.491990 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:53:51.492246 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m17:53:51.492444 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:53:51.497710 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:53:51.497979 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:53:51.498222 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    id AS opportunity_id,
    accountid AS account_id,
    ownerid AS user_id,
    contactid AS primary_contact_id,
    campaignid AS campaign_id,
    pricebook2id AS pricebook_id,
    contractid AS contract_id,
    primarypartneraccountid AS primary_partner_account_id,
    createdbyid AS created_by_id,
    lastmodifiedbyid AS last_modified_by_id,
    lastamountchangedhistoryid AS last_amount_change_history_id,
    lastclosedatechangedhistoryid AS last_close_date_change_history_id,

    /* Metrics */
    amount,
    probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,

    /* Dimensions */
    name AS opportunity_name,
    description,
    stagename AS stage_name,
    stagesortorder AS stage_sort_order,
    type AS opportunity_type,
    nextstep AS next_step,
    leadsource AS lead_source,
    isclosed AS is_closed,
    iswon AS is_won,
    forecastcategory AS forecast_category,
    forecastcategoryname AS forecast_category_name,
    hasopportunitylineitem AS has_opportunity_line_item,
    deliveryinstallationstatus__c AS delivery_installation_status,
    trackingnumber__c AS tracking_number,
    ordernumber__c AS order_number,
    currentgenerators__c AS current_generators,
    maincompetitors__c AS main_competitors,

    /* Dates */
    closedate AS close_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    laststagechangedate AS last_stage_change_date,
    fiscalyear AS fiscal_year,
    fiscalquarter AS fiscal_quarter

FROM "dbt"."staging"."stg_salesforce__opportunity"
    );
  
  
[0m17:53:51.499414 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 17:53:51.472535 => 17:53:51.499302
[0m17:53:51.499656 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: ROLLBACK
[0m17:53:51.503356 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_opportunity'
[0m17:53:51.503556 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m17:53:51.505259 [debug] [Thread-1  ]: Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__opportunity.iswon", "stg_salesforce__opportunity.ownerid"
  LINE 15:     id AS opportunity_id,
               ^
[0m17:53:51.505653 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_opportunity ..................... [[31mERROR[0m in 0.04s]
[0m17:53:51.505997 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m17:53:51.506762 [debug] [MainThread]: Using duckdb connection "master"
[0m17:53:51.506938 [debug] [MainThread]: On master: BEGIN
[0m17:53:51.507089 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:53:51.512558 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:53:51.512891 [debug] [MainThread]: On master: COMMIT
[0m17:53:51.513103 [debug] [MainThread]: Using duckdb connection "master"
[0m17:53:51.513264 [debug] [MainThread]: On master: COMMIT
[0m17:53:51.513531 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:53:51.513695 [debug] [MainThread]: On master: Close
[0m17:53:51.515117 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:53:51.515318 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_opportunity' was properly closed.
[0m17:53:51.515516 [info ] [MainThread]: 
[0m17:53:51.515720 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m17:53:51.516069 [debug] [MainThread]: Command end result
[0m17:53:51.550330 [info ] [MainThread]: 
[0m17:53:51.550649 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m17:53:51.550827 [info ] [MainThread]: 
[0m17:53:51.551002 [error] [MainThread]:   Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__opportunity.iswon", "stg_salesforce__opportunity.ownerid"
  LINE 15:     id AS opportunity_id,
               ^
[0m17:53:51.551183 [info ] [MainThread]: 
[0m17:53:51.551362 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m17:53:51.551696 [debug] [MainThread]: Command `dbt run` failed at 17:53:51.551645 after 0.46 seconds
[0m17:53:51.551890 [debug] [MainThread]: Flushing usage events


============================== 17:54:01.639483 | a1a7ed9a-ab70-4fd5-823a-97d8d28bc9f8 ==============================
[0m17:54:01.639483 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:54:01.642114 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m17:54:01.642368 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:54:01.717977 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:54:01.736921 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:54:01.780387 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:54:01.780863 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_opportunity.sql
[0m17:54:01.808164 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m17:54:01.821779 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m17:54:01.823142 [info ] [MainThread]: 
[0m17:54:01.823730 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:54:01.824278 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m17:54:01.831143 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:54:01.831403 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:54:01.831590 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:54:01.838765 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.839682 [debug] [ThreadPool]: On list_dbt: Close
[0m17:54:01.841610 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m17:54:01.842178 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m17:54:01.845342 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:54:01.845581 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m17:54:01.845739 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:54:01.851069 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.851337 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:54:01.851649 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m17:54:01.851992 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.852507 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:54:01.852686 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:54:01.852848 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:54:01.853081 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.853250 [debug] [ThreadPool]: On create_dbt_main: Close
[0m17:54:01.856028 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m17:54:01.859415 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:54:01.859628 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m17:54:01.859780 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:54:01.865555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.865822 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:54:01.865999 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m17:54:01.881776 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.885344 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m17:54:01.887193 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m17:54:01.887384 [debug] [ThreadPool]: On list_dbt_main: Close
[0m17:54:01.889263 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m17:54:01.891411 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:54:01.891592 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m17:54:01.891743 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:54:01.896834 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.897073 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:54:01.897241 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m17:54:01.915415 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.916540 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m17:54:01.916880 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m17:54:01.917052 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m17:54:01.919970 [debug] [MainThread]: Using duckdb connection "master"
[0m17:54:01.920187 [debug] [MainThread]: On master: BEGIN
[0m17:54:01.920346 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:54:01.926196 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:54:01.926441 [debug] [MainThread]: On master: COMMIT
[0m17:54:01.926622 [debug] [MainThread]: Using duckdb connection "master"
[0m17:54:01.926774 [debug] [MainThread]: On master: COMMIT
[0m17:54:01.926989 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:54:01.927152 [debug] [MainThread]: On master: Close
[0m17:54:01.928563 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:54:01.928767 [info ] [MainThread]: 
[0m17:54:01.930165 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m17:54:01.931261 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_opportunity .............................. [RUN]
[0m17:54:01.931637 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_opportunity)
[0m17:54:01.931833 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m17:54:01.937380 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.938111 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 17:54:01.931969 => 17:54:01.937894
[0m17:54:01.938340 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m17:54:01.957542 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.958151 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.958472 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m17:54:01.958665 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:54:01.964383 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:01.964656 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.964908 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    opportunity_id,
    accountid AS account_id,
    ownerid AS user_id,
    contactid AS primary_contact_id,
    campaignid AS campaign_id,
    pricebook2id AS pricebook_id,
    contractid AS contract_id,
    primarypartneraccountid AS primary_partner_account_id,
    createdbyid AS created_by_id,
    lastmodifiedbyid AS last_modified_by_id,
    lastamountchangedhistoryid AS last_amount_change_history_id,
    lastclosedatechangedhistoryid AS last_close_date_change_history_id,

    /* Metrics */
    amount,
    probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,

    /* Dimensions */
    name AS opportunity_name,
    description,
    stagename AS stage_name,
    stagesortorder AS stage_sort_order,
    type AS opportunity_type,
    nextstep AS next_step,
    leadsource AS lead_source,
    isclosed AS is_closed,
    iswon AS is_won,
    forecastcategory AS forecast_category,
    forecastcategoryname AS forecast_category_name,
    hasopportunitylineitem AS has_opportunity_line_item,
    deliveryinstallationstatus__c AS delivery_installation_status,
    trackingnumber__c AS tracking_number,
    ordernumber__c AS order_number,
    currentgenerators__c AS current_generators,
    maincompetitors__c AS main_competitors,

    /* Dates */
    closedate AS close_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    laststagechangedate AS last_stage_change_date,
    fiscalyear AS fiscal_year,
    fiscalquarter AS fiscal_quarter

FROM "dbt"."staging"."stg_salesforce__opportunity"
    );
  
  
[0m17:54:01.969368 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:01.973613 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.973889 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."main"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m17:54:01.974303 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:01.976122 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.976327 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."main"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m17:54:01.976645 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:01.987562 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m17:54:01.987936 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.988145 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m17:54:01.989716 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:02.017122 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:02.017493 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."main"."dim_opportunity__dbt_backup" cascade
[0m17:54:02.018215 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:02.018995 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 17:54:01.938490 => 17:54:02.018903
[0m17:54:02.019196 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m17:54:02.040662 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_opportunity ......................... [[32mOK[0m in 0.11s]
[0m17:54:02.041085 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m17:54:02.041842 [debug] [MainThread]: Using duckdb connection "master"
[0m17:54:02.042017 [debug] [MainThread]: On master: BEGIN
[0m17:54:02.042166 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:54:02.047823 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:54:02.048050 [debug] [MainThread]: On master: COMMIT
[0m17:54:02.048216 [debug] [MainThread]: Using duckdb connection "master"
[0m17:54:02.048364 [debug] [MainThread]: On master: COMMIT
[0m17:54:02.048559 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:54:02.048715 [debug] [MainThread]: On master: Close
[0m17:54:02.050006 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:54:02.050208 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_opportunity' was properly closed.
[0m17:54:02.050398 [info ] [MainThread]: 
[0m17:54:02.050588 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m17:54:02.050922 [debug] [MainThread]: Command end result
[0m17:54:02.058495 [info ] [MainThread]: 
[0m17:54:02.058822 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:54:02.059010 [info ] [MainThread]: 
[0m17:54:02.059200 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:54:02.059582 [debug] [MainThread]: Command `dbt run` succeeded at 17:54:02.059524 after 0.44 seconds
[0m17:54:02.059783 [debug] [MainThread]: Flushing usage events


============================== 18:00:09.168780 | fc8ec8e3-d567-429a-aa0d-dbc303f20176 ==============================
[0m18:00:09.168780 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:00:09.171971 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_case_status.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:00:09.172251 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:00:09.256930 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:00:09.277537 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:00:09.328401 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:00:09.328698 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:00:09.329693 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m18:00:09.351960 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:00:09.353422 [info ] [MainThread]: 
[0m18:00:09.353882 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:00:09.354609 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:00:09.361868 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:00:09.362129 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:00:09.362320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:00:09.374219 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.375174 [debug] [ThreadPool]: On list_dbt: Close
[0m18:00:09.377096 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:00:09.377611 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:00:09.380479 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:09.380687 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:00:09.380843 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:09.386305 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.386590 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:09.386757 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:00:09.387218 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.387823 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:09.388017 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:09.388176 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:09.388398 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.388561 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:00:09.391285 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m18:00:09.394816 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:09.395080 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:00:09.395237 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:09.400230 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.400477 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:09.400649 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:00:09.419112 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.420136 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:00:09.420775 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:00:09.420976 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:00:09.423091 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m18:00:09.425566 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:09.425754 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:00:09.425905 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:09.431466 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.431711 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:09.431885 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:00:09.447210 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.450738 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:00:09.450973 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:00:09.451127 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:00:09.454053 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:09.454316 [debug] [MainThread]: On master: BEGIN
[0m18:00:09.454484 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:00:09.459910 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:09.460173 [debug] [MainThread]: On master: COMMIT
[0m18:00:09.460337 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:09.460491 [debug] [MainThread]: On master: COMMIT
[0m18:00:09.460693 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:09.460854 [debug] [MainThread]: On master: Close
[0m18:00:09.462375 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:00:09.462620 [info ] [MainThread]: 
[0m18:00:09.464361 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m18:00:09.464697 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_case_status .............................. [RUN]
[0m18:00:09.465108 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_case_status)
[0m18:00:09.465324 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m18:00:09.470693 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.471774 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 18:00:09.465467 => 18:00:09.471578
[0m18:00:09.472039 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m18:00:09.491480 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.492678 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.492993 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m18:00:09.493191 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:00:09.498776 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.499042 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.499249 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."main"."dim_case_status__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        status AS status_name,
        status AS status_description  -- Adjust as needed, typically a description field should be separate
    FROM "dbt"."staging"."stg_salesforce__case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY status_name) AS status_id,  -- Surrogate Key
    status_name,
    status_description
FROM source
    );
  
  
[0m18:00:09.500978 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.504962 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.505225 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m18:00:09.505614 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.507408 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.507613 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m18:00:09.508022 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.519194 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m18:00:09.519485 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.519670 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m18:00:09.520362 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.523394 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.523618 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."main"."dim_case_status__dbt_backup" cascade
[0m18:00:09.524024 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.524879 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 18:00:09.472187 => 18:00:09.524773
[0m18:00:09.525096 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m18:00:09.544059 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_case_status ......................... [[32mOK[0m in 0.08s]
[0m18:00:09.544450 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m18:00:09.545188 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:09.545376 [debug] [MainThread]: On master: BEGIN
[0m18:00:09.545532 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:00:09.550990 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:09.551206 [debug] [MainThread]: On master: COMMIT
[0m18:00:09.551365 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:09.551511 [debug] [MainThread]: On master: COMMIT
[0m18:00:09.551705 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:09.551860 [debug] [MainThread]: On master: Close
[0m18:00:09.553438 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:00:09.553636 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_case_status' was properly closed.
[0m18:00:09.553827 [info ] [MainThread]: 
[0m18:00:09.554043 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m18:00:09.554392 [debug] [MainThread]: Command end result
[0m18:00:09.562296 [info ] [MainThread]: 
[0m18:00:09.562573 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:00:09.562744 [info ] [MainThread]: 
[0m18:00:09.562925 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m18:00:09.563292 [debug] [MainThread]: Command `dbt run` succeeded at 18:00:09.563243 after 0.41 seconds
[0m18:00:09.563484 [debug] [MainThread]: Flushing usage events


============================== 18:00:31.372193 | 81787442-1f31-4f75-ab64-0f178f442873 ==============================
[0m18:00:31.372193 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:00:31.374994 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:00:31.375272 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:00:31.452009 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:00:31.472496 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:00:31.518217 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:00:31.518525 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:00:31.519502 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m18:00:31.533321 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:00:31.534703 [info ] [MainThread]: 
[0m18:00:31.535124 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:00:31.535670 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:00:31.542473 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:00:31.542817 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:00:31.543008 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:00:31.554907 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.555861 [debug] [ThreadPool]: On list_dbt: Close
[0m18:00:31.557808 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:00:31.558370 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:00:31.561233 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:31.561440 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:00:31.561599 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:31.567395 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.567832 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:31.568087 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:00:31.568450 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.569020 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:31.569205 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:31.569356 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:31.569575 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.569734 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:00:31.572656 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m18:00:31.576549 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:31.576907 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:00:31.577139 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:31.582962 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.583227 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:31.583405 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:00:31.598599 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.602141 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:00:31.603006 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:00:31.603247 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:00:31.605300 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m18:00:31.607610 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:31.607788 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:00:31.607936 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:31.613398 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.613647 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:31.613816 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:00:31.631329 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.632265 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:00:31.632508 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:00:31.632669 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:00:31.635562 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:31.635785 [debug] [MainThread]: On master: BEGIN
[0m18:00:31.635940 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:00:31.641000 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:31.641257 [debug] [MainThread]: On master: COMMIT
[0m18:00:31.641431 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:31.641592 [debug] [MainThread]: On master: COMMIT
[0m18:00:31.641799 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:31.641966 [debug] [MainThread]: On master: Close
[0m18:00:31.643447 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:00:31.643714 [info ] [MainThread]: 
[0m18:00:31.645251 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:00:31.645554 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:00:31.645934 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_contact)
[0m18:00:31.646149 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:00:31.651317 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:00:31.651849 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:00:31.646288 => 18:00:31.651738
[0m18:00:31.652050 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:00:31.672716 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:00:31.673348 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:00:31.673556 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:00:31.673738 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:00:31.679347 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:31.679608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:00:31.679812 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

select
    id as contact_id,
    account_id,
    first_name,
    last_name,
    email,
    phone,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__contact"
where is_deleted = false;
    );
  
  
[0m18:00:31.680276 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:00:31.652186 => 18:00:31.680182
[0m18:00:31.680476 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m18:00:31.684368 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m18:00:31.684653 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:00:31.686386 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m18:00:31.686825 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_contact ......................... [[31mERROR[0m in 0.04s]
[0m18:00:31.687193 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:00:31.687922 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:31.688114 [debug] [MainThread]: On master: BEGIN
[0m18:00:31.688266 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:00:31.693530 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:31.693891 [debug] [MainThread]: On master: COMMIT
[0m18:00:31.694055 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:31.694205 [debug] [MainThread]: On master: COMMIT
[0m18:00:31.694411 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:31.694571 [debug] [MainThread]: On master: Close
[0m18:00:31.695900 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:00:31.696063 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:00:31.696326 [info ] [MainThread]: 
[0m18:00:31.696698 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m18:00:31.697195 [debug] [MainThread]: Command end result
[0m18:00:31.704172 [info ] [MainThread]: 
[0m18:00:31.704473 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:00:31.704646 [info ] [MainThread]: 
[0m18:00:31.704814 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m18:00:31.705126 [info ] [MainThread]: 
[0m18:00:31.705383 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:00:31.705820 [debug] [MainThread]: Command `dbt run` failed at 18:00:31.705761 after 0.36 seconds
[0m18:00:31.706032 [debug] [MainThread]: Flushing usage events


============================== 18:00:41.861836 | 408deb3a-0be0-4678-8899-96411c2855e4 ==============================
[0m18:00:41.861836 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:00:41.864506 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:00:41.864765 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:00:41.941501 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:00:41.960168 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:00:42.002645 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:00:42.003142 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:00:42.030750 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m18:00:42.044847 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:00:42.046651 [info ] [MainThread]: 
[0m18:00:42.047195 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:00:42.047920 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:00:42.055188 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:00:42.055473 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:00:42.055661 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:00:42.067454 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.068489 [debug] [ThreadPool]: On list_dbt: Close
[0m18:00:42.070280 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:00:42.070728 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:00:42.073977 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:42.074210 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:00:42.074383 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:42.080283 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.080579 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:42.080765 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:00:42.081015 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.081521 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:42.081696 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:42.081842 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:42.082053 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.082219 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:00:42.084841 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m18:00:42.088132 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:42.088335 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:00:42.088506 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:42.093650 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.093898 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:42.094079 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:00:42.112471 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.113457 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:00:42.114206 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:00:42.114400 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:00:42.116373 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m18:00:42.118749 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:42.118932 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:00:42.119085 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:42.124534 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.124795 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:42.124976 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:00:42.139857 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.143229 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:00:42.143468 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:00:42.143628 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:00:42.146110 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:42.146305 [debug] [MainThread]: On master: BEGIN
[0m18:00:42.146460 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:00:42.151347 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:42.151591 [debug] [MainThread]: On master: COMMIT
[0m18:00:42.151756 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:42.151907 [debug] [MainThread]: On master: COMMIT
[0m18:00:42.152115 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:42.152274 [debug] [MainThread]: On master: Close
[0m18:00:42.153991 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:00:42.154286 [info ] [MainThread]: 
[0m18:00:42.155727 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:00:42.156065 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:00:42.157407 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_contact)
[0m18:00:42.157712 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:00:42.163004 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:00:42.163665 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:00:42.157859 => 18:00:42.163553
[0m18:00:42.163866 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:00:42.183073 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:00:42.183790 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:00:42.184064 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:00:42.184249 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:00:42.189724 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:42.190001 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:00:42.190233 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

select
    id as contact_id,
    account_id,
    first_name,
    last_name,
    email,
    phone,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__contact"
where is_deleted = false
    );
  
  
[0m18:00:42.191886 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:00:42.164001 => 18:00:42.191748
[0m18:00:42.192151 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m18:00:42.195844 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m18:00:42.196117 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:00:42.197884 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "is_deleted" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.isdeleted"
  LINE 23: where is_deleted = false
                 ^
[0m18:00:42.198297 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_contact ......................... [[31mERROR[0m in 0.04s]
[0m18:00:42.198628 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:00:42.199452 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:42.199682 [debug] [MainThread]: On master: BEGIN
[0m18:00:42.199842 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:00:42.205255 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:42.205447 [debug] [MainThread]: On master: COMMIT
[0m18:00:42.205599 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:42.205743 [debug] [MainThread]: On master: COMMIT
[0m18:00:42.205951 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:42.206100 [debug] [MainThread]: On master: Close
[0m18:00:42.207501 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:00:42.207743 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:00:42.207927 [info ] [MainThread]: 
[0m18:00:42.208127 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m18:00:42.208502 [debug] [MainThread]: Command end result
[0m18:00:42.215962 [info ] [MainThread]: 
[0m18:00:42.216253 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:00:42.216587 [info ] [MainThread]: 
[0m18:00:42.216780 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "is_deleted" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.isdeleted"
  LINE 23: where is_deleted = false
                 ^
[0m18:00:42.216948 [info ] [MainThread]: 
[0m18:00:42.217133 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:00:42.217508 [debug] [MainThread]: Command `dbt run` failed at 18:00:42.217457 after 0.38 seconds
[0m18:00:42.217719 [debug] [MainThread]: Flushing usage events


============================== 18:01:31.805138 | 9f4dc736-c017-47a6-b2f3-40a0f61750a3 ==============================
[0m18:01:31.805138 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:01:31.808225 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:01:31.808482 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:01:31.892691 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:01:31.911309 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:01:31.954952 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:01:31.955477 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:01:31.984147 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m18:01:31.999760 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:01:32.001299 [info ] [MainThread]: 
[0m18:01:32.001808 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:01:32.002534 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:01:32.009541 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:01:32.009797 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:01:32.010138 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:01:32.021555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.022468 [debug] [ThreadPool]: On list_dbt: Close
[0m18:01:32.024441 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:01:32.024945 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:01:32.027896 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:32.028109 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:01:32.028280 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:32.034164 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.034497 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:32.034679 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:01:32.034991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.035534 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:01:32.035713 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:32.035856 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:01:32.036078 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.036241 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:01:32.039179 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m18:01:32.042833 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:01:32.043090 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:01:32.043257 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:32.114393 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.114708 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:01:32.114894 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:01:32.137078 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.138387 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:01:32.139229 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:01:32.139426 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:01:32.141888 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m18:01:32.144542 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:01:32.144732 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:01:32.144912 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:32.150774 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.151014 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:01:32.151193 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:01:32.166908 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.170675 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:01:32.170989 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:01:32.171163 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:01:32.175587 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:32.175830 [debug] [MainThread]: On master: BEGIN
[0m18:01:32.176002 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:01:32.183346 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:32.183661 [debug] [MainThread]: On master: COMMIT
[0m18:01:32.183839 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:32.184005 [debug] [MainThread]: On master: COMMIT
[0m18:01:32.184215 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:32.184387 [debug] [MainThread]: On master: Close
[0m18:01:32.187631 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:01:32.187956 [info ] [MainThread]: 
[0m18:01:32.189691 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:01:32.190021 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:01:32.191263 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_contact)
[0m18:01:32.191466 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:01:32.197139 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:01:32.198034 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:01:32.191607 => 18:01:32.197795
[0m18:01:32.198350 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:01:32.218059 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:01:32.218769 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:01:32.218988 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:01:32.219172 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:01:32.225093 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:01:32.225350 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:01:32.225553 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

select
    contact_id,
    account_id,
    first_name,
    last_name,
    email,
    phone,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__contact"
where is_deleted = false
    );
  
  
[0m18:01:32.226923 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:01:32.198506 => 18:01:32.226826
[0m18:01:32.227131 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m18:01:32.231620 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m18:01:32.231925 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:01:32.233755 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "is_deleted" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.isdeleted"
  LINE 23: where is_deleted = false
                 ^
[0m18:01:32.234193 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_contact ......................... [[31mERROR[0m in 0.04s]
[0m18:01:32.234534 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:01:32.235333 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:32.235576 [debug] [MainThread]: On master: BEGIN
[0m18:01:32.235745 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:01:32.240936 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:32.241213 [debug] [MainThread]: On master: COMMIT
[0m18:01:32.241373 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:32.241524 [debug] [MainThread]: On master: COMMIT
[0m18:01:32.241748 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:32.241908 [debug] [MainThread]: On master: Close
[0m18:01:32.243399 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:01:32.243664 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:01:32.243862 [info ] [MainThread]: 
[0m18:01:32.244066 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m18:01:32.244388 [debug] [MainThread]: Command end result
[0m18:01:32.251884 [info ] [MainThread]: 
[0m18:01:32.252165 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:01:32.252564 [info ] [MainThread]: 
[0m18:01:32.252782 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "is_deleted" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.isdeleted"
  LINE 23: where is_deleted = false
                 ^
[0m18:01:32.252975 [info ] [MainThread]: 
[0m18:01:32.253176 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:01:32.253598 [debug] [MainThread]: Command `dbt run` failed at 18:01:32.253542 after 0.47 seconds
[0m18:01:32.253812 [debug] [MainThread]: Flushing usage events


============================== 18:01:39.621627 | 126b03bd-1b4b-42ca-a0a6-edcdd983223c ==============================
[0m18:01:39.621627 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:01:39.625232 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'send_anonymous_usage_stats': 'False'}
[0m18:01:39.625624 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:01:39.703004 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:01:39.721435 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:01:39.765566 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:01:39.766111 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:01:39.798224 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m18:01:39.813456 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:01:39.815122 [info ] [MainThread]: 
[0m18:01:39.815612 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:01:39.816220 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:01:39.823325 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:01:39.823622 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:01:39.823799 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:01:39.831064 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.831988 [debug] [ThreadPool]: On list_dbt: Close
[0m18:01:39.833968 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:01:39.834464 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:01:39.837569 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:39.837793 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:01:39.837954 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:39.843248 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.843487 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:39.843648 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:01:39.844016 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.844605 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:01:39.844795 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:39.844955 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:01:39.845191 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.845384 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:01:39.848299 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m18:01:39.851604 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:01:39.851811 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:01:39.851968 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:39.858000 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.858256 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:01:39.858429 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:01:39.873931 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.877601 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:01:39.879062 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:01:39.879317 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:01:39.881543 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m18:01:39.883801 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:01:39.884007 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:01:39.884154 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:39.889333 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.889601 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:01:39.889776 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:01:39.907709 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.908707 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:01:39.908953 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:01:39.909107 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:01:39.912085 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:39.912268 [debug] [MainThread]: On master: BEGIN
[0m18:01:39.912426 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:01:39.917922 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:39.918170 [debug] [MainThread]: On master: COMMIT
[0m18:01:39.918334 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:39.918489 [debug] [MainThread]: On master: COMMIT
[0m18:01:39.918686 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:39.918847 [debug] [MainThread]: On master: Close
[0m18:01:39.920178 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:01:39.920389 [info ] [MainThread]: 
[0m18:01:39.921867 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:01:39.922917 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:01:39.923305 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_contact)
[0m18:01:39.923514 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:01:39.929063 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:01:39.929823 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:01:39.923661 => 18:01:39.929667
[0m18:01:39.930097 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:01:39.977128 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:01:39.977795 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:01:39.978001 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:01:39.978182 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:01:39.983530 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:01:39.983819 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:01:39.984035 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

select
    contact_id,
    account_id,
    first_name,
    last_name,
    email,
    phone,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__contact"
    );
  
  
[0m18:01:39.985482 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:01:39.930233 => 18:01:39.985378
[0m18:01:39.985692 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m18:01:39.988761 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m18:01:39.988969 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:01:39.990785 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "account_id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.accountid"
  LINE 15:     account_id,
               ^
[0m18:01:39.991189 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_contact ......................... [[31mERROR[0m in 0.07s]
[0m18:01:39.991549 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:01:39.992291 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:39.992468 [debug] [MainThread]: On master: BEGIN
[0m18:01:39.992614 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:01:39.997850 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:39.998321 [debug] [MainThread]: On master: COMMIT
[0m18:01:39.998508 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:39.998663 [debug] [MainThread]: On master: COMMIT
[0m18:01:39.998920 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:39.999081 [debug] [MainThread]: On master: Close
[0m18:01:40.000456 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:01:40.000653 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:01:40.001044 [info ] [MainThread]: 
[0m18:01:40.001267 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m18:01:40.001622 [debug] [MainThread]: Command end result
[0m18:01:40.009032 [info ] [MainThread]: 
[0m18:01:40.009296 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:01:40.009618 [info ] [MainThread]: 
[0m18:01:40.009829 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "account_id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.accountid"
  LINE 15:     account_id,
               ^
[0m18:01:40.010014 [info ] [MainThread]: 
[0m18:01:40.010243 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:01:40.010676 [debug] [MainThread]: Command `dbt run` failed at 18:01:40.010617 after 0.41 seconds
[0m18:01:40.010933 [debug] [MainThread]: Flushing usage events


============================== 18:04:54.525179 | 61b8da07-6663-4f95-a2e2-6fc202f52b5e ==============================
[0m18:04:54.525179 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:04:54.528288 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m18:04:54.528534 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:04:54.611547 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:04:54.630380 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:04:54.676061 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:04:54.676547 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:04:54.695149 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_contact (models/dimensions/dim_contact.sql)
  unexpected '}', expected ')'
    line 70
      FROM {{ ref(stg_salesforce__contact }}
[0m18:04:54.695682 [debug] [MainThread]: Command `dbt run` failed at 18:04:54.695609 after 0.20 seconds
[0m18:04:54.695932 [debug] [MainThread]: Flushing usage events


============================== 18:05:05.345055 | f865b42f-8994-4cdf-8e36-35cc0186bbfe ==============================
[0m18:05:05.345055 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:05:05.347664 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:05:05.347934 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:05:05.426775 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:05:05.445468 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:05:05.489681 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:05:05.490232 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:05:05.508988 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_contact (models/dimensions/dim_contact.sql)
  unexpected '}', expected ')'
    line 70
      FROM {{ ref(stg_salesforce__contact }}
[0m18:05:05.509524 [debug] [MainThread]: Command `dbt run` failed at 18:05:05.509453 after 0.19 seconds
[0m18:05:05.509753 [debug] [MainThread]: Flushing usage events


============================== 18:05:22.861219 | 2e447f76-37fc-41e3-8b4c-1bd54e4dc46c ==============================
[0m18:05:22.861219 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:05:22.864055 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:05:22.864312 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:05:22.940961 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:05:22.959238 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:05:23.002238 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:05:23.002831 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:05:23.032801 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m18:05:23.047383 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:05:23.048981 [info ] [MainThread]: 
[0m18:05:23.049459 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:05:23.050092 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:05:23.057509 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:05:23.057795 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:05:23.057976 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:05:23.069335 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.070209 [debug] [ThreadPool]: On list_dbt: Close
[0m18:05:23.072080 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:05:23.072582 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:05:23.075515 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:23.075714 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:05:23.075874 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:23.081453 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.081712 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:23.081947 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:05:23.082338 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.082877 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:05:23.083054 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:23.083208 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:05:23.083440 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.083596 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:05:23.086405 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m18:05:23.089774 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:05:23.090016 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:05:23.090174 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:23.095761 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.096025 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:05:23.096193 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:05:23.114343 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.115398 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:05:23.116075 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:05:23.116269 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:05:23.118246 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m18:05:23.120635 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:05:23.120838 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:05:23.121054 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:23.126594 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.126850 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:05:23.127035 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:05:23.142287 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.145650 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:05:23.145882 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:05:23.146040 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:05:23.148539 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:23.148712 [debug] [MainThread]: On master: BEGIN
[0m18:05:23.148859 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:05:23.153907 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:23.154145 [debug] [MainThread]: On master: COMMIT
[0m18:05:23.154303 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:23.154457 [debug] [MainThread]: On master: COMMIT
[0m18:05:23.154661 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:23.154825 [debug] [MainThread]: On master: Close
[0m18:05:23.156180 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:05:23.156385 [info ] [MainThread]: 
[0m18:05:23.157838 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:05:23.158127 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:05:23.159281 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_contact)
[0m18:05:23.159471 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:05:23.165176 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:05:23.166258 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:05:23.159608 => 18:05:23.166141
[0m18:05:23.166471 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:05:23.185782 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:05:23.186826 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:23.187133 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:05:23.187347 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:05:23.199316 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:23.199676 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:23.199936 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    id AS contact_id,
    masterrecordid AS master_record_id,
    accountid AS account_id,
    reportstoid AS reports_to_id,
    ownerid AS owner_id,
    jigsawcontactid AS jigsaw_contact_id,
    individualid AS individual_id,

    /* Dates */
    birthdate AS birth_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    lastcurequestdate AS last_cu_request_date,
    lastcuupdatedate AS last_cu_update_date,
    emailbounceddate AS email_bounced_date,

    /* Dimensions */
    salutation,
    firstname AS first_name,
    lastname AS last_name,
    otherstreet AS other_street,
    othercity AS other_city,
    otherstate AS other_state,
    otherpostalcode AS other_postal_code,
    othercountry AS other_country,
    otherlatitude AS other_latitude,
    otherlongitude AS other_longitude,
    othergeocodeaccuracy AS other_geocode_accuracy,
    mailingstreet AS mailing_street,
    mailingcity AS mailing_city,
    mailingstate AS mailing_state,
    mailingpostalcode AS mailing_postal_code,
    mailingcountry AS mailing_country,
    mailinglatitude AS mailing_latitude,
    mailinglongitude AS mailing_longitude,
    mailinggeocodeaccuracy AS mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone AS mobile_phone,
    homephone AS home_phone,
    otherphone AS other_phone,
    assistantphone AS assistant_phone,
    email,
    title,
    department,
    assistantname AS assistant_name,
    leadsource AS lead_source,
    description,
    pronouns,
    genderidentity AS gender_identity,
    cleanstatus AS clean_status,
    emailbouncedreason AS email_bounced_reason,
    level__c AS level,
    languages__c AS languages,

    /* Metrics */
    hasoptedoutofemail AS has_opted_out_of_email,
    hasoptedoutoffax AS has_opted_out_of_fax,
    donotcall AS do_not_call

FROM "dbt"."staging"."stg_salesforce__contact"
WHERE isdeleted = FALSE
    );
  
  
[0m18:05:23.201264 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:05:23.166607 => 18:05:23.201155
[0m18:05:23.201491 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m18:05:23.205868 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m18:05:23.206176 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:05:23.207950 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.fax", "stg_salesforce__contact.email", "stg_salesforce__contact.title", "stg_salesforce__contact.ownerid"
  LINE 15:     id AS contact_id,
               ^
[0m18:05:23.208441 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_contact ......................... [[31mERROR[0m in 0.05s]
[0m18:05:23.208823 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:05:23.209611 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:23.209792 [debug] [MainThread]: On master: BEGIN
[0m18:05:23.209945 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:05:23.215902 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:23.216157 [debug] [MainThread]: On master: COMMIT
[0m18:05:23.216328 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:23.216485 [debug] [MainThread]: On master: COMMIT
[0m18:05:23.216686 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:23.216845 [debug] [MainThread]: On master: Close
[0m18:05:23.219652 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:05:23.220282 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:05:23.221622 [info ] [MainThread]: 
[0m18:05:23.223226 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m18:05:23.226759 [debug] [MainThread]: Command end result
[0m18:05:23.263819 [info ] [MainThread]: 
[0m18:05:23.264169 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:05:23.264433 [info ] [MainThread]: 
[0m18:05:23.264747 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.fax", "stg_salesforce__contact.email", "stg_salesforce__contact.title", "stg_salesforce__contact.ownerid"
  LINE 15:     id AS contact_id,
               ^
[0m18:05:23.264992 [info ] [MainThread]: 
[0m18:05:23.265254 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:05:23.265793 [debug] [MainThread]: Command `dbt run` failed at 18:05:23.265724 after 0.43 seconds
[0m18:05:23.266599 [debug] [MainThread]: Flushing usage events


============================== 18:05:34.379688 | 956f9ec9-0795-4f23-a2af-22f6ae7604bc ==============================
[0m18:05:34.379688 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:05:34.382739 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m18:05:34.382986 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:05:34.472159 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:05:34.491283 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:05:34.537695 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:05:34.538191 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:05:34.566065 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m18:05:34.580590 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:05:34.582211 [info ] [MainThread]: 
[0m18:05:34.582704 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:05:34.583470 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:05:34.590360 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:05:34.590639 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:05:34.590825 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:05:34.602301 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.603185 [debug] [ThreadPool]: On list_dbt: Close
[0m18:05:34.605048 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:05:34.605577 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:05:34.608642 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:34.608854 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:05:34.609020 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:34.614697 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.614968 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:34.615139 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:05:34.615401 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.615924 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:05:34.616090 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:34.616258 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:05:34.616472 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.616635 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:05:34.619484 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m18:05:34.622876 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:05:34.623084 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:05:34.623243 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:34.628609 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.628847 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:05:34.629021 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:05:34.646843 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.647827 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:05:34.648430 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:05:34.648601 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:05:34.650586 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m18:05:34.652966 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:05:34.653141 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:05:34.653291 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:34.658496 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.658737 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:05:34.658911 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:05:34.674593 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.678012 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:05:34.678246 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:05:34.678410 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:05:34.683387 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:34.683601 [debug] [MainThread]: On master: BEGIN
[0m18:05:34.683764 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:05:34.691260 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:34.691550 [debug] [MainThread]: On master: COMMIT
[0m18:05:34.691727 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:34.691888 [debug] [MainThread]: On master: COMMIT
[0m18:05:34.692103 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:34.692266 [debug] [MainThread]: On master: Close
[0m18:05:34.693830 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:05:34.694091 [info ] [MainThread]: 
[0m18:05:34.695436 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:05:34.695736 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:05:34.696913 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_contact)
[0m18:05:34.697123 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:05:34.702925 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.703502 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:05:34.697267 => 18:05:34.703383
[0m18:05:34.703719 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:05:34.724761 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.726832 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.727096 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:05:34.727295 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:05:34.733399 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.733740 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.734020 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    contact_id,
    masterrecordid AS master_record_id,
    accountid AS account_id,
    reportstoid AS reports_to_id,
    ownerid AS owner_id,
    jigsawcontactid AS jigsaw_contact_id,
    individualid AS individual_id,

    /* Dates */
    birthdate AS birth_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    lastcurequestdate AS last_cu_request_date,
    lastcuupdatedate AS last_cu_update_date,
    emailbounceddate AS email_bounced_date,

    /* Dimensions */
    salutation,
    firstname AS first_name,
    lastname AS last_name,
    otherstreet AS other_street,
    othercity AS other_city,
    otherstate AS other_state,
    otherpostalcode AS other_postal_code,
    othercountry AS other_country,
    otherlatitude AS other_latitude,
    otherlongitude AS other_longitude,
    othergeocodeaccuracy AS other_geocode_accuracy,
    mailingstreet AS mailing_street,
    mailingcity AS mailing_city,
    mailingstate AS mailing_state,
    mailingpostalcode AS mailing_postal_code,
    mailingcountry AS mailing_country,
    mailinglatitude AS mailing_latitude,
    mailinglongitude AS mailing_longitude,
    mailinggeocodeaccuracy AS mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone AS mobile_phone,
    homephone AS home_phone,
    otherphone AS other_phone,
    assistantphone AS assistant_phone,
    email,
    title,
    department,
    assistantname AS assistant_name,
    leadsource AS lead_source,
    description,
    pronouns,
    genderidentity AS gender_identity,
    cleanstatus AS clean_status,
    emailbouncedreason AS email_bounced_reason,
    level__c AS level,
    languages__c AS languages,

    /* Metrics */
    hasoptedoutofemail AS has_opted_out_of_email,
    hasoptedoutoffax AS has_opted_out_of_fax,
    donotcall AS do_not_call

FROM "dbt"."staging"."stg_salesforce__contact"
WHERE isdeleted = FALSE
    );
  
  
[0m18:05:34.739220 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.743532 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.743845 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."main"."dim_contact" rename to "dim_contact__dbt_backup"
[0m18:05:34.744391 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.746573 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.746835 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."main"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m18:05:34.747301 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.759525 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m18:05:34.759893 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.760100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m18:05:34.761698 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.789963 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.790423 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."main"."dim_contact__dbt_backup" cascade
[0m18:05:34.791052 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.791936 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:05:34.703863 => 18:05:34.791836
[0m18:05:34.792149 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:05:34.817210 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_contact ............................. [[32mOK[0m in 0.12s]
[0m18:05:34.817665 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:05:34.818408 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:34.818585 [debug] [MainThread]: On master: BEGIN
[0m18:05:34.818735 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:05:34.824340 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:34.824563 [debug] [MainThread]: On master: COMMIT
[0m18:05:34.824723 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:34.824874 [debug] [MainThread]: On master: COMMIT
[0m18:05:34.825088 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:34.825246 [debug] [MainThread]: On master: Close
[0m18:05:34.826624 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:05:34.826835 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:05:34.827048 [info ] [MainThread]: 
[0m18:05:34.827246 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m18:05:34.827619 [debug] [MainThread]: Command end result
[0m18:05:34.834316 [info ] [MainThread]: 
[0m18:05:34.834580 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:05:34.834745 [info ] [MainThread]: 
[0m18:05:34.834922 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m18:05:34.835244 [debug] [MainThread]: Command `dbt run` succeeded at 18:05:34.835196 after 0.48 seconds
[0m18:05:34.835441 [debug] [MainThread]: Flushing usage events


============================== 19:21:28.476143 | 4146744c-3e5a-4ca9-b438-9f4d8a139559 ==============================
[0m19:21:28.476143 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:21:28.479536 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m19:21:28.479791 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:21:28.648782 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:21:28.669676 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:21:28.723702 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 5 files changed.
[0m19:21:28.724108 [debug] [MainThread]: Partial parsing: deleted file: elastic_dbt_interview://models/dimensions/dim_record_type.sql
[0m19:21:28.724375 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_campaign.sql
[0m19:21:28.724599 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:21:28.724812 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_solution.sql
[0m19:21:28.725014 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_pricebook.sql
[0m19:21:28.725215 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_product.sql
[0m19:21:28.762632 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m19:21:28.778296 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:21:28.780070 [info ] [MainThread]: 
[0m19:21:28.780495 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:21:28.781868 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:21:28.789015 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:21:28.789296 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:21:28.789478 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:21:28.807103 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.808451 [debug] [ThreadPool]: On list_dbt: Close
[0m19:21:28.812094 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:21:28.812340 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:21:28.812545 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:21:28.818831 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.819530 [debug] [ThreadPool]: On list_dbt: Close
[0m19:21:28.821375 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:21:28.821838 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:21:28.824682 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:21:28.824913 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:21:28.825066 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:21:28.830662 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.830922 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:21:28.831080 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:21:28.832062 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.832632 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:21:28.832895 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:21:28.833057 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:21:28.833287 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.833445 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:21:28.835303 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now create_dbt_staging)
[0m19:21:28.835828 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m19:21:28.837940 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m19:21:28.838135 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m19:21:28.838290 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:21:28.843758 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.844029 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m19:21:28.844208 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m19:21:28.844441 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.844926 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m19:21:28.845089 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m19:21:28.845242 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m19:21:28.845450 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.845700 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m19:21:28.848687 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_main)
[0m19:21:28.852163 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:21:28.852384 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:21:28.852551 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:21:28.858174 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.858446 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:21:28.858628 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:21:28.877208 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.881213 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:21:28.881860 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:21:28.882051 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:21:28.884288 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m19:21:28.886498 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:21:28.886674 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:21:28.886822 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:21:28.892161 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.892412 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:21:28.892587 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:21:28.910766 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.911683 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:21:28.911912 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:21:28.912068 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:21:28.915428 [debug] [MainThread]: Using duckdb connection "master"
[0m19:21:28.915702 [debug] [MainThread]: On master: BEGIN
[0m19:21:28.915875 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:21:28.921392 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:21:28.921658 [debug] [MainThread]: On master: COMMIT
[0m19:21:28.921828 [debug] [MainThread]: Using duckdb connection "master"
[0m19:21:28.921995 [debug] [MainThread]: On master: COMMIT
[0m19:21:28.922198 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:21:28.922359 [debug] [MainThread]: On master: Close
[0m19:21:28.923796 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:21:28.924033 [info ] [MainThread]: 
[0m19:21:28.925794 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m19:21:28.926181 [info ] [Thread-1  ]: 1 of 33 START sql table model main.dim_date .................................... [RUN]
[0m19:21:28.926731 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_date)
[0m19:21:28.926940 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m19:21:28.933127 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 19:21:28.927078 => 19:21:28.932867
[0m19:21:28.936765 [debug] [Thread-1  ]: Compilation Error in model dim_date (models/dimensions/dim_date.sql)
  'dict object' has no attribute 'generate_date_spine'. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m19:21:28.937191 [error] [Thread-1  ]: 1 of 33 ERROR creating sql table model main.dim_date ........................... [[31mERROR[0m in 0.01s]
[0m19:21:28.937490 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m19:21:28.937706 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m19:21:28.938099 [info ] [Thread-1  ]: 2 of 33 START sql table model main.fact_campaign_performance ................... [RUN]
[0m19:21:28.938583 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.fact_campaign_performance)
[0m19:21:28.938804 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m19:21:28.941163 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m19:21:28.942331 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 19:21:28.938998 => 19:21:28.942160
[0m19:21:28.942603 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m19:21:28.990743 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m19:21:28.991672 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m19:21:28.991967 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: BEGIN
[0m19:21:28.992163 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:28.997818 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:28.998108 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m19:21:28.998327 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */

  
    
    

    create  table
      "dbt"."main"."fact_campaign_performance__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."campaign"  -- Hypothetical source for campaign performance metrics
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS campaign_performance_id,  -- Surrogate Key
    campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
    leads_generated,
    opportunities_created,
    revenue_generated,
    expenses,
    createddate AS performance_created_at
FROM source
    );
  
  
[0m19:21:29.002888 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 19:21:28.942754 => 19:21:29.002715
[0m19:21:29.003200 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: ROLLBACK
[0m19:21:29.006046 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_campaign_performance'
[0m19:21:29.006273 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: Close
[0m19:21:29.008359 [debug] [Thread-1  ]: Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m19:21:29.008800 [error] [Thread-1  ]: 2 of 33 ERROR creating sql table model main.fact_campaign_performance .......... [[31mERROR[0m in 0.07s]
[0m19:21:29.009161 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m19:21:29.009412 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m19:21:29.009726 [info ] [Thread-1  ]: 3 of 33 START sql table model main.fact_product_sales .......................... [RUN]
[0m19:21:29.010142 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_product_sales)
[0m19:21:29.010357 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m19:21:29.012841 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m19:21:29.013721 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 19:21:29.010497 => 19:21:29.013576
[0m19:21:29.013987 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m19:21:29.016734 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_product_sales"
[0m19:21:29.017182 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m19:21:29.017371 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: BEGIN
[0m19:21:29.017549 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.023195 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.023496 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m19:21:29.023715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_product_sales"} */

  
    
    

    create  table
      "dbt"."main"."fact_product_sales__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."pricebook_entry"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS product_sales_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
    product2id AS product_fk,                                      -- Foreign Key to dim_product
    pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
    unitprice AS unit_price,
    isactive AS is_active,
    createddate AS product_sales_created_at,
    lastmodifieddate AS product_sales_last_modified_date
FROM source
    );
  
  
[0m19:21:29.024367 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 19:21:29.014125 => 19:21:29.024265
[0m19:21:29.024572 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: ROLLBACK
[0m19:21:29.025207 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_product_sales'
[0m19:21:29.025498 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: Close
[0m19:21:29.027692 [debug] [Thread-1  ]: Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m19:21:29.028101 [error] [Thread-1  ]: 3 of 33 ERROR creating sql table model main.fact_product_sales ................. [[31mERROR[0m in 0.02s]
[0m19:21:29.028424 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m19:21:29.028646 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m19:21:29.029016 [info ] [Thread-1  ]: 4 of 33 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m19:21:29.029465 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m19:21:29.029679 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m19:21:29.031755 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.032434 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 19:21:29.029819 => 19:21:29.032330
[0m19:21:29.032639 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m19:21:29.043376 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.044236 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.044444 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m19:21:29.044649 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.049950 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.050214 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.050449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m19:21:29.051275 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.056154 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.056465 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m19:21:29.057174 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.059231 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.059445 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m19:21:29.059833 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.069256 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m19:21:29.069498 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.069689 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m19:21:29.070581 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.073656 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.073910 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m19:21:29.074299 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.075060 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 19:21:29.032768 => 19:21:29.074964
[0m19:21:29.075265 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m19:21:29.094516 [info ] [Thread-1  ]: 4 of 33 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.07s]
[0m19:21:29.094935 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m19:21:29.095190 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m19:21:29.095507 [info ] [Thread-1  ]: 5 of 33 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m19:21:29.095904 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m19:21:29.096116 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m19:21:29.098222 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.098706 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 19:21:29.096252 => 19:21:29.098607
[0m19:21:29.098908 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m19:21:29.101689 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.102169 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.102400 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m19:21:29.102590 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.108382 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.108697 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.108936 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m19:21:29.109835 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.112759 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.113019 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m19:21:29.113359 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.115123 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.115337 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m19:21:29.115628 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.116738 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m19:21:29.116978 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.117173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m19:21:29.117894 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.119734 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.119968 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m19:21:29.120378 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.121173 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 19:21:29.099036 => 19:21:29.121072
[0m19:21:29.121383 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m19:21:29.130614 [info ] [Thread-1  ]: 5 of 33 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.03s]
[0m19:21:29.131004 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m19:21:29.131230 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m19:21:29.131652 [info ] [Thread-1  ]: 6 of 33 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m19:21:29.132129 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m19:21:29.132381 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m19:21:29.134528 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.135076 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 19:21:29.132517 => 19:21:29.134968
[0m19:21:29.135273 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m19:21:29.137980 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.138454 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.138665 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m19:21:29.138853 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.144724 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.145052 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.145287 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m19:21:29.146029 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.148500 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.148779 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m19:21:29.149115 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.151959 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.152277 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m19:21:29.152680 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.153681 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m19:21:29.153893 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.154070 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m19:21:29.154783 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.156750 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.157041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m19:21:29.157431 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.158324 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 19:21:29.135401 => 19:21:29.158214
[0m19:21:29.158559 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m19:21:29.166716 [info ] [Thread-1  ]: 6 of 33 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.03s]
[0m19:21:29.167180 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m19:21:29.167435 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m19:21:29.167727 [info ] [Thread-1  ]: 7 of 33 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m19:21:29.168098 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m19:21:29.168297 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m19:21:29.170299 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.170993 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 19:21:29.168429 => 19:21:29.170895
[0m19:21:29.171191 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m19:21:29.173868 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.174313 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.174512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m19:21:29.174690 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.180152 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.180441 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.180650 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m19:21:29.181085 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.183528 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.183855 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m19:21:29.184299 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.186945 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.187208 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m19:21:29.187564 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.188423 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m19:21:29.188622 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.188801 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m19:21:29.189449 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.191100 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.191304 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m19:21:29.191715 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.192680 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 19:21:29.171321 => 19:21:29.192573
[0m19:21:29.192950 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m19:21:29.202060 [info ] [Thread-1  ]: 7 of 33 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.03s]
[0m19:21:29.202467 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m19:21:29.202716 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m19:21:29.203041 [info ] [Thread-1  ]: 8 of 33 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m19:21:29.203408 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m19:21:29.203603 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m19:21:29.205735 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.206295 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 19:21:29.203740 => 19:21:29.206189
[0m19:21:29.206521 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m19:21:29.209427 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.209960 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.210302 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m19:21:29.210495 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.216063 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.216310 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.216545 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m19:21:29.217324 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.219467 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.219688 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m19:21:29.220031 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.221749 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.221946 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m19:21:29.222230 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.223068 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m19:21:29.223258 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.223429 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m19:21:29.224131 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.226862 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.227094 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m19:21:29.227542 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.228379 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 19:21:29.206656 => 19:21:29.228278
[0m19:21:29.228594 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m19:21:29.236552 [info ] [Thread-1  ]: 8 of 33 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.03s]
[0m19:21:29.236938 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m19:21:29.237164 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m19:21:29.237561 [info ] [Thread-1  ]: 9 of 33 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m19:21:29.238038 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m19:21:29.238255 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m19:21:29.240400 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.240896 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 19:21:29.238395 => 19:21:29.240789
[0m19:21:29.241098 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m19:21:29.244471 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.244975 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.245173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m19:21:29.245351 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.250909 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.251172 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.251406 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m19:21:29.252190 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.254346 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.254559 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m19:21:29.254858 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.256578 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.256789 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m19:21:29.257068 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.258090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m19:21:29.258382 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.258570 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m19:21:29.259248 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.261719 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.261934 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m19:21:29.262336 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.263149 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 19:21:29.241228 => 19:21:29.263054
[0m19:21:29.263356 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m19:21:29.272043 [info ] [Thread-1  ]: 9 of 33 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.03s]
[0m19:21:29.272462 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m19:21:29.272717 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m19:21:29.273057 [info ] [Thread-1  ]: 10 of 33 START sql view model staging.stg_salesforce__opportunity .............. [RUN]
[0m19:21:29.273432 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m19:21:29.273628 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m19:21:29.275672 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.276164 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 19:21:29.273762 => 19:21:29.276063
[0m19:21:29.276363 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m19:21:29.278951 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.279406 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.279598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m19:21:29.279771 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.285254 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.285565 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.285806 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m19:21:29.286554 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.288872 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.289211 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m19:21:29.289592 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.291581 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.291795 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m19:21:29.292106 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.292980 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m19:21:29.293244 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.293426 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m19:21:29.294274 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.296113 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.296353 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m19:21:29.296751 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.297665 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 19:21:29.276502 => 19:21:29.297563
[0m19:21:29.297883 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m19:21:29.307064 [info ] [Thread-1  ]: 10 of 33 OK created sql view model staging.stg_salesforce__opportunity ......... [[32mOK[0m in 0.03s]
[0m19:21:29.307490 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m19:21:29.307734 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m19:21:29.308074 [info ] [Thread-1  ]: 11 of 33 START sql view model staging.stg_salesforce__opportunity_history ...... [RUN]
[0m19:21:29.308468 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m19:21:29.308702 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m19:21:29.311450 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.312194 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 19:21:29.308838 => 19:21:29.312084
[0m19:21:29.312390 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m19:21:29.315521 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.316075 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.316342 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m19:21:29.316589 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.322273 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.322559 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.322772 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m19:21:29.323323 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.325706 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.325984 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m19:21:29.326360 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.328316 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.328527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m19:21:29.328827 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.329705 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m19:21:29.329899 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.330096 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m19:21:29.330671 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.332519 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.332726 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m19:21:29.333122 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.333958 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 19:21:29.312522 => 19:21:29.333856
[0m19:21:29.334176 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m19:21:29.342892 [info ] [Thread-1  ]: 11 of 33 OK created sql view model staging.stg_salesforce__opportunity_history . [[32mOK[0m in 0.03s]
[0m19:21:29.343319 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m19:21:29.343567 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m19:21:29.343863 [info ] [Thread-1  ]: 12 of 33 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m19:21:29.344258 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m19:21:29.344466 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m19:21:29.346476 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.346964 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 19:21:29.344602 => 19:21:29.346868
[0m19:21:29.347156 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m19:21:29.350563 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.350971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.351175 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m19:21:29.351399 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.356793 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.357111 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.357337 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m19:21:29.357840 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.360234 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.360472 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m19:21:29.360810 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.362582 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.362789 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m19:21:29.363106 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.364022 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m19:21:29.364222 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.364400 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m19:21:29.364980 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.366594 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.366804 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m19:21:29.367170 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.367984 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 19:21:29.347281 => 19:21:29.367882
[0m19:21:29.368231 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m19:21:29.376214 [info ] [Thread-1  ]: 12 of 33 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.03s]
[0m19:21:29.376626 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m19:21:29.376877 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m19:21:29.377198 [info ] [Thread-1  ]: 13 of 33 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m19:21:29.377567 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m19:21:29.377845 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m19:21:29.379907 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.380377 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 19:21:29.377997 => 19:21:29.380281
[0m19:21:29.380569 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m19:21:29.383734 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.384153 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.384344 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m19:21:29.384523 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.389902 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.390193 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.390425 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m19:21:29.391112 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.393241 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.393467 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m19:21:29.393769 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.395451 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.395658 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m19:21:29.395940 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.396794 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m19:21:29.396983 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.397157 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m19:21:29.397883 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.399723 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.399943 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m19:21:29.400340 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.401329 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 19:21:29.380696 => 19:21:29.401224
[0m19:21:29.401598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m19:21:29.412218 [info ] [Thread-1  ]: 13 of 33 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.03s]
[0m19:21:29.412608 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m19:21:29.412836 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m19:21:29.413295 [info ] [Thread-1  ]: 14 of 33 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m19:21:29.413801 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m19:21:29.414043 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m19:21:29.416158 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.416657 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 19:21:29.414184 => 19:21:29.416557
[0m19:21:29.416869 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m19:21:29.419955 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.420428 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.420626 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m19:21:29.420801 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.426184 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.426461 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.426667 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m19:21:29.427127 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.430010 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.430219 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m19:21:29.430522 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.432230 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.432439 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m19:21:29.432723 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.433757 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m19:21:29.434106 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.434337 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m19:21:29.434990 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.436819 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.437051 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m19:21:29.437424 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.438196 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 19:21:29.417000 => 19:21:29.438099
[0m19:21:29.438405 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m19:21:29.447256 [info ] [Thread-1  ]: 14 of 33 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.03s]
[0m19:21:29.447644 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m19:21:29.447880 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m19:21:29.448226 [info ] [Thread-1  ]: 15 of 33 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m19:21:29.448697 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m19:21:29.448908 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m19:21:29.450944 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.451588 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 19:21:29.449044 => 19:21:29.451484
[0m19:21:29.451796 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m19:21:29.454507 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.455016 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.455211 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m19:21:29.455387 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.460809 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.461075 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.461281 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m19:21:29.461785 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.464986 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.465311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m19:21:29.465716 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.467608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.467815 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m19:21:29.468127 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.469000 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m19:21:29.469188 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.469362 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m19:21:29.469915 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.471442 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.471642 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m19:21:29.471976 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.472657 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 19:21:29.451927 => 19:21:29.472565
[0m19:21:29.472860 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m19:21:29.480720 [info ] [Thread-1  ]: 15 of 33 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.03s]
[0m19:21:29.481130 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m19:21:29.481385 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m19:21:29.481738 [info ] [Thread-1  ]: 16 of 33 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m19:21:29.482142 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m19:21:29.482357 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m19:21:29.484464 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.484916 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 19:21:29.482495 => 19:21:29.484818
[0m19:21:29.485112 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m19:21:29.487918 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.488403 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.488604 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m19:21:29.488780 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.494163 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.494407 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.494654 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m19:21:29.495639 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.497771 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.497986 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m19:21:29.498312 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.500700 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.500901 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m19:21:29.501203 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.502257 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m19:21:29.502544 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.502736 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m19:21:29.503413 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.505190 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.505393 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m19:21:29.505767 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.506619 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 19:21:29.485247 => 19:21:29.506522
[0m19:21:29.506860 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m19:21:29.515639 [info ] [Thread-1  ]: 16 of 33 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.03s]
[0m19:21:29.516066 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m19:21:29.516316 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m19:21:29.516592 [info ] [Thread-1  ]: 17 of 33 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m19:21:29.516946 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m19:21:29.517142 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m19:21:29.519129 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.519606 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 19:21:29.517275 => 19:21:29.519498
[0m19:21:29.519812 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m19:21:29.522570 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.523036 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.523229 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m19:21:29.523405 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.528822 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.529142 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.529373 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m19:21:29.530010 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.532584 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.532858 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m19:21:29.533216 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.535787 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.536015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m19:21:29.536314 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.537351 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m19:21:29.537623 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.537817 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m19:21:29.538447 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.566914 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.567252 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m19:21:29.567766 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.568545 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 19:21:29.519941 => 19:21:29.568451
[0m19:21:29.568748 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m19:21:29.576778 [info ] [Thread-1  ]: 17 of 33 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.06s]
[0m19:21:29.577171 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m19:21:29.577418 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m19:21:29.577697 [info ] [Thread-1  ]: 18 of 33 START sql table model main.dim_account ................................ [RUN]
[0m19:21:29.578131 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m19:21:29.578331 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m19:21:29.580349 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m19:21:29.580839 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 19:21:29.578466 => 19:21:29.580741
[0m19:21:29.581032 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m19:21:29.583675 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m19:21:29.584422 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m19:21:29.584623 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m19:21:29.584794 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.590207 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.590463 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m19:21:29.590655 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."main"."dim_account__dbt_tmp"
  
    as (
      

select
    id as account_id,
    name as account_name,
    industry,
    type as account_type,
    billing_city,
    billing_state,
    billing_country,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__account"
where is_deleted = false;
    );
  
  
[0m19:21:29.592017 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 19:21:29.581162 => 19:21:29.591859
[0m19:21:29.592290 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: ROLLBACK
[0m19:21:29.592830 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_account'
[0m19:21:29.593016 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m19:21:29.594964 [debug] [Thread-1  ]: Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.595334 [error] [Thread-1  ]: 18 of 33 ERROR creating sql table model main.dim_account ....................... [[31mERROR[0m in 0.02s]
[0m19:21:29.595652 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m19:21:29.595897 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m19:21:29.596183 [info ] [Thread-1  ]: 19 of 33 START sql table model main.dim_campaign ............................... [RUN]
[0m19:21:29.596539 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m19:21:29.596744 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m19:21:29.599068 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m19:21:29.599597 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 19:21:29.596874 => 19:21:29.599497
[0m19:21:29.599796 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m19:21:29.602492 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m19:21:29.603102 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m19:21:29.603295 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m19:21:29.603467 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.608581 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.608834 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m19:21:29.609082 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."main"."dim_campaign__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    id AS campaign_id,
    parentid AS parent_campaign_id,
    ownerid AS owner_id,
    createdbyid AS created_by_id,
    lastmodifiedbyid AS last_modified_by_id,
    campaignmemberrecordtypeid AS campaign_member_record_type_id,

    /* Dates */
    startdate AS start_date,
    enddate AS end_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,

    /* Dimensions */
    name AS campaign_name,
    type AS campaign_type,
    status,
    description,
    isactive AS is_active,

    /* Metrics */
    expectedrevenue AS expected_revenue,
    budgetedcost AS budgeted_cost,
    actualcost AS actual_cost,
    expectedresponse AS expected_response,
    numbersent AS number_sent,
    numberofleads AS number_of_leads,
    numberofconvertedleads AS number_of_converted_leads,
    numberofcontacts AS number_of_contacts,
    numberofresponses AS number_of_responses,
    numberofopportunities AS number_of_opportunities,
    numberofwonopportunities AS number_of_won_opportunities,
    amountallopportunities AS amount_all_opportunities,
    amountwonopportunities AS amount_won_opportunities,
    hierarchynumberofleads AS hierarchy_number_of_leads,
    hierarchynumberofconvertedleads AS hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts AS hierarchy_number_of_contacts,
    hierarchynumberofresponses AS hierarchy_number_of_responses,
    hierarchynumberofopportunities AS hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities AS hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities AS hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities AS hierarchy_amount_won_opportunities,
    hierarchynumbersent AS hierarchy_number_sent,
    hierarchyexpectedrevenue AS hierarchy_expected_revenue,
    hierarchybudgetedcost AS hierarchy_budgeted_cost,
    hierarchyactualcost AS hierarchy_actual_cost

FROM "dbt"."staging"."stg_salesforce__campaign"
WHERE isdeleted = FALSE;
    );
  
  
[0m19:21:29.609581 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 19:21:29.599931 => 19:21:29.609487
[0m19:21:29.609778 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: ROLLBACK
[0m19:21:29.610291 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_campaign'
[0m19:21:29.610482 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m19:21:29.612457 [debug] [Thread-1  ]: Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.612917 [error] [Thread-1  ]: 19 of 33 ERROR creating sql table model main.dim_campaign ...................... [[31mERROR[0m in 0.02s]
[0m19:21:29.613248 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m19:21:29.613486 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m19:21:29.613782 [info ] [Thread-1  ]: 20 of 33 START sql table model main.dim_case_status ............................ [RUN]
[0m19:21:29.614200 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m19:21:29.614448 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m19:21:29.617337 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.618424 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 19:21:29.614587 => 19:21:29.618287
[0m19:21:29.618668 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m19:21:29.621244 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.622095 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.622351 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m19:21:29.622524 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.627884 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.628151 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.628353 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."main"."dim_case_status__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        status AS status_name,
        status AS status_description  -- Adjust as needed, typically a description field should be separate
    FROM "dbt"."staging"."stg_salesforce__case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY status_name) AS status_id,  -- Surrogate Key
    status_name,
    status_description
FROM source
    );
  
  
[0m19:21:29.631412 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.633504 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.633726 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m19:21:29.634041 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.635751 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.635950 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m19:21:29.636234 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.639079 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m19:21:29.639288 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.639465 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m19:21:29.640051 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.641834 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.642133 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."main"."dim_case_status__dbt_backup" cascade
[0m19:21:29.642565 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.643415 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 19:21:29.618802 => 19:21:29.643319
[0m19:21:29.643639 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m19:21:29.653326 [info ] [Thread-1  ]: 20 of 33 OK created sql table model main.dim_case_status ....................... [[32mOK[0m in 0.04s]
[0m19:21:29.653728 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m19:21:29.653956 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m19:21:29.654351 [info ] [Thread-1  ]: 21 of 33 START sql table model main.dim_contact ................................ [RUN]
[0m19:21:29.654899 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m19:21:29.655164 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m19:21:29.657649 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.658641 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 19:21:29.655321 => 19:21:29.658514
[0m19:21:29.659240 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m19:21:29.663942 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.664740 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.665076 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m19:21:29.665297 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.672350 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.672668 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.672940 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    contact_id,
    masterrecordid AS master_record_id,
    accountid AS account_id,
    reportstoid AS reports_to_id,
    ownerid AS owner_id,
    jigsawcontactid AS jigsaw_contact_id,
    individualid AS individual_id,

    /* Dates */
    birthdate AS birth_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    lastcurequestdate AS last_cu_request_date,
    lastcuupdatedate AS last_cu_update_date,
    emailbounceddate AS email_bounced_date,

    /* Dimensions */
    salutation,
    firstname AS first_name,
    lastname AS last_name,
    otherstreet AS other_street,
    othercity AS other_city,
    otherstate AS other_state,
    otherpostalcode AS other_postal_code,
    othercountry AS other_country,
    otherlatitude AS other_latitude,
    otherlongitude AS other_longitude,
    othergeocodeaccuracy AS other_geocode_accuracy,
    mailingstreet AS mailing_street,
    mailingcity AS mailing_city,
    mailingstate AS mailing_state,
    mailingpostalcode AS mailing_postal_code,
    mailingcountry AS mailing_country,
    mailinglatitude AS mailing_latitude,
    mailinglongitude AS mailing_longitude,
    mailinggeocodeaccuracy AS mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone AS mobile_phone,
    homephone AS home_phone,
    otherphone AS other_phone,
    assistantphone AS assistant_phone,
    email,
    title,
    department,
    assistantname AS assistant_name,
    leadsource AS lead_source,
    description,
    pronouns,
    genderidentity AS gender_identity,
    cleanstatus AS clean_status,
    emailbouncedreason AS email_bounced_reason,
    level__c AS level,
    languages__c AS languages,

    /* Metrics */
    hasoptedoutofemail AS has_opted_out_of_email,
    hasoptedoutoffax AS has_opted_out_of_fax,
    donotcall AS do_not_call

FROM "dbt"."staging"."stg_salesforce__contact"
WHERE isdeleted = FALSE
    );
  
  
[0m19:21:29.677660 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.680067 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.680388 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."main"."dim_contact" rename to "dim_contact__dbt_backup"
[0m19:21:29.680866 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.683160 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.683446 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."main"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m19:21:29.683890 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.685313 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m19:21:29.685607 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.685827 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m19:21:29.687385 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.689394 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.689647 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."main"."dim_contact__dbt_backup" cascade
[0m19:21:29.690321 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.691253 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 19:21:29.659682 => 19:21:29.691146
[0m19:21:29.691658 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m19:21:29.711888 [info ] [Thread-1  ]: 21 of 33 OK created sql table model main.dim_contact ........................... [[32mOK[0m in 0.06s]
[0m19:21:29.712324 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m19:21:29.712558 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m19:21:29.712959 [info ] [Thread-1  ]: 22 of 33 START sql table model main.dim_lead ................................... [RUN]
[0m19:21:29.713467 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m19:21:29.713687 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m19:21:29.715814 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m19:21:29.716343 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 19:21:29.713824 => 19:21:29.716236
[0m19:21:29.716544 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m19:21:29.720208 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m19:21:29.720710 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m19:21:29.720901 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m19:21:29.721072 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.726536 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.726811 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m19:21:29.727007 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."main"."dim_lead__dbt_tmp"
  
    as (
      

select
    id as lead_id,
    first_name,
    last_name,
    company,
    status,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__lead";
    );
  
  
[0m19:21:29.727416 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 19:21:29.716677 => 19:21:29.727321
[0m19:21:29.727610 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: ROLLBACK
[0m19:21:29.728135 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_lead'
[0m19:21:29.728313 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m19:21:29.730296 [debug] [Thread-1  ]: Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.730632 [error] [Thread-1  ]: 22 of 33 ERROR creating sql table model main.dim_lead .......................... [[31mERROR[0m in 0.02s]
[0m19:21:29.730940 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m19:21:29.731163 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m19:21:29.731472 [info ] [Thread-1  ]: 23 of 33 START sql table model main.dim_opportunity ............................ [RUN]
[0m19:21:29.731810 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m19:21:29.731995 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m19:21:29.734185 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.735221 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 19:21:29.732124 => 19:21:29.735042
[0m19:21:29.735517 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m19:21:29.738225 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.738974 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.739209 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m19:21:29.739392 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.744852 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.745131 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.745367 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    opportunity_id,
    accountid AS account_id,
    ownerid AS user_id,
    contactid AS primary_contact_id,
    campaignid AS campaign_id,
    pricebook2id AS pricebook_id,
    contractid AS contract_id,
    primarypartneraccountid AS primary_partner_account_id,
    createdbyid AS created_by_id,
    lastmodifiedbyid AS last_modified_by_id,
    lastamountchangedhistoryid AS last_amount_change_history_id,
    lastclosedatechangedhistoryid AS last_close_date_change_history_id,

    /* Metrics */
    amount,
    probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,

    /* Dimensions */
    name AS opportunity_name,
    description,
    stagename AS stage_name,
    stagesortorder AS stage_sort_order,
    type AS opportunity_type,
    nextstep AS next_step,
    leadsource AS lead_source,
    isclosed AS is_closed,
    iswon AS is_won,
    forecastcategory AS forecast_category,
    forecastcategoryname AS forecast_category_name,
    hasopportunitylineitem AS has_opportunity_line_item,
    deliveryinstallationstatus__c AS delivery_installation_status,
    trackingnumber__c AS tracking_number,
    ordernumber__c AS order_number,
    currentgenerators__c AS current_generators,
    maincompetitors__c AS main_competitors,

    /* Dates */
    closedate AS close_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    laststagechangedate AS last_stage_change_date,
    fiscalyear AS fiscal_year,
    fiscalquarter AS fiscal_quarter

FROM "dbt"."staging"."stg_salesforce__opportunity"
    );
  
  
[0m19:21:29.748543 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.750820 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.751077 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."main"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m19:21:29.751491 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.753290 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.753494 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."main"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m19:21:29.753821 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.754863 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m19:21:29.755055 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.755230 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m19:21:29.756682 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.758311 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.758520 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."main"."dim_opportunity__dbt_backup" cascade
[0m19:21:29.759025 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.759744 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 19:21:29.735675 => 19:21:29.759649
[0m19:21:29.759960 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m19:21:29.773903 [info ] [Thread-1  ]: 23 of 33 OK created sql table model main.dim_opportunity ....................... [[32mOK[0m in 0.04s]
[0m19:21:29.774282 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m19:21:29.774514 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m19:21:29.774895 [info ] [Thread-1  ]: 24 of 33 START sql table model main.dim_opportunity_stage ...................... [RUN]
[0m19:21:29.775376 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m19:21:29.775584 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m19:21:29.778720 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.779210 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 19:21:29.775725 => 19:21:29.779102
[0m19:21:29.779414 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m19:21:29.782179 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.782643 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.782834 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m19:21:29.783006 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.788842 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.789181 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.789400 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        stagename AS stage_name,
        stagesortorder AS stage_sort_order
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY stage_sort_order) AS stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
FROM source
    );
  
  
[0m19:21:29.793078 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.795305 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.795539 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m19:21:29.795912 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.797679 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.797880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m19:21:29.798258 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.799207 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m19:21:29.799398 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.799573 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m19:21:29.800195 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.801836 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.802045 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."main"."dim_opportunity_stage__dbt_backup" cascade
[0m19:21:29.802416 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.803175 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 19:21:29.779546 => 19:21:29.803078
[0m19:21:29.803384 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m19:21:29.818996 [info ] [Thread-1  ]: 24 of 33 OK created sql table model main.dim_opportunity_stage ................. [[32mOK[0m in 0.04s]
[0m19:21:29.819426 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m19:21:29.819686 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m19:21:29.819963 [info ] [Thread-1  ]: 25 of 33 START sql table model main.dim_pricebook .............................. [RUN]
[0m19:21:29.820329 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m19:21:29.820519 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m19:21:29.822632 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m19:21:29.823133 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 19:21:29.820651 => 19:21:29.823032
[0m19:21:29.823323 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m19:21:29.826904 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m19:21:29.827720 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m19:21:29.827988 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m19:21:29.828168 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.833850 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.834125 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m19:21:29.834329 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."main"."dim_pricebook__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        pricebook_entry_id AS pricebook_id,        -- Surrogate Key
        name AS pricebook_name,
        isactive AS is_active,
        description,
        createddate AS pricebook_created_date,
        lastmodifieddate AS pricebook_last_modified_date
    FROM "dbt"."staging"."stg_salesforce__pricebook_entry"
)

SELECT * FROM source
    );
  
  
[0m19:21:29.835073 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 19:21:29.823450 => 19:21:29.834975
[0m19:21:29.835273 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: ROLLBACK
[0m19:21:29.835785 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_pricebook'
[0m19:21:29.835966 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m19:21:29.837992 [debug] [Thread-1  ]: Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m19:21:29.838445 [error] [Thread-1  ]: 25 of 33 ERROR creating sql table model main.dim_pricebook ..................... [[31mERROR[0m in 0.02s]
[0m19:21:29.838796 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m19:21:29.839047 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m19:21:29.839365 [info ] [Thread-1  ]: 26 of 33 START sql table model main.dim_product ................................ [RUN]
[0m19:21:29.839809 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m19:21:29.840042 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m19:21:29.842105 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m19:21:29.842615 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 19:21:29.840175 => 19:21:29.842522
[0m19:21:29.842813 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m19:21:29.845160 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m19:21:29.845526 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m19:21:29.845711 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m19:21:29.845879 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.851408 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.851689 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m19:21:29.851884 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."main"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    product_code,
    is_active,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__product_2"
where is_active = true;
    );
  
  
[0m19:21:29.852299 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 19:21:29.842942 => 19:21:29.852204
[0m19:21:29.852496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: ROLLBACK
[0m19:21:29.853015 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_product'
[0m19:21:29.853194 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m19:21:29.855297 [debug] [Thread-1  ]: Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.855740 [error] [Thread-1  ]: 26 of 33 ERROR creating sql table model main.dim_product ....................... [[31mERROR[0m in 0.02s]
[0m19:21:29.856102 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m19:21:29.856408 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m19:21:29.856679 [info ] [Thread-1  ]: 27 of 33 START sql table model main.dim_solution ............................... [RUN]
[0m19:21:29.857055 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_solution)
[0m19:21:29.857358 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m19:21:29.859616 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m19:21:29.860134 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 19:21:29.857549 => 19:21:29.860043
[0m19:21:29.860335 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m19:21:29.862794 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m19:21:29.863208 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m19:21:29.863399 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m19:21:29.863577 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.869145 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.869435 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m19:21:29.869631 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."main"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solution_name,
    status,
    is_active,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__solution";
    );
  
  
[0m19:21:29.870058 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 19:21:29.860464 => 19:21:29.869956
[0m19:21:29.870375 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: ROLLBACK
[0m19:21:29.870894 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_solution'
[0m19:21:29.871087 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m19:21:29.873238 [debug] [Thread-1  ]: Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.873678 [error] [Thread-1  ]: 27 of 33 ERROR creating sql table model main.dim_solution ...................... [[31mERROR[0m in 0.02s]
[0m19:21:29.874000 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m19:21:29.874256 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:21:29.874641 [info ] [Thread-1  ]: 28 of 33 START sql table model main.dim_user ................................... [RUN]
[0m19:21:29.875113 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m19:21:29.875324 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:21:29.878347 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:21:29.878863 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:21:29.875460 => 19:21:29.878765
[0m19:21:29.879058 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:21:29.881709 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:21:29.882171 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:21:29.882362 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:21:29.882532 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.888193 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.888499 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:21:29.888712 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.first_name,
    u.last_name,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.role_id = r.id
where u.is_active = true;
    );
  
  
[0m19:21:29.889141 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:21:29.879189 => 19:21:29.889040
[0m19:21:29.889336 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:21:29.889998 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:21:29.890318 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:21:29.892415 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.892858 [error] [Thread-1  ]: 28 of 33 ERROR creating sql table model main.dim_user .......................... [[31mERROR[0m in 0.02s]
[0m19:21:29.893192 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:21:29.893410 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user_role
[0m19:21:29.893886 [info ] [Thread-1  ]: 29 of 33 START sql table model main.dim_user_role .............................. [RUN]
[0m19:21:29.894313 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.dim_user_role)
[0m19:21:29.894519 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user_role
[0m19:21:29.896751 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user_role"
[0m19:21:29.897266 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (compile): 19:21:29.894657 => 19:21:29.897160
[0m19:21:29.897466 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user_role
[0m19:21:29.900147 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user_role"
[0m19:21:29.900686 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m19:21:29.900954 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: BEGIN
[0m19:21:29.901150 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.906603 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.906921 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m19:21:29.907124 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user_role"} */

  
    
    

    create  table
      "dbt"."main"."dim_user_role__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__user_role"
)

SELECT
    user_role_id,                           -- Surrogate Key
    name AS role_name,
    roledescription AS role_description
FROM source
    );
  
  
[0m19:21:29.907948 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (execute): 19:21:29.897597 => 19:21:29.907847
[0m19:21:29.908148 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: ROLLBACK
[0m19:21:29.908667 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user_role'
[0m19:21:29.908858 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: Close
[0m19:21:29.911023 [debug] [Thread-1  ]: Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "roledescription" not found in FROM clause!
  Candidate bindings: "source.rollupdescription"
  LINE 21:     roledescription AS role_description
               ^
[0m19:21:29.911481 [error] [Thread-1  ]: 29 of 33 ERROR creating sql table model main.dim_user_role ..................... [[31mERROR[0m in 0.02s]
[0m19:21:29.911813 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user_role
[0m19:21:29.912045 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m19:21:29.912255 [info ] [Thread-1  ]: 30 of 33 SKIP relation main.fact_case .......................................... [[33mSKIP[0m]
[0m19:21:29.912542 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m19:21:29.912744 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m19:21:29.912937 [info ] [Thread-1  ]: 31 of 33 SKIP relation main.fact_opportunity ................................... [[33mSKIP[0m]
[0m19:21:29.913271 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m19:21:29.913523 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m19:21:29.913733 [info ] [Thread-1  ]: 32 of 33 SKIP relation main.fact_case_history .................................. [[33mSKIP[0m]
[0m19:21:29.914060 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m19:21:29.914327 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m19:21:29.914623 [info ] [Thread-1  ]: 33 of 33 SKIP relation main.fact_opportunity_history ........................... [[33mSKIP[0m]
[0m19:21:29.914965 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m19:21:29.915812 [debug] [MainThread]: Using duckdb connection "master"
[0m19:21:29.916043 [debug] [MainThread]: On master: BEGIN
[0m19:21:29.916204 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:21:29.921869 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:21:29.922134 [debug] [MainThread]: On master: COMMIT
[0m19:21:29.922294 [debug] [MainThread]: Using duckdb connection "master"
[0m19:21:29.922452 [debug] [MainThread]: On master: COMMIT
[0m19:21:29.922656 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:21:29.922810 [debug] [MainThread]: On master: Close
[0m19:21:29.924289 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:21:29.924475 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user_role' was properly closed.
[0m19:21:29.924717 [info ] [MainThread]: 
[0m19:21:29.924899 [info ] [MainThread]: Finished running 15 table models, 14 view models, 4 incremental models in 0 hours 0 minutes and 1.14 seconds (1.14s).
[0m19:21:29.926877 [debug] [MainThread]: Command end result
[0m19:21:29.935910 [info ] [MainThread]: 
[0m19:21:29.936212 [info ] [MainThread]: [31mCompleted with 11 errors and 0 warnings:[0m
[0m19:21:29.936387 [info ] [MainThread]: 
[0m19:21:29.936559 [error] [MainThread]:   Compilation Error in model dim_date (models/dimensions/dim_date.sql)
  'dict object' has no attribute 'generate_date_spine'. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m19:21:29.936724 [info ] [MainThread]: 
[0m19:21:29.936893 [error] [MainThread]:   Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m19:21:29.937072 [info ] [MainThread]: 
[0m19:21:29.937243 [error] [MainThread]:   Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m19:21:29.937430 [info ] [MainThread]: 
[0m19:21:29.937580 [error] [MainThread]:   Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.937737 [info ] [MainThread]: 
[0m19:21:29.937890 [error] [MainThread]:   Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.938038 [info ] [MainThread]: 
[0m19:21:29.938188 [error] [MainThread]:   Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.938348 [info ] [MainThread]: 
[0m19:21:29.938612 [error] [MainThread]:   Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m19:21:29.938818 [info ] [MainThread]: 
[0m19:21:29.938973 [error] [MainThread]:   Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.939126 [info ] [MainThread]: 
[0m19:21:29.939274 [error] [MainThread]:   Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.939418 [info ] [MainThread]: 
[0m19:21:29.939656 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.939874 [info ] [MainThread]: 
[0m19:21:29.940039 [error] [MainThread]:   Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "roledescription" not found in FROM clause!
  Candidate bindings: "source.rollupdescription"
  LINE 21:     roledescription AS role_description
               ^
[0m19:21:29.940217 [info ] [MainThread]: 
[0m19:21:29.940384 [info ] [MainThread]: Done. PASS=18 WARN=0 ERROR=11 SKIP=4 TOTAL=33
[0m19:21:29.940741 [debug] [MainThread]: Command `dbt run` failed at 19:21:29.940690 after 1.49 seconds
[0m19:21:29.940943 [debug] [MainThread]: Flushing usage events


============================== 19:22:27.482415 | 395eca3c-b778-45ad-ab1a-dad8e2026b5e ==============================
[0m19:22:27.482415 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:22:27.485517 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'send_anonymous_usage_stats': 'False'}
[0m19:22:27.485781 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:22:27.571315 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:22:27.590783 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:22:27.634297 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m19:22:27.634769 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user_role.sql
[0m19:22:27.635007 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:22:27.665676 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m19:22:27.680083 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:22:27.681631 [info ] [MainThread]: 
[0m19:22:27.682118 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:22:27.682681 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:22:27.689852 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:22:27.690103 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:22:27.690280 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:22:27.702991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.704017 [debug] [ThreadPool]: On list_dbt: Close
[0m19:22:27.706028 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:22:27.706495 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:22:27.709484 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:27.709785 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:22:27.709959 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:27.716190 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.716484 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:27.716655 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:22:27.716903 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.717437 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:22:27.717614 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:27.717769 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:22:27.718025 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.718198 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:22:27.721683 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m19:22:27.725603 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:22:27.725898 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:22:27.726060 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:27.731874 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.732267 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:22:27.732500 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:22:27.752413 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.753648 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:22:27.754270 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:22:27.754481 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:22:27.756891 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m19:22:27.759524 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:22:27.759747 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:22:27.759917 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:27.765579 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.765833 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:22:27.766011 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:22:27.781646 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.785305 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:22:27.785580 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:22:27.785742 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:22:27.788503 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:27.788686 [debug] [MainThread]: On master: BEGIN
[0m19:22:27.788837 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:22:27.794316 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:27.794557 [debug] [MainThread]: On master: COMMIT
[0m19:22:27.794716 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:27.794868 [debug] [MainThread]: On master: COMMIT
[0m19:22:27.795070 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:27.795229 [debug] [MainThread]: On master: Close
[0m19:22:27.796841 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:22:27.797116 [info ] [MainThread]: 
[0m19:22:27.798619 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:22:27.798951 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:22:27.800225 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m19:22:27.800468 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:22:27.805931 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:22:27.806437 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:22:27.800614 => 19:22:27.806328
[0m19:22:27.806635 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:22:27.826172 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:22:27.826774 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:22:27.826981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:22:27.827159 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:22:27.832841 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:22:27.833125 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:22:27.833329 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.first_name,
    u.last_name,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.role_id = r.id
where u.is_active = true
    );
  
  
[0m19:22:27.834855 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:22:27.806766 => 19:22:27.834751
[0m19:22:27.835068 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:22:27.838387 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:22:27.838575 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:22:27.840348 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "role_id"
  LINE 24:     on u.role_id = r.id
                  ^
[0m19:22:27.840730 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:22:27.841060 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:22:27.841804 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:27.841974 [debug] [MainThread]: On master: BEGIN
[0m19:22:27.842122 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:22:27.847725 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:27.848031 [debug] [MainThread]: On master: COMMIT
[0m19:22:27.848205 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:27.848357 [debug] [MainThread]: On master: COMMIT
[0m19:22:27.848569 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:27.848734 [debug] [MainThread]: On master: Close
[0m19:22:27.850258 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:22:27.850501 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:22:27.850685 [info ] [MainThread]: 
[0m19:22:27.850878 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m19:22:27.851218 [debug] [MainThread]: Command end result
[0m19:22:27.859828 [info ] [MainThread]: 
[0m19:22:27.860112 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:22:27.860285 [info ] [MainThread]: 
[0m19:22:27.860455 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "role_id"
  LINE 24:     on u.role_id = r.id
                  ^
[0m19:22:27.860621 [info ] [MainThread]: 
[0m19:22:27.860805 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:22:27.861179 [debug] [MainThread]: Command `dbt run` failed at 19:22:27.861131 after 0.40 seconds
[0m19:22:27.861382 [debug] [MainThread]: Flushing usage events


============================== 19:22:50.914008 | 7281f893-b6ce-4563-b77f-90ad3724ee8f ==============================
[0m19:22:50.914008 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:22:50.917176 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m19:22:50.917443 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:22:50.993361 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:22:51.011860 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:22:51.055102 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:22:51.055579 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:22:51.083686 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m19:22:51.098577 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:22:51.100167 [info ] [MainThread]: 
[0m19:22:51.100662 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:22:51.101242 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:22:51.108654 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:22:51.108956 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:22:51.109132 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:22:51.121134 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.122071 [debug] [ThreadPool]: On list_dbt: Close
[0m19:22:51.123944 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:22:51.124413 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:22:51.127598 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:51.127833 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:22:51.127999 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:51.133834 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.134112 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:51.134290 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:22:51.134536 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.135099 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:22:51.135333 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:51.135494 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:22:51.135739 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.135893 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:22:51.138704 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m19:22:51.142175 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:22:51.142444 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:22:51.142604 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:51.147650 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.147893 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:22:51.148065 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:22:51.165726 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.166836 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:22:51.167460 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:22:51.167635 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:22:51.169650 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m19:22:51.172082 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:22:51.172260 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:22:51.172407 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:51.178128 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.178413 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:22:51.178598 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:22:51.193997 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.197474 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:22:51.197716 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:22:51.197881 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:22:51.200729 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:51.200912 [debug] [MainThread]: On master: BEGIN
[0m19:22:51.201066 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:22:51.206300 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:51.206565 [debug] [MainThread]: On master: COMMIT
[0m19:22:51.206725 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:51.206904 [debug] [MainThread]: On master: COMMIT
[0m19:22:51.207113 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:51.207291 [debug] [MainThread]: On master: Close
[0m19:22:51.208609 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:22:51.208807 [info ] [MainThread]: 
[0m19:22:51.210215 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:22:51.210499 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:22:51.211621 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m19:22:51.211814 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:22:51.217272 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:22:51.218162 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:22:51.211953 => 19:22:51.217978
[0m19:22:51.218545 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:22:51.237784 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:22:51.238452 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:22:51.238655 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:22:51.238832 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:22:51.244408 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:22:51.244689 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:22:51.244895 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.first_name,
    u.last_name,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.user_role_id = r.id
where u.is_active = true
    );
  
  
[0m19:22:51.246382 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:22:51.218731 => 19:22:51.246271
[0m19:22:51.246596 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:22:51.250176 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:22:51.250392 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:22:51.252187 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "user_role_id"
  LINE 24:     on u.user_role_id = r.id
                  ^
[0m19:22:51.252584 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:22:51.252930 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:22:51.253682 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:51.253859 [debug] [MainThread]: On master: BEGIN
[0m19:22:51.254007 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:22:51.259132 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:51.259436 [debug] [MainThread]: On master: COMMIT
[0m19:22:51.259642 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:51.259799 [debug] [MainThread]: On master: COMMIT
[0m19:22:51.260060 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:51.260215 [debug] [MainThread]: On master: Close
[0m19:22:51.261763 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:22:51.262096 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:22:51.262286 [info ] [MainThread]: 
[0m19:22:51.262500 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m19:22:51.262882 [debug] [MainThread]: Command end result
[0m19:22:51.271162 [info ] [MainThread]: 
[0m19:22:51.271455 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:22:51.271650 [info ] [MainThread]: 
[0m19:22:51.271845 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "user_role_id"
  LINE 24:     on u.user_role_id = r.id
                  ^
[0m19:22:51.272077 [info ] [MainThread]: 
[0m19:22:51.272291 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:22:51.272785 [debug] [MainThread]: Command `dbt run` failed at 19:22:51.272715 after 0.38 seconds
[0m19:22:51.272995 [debug] [MainThread]: Flushing usage events


============================== 19:23:59.129958 | 3d62a8ac-c8ca-41e6-902a-b3d7a0788c2b ==============================
[0m19:23:59.129958 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:23:59.132809 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'send_anonymous_usage_stats': 'False'}
[0m19:23:59.133047 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:23:59.209455 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:23:59.228157 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:23:59.270434 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:23:59.270919 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:23:59.298823 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m19:23:59.313157 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:23:59.314696 [info ] [MainThread]: 
[0m19:23:59.315158 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:23:59.315869 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:23:59.323289 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:23:59.323517 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:23:59.323685 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:23:59.335808 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.336885 [debug] [ThreadPool]: On list_dbt: Close
[0m19:23:59.338831 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:23:59.339331 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:23:59.342217 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:23:59.342430 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:23:59.342582 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:23:59.348326 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.348635 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:23:59.348910 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:23:59.349232 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.349763 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:23:59.349945 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:23:59.350096 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:23:59.350317 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.350477 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:23:59.353312 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m19:23:59.356678 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:23:59.356920 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:23:59.357076 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:23:59.362860 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.363127 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:23:59.363296 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:23:59.378839 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.382436 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:23:59.383158 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:23:59.383354 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:23:59.385553 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m19:23:59.387884 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:23:59.388093 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:23:59.388246 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:23:59.393560 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.393810 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:23:59.393982 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:23:59.411810 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.412815 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:23:59.413052 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:23:59.413223 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:23:59.416157 [debug] [MainThread]: Using duckdb connection "master"
[0m19:23:59.416360 [debug] [MainThread]: On master: BEGIN
[0m19:23:59.416516 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:23:59.421979 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:23:59.422245 [debug] [MainThread]: On master: COMMIT
[0m19:23:59.422411 [debug] [MainThread]: Using duckdb connection "master"
[0m19:23:59.422558 [debug] [MainThread]: On master: COMMIT
[0m19:23:59.422758 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:23:59.422914 [debug] [MainThread]: On master: Close
[0m19:23:59.424305 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:23:59.424546 [info ] [MainThread]: 
[0m19:23:59.426023 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:23:59.427127 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:23:59.427533 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_user)
[0m19:23:59.427728 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:23:59.433051 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:23:59.433606 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:23:59.427866 => 19:23:59.433501
[0m19:23:59.433808 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:23:59.453154 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:23:59.453834 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:23:59.454040 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:23:59.454218 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:23:59.459651 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:23:59.459933 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:23:59.460220 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.first_name,
    u.last_name,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.userroleid = r.user_role_id
where u.is_active = true
    );
  
  
[0m19:23:59.461824 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:23:59.433947 => 19:23:59.461722
[0m19:23:59.462044 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:23:59.465470 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:23:59.465675 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:23:59.467560 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "is_active"
  LINE 25: where u.is_active = true
                 ^
[0m19:23:59.467983 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:23:59.468325 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:23:59.469102 [debug] [MainThread]: Using duckdb connection "master"
[0m19:23:59.469342 [debug] [MainThread]: On master: BEGIN
[0m19:23:59.469501 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:23:59.475119 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:23:59.475313 [debug] [MainThread]: On master: COMMIT
[0m19:23:59.475466 [debug] [MainThread]: Using duckdb connection "master"
[0m19:23:59.475609 [debug] [MainThread]: On master: COMMIT
[0m19:23:59.475800 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:23:59.475949 [debug] [MainThread]: On master: Close
[0m19:23:59.477433 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:23:59.477599 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:23:59.477764 [info ] [MainThread]: 
[0m19:23:59.477941 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m19:23:59.478258 [debug] [MainThread]: Command end result
[0m19:23:59.486421 [info ] [MainThread]: 
[0m19:23:59.486774 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:23:59.486982 [info ] [MainThread]: 
[0m19:23:59.487196 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "is_active"
  LINE 25: where u.is_active = true
                 ^
[0m19:23:59.487396 [info ] [MainThread]: 
[0m19:23:59.487736 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:23:59.488250 [debug] [MainThread]: Command `dbt run` failed at 19:23:59.488177 after 0.38 seconds
[0m19:23:59.488472 [debug] [MainThread]: Flushing usage events


============================== 19:24:35.236642 | 5d8f4b67-3272-4cd4-bd65-b4b223658678 ==============================
[0m19:24:35.236642 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:24:35.239725 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m19:24:35.240022 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:24:35.316527 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:24:35.335010 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:24:35.377077 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:24:35.377603 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:24:35.405032 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m19:24:35.419133 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:24:35.420547 [info ] [MainThread]: 
[0m19:24:35.420983 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:24:35.421648 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:24:35.428890 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:24:35.429157 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:24:35.429339 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:24:35.441308 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.442415 [debug] [ThreadPool]: On list_dbt: Close
[0m19:24:35.444300 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:24:35.444737 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:24:35.447666 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:35.447868 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:24:35.448027 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:35.453563 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.453811 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:35.453978 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:24:35.454240 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.454752 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:24:35.454931 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:35.455080 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:24:35.455298 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.455455 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:24:35.458263 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m19:24:35.461603 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:24:35.461828 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:24:35.462000 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:35.467612 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.467859 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:24:35.468034 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:24:35.483964 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.487487 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:24:35.488199 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:24:35.488448 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:24:35.490547 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m19:24:35.492966 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:24:35.493192 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:24:35.493356 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:35.498861 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.499120 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:24:35.499290 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:24:35.516932 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.517856 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:24:35.518085 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:24:35.518244 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:24:35.521083 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:35.521281 [debug] [MainThread]: On master: BEGIN
[0m19:24:35.521436 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:24:35.526545 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:35.526786 [debug] [MainThread]: On master: COMMIT
[0m19:24:35.526949 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:35.527100 [debug] [MainThread]: On master: COMMIT
[0m19:24:35.527300 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:35.527456 [debug] [MainThread]: On master: Close
[0m19:24:35.529019 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:24:35.529281 [info ] [MainThread]: 
[0m19:24:35.530663 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:24:35.531800 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:24:35.532209 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_user)
[0m19:24:35.532412 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:24:35.537857 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:24:35.538432 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:24:35.532553 => 19:24:35.538313
[0m19:24:35.538633 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:24:35.557069 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:24:35.557575 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:24:35.557772 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:24:35.557952 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:24:35.563538 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:24:35.563839 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:24:35.564050 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.first_name,
    u.last_name,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.userroleid = r.user_role_id
where u.isactive = true
    );
  
  
[0m19:24:35.565822 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:24:35.538766 => 19:24:35.565696
[0m19:24:35.566053 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:24:35.569626 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:24:35.569855 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:24:35.571629 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "first_name"
  LINE 16:     u.first_name,
               ^
[0m19:24:35.572090 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:24:35.572428 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:24:35.573200 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:35.573435 [debug] [MainThread]: On master: BEGIN
[0m19:24:35.573594 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:24:35.579351 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:35.579568 [debug] [MainThread]: On master: COMMIT
[0m19:24:35.579729 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:35.579878 [debug] [MainThread]: On master: COMMIT
[0m19:24:35.580105 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:35.580273 [debug] [MainThread]: On master: Close
[0m19:24:35.581667 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:24:35.581832 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:24:35.582012 [info ] [MainThread]: 
[0m19:24:35.582196 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m19:24:35.582528 [debug] [MainThread]: Command end result
[0m19:24:35.589874 [info ] [MainThread]: 
[0m19:24:35.590151 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:24:35.590324 [info ] [MainThread]: 
[0m19:24:35.590505 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "first_name"
  LINE 16:     u.first_name,
               ^
[0m19:24:35.590670 [info ] [MainThread]: 
[0m19:24:35.590852 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:24:35.591262 [debug] [MainThread]: Command `dbt run` failed at 19:24:35.591212 after 0.37 seconds
[0m19:24:35.591469 [debug] [MainThread]: Flushing usage events


============================== 19:24:47.121376 | a2ce601d-6d0d-497b-9afc-589f9ddc6631 ==============================
[0m19:24:47.121376 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:24:47.124526 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m19:24:47.124785 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:24:47.209731 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:24:47.227752 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:24:47.274045 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:24:47.274567 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:24:47.303578 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m19:24:47.318146 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:24:47.319555 [info ] [MainThread]: 
[0m19:24:47.319987 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:24:47.320580 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:24:47.327912 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:24:47.328177 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:24:47.328357 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:24:47.340546 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.341459 [debug] [ThreadPool]: On list_dbt: Close
[0m19:24:47.343498 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:24:47.344034 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:24:47.347015 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:47.347226 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:24:47.347380 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:47.352955 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.353224 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:47.353579 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:24:47.353924 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.354486 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:24:47.354673 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:47.354830 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:24:47.355077 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.355257 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:24:47.358340 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m19:24:47.361906 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:24:47.362221 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:24:47.362424 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:47.368065 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.368287 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:24:47.368455 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:24:47.383664 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.387437 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:24:47.388094 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:24:47.388282 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:24:47.390612 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m19:24:47.393021 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:24:47.393278 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:24:47.393443 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:47.398834 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.399078 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:24:47.399260 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:24:47.417027 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.418026 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:24:47.418270 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:24:47.418432 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:24:47.421417 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:47.421635 [debug] [MainThread]: On master: BEGIN
[0m19:24:47.421800 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:24:47.427164 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:47.427415 [debug] [MainThread]: On master: COMMIT
[0m19:24:47.427584 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:47.427734 [debug] [MainThread]: On master: COMMIT
[0m19:24:47.427938 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:47.428104 [debug] [MainThread]: On master: Close
[0m19:24:47.429537 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:24:47.429749 [info ] [MainThread]: 
[0m19:24:47.431221 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:24:47.432341 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:24:47.432739 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_user)
[0m19:24:47.432951 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:24:47.438393 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:24:47.439261 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:24:47.433094 => 19:24:47.439090
[0m19:24:47.439521 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:24:47.458824 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:24:47.459634 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:24:47.459935 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:24:47.460147 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:24:47.465794 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:24:47.466076 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:24:47.466285 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.firstname,
    u.lastname,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.userroleid = r.user_role_id
where u.isactive = true
    );
  
  
[0m19:24:47.467860 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:24:47.439662 => 19:24:47.467753
[0m19:24:47.468072 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:24:47.471925 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:24:47.472131 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:24:47.473969 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "created_date"
  LINE 20:     u.created_date,
               ^
[0m19:24:47.474364 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:24:47.474702 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:24:47.475492 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:47.475698 [debug] [MainThread]: On master: BEGIN
[0m19:24:47.475860 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:24:47.481436 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:47.481671 [debug] [MainThread]: On master: COMMIT
[0m19:24:47.482009 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:47.482286 [debug] [MainThread]: On master: COMMIT
[0m19:24:47.482523 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:47.482941 [debug] [MainThread]: On master: Close
[0m19:24:47.484645 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:24:47.487293 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:24:47.487565 [info ] [MainThread]: 
[0m19:24:47.487767 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m19:24:47.488116 [debug] [MainThread]: Command end result
[0m19:24:47.495260 [info ] [MainThread]: 
[0m19:24:47.495530 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:24:47.495697 [info ] [MainThread]: 
[0m19:24:47.495865 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "created_date"
  LINE 20:     u.created_date,
               ^
[0m19:24:47.496028 [info ] [MainThread]: 
[0m19:24:47.496209 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:24:47.496582 [debug] [MainThread]: Command `dbt run` failed at 19:24:47.496532 after 0.40 seconds
[0m19:24:47.496782 [debug] [MainThread]: Flushing usage events


============================== 19:31:20.537559 | 94e1354e-b587-4707-8b61-23cbc85b335f ==============================
[0m19:31:20.537559 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:31:20.540735 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'send_anonymous_usage_stats': 'False'}
[0m19:31:20.541004 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:31:20.625475 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:31:20.643553 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:31:20.688896 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:31:20.689413 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:31:20.718633 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m19:31:20.734326 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:31:20.735832 [info ] [MainThread]: 
[0m19:31:20.736320 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:31:20.736891 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:31:20.744284 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:31:20.744633 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:31:20.744821 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:31:20.765431 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.766648 [debug] [ThreadPool]: On list_dbt: Close
[0m19:31:20.769599 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:31:20.770175 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:31:20.773456 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:20.773725 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:31:20.773899 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:20.779940 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.780228 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:20.780405 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:31:20.780657 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.781202 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:31:20.781374 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:20.781525 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:31:20.781790 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.781958 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:31:20.785274 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m19:31:20.788983 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:31:20.789357 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:31:20.789577 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:20.795642 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.795951 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:31:20.796139 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:31:20.812517 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.816138 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:31:20.816788 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:31:20.816958 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:31:20.819223 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m19:31:20.821576 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:31:20.821774 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:31:20.821922 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:20.827646 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.827893 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:31:20.828062 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:31:20.845623 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.846531 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:31:20.846756 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:31:20.846911 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:31:20.849837 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:20.850033 [debug] [MainThread]: On master: BEGIN
[0m19:31:20.850185 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:31:20.855276 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:20.855513 [debug] [MainThread]: On master: COMMIT
[0m19:31:20.855672 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:20.855819 [debug] [MainThread]: On master: COMMIT
[0m19:31:20.856012 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:20.856172 [debug] [MainThread]: On master: Close
[0m19:31:20.857498 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:31:20.857701 [info ] [MainThread]: 
[0m19:31:20.859220 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:31:20.860272 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:31:20.860658 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_user)
[0m19:31:20.860859 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:31:20.866609 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:31:20.867769 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:31:20.861004 => 19:31:20.867658
[0m19:31:20.867975 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:31:20.886072 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:31:20.886606 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:31:20.886805 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:31:20.886987 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:31:20.892616 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:31:20.892919 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:31:20.893248 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    -- IDs
    u.id as user_id,
    u.userroleid as user_role_id,
    u.profileid as user_profile_id,
    u.contactid as contact_id,
    u.accountid as account_id,
    u.managerid as manager_id,
    u.delegatedapproverid as delegated_approver_id,
    u.createdbyid as created_by_id,
    u.lastmodifiedbyid as last_modified_by_id,
    u.workspaceid as workspace_id,
    u.individualid as individual_id,
    u.globalidentity as global_identity,
    r.name as user_role_name,
    
    -- Metrics
    u.numberoffailedlogins as number_of_failed_logins,
    u.loginlimit as login_limit,
    u.forecastenabled as forecast_enabled,
    
    -- Dimensions
    u.username as user_username,
    u.firstname as user_first_name,
    u.lastname as user_last_name,
    u.companyname as company_name,
    u.division as user_division,
    u.department as user_department,
    u.title as user_title,
    u.street as user_street,
    u.city as user_city,
    u.state as user_state,
    u.postalcode as user_postal_code,
    u.country as user_country,
    u.latitude as user_latitude,
    u.longitude as user_longitude,
    u.geocodeaccuracy as user_geocode_accuracy,
    u.email as user_email,
    u.senderemail as user_sender_email,
    u.sendername as user_sender_name,
    u.signature as user_signature,
    u.stayintouchsubject as stay_in_touch_subject,
    u.stayintouchsignature as stay_in_touch_signature,
    u.stayintouchnote as stay_in_touch_note,
    u.phone as user_phone,
    u.fax as user_fax,
    u.mobilephone as user_mobile_phone,
    u.alias as user_alias,
    u.communitynickname as user_community_nickname,
    u.isactive as user_is_active,
    u.issystemcontrolled as user_is_system_controlled,
    u.timezonesidkey as user_timezone_sid_key,
    u.localesidkey as user_locale_sid_key,
    u.receivesinfoemails as user_receives_info_emails,
    u.receivesadmininfoemails as user_receives_admin_info_emails,
    u.emailencodingkey as user_email_encoding_key,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.startday as user_start_day,
    u.endday as user_end_day,
    u.languagelocalekey as user_language_locale_key,
    u.employee_number,
    u.callcenterid as call_center_id,
    u.extension as user_extension,
    u.federationidentifier as federation_identifier,
    u.aboutme as about_me,
    u.profilephotoid as profile_photo_id,
    u.digestfrequency as digest_frequency,
    u.defaultgroupnotificationfrequency as default_group_notification_frequency,
    u.jigsawimportlimitoverride as jigsaw_import_limit_override,
    u.sharingtype as sharing_type,
    u.chatteradoptionstage as chatter_adoption_stage,
    u.bannerphotoid as banner_photo_id,
    u.isprofilephotoactive as is_profile_photo_active,
    
    -- Dates
    u.lastlogindate as last_login_date,
    u.lastpasswordchangedate as last_password_change_date,
    u.createddate as created_date,
    u.lastmodifieddate as last_modified_date,
    u.systemmodstamp as system_mod_stamp,
    u.suaccessexpirationdate as su_access_expiration_date,
    u.suorgadminexpirationdate as su_org_admin_expiration_date,
    u.offlinetrialexpirationdate as offline_trial_expiration_date,
    u.wirelesstrialexpirationdate as wireless_trial_expiration_date,
    u.offlinepdatrialexpirationdate as offline_pda_trial_expiration_date,
    u.chatteradoptionstagemodifieddate as chatter_adoption_stage_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.userroleid = r.user_role_id
where u.isactive = true
    );
  
  
[0m19:31:20.895064 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:31:20.868110 => 19:31:20.894951
[0m19:31:20.895278 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:31:20.899178 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:31:20.899377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:31:20.901211 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "id"
  LINE 15:     u.id as user_id,
               ^
[0m19:31:20.901659 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:31:20.902021 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:31:20.902824 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:20.903022 [debug] [MainThread]: On master: BEGIN
[0m19:31:20.903184 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:31:20.908685 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:20.908902 [debug] [MainThread]: On master: COMMIT
[0m19:31:20.909061 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:20.909212 [debug] [MainThread]: On master: COMMIT
[0m19:31:20.909405 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:20.909561 [debug] [MainThread]: On master: Close
[0m19:31:20.911023 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:31:20.911204 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:31:20.911377 [info ] [MainThread]: 
[0m19:31:20.911566 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m19:31:20.911933 [debug] [MainThread]: Command end result
[0m19:31:20.920381 [info ] [MainThread]: 
[0m19:31:20.920695 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:31:20.920868 [info ] [MainThread]: 
[0m19:31:20.921036 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "id"
  LINE 15:     u.id as user_id,
               ^
[0m19:31:20.921204 [info ] [MainThread]: 
[0m19:31:20.921382 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:31:20.921765 [debug] [MainThread]: Command `dbt run` failed at 19:31:20.921712 after 0.41 seconds
[0m19:31:20.921980 [debug] [MainThread]: Flushing usage events


============================== 19:31:35.615753 | 14034102-de5a-4c96-9f62-eced90b61d32 ==============================
[0m19:31:35.615753 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:31:35.618365 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m19:31:35.618625 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:31:35.694139 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:31:35.712258 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:31:35.755279 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:31:35.755794 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:31:35.783281 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m19:31:35.797420 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:31:35.798925 [info ] [MainThread]: 
[0m19:31:35.799420 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:31:35.800277 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:31:35.807222 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:31:35.807485 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:31:35.807672 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:31:35.815264 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.816220 [debug] [ThreadPool]: On list_dbt: Close
[0m19:31:35.818105 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:31:35.818611 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:31:35.821465 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:35.821657 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:31:35.821813 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:35.827555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.827895 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:35.828184 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:31:35.828532 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.829091 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:31:35.829278 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:35.829430 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:31:35.829648 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.829808 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:31:35.832641 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m19:31:35.836188 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:31:35.836391 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:31:35.836548 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:35.842564 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.842848 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:31:35.843038 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:31:35.860890 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.861978 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:31:35.863346 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:31:35.863532 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:31:35.865691 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m19:31:35.868244 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:31:35.868434 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:31:35.868580 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:35.873979 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.874252 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:31:35.874430 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:31:35.890214 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.893925 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:31:35.894174 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:31:35.894336 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:31:35.897097 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:35.897304 [debug] [MainThread]: On master: BEGIN
[0m19:31:35.897468 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:31:35.903161 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:35.903436 [debug] [MainThread]: On master: COMMIT
[0m19:31:35.903600 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:35.903755 [debug] [MainThread]: On master: COMMIT
[0m19:31:35.903961 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:35.904156 [debug] [MainThread]: On master: Close
[0m19:31:35.905558 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:31:35.905760 [info ] [MainThread]: 
[0m19:31:35.907299 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:31:35.907594 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:31:35.908782 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m19:31:35.908978 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:31:35.914571 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:31:35.915543 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:31:35.909113 => 19:31:35.915349
[0m19:31:35.915893 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:31:35.934294 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:31:35.934831 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:31:35.935031 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:31:35.935210 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:31:35.940844 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:31:35.941120 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:31:35.941416 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    -- IDs
    user_id,
    u.userroleid as user_role_id,
    u.profileid as user_profile_id,
    u.contactid as contact_id,
    u.accountid as account_id,
    u.managerid as manager_id,
    u.delegatedapproverid as delegated_approver_id,
    u.createdbyid as created_by_id,
    u.lastmodifiedbyid as last_modified_by_id,
    u.workspaceid as workspace_id,
    u.individualid as individual_id,
    u.globalidentity as global_identity,
    r.name as user_role_name,
    
    -- Metrics
    u.numberoffailedlogins as number_of_failed_logins,
    u.loginlimit as login_limit,
    u.forecastenabled as forecast_enabled,
    
    -- Dimensions
    u.username as user_username,
    u.firstname as user_first_name,
    u.lastname as user_last_name,
    u.companyname as company_name,
    u.division as user_division,
    u.department as user_department,
    u.title as user_title,
    u.street as user_street,
    u.city as user_city,
    u.state as user_state,
    u.postalcode as user_postal_code,
    u.country as user_country,
    u.latitude as user_latitude,
    u.longitude as user_longitude,
    u.geocodeaccuracy as user_geocode_accuracy,
    u.email as user_email,
    u.senderemail as user_sender_email,
    u.sendername as user_sender_name,
    u.signature as user_signature,
    u.stayintouchsubject as stay_in_touch_subject,
    u.stayintouchsignature as stay_in_touch_signature,
    u.stayintouchnote as stay_in_touch_note,
    u.phone as user_phone,
    u.fax as user_fax,
    u.mobilephone as user_mobile_phone,
    u.alias as user_alias,
    u.communitynickname as user_community_nickname,
    u.isactive as user_is_active,
    u.issystemcontrolled as user_is_system_controlled,
    u.timezonesidkey as user_timezone_sid_key,
    u.localesidkey as user_locale_sid_key,
    u.receivesinfoemails as user_receives_info_emails,
    u.receivesadmininfoemails as user_receives_admin_info_emails,
    u.emailencodingkey as user_email_encoding_key,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.startday as user_start_day,
    u.endday as user_end_day,
    u.languagelocalekey as user_language_locale_key,
    u.employee_number,
    u.callcenterid as call_center_id,
    u.extension as user_extension,
    u.federationidentifier as federation_identifier,
    u.aboutme as about_me,
    u.profilephotoid as profile_photo_id,
    u.digestfrequency as digest_frequency,
    u.defaultgroupnotificationfrequency as default_group_notification_frequency,
    u.jigsawimportlimitoverride as jigsaw_import_limit_override,
    u.sharingtype as sharing_type,
    u.chatteradoptionstage as chatter_adoption_stage,
    u.bannerphotoid as banner_photo_id,
    u.isprofilephotoactive as is_profile_photo_active,
    
    -- Dates
    u.lastlogindate as last_login_date,
    u.lastpasswordchangedate as last_password_change_date,
    u.createddate as created_date,
    u.lastmodifieddate as last_modified_date,
    u.systemmodstamp as system_mod_stamp,
    u.suaccessexpirationdate as su_access_expiration_date,
    u.suorgadminexpirationdate as su_org_admin_expiration_date,
    u.offlinetrialexpirationdate as offline_trial_expiration_date,
    u.wirelesstrialexpirationdate as wireless_trial_expiration_date,
    u.offlinepdatrialexpirationdate as offline_pda_trial_expiration_date,
    u.chatteradoptionstagemodifieddate as chatter_adoption_stage_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.userroleid = r.user_role_id
where u.isactive = true
    );
  
  
[0m19:31:35.943235 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:31:35.916082 => 19:31:35.943127
[0m19:31:35.943448 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:31:35.946809 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:31:35.947122 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:31:35.948975 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "employee_number"
  LINE 74:     u.employee_number,
               ^
[0m19:31:35.949360 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:31:35.949705 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:31:35.950461 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:35.950644 [debug] [MainThread]: On master: BEGIN
[0m19:31:35.950792 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:31:35.955911 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:35.956198 [debug] [MainThread]: On master: COMMIT
[0m19:31:35.956375 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:35.956535 [debug] [MainThread]: On master: COMMIT
[0m19:31:35.956756 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:35.956912 [debug] [MainThread]: On master: Close
[0m19:31:35.958406 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:31:35.958645 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:31:35.958833 [info ] [MainThread]: 
[0m19:31:35.959028 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m19:31:35.959395 [debug] [MainThread]: Command end result
[0m19:31:35.970212 [info ] [MainThread]: 
[0m19:31:35.970618 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:31:35.970827 [info ] [MainThread]: 
[0m19:31:35.971007 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "employee_number"
  LINE 74:     u.employee_number,
               ^
[0m19:31:35.971177 [info ] [MainThread]: 
[0m19:31:35.971438 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:31:35.971845 [debug] [MainThread]: Command `dbt run` failed at 19:31:35.971789 after 0.37 seconds
[0m19:31:35.972061 [debug] [MainThread]: Flushing usage events


============================== 21:03:39.118315 | 1ca0cf8b-793a-4ca1-8fe2-d85d1334a802 ==============================
[0m21:03:39.118315 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:03:39.121457 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:03:39.121721 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:03:39.206536 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:03:39.226503 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:03:39.275914 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:03:39.276421 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:03:39.304866 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m21:03:39.319929 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:03:39.321520 [info ] [MainThread]: 
[0m21:03:39.322024 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:03:39.322684 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:03:39.329688 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:03:39.329964 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:03:39.330263 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:03:39.340086 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:03:39.340450 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m21:03:39.340638 [info ] [MainThread]: 
[0m21:03:39.340856 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m21:03:39.341163 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 7815) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m21:03:39.341614 [debug] [MainThread]: Command `dbt run` failed at 21:03:39.341551 after 0.24 seconds
[0m21:03:39.341816 [debug] [MainThread]: Flushing usage events


============================== 21:04:02.660952 | 244df88e-7119-4283-95f9-20c1e31e3328 ==============================
[0m21:04:02.660952 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:04:02.663640 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:04:02.663908 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:04:02.741145 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:04:02.761471 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:04:02.808797 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:04:02.809139 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:04:02.810105 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:04:02.824435 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:04:02.825844 [info ] [MainThread]: 
[0m21:04:02.826285 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:04:02.826970 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:04:02.834662 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:04:02.834932 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:04:02.835114 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:04:02.847322 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.848257 [debug] [ThreadPool]: On list_dbt: Close
[0m21:04:02.850144 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:04:02.850649 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:04:02.853864 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:02.854199 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:04:02.854376 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:02.860346 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.860550 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:02.860713 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:04:02.860965 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.861465 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:04:02.861635 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:02.861782 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:04:02.861995 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.862174 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:04:02.865245 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:04:02.868831 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:04:02.869095 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:04:02.869262 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:02.874976 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.875230 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:04:02.875414 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:04:02.893683 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.894739 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:04:02.895459 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:04:02.895638 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:04:02.897831 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:04:02.900414 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:04:02.900618 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:04:02.900772 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:02.906278 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.906522 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:04:02.906693 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:04:02.921868 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.925618 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:04:02.925878 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:04:02.926044 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:04:02.928799 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:02.928978 [debug] [MainThread]: On master: BEGIN
[0m21:04:02.929129 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:04:02.934432 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:02.934672 [debug] [MainThread]: On master: COMMIT
[0m21:04:02.934833 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:02.934982 [debug] [MainThread]: On master: COMMIT
[0m21:04:02.935185 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:02.935342 [debug] [MainThread]: On master: Close
[0m21:04:02.936734 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:04:02.936979 [info ] [MainThread]: 
[0m21:04:02.938558 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:04:02.938847 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:04:02.939238 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:04:02.939451 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:04:02.945059 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:04:02.946435 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:04:02.939592 => 21:04:02.946258
[0m21:04:02.946751 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:04:02.966665 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:04:02.967325 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:02.967530 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:04:02.967711 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:04:02.973261 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:02.973529 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:02.973758 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

SELECT
    u.id AS user_id,
    u.username AS user_name,
    u.firstname AS first_name,
    u.lastname AS last_name,
    u.companyname AS company_name,
    u.division AS division,
    u.department AS department,
    u.title AS title,
    u.street AS street,
    u.city AS city,
    u.state AS state,
    u.postalcode AS postal_code,
    u.country AS country,
    u.latitude AS latitude,
    u.longitude AS longitude,
    u.email AS email,
    u.phone AS phone,
    u.mobilephone AS mobile_phone,
    u.alias AS alias,
    u.isactive AS is_active,
    u.timezonesidkey AS timezone_sid_key,
    u.localesidkey AS locale_sid_key,
    u.emailencodingkey AS email_encoding_key,
    u.profileid AS profile_id,
    u.usertype AS user_type,
    u.usersubtype AS user_subtype,
    u.lastlogindate AS last_login_date,
    u.createddate AS created_date,
    r.name AS role_name,
    r.parentroleid AS parent_role_id,
    r.opportunityaccessforaccountowner AS opportunity_access_for_account_owner,
    r.caseaccessforaccountowner AS case_access_for_account_owner,
    r.contactaccessforaccountowner AS contact_access_for_account_owner
FROM
    "dbt"."raw"."user" u
LEFT JOIN
    "dbt"."raw"."user_role" r
ON
    u.userroleid = r.id;
    );
  
  
[0m21:04:02.974292 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:04:02.946929 => 21:04:02.974190
[0m21:04:02.974496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m21:04:02.978370 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m21:04:02.978632 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:04:02.980441 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m21:04:02.980872 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m21:04:02.981273 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:04:02.982026 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:02.982203 [debug] [MainThread]: On master: BEGIN
[0m21:04:02.982350 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:04:02.988005 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:02.988243 [debug] [MainThread]: On master: COMMIT
[0m21:04:02.988413 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:02.988561 [debug] [MainThread]: On master: COMMIT
[0m21:04:02.988763 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:02.988918 [debug] [MainThread]: On master: Close
[0m21:04:02.990441 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:04:02.990719 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:04:02.990943 [info ] [MainThread]: 
[0m21:04:02.991170 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m21:04:02.991596 [debug] [MainThread]: Command end result
[0m21:04:02.999086 [info ] [MainThread]: 
[0m21:04:02.999485 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:04:02.999697 [info ] [MainThread]: 
[0m21:04:02.999865 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m21:04:03.000028 [info ] [MainThread]: 
[0m21:04:03.000250 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:04:03.000648 [debug] [MainThread]: Command `dbt run` failed at 21:04:03.000568 after 0.36 seconds
[0m21:04:03.000861 [debug] [MainThread]: Flushing usage events


============================== 21:04:10.440746 | 5c86d2f5-f6de-4575-9cb2-905a9d05e8af ==============================
[0m21:04:10.440746 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:04:10.443095 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:04:10.443333 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:04:10.517116 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:04:10.535450 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:04:10.571394 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:04:10.571865 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:04:10.599539 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:04:10.612424 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:04:10.613819 [info ] [MainThread]: 
[0m21:04:10.614256 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:04:10.614898 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:04:10.621914 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:04:10.622195 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:04:10.622389 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:04:10.629946 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.630886 [debug] [ThreadPool]: On list_dbt: Close
[0m21:04:10.632728 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:04:10.633275 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:04:10.636378 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:10.636585 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:04:10.636745 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:10.642785 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.643056 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:10.643229 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:04:10.643491 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.644028 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:04:10.644202 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:10.644350 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:04:10.644562 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.644728 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:04:10.647547 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:04:10.651069 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:04:10.651302 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:04:10.651466 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:10.656919 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.657172 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:04:10.657350 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:04:10.675382 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.676353 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:04:10.676745 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:04:10.676911 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:04:10.679114 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:04:10.681744 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:04:10.681942 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:04:10.682101 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:10.687738 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.687992 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:04:10.688168 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:04:10.703843 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.707500 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:04:10.707733 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:04:10.707898 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:04:10.710674 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:10.710859 [debug] [MainThread]: On master: BEGIN
[0m21:04:10.711011 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:04:10.716390 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:10.716653 [debug] [MainThread]: On master: COMMIT
[0m21:04:10.716814 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:10.716968 [debug] [MainThread]: On master: COMMIT
[0m21:04:10.717169 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:10.717327 [debug] [MainThread]: On master: Close
[0m21:04:10.718886 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:04:10.719135 [info ] [MainThread]: 
[0m21:04:10.720191 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:04:10.720529 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:04:10.721778 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:04:10.721984 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:04:10.727539 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:04:10.728072 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:04:10.722125 => 21:04:10.727967
[0m21:04:10.728273 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:04:10.747323 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:04:10.747880 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:10.748086 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:04:10.748263 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:04:10.753895 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:10.754207 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:10.754444 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

SELECT
    u.id AS user_id,
    u.username AS user_name,
    u.firstname AS first_name,
    u.lastname AS last_name,
    u.companyname AS company_name,
    u.division AS division,
    u.department AS department,
    u.title AS title,
    u.street AS street,
    u.city AS city,
    u.state AS state,
    u.postalcode AS postal_code,
    u.country AS country,
    u.latitude AS latitude,
    u.longitude AS longitude,
    u.email AS email,
    u.phone AS phone,
    u.mobilephone AS mobile_phone,
    u.alias AS alias,
    u.isactive AS is_active,
    u.timezonesidkey AS timezone_sid_key,
    u.localesidkey AS locale_sid_key,
    u.emailencodingkey AS email_encoding_key,
    u.profileid AS profile_id,
    u.usertype AS user_type,
    u.usersubtype AS user_subtype,
    u.lastlogindate AS last_login_date,
    u.createddate AS created_date,
    r.name AS role_name,
    r.parentroleid AS parent_role_id,
    r.opportunityaccessforaccountowner AS opportunity_access_for_account_owner,
    r.caseaccessforaccountowner AS case_access_for_account_owner,
    r.contactaccessforaccountowner AS contact_access_for_account_owner
FROM
    "dbt"."raw"."user" u
LEFT JOIN
    "dbt"."raw"."user_role" r
ON
    u.userroleid = r.id
    );
  
  
[0m21:04:10.758409 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:10.762406 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:10.762676 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."main"."dim_user__dbt_tmp" rename to "dim_user"
[0m21:04:10.763097 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:10.772966 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:04:10.773278 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:10.773469 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:04:10.774591 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:10.777846 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:10.778102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."main"."dim_user__dbt_backup" cascade
[0m21:04:10.778445 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:10.779221 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:04:10.728409 => 21:04:10.779128
[0m21:04:10.779437 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:04:10.801299 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_user ................................ [[32mOK[0m in 0.08s]
[0m21:04:10.801728 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:04:10.802497 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:10.802690 [debug] [MainThread]: On master: BEGIN
[0m21:04:10.802848 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:04:10.808734 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:10.808956 [debug] [MainThread]: On master: COMMIT
[0m21:04:10.809113 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:10.809262 [debug] [MainThread]: On master: COMMIT
[0m21:04:10.809459 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:10.809615 [debug] [MainThread]: On master: Close
[0m21:04:10.811034 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:04:10.811275 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:04:10.811463 [info ] [MainThread]: 
[0m21:04:10.811657 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m21:04:10.811997 [debug] [MainThread]: Command end result
[0m21:04:10.845029 [info ] [MainThread]: 
[0m21:04:10.845317 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:04:10.845486 [info ] [MainThread]: 
[0m21:04:10.845666 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:04:10.845990 [debug] [MainThread]: Command `dbt run` succeeded at 21:04:10.845934 after 0.42 seconds
[0m21:04:10.846175 [debug] [MainThread]: Flushing usage events


============================== 21:08:20.980737 | 6d771c70-971c-49e9-8441-fd3b16b272ec ==============================
[0m21:08:20.980737 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:08:20.983907 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:08:20.984174 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:08:21.066354 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:08:21.084968 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:08:21.129268 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 1 files changed.
[0m21:08:21.129676 [debug] [MainThread]: Partial parsing: deleted file: elastic_dbt_interview://models/dimensions/dim_user_role.sql
[0m21:08:21.129960 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:08:21.158855 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:08:21.175590 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:08:21.177076 [info ] [MainThread]: 
[0m21:08:21.177516 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:08:21.178187 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:08:21.185067 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:08:21.185320 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:08:21.185498 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:08:21.191791 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:08:21.191989 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m21:08:21.192154 [info ] [MainThread]: 
[0m21:08:21.192451 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m21:08:21.192782 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 10867) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m21:08:21.193174 [debug] [MainThread]: Command `dbt run` failed at 21:08:21.193119 after 0.24 seconds
[0m21:08:21.193373 [debug] [MainThread]: Flushing usage events


============================== 21:08:29.071797 | 1e7ea831-adc5-437c-b580-b54ae928cd31 ==============================
[0m21:08:29.071797 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:08:29.074537 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:08:29.074809 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:08:29.151855 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:08:29.171189 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:08:29.213756 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:08:29.214048 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:08:29.215000 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:08:29.229176 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:08:29.230573 [info ] [MainThread]: 
[0m21:08:29.231004 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:08:29.231677 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:08:29.239146 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:08:29.239430 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:08:29.239600 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:08:29.246040 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:08:29.246363 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m21:08:29.246549 [info ] [MainThread]: 
[0m21:08:29.246809 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m21:08:29.247149 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 10867) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m21:08:29.247805 [debug] [MainThread]: Command `dbt run` failed at 21:08:29.247715 after 0.20 seconds
[0m21:08:29.248063 [debug] [MainThread]: Flushing usage events


============================== 21:08:36.519430 | a4d343e9-3ef9-496d-add2-1a3ad9df97ed ==============================
[0m21:08:36.519430 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:08:36.521889 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:08:36.522167 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:08:36.598618 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:08:36.617630 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:08:36.654087 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:08:36.654383 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:08:36.655336 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m21:08:36.669118 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:08:36.670501 [info ] [MainThread]: 
[0m21:08:36.670933 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:08:36.671594 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:08:36.678733 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:08:36.679014 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:08:36.679204 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:08:36.690896 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.691984 [debug] [ThreadPool]: On list_dbt: Close
[0m21:08:36.693908 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:08:36.694407 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:08:36.697336 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:36.697540 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:08:36.697691 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:36.703342 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.703586 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:36.703749 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:08:36.704284 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.704876 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:08:36.705098 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:36.705249 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:08:36.705482 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.705648 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:08:36.708536 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:08:36.712140 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:08:36.712355 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:08:36.712518 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:36.718264 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.718563 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:08:36.718773 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:08:36.736994 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.738021 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:08:36.738741 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:08:36.738941 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:08:36.741091 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:08:36.743708 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:08:36.743906 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:08:36.744057 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:36.749681 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.749924 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:08:36.750099 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:08:36.765705 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.769322 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:08:36.769565 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:08:36.769724 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:08:36.772368 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:36.772542 [debug] [MainThread]: On master: BEGIN
[0m21:08:36.772694 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:08:36.778200 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:36.778447 [debug] [MainThread]: On master: COMMIT
[0m21:08:36.778609 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:36.778753 [debug] [MainThread]: On master: COMMIT
[0m21:08:36.778957 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:36.779109 [debug] [MainThread]: On master: Close
[0m21:08:36.780632 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:08:36.780898 [info ] [MainThread]: 
[0m21:08:36.782874 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:08:36.783207 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:08:36.783642 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:08:36.783848 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:08:36.789346 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:08:36.790716 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:08:36.783984 => 21:08:36.790525
[0m21:08:36.791008 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:08:36.811271 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:08:36.811968 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:36.812179 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:08:36.812357 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:08:36.817938 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:36.818216 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:36.818445 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    u.id as user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."raw"."user" u
left join "dbt"."raw"."user_role" ur on u.userroleid = r.id
where is_active = 1
    );
  
  
[0m21:08:36.819197 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:08:36.791162 => 21:08:36.819094
[0m21:08:36.819402 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m21:08:36.823266 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m21:08:36.823489 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:08:36.825468 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Referenced table "r" not found!
  Candidate tables: "u", "ur"
  LINE 48: ..., "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.id as user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."raw"."user" u
  left join "dbt"."raw"."user_role" ur on u.userroleid = r.id
                                                     ^
[0m21:08:36.825918 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m21:08:36.826323 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:08:36.827073 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:36.827245 [debug] [MainThread]: On master: BEGIN
[0m21:08:36.827393 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:08:36.832396 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:36.832817 [debug] [MainThread]: On master: COMMIT
[0m21:08:36.833033 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:36.833202 [debug] [MainThread]: On master: COMMIT
[0m21:08:36.833529 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:36.833729 [debug] [MainThread]: On master: Close
[0m21:08:36.835154 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:08:36.835395 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:08:36.835574 [info ] [MainThread]: 
[0m21:08:36.835896 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m21:08:36.836324 [debug] [MainThread]: Command end result
[0m21:08:36.843700 [info ] [MainThread]: 
[0m21:08:36.844042 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:08:36.844248 [info ] [MainThread]: 
[0m21:08:36.844461 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Referenced table "r" not found!
  Candidate tables: "u", "ur"
  LINE 48: ..., "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.id as user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."raw"."user" u
  left join "dbt"."raw"."user_role" ur on u.userroleid = r.id
                                                     ^
[0m21:08:36.844868 [info ] [MainThread]: 
[0m21:08:36.845120 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:08:36.845537 [debug] [MainThread]: Command `dbt run` failed at 21:08:36.845474 after 0.34 seconds
[0m21:08:36.845755 [debug] [MainThread]: Flushing usage events


============================== 21:08:56.848351 | c527ff2a-1bc4-41bc-b649-c738ed51ba86 ==============================
[0m21:08:56.848351 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:08:56.850726 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:08:56.850974 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:08:56.925611 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:08:56.945092 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:08:56.980694 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:08:56.981238 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:08:57.008659 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:08:57.022443 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:08:57.023910 [info ] [MainThread]: 
[0m21:08:57.024361 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:08:57.025040 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:08:57.032383 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:08:57.032635 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:08:57.032804 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:08:57.040232 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.041224 [debug] [ThreadPool]: On list_dbt: Close
[0m21:08:57.043214 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:08:57.043732 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:08:57.046811 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:57.047028 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:08:57.047180 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:57.052749 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.053025 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:57.053398 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:08:57.053768 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.054343 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:08:57.054518 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:57.054668 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:08:57.054893 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.055054 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:08:57.058002 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:08:57.061633 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:08:57.061945 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:08:57.062165 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:57.068117 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.068385 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:08:57.068563 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:08:57.086428 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.087641 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:08:57.088104 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:08:57.088274 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:08:57.090354 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:08:57.092790 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:08:57.092975 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:08:57.093124 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:57.098557 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.098795 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:08:57.098972 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:08:57.114785 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.118440 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:08:57.118687 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:08:57.118852 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:08:57.121673 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:57.121883 [debug] [MainThread]: On master: BEGIN
[0m21:08:57.122046 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:08:57.127543 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:57.127796 [debug] [MainThread]: On master: COMMIT
[0m21:08:57.127957 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:57.128110 [debug] [MainThread]: On master: COMMIT
[0m21:08:57.128309 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:57.128468 [debug] [MainThread]: On master: Close
[0m21:08:57.129916 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:08:57.130148 [info ] [MainThread]: 
[0m21:08:57.131267 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:08:57.132345 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:08:57.132719 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:08:57.132919 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:08:57.138550 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:08:57.139190 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:08:57.133061 => 21:08:57.139073
[0m21:08:57.139418 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:08:57.158730 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:08:57.159912 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.160226 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:08:57.160418 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:08:57.166528 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.166771 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.167001 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    u.id as user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."raw"."user" u
left join "dbt"."raw"."user_role" ur on u.userroleid = ur.id
where is_active = 1
    );
  
  
[0m21:08:57.170153 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.174323 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.174608 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."main"."dim_user" rename to "dim_user__dbt_backup"
[0m21:08:57.175026 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.176850 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.177055 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."main"."dim_user__dbt_tmp" rename to "dim_user"
[0m21:08:57.177363 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.188077 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:08:57.188327 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.188520 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:08:57.189635 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.216522 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.216865 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."main"."dim_user__dbt_backup" cascade
[0m21:08:57.217526 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.218342 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:08:57.139552 => 21:08:57.218247
[0m21:08:57.218556 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:08:57.236609 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_user ................................ [[32mOK[0m in 0.10s]
[0m21:08:57.237001 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:08:57.237758 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:57.237947 [debug] [MainThread]: On master: BEGIN
[0m21:08:57.238102 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:08:57.243770 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:57.244006 [debug] [MainThread]: On master: COMMIT
[0m21:08:57.244172 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:57.244319 [debug] [MainThread]: On master: COMMIT
[0m21:08:57.244522 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:57.244678 [debug] [MainThread]: On master: Close
[0m21:08:57.246079 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:08:57.246263 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:08:57.246440 [info ] [MainThread]: 
[0m21:08:57.246687 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m21:08:57.247030 [debug] [MainThread]: Command end result
[0m21:08:57.254478 [info ] [MainThread]: 
[0m21:08:57.254777 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:08:57.254953 [info ] [MainThread]: 
[0m21:08:57.255140 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:08:57.255487 [debug] [MainThread]: Command `dbt run` succeeded at 21:08:57.255434 after 0.42 seconds
[0m21:08:57.255693 [debug] [MainThread]: Flushing usage events


============================== 21:11:44.233266 | 035feb77-c5fd-445e-a12e-d55000c1844f ==============================
[0m21:11:44.233266 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:11:44.235595 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_date.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:11:44.235846 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:11:44.311854 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:11:44.330064 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:11:44.368340 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:11:44.368817 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_date.sql
[0m21:11:44.456051 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:11:44.468648 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:11:44.469987 [info ] [MainThread]: 
[0m21:11:44.470404 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:11:44.471098 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:11:44.475674 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:11:44.475940 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:11:44.476118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:11:44.483329 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.484296 [debug] [ThreadPool]: On list_dbt: Close
[0m21:11:44.485984 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:11:44.486358 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:11:44.489263 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:44.489474 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:11:44.489628 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:44.494924 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.495207 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:44.495368 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:11:44.495607 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.496250 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:11:44.496551 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:44.496721 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:11:44.497034 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.497238 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:11:44.500307 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:11:44.503977 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:11:44.504201 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:11:44.504361 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:44.509622 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.509878 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:11:44.510045 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:11:44.528897 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.529916 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:11:44.530321 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:11:44.530485 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:11:44.532802 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:11:44.534500 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:11:44.535684 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:11:44.535862 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:44.541643 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.541899 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:11:44.542080 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:11:44.557841 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.561480 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:11:44.561731 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:11:44.561892 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:11:44.564622 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:44.564824 [debug] [MainThread]: On master: BEGIN
[0m21:11:44.564984 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:11:44.570430 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:44.570678 [debug] [MainThread]: On master: COMMIT
[0m21:11:44.570843 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:44.570990 [debug] [MainThread]: On master: COMMIT
[0m21:11:44.571193 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:44.571349 [debug] [MainThread]: On master: Close
[0m21:11:44.572715 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:11:44.572914 [info ] [MainThread]: 
[0m21:11:44.573954 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m21:11:44.574230 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_date ..................................... [RUN]
[0m21:11:44.574582 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_date)
[0m21:11:44.574771 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m21:11:44.610973 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:44.611292 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m21:11:44.611484 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:11:44.616964 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:44.617196 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:44.617389 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m21:11:44.619844 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:44.629699 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m21:11:44.630337 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 21:11:44.574906 => 21:11:44.630175
[0m21:11:44.630604 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m21:11:44.649042 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m21:11:44.649516 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:44.649937 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."main"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    *,
    CASE 
        WHEN date_day = CURRENT_DATE THEN TRUE 
        ELSE FALSE 
    END AS is_current_date,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
            AND EXTRACT(month FROM date_day) = EXTRACT(month FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_month,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
            AND EXTRACT(quarter FROM date_day) = EXTRACT(quarter FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_quarter,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_year,
    
    CONCAT(month_name, ' ', year_number) AS year_qualified_month_name
FROM 
    date_range;
    );
  
  
[0m21:11:44.650820 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 21:11:44.630754 => 21:11:44.650723
[0m21:11:44.651018 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: ROLLBACK
[0m21:11:44.654351 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_date'
[0m21:11:44.654577 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m21:11:44.656470 [debug] [Thread-1  ]: Runtime Error in model dim_date (models/dimensions/dim_date.sql)
  Parser Error: syntax error at or near ";"
[0m21:11:44.656822 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_date ............................ [[31mERROR[0m in 0.08s]
[0m21:11:44.657167 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m21:11:44.657984 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:44.658168 [debug] [MainThread]: On master: BEGIN
[0m21:11:44.658313 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:11:44.663793 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:44.664011 [debug] [MainThread]: On master: COMMIT
[0m21:11:44.664169 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:44.664321 [debug] [MainThread]: On master: COMMIT
[0m21:11:44.664515 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:44.664674 [debug] [MainThread]: On master: Close
[0m21:11:44.666179 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:11:44.666481 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_date' was properly closed.
[0m21:11:44.666687 [info ] [MainThread]: 
[0m21:11:44.666985 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m21:11:44.667340 [debug] [MainThread]: Command end result
[0m21:11:44.674772 [info ] [MainThread]: 
[0m21:11:44.675104 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:11:44.675399 [info ] [MainThread]: 
[0m21:11:44.675591 [error] [MainThread]:   Runtime Error in model dim_date (models/dimensions/dim_date.sql)
  Parser Error: syntax error at or near ";"
[0m21:11:44.675772 [info ] [MainThread]: 
[0m21:11:44.676002 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:11:44.676383 [debug] [MainThread]: Command `dbt run` failed at 21:11:44.676322 after 0.46 seconds
[0m21:11:44.676603 [debug] [MainThread]: Flushing usage events


============================== 21:11:50.848568 | f426a088-d423-40f4-8487-fad3f1f76f41 ==============================
[0m21:11:50.848568 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:11:50.851011 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_date.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:11:50.851339 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:11:50.925910 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:11:50.944284 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:11:50.983953 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:11:50.984451 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_date.sql
[0m21:11:51.076471 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:11:51.089668 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:11:51.091059 [info ] [MainThread]: 
[0m21:11:51.091508 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:11:51.092232 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:11:51.096932 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:11:51.097187 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:11:51.097359 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:11:51.104483 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.105481 [debug] [ThreadPool]: On list_dbt: Close
[0m21:11:51.107258 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:11:51.107658 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:11:51.110549 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:51.110795 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:11:51.111010 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:51.116718 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.116982 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:51.117358 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:11:51.117790 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.118391 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:11:51.118597 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:51.118766 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:11:51.119014 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.119185 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:11:51.122251 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m21:11:51.125780 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:11:51.125977 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:11:51.126134 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:51.132104 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.132410 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:11:51.132715 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:11:51.148527 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.152210 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:11:51.152649 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:11:51.152824 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:11:51.155083 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m21:11:51.157519 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:11:51.157761 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:11:51.157920 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:51.163585 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.163784 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:11:51.163952 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:11:51.182659 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.183750 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:11:51.183991 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:11:51.184155 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:11:51.187379 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:51.187576 [debug] [MainThread]: On master: BEGIN
[0m21:11:51.187734 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:11:51.193434 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:51.193687 [debug] [MainThread]: On master: COMMIT
[0m21:11:51.193850 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:51.193998 [debug] [MainThread]: On master: COMMIT
[0m21:11:51.194200 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:51.194364 [debug] [MainThread]: On master: Close
[0m21:11:51.195754 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:11:51.195960 [info ] [MainThread]: 
[0m21:11:51.197046 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m21:11:51.197333 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_date ..................................... [RUN]
[0m21:11:51.197684 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_date)
[0m21:11:51.197873 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m21:11:51.236608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.236958 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m21:11:51.237153 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:11:51.243082 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.243391 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.243592 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m21:11:51.243963 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.254683 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m21:11:51.255402 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 21:11:51.198007 => 21:11:51.255222
[0m21:11:51.255698 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m21:11:51.275203 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m21:11:51.275802 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.276232 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."main"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    *,
    CASE 
        WHEN date_day = CURRENT_DATE THEN TRUE 
        ELSE FALSE 
    END AS is_current_date,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
            AND EXTRACT(month FROM date_day) = EXTRACT(month FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_month,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
            AND EXTRACT(quarter FROM date_day) = EXTRACT(quarter FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_quarter,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_year,
    
    CONCAT(month_name, ' ', year_number) AS year_qualified_month_name
FROM 
    date_range
    );
  
  
[0m21:11:51.363242 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.367492 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.367817 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."main"."dim_date__dbt_tmp" rename to "dim_date"
[0m21:11:51.368282 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.378062 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m21:11:51.378301 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.378496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m21:11:51.382137 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.385233 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.385447 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."main"."dim_date__dbt_backup" cascade
[0m21:11:51.385739 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.386510 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 21:11:51.255848 => 21:11:51.386414
[0m21:11:51.386718 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m21:11:51.411395 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_date ................................ [[32mOK[0m in 0.21s]
[0m21:11:51.411807 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m21:11:51.412550 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:51.412744 [debug] [MainThread]: On master: BEGIN
[0m21:11:51.412899 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:11:51.418638 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:51.418890 [debug] [MainThread]: On master: COMMIT
[0m21:11:51.419061 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:51.419212 [debug] [MainThread]: On master: COMMIT
[0m21:11:51.419426 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:51.419583 [debug] [MainThread]: On master: Close
[0m21:11:51.420995 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:11:51.421205 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_date' was properly closed.
[0m21:11:51.421391 [info ] [MainThread]: 
[0m21:11:51.421581 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.33 seconds (0.33s).
[0m21:11:51.421933 [debug] [MainThread]: Command end result
[0m21:11:51.428912 [info ] [MainThread]: 
[0m21:11:51.429248 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:11:51.429449 [info ] [MainThread]: 
[0m21:11:51.429789 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:11:51.430376 [debug] [MainThread]: Command `dbt run` succeeded at 21:11:51.430259 after 0.60 seconds
[0m21:11:51.430601 [debug] [MainThread]: Flushing usage events


============================== 21:17:35.542956 | b296a61c-2526-43f1-8ab5-140cb2724669 ==============================
[0m21:17:35.542956 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:17:35.545779 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:17:35.546053 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:17:35.622342 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:17:35.639861 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:17:35.682275 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:17:35.682731 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:17:35.701552 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_user (models/dimensions/dim_user.sql)
  expected token ':', got '}'
    line 40
      left join {{{ ref('stg_salesforce__user_role') }}} ur on u.userroleid = ur.id
[0m21:17:35.702098 [debug] [MainThread]: Command `dbt run` failed at 21:17:35.702024 after 0.18 seconds
[0m21:17:35.702335 [debug] [MainThread]: Flushing usage events


============================== 21:17:44.700111 | e03c4b5f-3998-4ca9-943d-aae116cfde33 ==============================
[0m21:17:44.700111 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:17:44.703059 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:17:44.703313 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:17:44.785935 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:17:44.803165 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:17:44.847244 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:17:44.847779 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:17:44.866388 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_user (models/dimensions/dim_user.sql)
  expected token ':', got '}'
    line 40
      left join {{{ ref('stg_salesforce__user_role') }} ur on u.userroleid = ur.id
[0m21:17:44.866915 [debug] [MainThread]: Command `dbt run` failed at 21:17:44.866840 after 0.19 seconds
[0m21:17:44.867175 [debug] [MainThread]: Flushing usage events


============================== 21:18:10.292856 | 1a29f0d7-feef-487a-b439-5b86ada52224 ==============================
[0m21:18:10.292856 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:18:10.295321 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:18:10.295569 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:18:10.369596 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:18:10.388348 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:18:10.423468 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:18:10.423956 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:18:10.452741 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m21:18:10.466203 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:18:10.467566 [info ] [MainThread]: 
[0m21:18:10.467995 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:18:10.468530 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:18:10.476169 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:18:10.476444 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:18:10.476618 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:10.489280 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.490330 [debug] [ThreadPool]: On list_dbt: Close
[0m21:18:10.492138 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:18:10.492559 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:18:10.495590 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:10.495851 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:18:10.496013 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:10.502045 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.502320 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:10.502484 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:18:10.502807 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.503317 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:18:10.503482 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:10.503633 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:18:10.503859 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.504023 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:18:10.506894 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:18:10.510422 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:18:10.510641 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:18:10.510806 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:10.516668 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.516945 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:18:10.517151 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:18:10.535267 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.536309 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:18:10.537008 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:18:10.537204 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:18:10.539239 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:18:10.541607 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:18:10.541920 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:18:10.542134 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:10.547311 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.547567 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:18:10.547743 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:18:10.563392 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.567080 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:18:10.567339 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:18:10.567499 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:18:10.570291 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:10.570503 [debug] [MainThread]: On master: BEGIN
[0m21:18:10.570661 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:18:10.576173 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:10.576423 [debug] [MainThread]: On master: COMMIT
[0m21:18:10.576589 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:10.576742 [debug] [MainThread]: On master: COMMIT
[0m21:18:10.576951 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:10.577113 [debug] [MainThread]: On master: Close
[0m21:18:10.578530 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:10.578744 [info ] [MainThread]: 
[0m21:18:10.580903 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:18:10.581205 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:18:10.581583 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:18:10.581776 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:18:10.587519 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:18:10.588106 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:18:10.581913 => 21:18:10.587997
[0m21:18:10.588339 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:18:10.607793 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:18:10.608449 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:18:10.608650 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:18:10.608823 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:18:10.614594 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:18:10.614877 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:18:10.615207 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    u.id as user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
where is_active = 1
    );
  
  
[0m21:18:10.616818 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:18:10.588468 => 21:18:10.616710
[0m21:18:10.617031 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m21:18:10.620861 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m21:18:10.621146 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:18:10.623164 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "ur" does not have a column named "id"
  LINE 48: ...6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.id as user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."staging"."stg_salesforce__user" u
  left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
                                                     ^
[0m21:18:10.623663 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m21:18:10.624137 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:18:10.624893 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:10.625073 [debug] [MainThread]: On master: BEGIN
[0m21:18:10.625223 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:18:10.631361 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:10.631613 [debug] [MainThread]: On master: COMMIT
[0m21:18:10.631778 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:10.631940 [debug] [MainThread]: On master: COMMIT
[0m21:18:10.632156 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:10.632312 [debug] [MainThread]: On master: Close
[0m21:18:10.633937 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:10.634234 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:18:10.634430 [info ] [MainThread]: 
[0m21:18:10.634630 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m21:18:10.635048 [debug] [MainThread]: Command end result
[0m21:18:10.642995 [info ] [MainThread]: 
[0m21:18:10.643339 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:18:10.643526 [info ] [MainThread]: 
[0m21:18:10.643754 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "ur" does not have a column named "id"
  LINE 48: ...6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.id as user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."staging"."stg_salesforce__user" u
  left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
                                                     ^
[0m21:18:10.643998 [info ] [MainThread]: 
[0m21:18:10.644189 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:18:10.644644 [debug] [MainThread]: Command `dbt run` failed at 21:18:10.644568 after 0.37 seconds
[0m21:18:10.644902 [debug] [MainThread]: Flushing usage events


============================== 21:18:59.437144 | 1640fdfd-bba2-4c8a-ac2f-1a2258167220 ==============================
[0m21:18:59.437144 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:18:59.439651 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m21:18:59.439941 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:18:59.517060 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:18:59.535812 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:18:59.571747 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:18:59.572239 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:18:59.600958 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m21:18:59.614643 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:18:59.615993 [info ] [MainThread]: 
[0m21:18:59.616419 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:18:59.616951 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:18:59.624135 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:18:59.624389 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:18:59.624563 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:59.632423 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.633392 [debug] [ThreadPool]: On list_dbt: Close
[0m21:18:59.635376 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:18:59.635880 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:18:59.638990 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:59.639240 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:18:59.639395 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:59.645228 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.645570 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:59.645761 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:18:59.646124 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.646725 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:18:59.646931 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:59.647089 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:18:59.647308 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.647473 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:18:59.650477 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:18:59.654366 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:18:59.654585 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:18:59.654735 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:59.660517 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.660793 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:18:59.661163 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:18:59.680171 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.681362 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:18:59.681903 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:18:59.682087 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:18:59.684279 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:18:59.686946 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:18:59.687175 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:18:59.687335 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:59.693663 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.693919 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:18:59.694098 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:18:59.709508 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.713103 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:18:59.713412 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:18:59.713585 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:18:59.716430 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:59.716670 [debug] [MainThread]: On master: BEGIN
[0m21:18:59.716840 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:18:59.722331 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:59.722587 [debug] [MainThread]: On master: COMMIT
[0m21:18:59.722762 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:59.722909 [debug] [MainThread]: On master: COMMIT
[0m21:18:59.723112 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:59.723283 [debug] [MainThread]: On master: Close
[0m21:18:59.724700 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:59.724902 [info ] [MainThread]: 
[0m21:18:59.726897 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:18:59.727218 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:18:59.727610 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:18:59.727812 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:18:59.733289 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:18:59.733889 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:18:59.727951 => 21:18:59.733773
[0m21:18:59.734146 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:18:59.753994 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:18:59.754579 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:18:59.754782 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:18:59.754972 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:18:59.761001 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:18:59.761359 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:18:59.761605 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
where is_active = 1
    );
  
  
[0m21:18:59.763178 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:18:59.734281 => 21:18:59.763066
[0m21:18:59.763403 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m21:18:59.764876 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m21:18:59.765077 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:18:59.767019 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "ur" does not have a column named "id"
  LINE 48: ...6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."staging"."stg_salesforce__user" u
  left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
                                                     ^
[0m21:18:59.767479 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m21:18:59.767868 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:18:59.768605 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:59.768773 [debug] [MainThread]: On master: BEGIN
[0m21:18:59.768922 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:18:59.774429 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:59.774726 [debug] [MainThread]: On master: COMMIT
[0m21:18:59.774911 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:59.775199 [debug] [MainThread]: On master: COMMIT
[0m21:18:59.775444 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:59.775618 [debug] [MainThread]: On master: Close
[0m21:18:59.777104 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:59.777324 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:18:59.777536 [info ] [MainThread]: 
[0m21:18:59.777740 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m21:18:59.778132 [debug] [MainThread]: Command end result
[0m21:18:59.785498 [info ] [MainThread]: 
[0m21:18:59.785778 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:18:59.785952 [info ] [MainThread]: 
[0m21:18:59.786345 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "ur" does not have a column named "id"
  LINE 48: ...6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."staging"."stg_salesforce__user" u
  left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
                                                     ^
[0m21:18:59.786633 [info ] [MainThread]: 
[0m21:18:59.786852 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:18:59.787273 [debug] [MainThread]: Command `dbt run` failed at 21:18:59.787215 after 0.37 seconds
[0m21:18:59.787512 [debug] [MainThread]: Flushing usage events


============================== 21:19:45.523718 | 9201c432-444c-4e01-a757-61d8e8fbd4a4 ==============================
[0m21:19:45.523718 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:19:45.526146 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:19:45.526404 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:19:45.601770 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:19:45.620550 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:19:45.656920 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:19:45.657450 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:19:45.684128 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:19:45.696783 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:19:45.698711 [info ] [MainThread]: 
[0m21:19:45.699391 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:19:45.700016 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:19:45.707421 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:19:45.707745 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:19:45.707936 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:19:45.715600 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.716712 [debug] [ThreadPool]: On list_dbt: Close
[0m21:19:45.718772 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:19:45.719274 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:19:45.722204 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:19:45.722452 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:19:45.722614 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:19:45.728412 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.728832 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:19:45.729040 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:19:45.729399 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.730030 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:19:45.730299 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:19:45.730475 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:19:45.730719 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.730912 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:19:45.733966 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m21:19:45.737593 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:19:45.737863 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:19:45.738048 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:19:45.744276 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.744569 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:19:45.744754 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:19:45.760114 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.763825 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:19:45.764270 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:19:45.764437 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:19:45.766521 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m21:19:45.768793 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:19:45.768979 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:19:45.769129 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:19:45.774697 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.774898 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:19:45.775070 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:19:45.793279 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.794330 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:19:45.794573 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:19:45.794735 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:19:45.797817 [debug] [MainThread]: Using duckdb connection "master"
[0m21:19:45.798021 [debug] [MainThread]: On master: BEGIN
[0m21:19:45.798182 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:19:45.803650 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:19:45.803897 [debug] [MainThread]: On master: COMMIT
[0m21:19:45.804060 [debug] [MainThread]: Using duckdb connection "master"
[0m21:19:45.804209 [debug] [MainThread]: On master: COMMIT
[0m21:19:45.804407 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:19:45.804562 [debug] [MainThread]: On master: Close
[0m21:19:45.806142 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:19:45.806407 [info ] [MainThread]: 
[0m21:19:45.808445 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:19:45.808788 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:19:45.809182 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_user)
[0m21:19:45.809372 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:19:45.814852 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:19:45.815397 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:19:45.809516 => 21:19:45.815291
[0m21:19:45.815604 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:19:45.834899 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:19:45.835583 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.835786 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:19:45.835963 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:19:45.841945 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.842239 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.842475 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m21:19:45.846941 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.850781 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.851113 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."main"."dim_user" rename to "dim_user__dbt_backup"
[0m21:19:45.851536 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.853290 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.853496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."main"."dim_user__dbt_tmp" rename to "dim_user"
[0m21:19:45.853810 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.865092 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:19:45.865391 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.865588 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:19:45.866653 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.893955 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.894301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."main"."dim_user__dbt_backup" cascade
[0m21:19:45.894945 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.895747 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:19:45.815745 => 21:19:45.895652
[0m21:19:45.895988 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:19:45.910969 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_user ................................ [[32mOK[0m in 0.10s]
[0m21:19:45.911376 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:19:45.912116 [debug] [MainThread]: Using duckdb connection "master"
[0m21:19:45.912325 [debug] [MainThread]: On master: BEGIN
[0m21:19:45.912475 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:19:45.918233 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:19:45.918488 [debug] [MainThread]: On master: COMMIT
[0m21:19:45.918655 [debug] [MainThread]: Using duckdb connection "master"
[0m21:19:45.918805 [debug] [MainThread]: On master: COMMIT
[0m21:19:45.919005 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:19:45.919164 [debug] [MainThread]: On master: Close
[0m21:19:45.920747 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:19:45.920954 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:19:45.921148 [info ] [MainThread]: 
[0m21:19:45.921328 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m21:19:45.921748 [debug] [MainThread]: Command end result
[0m21:19:45.928732 [info ] [MainThread]: 
[0m21:19:45.929005 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:19:45.929173 [info ] [MainThread]: 
[0m21:19:45.929347 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:19:45.929867 [debug] [MainThread]: Command `dbt run` succeeded at 21:19:45.929781 after 0.42 seconds
[0m21:19:45.930088 [debug] [MainThread]: Flushing usage events


============================== 21:27:43.281992 | 726474dd-a0e2-42bf-8dba-032bb69e6e28 ==============================
[0m21:27:43.281992 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:27:43.285091 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m21:27:43.285342 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:27:43.369373 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:27:43.388204 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:27:43.429259 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:27:43.429664 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:27:43.430739 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:27:43.446246 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:27:43.447663 [info ] [MainThread]: 
[0m21:27:43.448126 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:27:43.448832 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:27:43.455794 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:27:43.456045 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:27:43.456218 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:27:43.469116 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.470083 [debug] [ThreadPool]: On list_dbt: Close
[0m21:27:43.471953 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:27:43.472511 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:27:43.475696 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:27:43.475968 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:27:43.476159 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:27:43.482106 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.482383 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:27:43.482559 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:27:43.482821 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.483450 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:27:43.483743 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:27:43.483930 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:27:43.484577 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.484808 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:27:43.504820 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m21:27:43.508691 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:27:43.508961 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:27:43.509123 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:27:43.514734 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.515022 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:27:43.515329 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:27:43.531241 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.535381 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:27:43.536277 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:27:43.536525 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:27:43.538833 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:27:43.541348 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:27:43.541570 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:27:43.541804 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:27:43.547177 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.547551 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:27:43.547739 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:27:43.566524 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.567420 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:27:43.567662 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:27:43.567824 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:27:43.570021 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m21:27:43.571555 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:27:43.571772 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:27:43.571932 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:27:43.577774 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.578028 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:27:43.578199 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:27:43.595985 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.596875 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:27:43.597106 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:27:43.597262 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:27:43.599372 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m21:27:43.601734 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:27:43.601909 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:27:43.602054 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:27:43.607591 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.607845 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:27:43.608018 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:27:43.625828 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.626517 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:27:43.626742 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:27:43.626896 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:27:43.628825 [debug] [MainThread]: Using duckdb connection "master"
[0m21:27:43.629035 [debug] [MainThread]: On master: BEGIN
[0m21:27:43.629198 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:27:43.634847 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:27:43.635116 [debug] [MainThread]: On master: COMMIT
[0m21:27:43.635279 [debug] [MainThread]: Using duckdb connection "master"
[0m21:27:43.635433 [debug] [MainThread]: On master: COMMIT
[0m21:27:43.635640 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:27:43.635800 [debug] [MainThread]: On master: Close
[0m21:27:43.637191 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:27:43.637396 [info ] [MainThread]: 
[0m21:27:43.640657 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:27:43.640948 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_user ...................................... [RUN]
[0m21:27:43.641309 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_user)
[0m21:27:43.641507 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:27:43.647269 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:27:43.648507 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:27:43.641646 => 21:27:43.648227
[0m21:27:43.648822 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:27:43.668092 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:27:43.668729 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:27:43.668935 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:27:43.669111 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:27:43.674764 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:27:43.675059 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:27:43.675312 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m21:27:43.679614 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:27:43.683516 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:27:43.683775 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m21:27:43.684132 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:27:43.693599 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:27:43.693833 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:27:43.694015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:27:43.695041 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:27:43.698355 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:27:43.698583 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m21:27:43.698913 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:27:43.699681 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:27:43.648975 => 21:27:43.699584
[0m21:27:43.699897 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:27:43.714493 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_user ................................. [[32mOK[0m in 0.07s]
[0m21:27:43.714945 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:27:43.715752 [debug] [MainThread]: Using duckdb connection "master"
[0m21:27:43.715948 [debug] [MainThread]: On master: BEGIN
[0m21:27:43.716106 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:27:43.721795 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:27:43.722048 [debug] [MainThread]: On master: COMMIT
[0m21:27:43.722213 [debug] [MainThread]: Using duckdb connection "master"
[0m21:27:43.722365 [debug] [MainThread]: On master: COMMIT
[0m21:27:43.722567 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:27:43.722733 [debug] [MainThread]: On master: Close
[0m21:27:43.724249 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:27:43.724480 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:27:43.724668 [info ] [MainThread]: 
[0m21:27:43.724855 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m21:27:43.725201 [debug] [MainThread]: Command end result
[0m21:27:43.733945 [info ] [MainThread]: 
[0m21:27:43.734225 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:27:43.734388 [info ] [MainThread]: 
[0m21:27:43.734586 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:27:43.734995 [debug] [MainThread]: Command `dbt run` succeeded at 21:27:43.734945 after 0.47 seconds
[0m21:27:43.735196 [debug] [MainThread]: Flushing usage events


============================== 21:28:04.708645 | b143d771-27ab-4b67-a941-58ebe35d5318 ==============================
[0m21:28:04.708645 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:28:04.711408 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:28:04.711668 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:28:04.794503 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:28:04.814366 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:28:04.862259 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:28:04.862564 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:28:04.863534 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m21:28:04.877782 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:28:04.878666 [warn ] [MainThread]: The selection criterion 'models/dimensions/dim_record_type.sql' does not match any nodes
[0m21:28:04.879302 [info ] [MainThread]: 
[0m21:28:04.879482 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:28:04.879740 [debug] [MainThread]: Command end result
[0m21:28:04.886745 [debug] [MainThread]: Command `dbt run` succeeded at 21:28:04.886667 after 0.20 seconds
[0m21:28:04.886997 [debug] [MainThread]: Flushing usage events


============================== 21:28:21.524853 | 59de5ae6-a8f3-411e-8e10-adf5bfa648bd ==============================
[0m21:28:21.524853 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:28:21.527748 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select dim_record_type.sql', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:28:21.528009 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:28:21.605522 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:28:21.625406 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:28:21.671930 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:28:21.672252 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:28:21.673255 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:28:21.687381 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:28:21.688240 [warn ] [MainThread]: The selection criterion 'dim_record_type.sql' does not match any nodes
[0m21:28:21.688928 [info ] [MainThread]: 
[0m21:28:21.689122 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:28:21.689380 [debug] [MainThread]: Command end result
[0m21:28:21.696484 [debug] [MainThread]: Command `dbt run` succeeded at 21:28:21.696371 after 0.20 seconds
[0m21:28:21.696803 [debug] [MainThread]: Flushing usage events


============================== 21:28:50.879388 | 1b87cc3f-f2f1-4604-a2bf-3327eabcebc9 ==============================
[0m21:28:50.879388 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:28:50.881791 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:28:50.882040 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:28:50.957290 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:28:50.975865 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:28:51.011124 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:28:51.011438 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:28:51.012441 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:28:51.026319 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:28:51.027222 [warn ] [MainThread]: The selection criterion 'models/dimensions/dim_record_type.sql' does not match any nodes
[0m21:28:51.027862 [info ] [MainThread]: 
[0m21:28:51.028047 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:28:51.028305 [debug] [MainThread]: Command end result
[0m21:28:51.035704 [debug] [MainThread]: Command `dbt run` succeeded at 21:28:51.035590 after 0.17 seconds
[0m21:28:51.036145 [debug] [MainThread]: Flushing usage events


============================== 21:29:50.676266 | 3d0c7d58-e844-4d8e-b9fd-25eda35c8cd6 ==============================
[0m21:29:50.676266 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:29:50.679467 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:29:50.679767 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:29:50.756843 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:29:50.775160 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:29:50.811318 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:29:50.811796 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_record_type.sql
[0m21:29:50.830610 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_record_type (models/dimensions/dim_record_type.sql)
  expected token ',', got 'the'
    line 3
      schema='dim'  -- Specifies the schema for dimension tables
[0m21:29:50.831198 [debug] [MainThread]: Command `dbt run` failed at 21:29:50.831119 after 0.18 seconds
[0m21:29:50.831463 [debug] [MainThread]: Flushing usage events


============================== 21:30:00.324549 | c675d5e1-7a06-423b-8ce9-5ff68f40c061 ==============================
[0m21:30:00.324549 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:30:00.327533 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:30:00.327812 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:30:00.410112 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:30:00.428149 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:30:00.466629 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:30:00.467070 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_record_type.sql
[0m21:30:00.494784 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m21:30:00.508559 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:30:00.510149 [info ] [MainThread]: 
[0m21:30:00.510680 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:30:00.511267 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:30:00.518907 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:30:00.519409 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:30:00.519804 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:30:00.534780 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.535741 [debug] [ThreadPool]: On list_dbt: Close
[0m21:30:00.537607 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:30:00.538047 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:30:00.540966 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:00.541202 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:30:00.541378 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:00.547309 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.547675 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:00.547884 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:30:00.548206 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.548724 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:00.548902 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:00.549056 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:00.549284 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.549446 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:30:00.552231 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:30:00.555696 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:00.556047 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:30:00.556409 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:00.562292 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.562538 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:00.562730 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:30:00.582081 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.583137 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:30:00.584922 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:30:00.585154 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:30:00.589473 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m21:30:00.592514 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:00.592744 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:30:00.592924 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:00.600637 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.600933 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:00.601163 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:30:00.620042 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.620818 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:30:00.621057 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:30:00.621221 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:30:00.623576 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m21:30:00.625649 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:00.625833 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:30:00.625988 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:00.631577 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.631853 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:00.632040 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:30:00.647810 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.651341 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:30:00.651671 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:30:00.651927 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:30:00.654122 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:30:00.655594 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:00.656000 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:30:00.656173 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:00.661436 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.661674 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:00.661850 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:30:00.678027 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.681583 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:30:00.681829 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:30:00.682001 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:30:00.684808 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:00.685014 [debug] [MainThread]: On master: BEGIN
[0m21:30:00.685176 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:30:00.690738 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:00.691002 [debug] [MainThread]: On master: COMMIT
[0m21:30:00.691172 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:00.691332 [debug] [MainThread]: On master: COMMIT
[0m21:30:00.691538 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:00.691706 [debug] [MainThread]: On master: Close
[0m21:30:00.693239 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:30:00.693469 [info ] [MainThread]: 
[0m21:30:00.696486 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m21:30:00.696927 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_record_type ............................... [RUN]
[0m21:30:00.697320 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_record_type)
[0m21:30:00.697528 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m21:30:00.703204 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:00.703847 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 21:30:00.697677 => 21:30:00.703730
[0m21:30:00.704061 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m21:30:00.722835 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:00.723423 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:00.723626 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m21:30:00.723806 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:30:00.729670 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:00.730061 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:00.730302 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

SELECT
    id AS record_type_id,
    name AS record_type_name,
    modulenamespace AS module_namespace,
    description AS record_type_description,
    businessprocessid AS business_process_id,
    sobjecttype AS sobject_type,
    isactive AS is_active,
    createdbyid AS created_by_id,
    createddate AS created_date,
    lastmodifiedbyid AS last_modified_by_id,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."raw"."record_type"
WHERE
    isdeleted = FALSE
ORDER BY
    record_type_name;
    );
  
  
[0m21:30:00.730789 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 21:30:00.704336 => 21:30:00.730690
[0m21:30:00.730993 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: ROLLBACK
[0m21:30:00.738443 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_record_type'
[0m21:30:00.738746 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m21:30:00.740759 [debug] [Thread-1  ]: Runtime Error in model dim_record_type (models/dimensions/dim_record_type.sql)
  Parser Error: syntax error at or near ";"
[0m21:30:00.741357 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_record_type ...................... [[31mERROR[0m in 0.04s]
[0m21:30:00.741849 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m21:30:00.742692 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:00.742877 [debug] [MainThread]: On master: BEGIN
[0m21:30:00.743030 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:30:00.749213 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:00.749510 [debug] [MainThread]: On master: COMMIT
[0m21:30:00.749690 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:00.749865 [debug] [MainThread]: On master: COMMIT
[0m21:30:00.750083 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:00.750287 [debug] [MainThread]: On master: Close
[0m21:30:00.752052 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:30:00.752345 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_record_type' was properly closed.
[0m21:30:00.754793 [info ] [MainThread]: 
[0m21:30:00.755308 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m21:30:00.755892 [debug] [MainThread]: Command end result
[0m21:30:00.792347 [info ] [MainThread]: 
[0m21:30:00.792637 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:30:00.792809 [info ] [MainThread]: 
[0m21:30:00.792972 [error] [MainThread]:   Runtime Error in model dim_record_type (models/dimensions/dim_record_type.sql)
  Parser Error: syntax error at or near ";"
[0m21:30:00.793129 [info ] [MainThread]: 
[0m21:30:00.793300 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:30:00.793612 [debug] [MainThread]: Command `dbt run` failed at 21:30:00.793565 after 0.49 seconds
[0m21:30:00.793806 [debug] [MainThread]: Flushing usage events


============================== 21:30:06.839850 | 051bf2af-3efe-43cd-a170-56507136583f ==============================
[0m21:30:06.839850 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:30:06.842245 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:30:06.842505 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:30:06.916030 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:30:06.934679 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:30:06.969528 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:30:06.969974 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_record_type.sql
[0m21:30:06.998465 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:30:07.012666 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:30:07.014343 [info ] [MainThread]: 
[0m21:30:07.014822 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:30:07.015527 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:30:07.023888 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:30:07.024289 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:30:07.024482 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:30:07.032498 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.033578 [debug] [ThreadPool]: On list_dbt: Close
[0m21:30:07.035556 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:30:07.036058 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:30:07.039036 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:07.039269 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:30:07.039438 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:07.045498 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.045674 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:07.045824 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:30:07.046065 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.046542 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:07.046706 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:07.046851 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:07.047051 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.047210 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:30:07.050183 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:30:07.053382 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:07.053628 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:30:07.053780 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:07.059742 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.059993 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:07.060165 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:30:07.079026 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.079971 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:30:07.080350 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:30:07.080516 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:30:07.082637 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m21:30:07.084956 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:07.085125 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:30:07.085268 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:07.090879 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.091161 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:07.091366 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:30:07.110555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.111316 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:30:07.111550 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:30:07.111709 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:30:07.113829 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m21:30:07.116179 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:07.116468 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:30:07.116638 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:07.122780 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.123015 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:07.123190 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:30:07.138340 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.141793 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:30:07.142031 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:30:07.142191 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:30:07.144263 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:30:07.145815 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:07.145992 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:30:07.146144 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:07.151428 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.151653 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:07.151822 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:30:07.167471 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.171150 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:30:07.171388 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:30:07.171546 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:30:07.174408 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:07.174609 [debug] [MainThread]: On master: BEGIN
[0m21:30:07.174770 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:30:07.180223 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:07.180465 [debug] [MainThread]: On master: COMMIT
[0m21:30:07.180626 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:07.180839 [debug] [MainThread]: On master: COMMIT
[0m21:30:07.181116 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:07.181298 [debug] [MainThread]: On master: Close
[0m21:30:07.182705 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:30:07.182910 [info ] [MainThread]: 
[0m21:30:07.183961 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m21:30:07.184391 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_record_type ............................... [RUN]
[0m21:30:07.184755 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_record_type)
[0m21:30:07.184947 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m21:30:07.190253 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.190898 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 21:30:07.185086 => 21:30:07.190767
[0m21:30:07.191192 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m21:30:07.210462 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.211078 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.211285 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m21:30:07.211464 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:30:07.217162 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:07.217442 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.217660 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

SELECT
    id AS record_type_id,
    name AS record_type_name,
    modulenamespace AS module_namespace,
    description AS record_type_description,
    businessprocessid AS business_process_id,
    sobjecttype AS sobject_type,
    isactive AS is_active,
    createdbyid AS created_by_id,
    createddate AS created_date,
    lastmodifiedbyid AS last_modified_by_id,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."raw"."record_type"
WHERE
    isdeleted = FALSE
ORDER BY
    record_type_name
    );
  
  
[0m21:30:07.219368 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:07.223861 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.224111 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m21:30:07.224493 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:07.256930 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m21:30:07.257267 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.257459 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m21:30:07.258271 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:07.261236 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.261460 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m21:30:07.261763 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:07.262560 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 21:30:07.191423 => 21:30:07.262463
[0m21:30:07.262764 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m21:30:07.274473 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_record_type .......................... [[32mOK[0m in 0.09s]
[0m21:30:07.274860 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m21:30:07.275585 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:07.275763 [debug] [MainThread]: On master: BEGIN
[0m21:30:07.275911 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:30:07.281447 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:07.281737 [debug] [MainThread]: On master: COMMIT
[0m21:30:07.281903 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:07.282056 [debug] [MainThread]: On master: COMMIT
[0m21:30:07.282351 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:07.282656 [debug] [MainThread]: On master: Close
[0m21:30:07.284275 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:30:07.284521 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_record_type' was properly closed.
[0m21:30:07.284722 [info ] [MainThread]: 
[0m21:30:07.284911 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m21:30:07.285252 [debug] [MainThread]: Command end result
[0m21:30:07.292037 [info ] [MainThread]: 
[0m21:30:07.292314 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:30:07.292490 [info ] [MainThread]: 
[0m21:30:07.292670 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:30:07.293098 [debug] [MainThread]: Command `dbt run` succeeded at 21:30:07.292989 after 0.47 seconds
[0m21:30:07.293402 [debug] [MainThread]: Flushing usage events


============================== 21:30:27.137626 | 3f8a4e2f-c184-46ae-b3ed-7af5d0969fc5 ==============================
[0m21:30:27.137626 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:30:27.140032 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:30:27.140282 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:30:27.214054 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:30:27.232067 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:30:27.268395 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:30:27.268708 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:30:27.269683 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m21:30:27.283566 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:30:27.285087 [info ] [MainThread]: 
[0m21:30:27.285522 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:30:27.286194 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:30:27.293348 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:30:27.293617 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:30:27.293802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:30:27.300894 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.301805 [debug] [ThreadPool]: On list_dbt: Close
[0m21:30:27.303804 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:30:27.304366 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:30:27.307422 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:27.307648 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:30:27.307813 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:27.314039 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.314292 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:27.314463 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:30:27.314712 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.315225 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:27.315464 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:27.315657 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:27.315902 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.316067 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:30:27.318842 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m21:30:27.322301 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:27.322548 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:30:27.322722 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:27.328415 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.328664 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:27.328844 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:30:27.344854 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.348303 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:30:27.348749 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:30:27.348922 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:30:27.351162 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:30:27.352860 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:27.353062 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:30:27.353221 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:27.358873 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.359127 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:27.359298 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:30:27.374683 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.378359 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:30:27.378615 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:30:27.378784 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:30:27.380960 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:30:27.383188 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:27.383373 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:30:27.383523 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:27.389305 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.389552 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:27.389725 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:30:27.407295 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.407977 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:30:27.408196 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:30:27.408349 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:30:27.410335 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m21:30:27.411800 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:27.411990 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:30:27.412143 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:27.417514 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.417748 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:27.417918 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:30:27.436404 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.437430 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:30:27.437665 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:30:27.437824 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:30:27.440985 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:27.441234 [debug] [MainThread]: On master: BEGIN
[0m21:30:27.441397 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:30:27.447223 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:27.447487 [debug] [MainThread]: On master: COMMIT
[0m21:30:27.447666 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:27.447834 [debug] [MainThread]: On master: COMMIT
[0m21:30:27.448049 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:27.448218 [debug] [MainThread]: On master: Close
[0m21:30:27.449770 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:30:27.450011 [info ] [MainThread]: 
[0m21:30:27.451947 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m21:30:27.452234 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_record_type ............................... [RUN]
[0m21:30:27.452594 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_record_type)
[0m21:30:27.452786 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m21:30:27.458038 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:27.458619 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 21:30:27.452921 => 21:30:27.458498
[0m21:30:27.458824 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m21:30:27.477649 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:27.478112 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:27.478311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m21:30:27.478490 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:30:27.484370 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:27.484664 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:27.484873 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

SELECT
    id AS record_type_id,
    name AS record_type_name,
    modulenamespace AS module_namespace,
    description AS record_type_description,
    businessprocessid AS business_process_id,
    sobjecttype AS sobject_type,
    isactive AS is_active,
    createdbyid AS created_by_id,
    createddate AS created_date,
    lastmodifiedbyid AS last_modified_by_id,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."staging"."stg_salesforce__record_type"
WHERE
    isdeleted = FALSE
ORDER BY
    record_type_name
    );
  
  
[0m21:30:27.485659 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 21:30:27.458958 => 21:30:27.485559
[0m21:30:27.485859 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: ROLLBACK
[0m21:30:27.487172 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_record_type'
[0m21:30:27.487366 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m21:30:27.489309 [debug] [Thread-1  ]: Runtime Error in model dim_record_type (models/dimensions/dim_record_type.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__record_type.name"
  LINE 14:     id AS record_type_id,
               ^
[0m21:30:27.489721 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_record_type ...................... [[31mERROR[0m in 0.04s]
[0m21:30:27.490064 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m21:30:27.490979 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:27.491162 [debug] [MainThread]: On master: BEGIN
[0m21:30:27.491310 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:30:27.496856 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:27.497115 [debug] [MainThread]: On master: COMMIT
[0m21:30:27.497271 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:27.497420 [debug] [MainThread]: On master: COMMIT
[0m21:30:27.497620 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:27.497777 [debug] [MainThread]: On master: Close
[0m21:30:27.499222 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:30:27.499400 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_record_type' was properly closed.
[0m21:30:27.499587 [info ] [MainThread]: 
[0m21:30:27.499785 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m21:30:27.500129 [debug] [MainThread]: Command end result
[0m21:30:27.507289 [info ] [MainThread]: 
[0m21:30:27.507537 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:30:27.507698 [info ] [MainThread]: 
[0m21:30:27.507861 [error] [MainThread]:   Runtime Error in model dim_record_type (models/dimensions/dim_record_type.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__record_type.name"
  LINE 14:     id AS record_type_id,
               ^
[0m21:30:27.508027 [info ] [MainThread]: 
[0m21:30:27.508213 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:30:27.508831 [debug] [MainThread]: Command `dbt run` failed at 21:30:27.508718 after 0.39 seconds
[0m21:30:27.509169 [debug] [MainThread]: Flushing usage events


============================== 21:30:47.471326 | d68433ed-97e8-4159-8bc7-a2ae2865186c ==============================
[0m21:30:47.471326 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:30:47.473788 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:30:47.474053 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:30:47.549484 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:30:47.567191 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:30:47.604266 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:30:47.604608 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:30:47.605601 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:30:47.619208 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:30:47.620580 [info ] [MainThread]: 
[0m21:30:47.621011 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:30:47.621580 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:30:47.628630 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:30:47.628911 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:30:47.629092 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:30:47.636937 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.637867 [debug] [ThreadPool]: On list_dbt: Close
[0m21:30:47.639769 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:30:47.640136 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:30:47.642948 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:47.643143 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:30:47.643296 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:47.649220 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.649490 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:47.649803 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:30:47.650199 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.650738 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:47.650927 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:47.651084 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:47.651309 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.651474 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:30:47.654549 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m21:30:47.658030 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:47.658233 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:30:47.658396 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:47.664422 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.664678 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:47.664859 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:30:47.680323 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.683831 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:30:47.684255 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:30:47.684424 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:30:47.686728 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:30:47.688464 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:47.688652 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:30:47.688806 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:47.694394 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.694650 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:47.694824 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:30:47.710596 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.714154 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:30:47.714529 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:30:47.714786 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:30:47.716928 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:30:47.718592 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:47.718850 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:30:47.719105 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:47.725586 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.725840 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:47.726021 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:30:47.744236 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.745050 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:30:47.745300 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:30:47.745463 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:30:47.747776 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m21:30:47.749450 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:47.749681 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:30:47.749830 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:47.755576 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.755848 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:47.756025 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:30:47.773768 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.774704 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:30:47.774936 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:30:47.775110 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:30:47.778205 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:47.778417 [debug] [MainThread]: On master: BEGIN
[0m21:30:47.778579 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:30:47.784478 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:47.784755 [debug] [MainThread]: On master: COMMIT
[0m21:30:47.784919 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:47.785067 [debug] [MainThread]: On master: COMMIT
[0m21:30:47.785271 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:47.785429 [debug] [MainThread]: On master: Close
[0m21:30:47.786947 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:30:47.787175 [info ] [MainThread]: 
[0m21:30:47.789207 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m21:30:47.789507 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_record_type ............................... [RUN]
[0m21:30:47.789883 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_record_type)
[0m21:30:47.790081 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m21:30:47.795493 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.796116 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 21:30:47.790220 => 21:30:47.796001
[0m21:30:47.796331 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m21:30:47.815715 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.816279 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.816481 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m21:30:47.816660 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:30:47.822392 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.822688 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.822908 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

SELECT
    record_type_id,
    name AS record_type_name,
    modulenamespace AS module_namespace,
    description AS record_type_description,
    businessprocessid AS business_process_id,
    sobjecttype AS sobject_type,
    isactive AS is_active,
    createdbyid AS created_by_id,
    createddate AS created_date,
    lastmodifiedbyid AS last_modified_by_id,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."staging"."stg_salesforce__record_type"
WHERE
    isdeleted = FALSE
ORDER BY
    record_type_name
    );
  
  
[0m21:30:47.824409 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.828082 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.828338 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m21:30:47.828701 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.830458 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.830663 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m21:30:47.830967 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.841224 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m21:30:47.841452 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.841638 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m21:30:47.842421 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.845983 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.846230 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m21:30:47.846681 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.847436 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 21:30:47.796473 => 21:30:47.847344
[0m21:30:47.847639 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m21:30:47.859485 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_record_type .......................... [[32mOK[0m in 0.07s]
[0m21:30:47.859881 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m21:30:47.860627 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:47.860827 [debug] [MainThread]: On master: BEGIN
[0m21:30:47.860985 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:30:47.867172 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:47.867427 [debug] [MainThread]: On master: COMMIT
[0m21:30:47.867599 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:47.867763 [debug] [MainThread]: On master: COMMIT
[0m21:30:47.867983 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:47.868157 [debug] [MainThread]: On master: Close
[0m21:30:47.869595 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:30:47.869802 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_record_type' was properly closed.
[0m21:30:47.869993 [info ] [MainThread]: 
[0m21:30:47.870193 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m21:30:47.870531 [debug] [MainThread]: Command end result
[0m21:30:47.877554 [info ] [MainThread]: 
[0m21:30:47.877800 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:30:47.877969 [info ] [MainThread]: 
[0m21:30:47.878154 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:30:47.878618 [debug] [MainThread]: Command `dbt run` succeeded at 21:30:47.878555 after 0.42 seconds
[0m21:30:47.878835 [debug] [MainThread]: Flushing usage events


============================== 21:31:59.894984 | 3e0fedb3-a980-42bf-bfc3-542304061ef6 ==============================
[0m21:31:59.894984 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:31:59.897350 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:31:59.897594 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:31:59.972085 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:31:59.990128 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:32:00.026202 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:32:00.026734 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/staging/stg_salesforce__solution.sql
[0m21:32:00.045404 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_salesforce__solution (models/staging/stg_salesforce__solution.sql)
  expected token ',', got 'the'
    line 3
      schema='dim'  -- Ensures the model is written to the 'dim' schema
[0m21:32:00.045952 [debug] [MainThread]: Command `dbt run` failed at 21:32:00.045885 after 0.17 seconds
[0m21:32:00.046179 [debug] [MainThread]: Flushing usage events


============================== 21:32:05.575479 | b9e35f89-a836-4488-9d58-05ffb5034810 ==============================
[0m21:32:05.575479 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:32:05.577932 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:32:05.578174 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:32:05.652287 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:32:05.670975 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:32:05.713297 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:32:05.713811 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/staging/stg_salesforce__solution.sql
[0m21:32:05.732665 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_salesforce__solution (models/staging/stg_salesforce__solution.sql)
  expected token ',', got 'the'
    line 3
      schema='dim'  -- Ensures the model is written to the 'dim' schema
[0m21:32:05.733142 [debug] [MainThread]: Command `dbt run` failed at 21:32:05.733081 after 0.17 seconds
[0m21:32:05.733356 [debug] [MainThread]: Flushing usage events


============================== 21:32:17.396327 | 114e6dc1-4619-48e9-a16e-0e24fc5eb251 ==============================
[0m21:32:17.396327 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:32:17.398726 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:32:17.398976 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:32:17.476924 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:32:17.495165 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:32:17.532675 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:32:17.532989 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:32:17.533985 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m21:32:17.546341 [error] [MainThread]: Encountered an error:
Found a cycle: model.elastic_dbt_interview.stg_salesforce__solution
[0m21:32:17.548884 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 87, in wrapper
    result, success = func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 143, in wrapper
    return func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 172, in wrapper
    return func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 219, in wrapper
    return func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/main.py", line 596, in run
    results = task.run()
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/task/runnable.py", line 449, in run
    self._runtime_initialize()
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/task/compile.py", line 147, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/task/runnable.py", line 142, in _runtime_initialize
    self.compile_manifest()
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/task/base.py", line 173, in compile_manifest
    self.graph = compiler.compile(self.manifest)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/compilation.py", line 453, in compile
    linker.link_graph(manifest)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/compilation.py", line 198, in link_graph
    raise RuntimeError("Found a cycle: {}".format(cycle))
RuntimeError: Found a cycle: model.elastic_dbt_interview.stg_salesforce__solution

[0m21:32:17.549716 [debug] [MainThread]: Command `dbt run` failed at 21:32:17.549622 after 0.17 seconds
[0m21:32:17.549973 [debug] [MainThread]: Flushing usage events


============================== 21:35:42.330365 | be7326a5-44c4-4143-8ead-a93eb2654bae ==============================
[0m21:35:42.330365 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:35:42.333075 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:35:42.333342 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:35:42.410802 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:35:42.429388 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:35:42.466034 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:35:42.466334 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:35:42.467318 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:35:42.481174 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:35:42.482564 [info ] [MainThread]: 
[0m21:35:42.482993 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:35:42.483663 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:35:42.490613 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:35:42.490864 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:35:42.491042 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:35:42.498657 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.499600 [debug] [ThreadPool]: On list_dbt: Close
[0m21:35:42.501572 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:35:42.502009 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:35:42.504827 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:35:42.505031 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:35:42.505190 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:35:42.511266 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.511653 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:35:42.511861 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:35:42.512238 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.512807 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:35:42.512980 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:35:42.513131 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:35:42.513353 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.513517 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:35:42.516652 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m21:35:42.520485 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:35:42.520717 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:35:42.520884 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:35:42.526655 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.526942 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:35:42.527215 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:35:42.542726 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.546521 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:35:42.547869 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:35:42.548065 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:35:42.550263 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m21:35:42.552079 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:35:42.552286 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:35:42.552440 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:35:42.558581 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.558827 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:35:42.558999 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:35:42.577223 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.578256 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:35:42.578501 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:35:42.578666 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:35:42.580972 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:35:42.583697 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:35:42.583904 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:35:42.584054 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:35:42.589534 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.589801 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:35:42.589982 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:35:42.606665 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.610357 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:35:42.610611 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:35:42.610782 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:35:42.613175 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:35:42.614830 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:35:42.615826 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:35:42.616002 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:35:42.622082 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.622321 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:35:42.622497 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:35:42.641369 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.643281 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:35:42.643624 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:35:42.643807 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:35:42.646308 [debug] [MainThread]: Using duckdb connection "master"
[0m21:35:42.646581 [debug] [MainThread]: On master: BEGIN
[0m21:35:42.646767 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:35:42.652693 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:35:42.652989 [debug] [MainThread]: On master: COMMIT
[0m21:35:42.653155 [debug] [MainThread]: Using duckdb connection "master"
[0m21:35:42.653311 [debug] [MainThread]: On master: COMMIT
[0m21:35:42.653514 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:35:42.653682 [debug] [MainThread]: On master: Close
[0m21:35:42.655381 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:35:42.655683 [info ] [MainThread]: 
[0m21:35:42.658007 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m21:35:42.658368 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_solution .................................. [RUN]
[0m21:35:42.658818 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_solution)
[0m21:35:42.659032 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m21:35:42.664684 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m21:35:42.665726 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 21:35:42.659179 => 21:35:42.665551
[0m21:35:42.665992 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m21:35:42.687509 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m21:35:42.688152 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:35:42.688368 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m21:35:42.688548 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:35:42.694348 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:35:42.694622 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:35:42.694825 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solution_name,
    status,
    is_active,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__solution";
    );
  
  
[0m21:35:42.695275 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 21:35:42.666135 => 21:35:42.695174
[0m21:35:42.695472 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: ROLLBACK
[0m21:35:42.699359 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_solution'
[0m21:35:42.699654 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m21:35:42.701656 [debug] [Thread-1  ]: Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Parser Error: syntax error at or near ";"
[0m21:35:42.702077 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_solution ......................... [[31mERROR[0m in 0.04s]
[0m21:35:42.702465 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m21:35:42.703229 [debug] [MainThread]: Using duckdb connection "master"
[0m21:35:42.703417 [debug] [MainThread]: On master: BEGIN
[0m21:35:42.703572 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:35:42.709217 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:35:42.709448 [debug] [MainThread]: On master: COMMIT
[0m21:35:42.709619 [debug] [MainThread]: Using duckdb connection "master"
[0m21:35:42.709772 [debug] [MainThread]: On master: COMMIT
[0m21:35:42.709968 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:35:42.710124 [debug] [MainThread]: On master: Close
[0m21:35:42.711757 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:35:42.711962 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_solution' was properly closed.
[0m21:35:42.712149 [info ] [MainThread]: 
[0m21:35:42.712339 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m21:35:42.712677 [debug] [MainThread]: Command end result
[0m21:35:42.719961 [info ] [MainThread]: 
[0m21:35:42.720220 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:35:42.720385 [info ] [MainThread]: 
[0m21:35:42.720680 [error] [MainThread]:   Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Parser Error: syntax error at or near ";"
[0m21:35:42.720933 [info ] [MainThread]: 
[0m21:35:42.721148 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:35:42.721560 [debug] [MainThread]: Command `dbt run` failed at 21:35:42.721504 after 0.41 seconds
[0m21:35:42.721776 [debug] [MainThread]: Flushing usage events


============================== 21:36:13.863390 | c1d618e7-94b9-48e6-a75a-f1f73eb00789 ==============================
[0m21:36:13.863390 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:36:13.866783 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:36:13.867035 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:36:13.950424 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:36:13.968133 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:36:14.004643 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:36:14.005137 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_solution.sql
[0m21:36:14.024253 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_solution (models/dimensions/dim_solution.sql)
  expected token ',', got 'the'
    line 3
      schema='dim'  -- Ensures the model is written to the 'dim' schema
[0m21:36:14.024770 [debug] [MainThread]: Command `dbt run` failed at 21:36:14.024704 after 0.18 seconds
[0m21:36:14.024990 [debug] [MainThread]: Flushing usage events


============================== 21:36:23.056459 | 05372af6-6191-4ac1-9a4e-7d0d86ad4ec7 ==============================
[0m21:36:23.056459 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:36:23.058950 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:36:23.059202 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:36:23.133272 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:36:23.151647 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:36:23.187213 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:36:23.187503 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:36:23.188502 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:36:23.203276 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:36:23.204662 [info ] [MainThread]: 
[0m21:36:23.205109 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:36:23.205834 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:36:23.213496 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:36:23.213786 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:36:23.213961 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:36:23.227175 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.228071 [debug] [ThreadPool]: On list_dbt: Close
[0m21:36:23.230072 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:36:23.230585 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:36:23.233613 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:36:23.233843 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:36:23.234007 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:23.240200 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.240473 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:36:23.240646 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:36:23.240918 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.241515 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:36:23.241697 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:36:23.241849 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:36:23.242079 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.242248 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:36:23.245308 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:36:23.248785 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:36:23.249007 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:36:23.249166 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:23.255257 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.255521 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:36:23.255697 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:36:23.274524 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.275583 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:36:23.278997 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:36:23.279189 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:36:23.281461 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m21:36:23.283836 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:36:23.284016 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:36:23.284166 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:23.290015 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.290267 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:36:23.290437 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:36:23.305654 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.309020 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:36:23.309239 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:36:23.309393 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:36:23.311486 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m21:36:23.313122 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:36:23.313303 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:36:23.313454 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:23.318853 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.319097 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:36:23.319265 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:36:23.336787 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.337468 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:36:23.337686 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:36:23.337841 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:36:23.339938 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m21:36:23.341365 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:36:23.341545 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:36:23.341703 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:23.347044 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.347285 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:36:23.347463 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:36:23.363463 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.366955 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:36:23.367178 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:36:23.367529 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:36:23.370701 [debug] [MainThread]: Using duckdb connection "master"
[0m21:36:23.370934 [debug] [MainThread]: On master: BEGIN
[0m21:36:23.371109 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:36:23.377304 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:36:23.377564 [debug] [MainThread]: On master: COMMIT
[0m21:36:23.377729 [debug] [MainThread]: Using duckdb connection "master"
[0m21:36:23.377883 [debug] [MainThread]: On master: COMMIT
[0m21:36:23.378089 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:36:23.378251 [debug] [MainThread]: On master: Close
[0m21:36:23.379877 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:36:23.380190 [info ] [MainThread]: 
[0m21:36:23.383820 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m21:36:23.384135 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_solution .................................. [RUN]
[0m21:36:23.384520 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_solution)
[0m21:36:23.384719 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m21:36:23.390262 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.390872 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 21:36:23.384861 => 21:36:23.390762
[0m21:36:23.391082 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m21:36:23.410390 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.410982 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.411188 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m21:36:23.411370 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:36:23.417436 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:36:23.417729 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.417973 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

SELECT
    id AS solution_id,
    solutionnumber AS solution_number,
    solutionname AS solution_name,
    ispublished AS is_published,
    ispublishedinpublickb AS is_published_in_public_kb,
    status AS solution_status,
    isreviewed AS is_reviewed,
    solutionnote AS solution_note,
    caseid AS case_id,
    ownerid AS owner_id,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    timesused AS times_used,
    ishtml AS is_html
FROM
    "dbt"."raw"."solution"
WHERE
    isdeleted = FALSE
ORDER BY
    solution_name
    );
  
  
[0m21:36:23.420008 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:36:23.423832 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.424090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m21:36:23.424453 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:36:23.433765 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m21:36:23.433999 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.434184 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m21:36:23.435034 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:36:23.437911 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.438149 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m21:36:23.438441 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:36:23.439201 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 21:36:23.391220 => 21:36:23.439108
[0m21:36:23.439405 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m21:36:23.462183 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_solution ............................. [[32mOK[0m in 0.08s]
[0m21:36:23.462603 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m21:36:23.463370 [debug] [MainThread]: Using duckdb connection "master"
[0m21:36:23.463570 [debug] [MainThread]: On master: BEGIN
[0m21:36:23.463731 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:36:23.474852 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:36:23.476859 [debug] [MainThread]: On master: COMMIT
[0m21:36:23.477069 [debug] [MainThread]: Using duckdb connection "master"
[0m21:36:23.477243 [debug] [MainThread]: On master: COMMIT
[0m21:36:23.483467 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:36:23.484220 [debug] [MainThread]: On master: Close
[0m21:36:23.492411 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:36:23.492691 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_solution' was properly closed.
[0m21:36:23.492882 [info ] [MainThread]: 
[0m21:36:23.493078 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m21:36:23.493420 [debug] [MainThread]: Command end result
[0m21:36:23.503036 [info ] [MainThread]: 
[0m21:36:23.503337 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:36:23.503528 [info ] [MainThread]: 
[0m21:36:23.503730 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:36:23.504103 [debug] [MainThread]: Command `dbt run` succeeded at 21:36:23.504054 after 0.46 seconds
[0m21:36:23.504301 [debug] [MainThread]: Flushing usage events


============================== 21:38:49.988415 | b7e638c6-95d4-43ea-ac03-8d193bb7829f ==============================
[0m21:38:49.988415 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:38:49.991502 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_product.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:38:49.991801 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:38:50.069853 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:38:50.087579 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:38:50.125532 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:38:50.125818 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:38:50.126772 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:38:50.141216 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:38:50.142670 [info ] [MainThread]: 
[0m21:38:50.143100 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:38:50.143765 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:38:50.151115 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:38:50.151461 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:38:50.151643 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:38:50.159987 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.160977 [debug] [ThreadPool]: On list_dbt: Close
[0m21:38:50.163081 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:38:50.163546 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:38:50.166556 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:50.166810 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:38:50.166969 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:50.173303 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.173567 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:50.173732 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:38:50.173989 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.174495 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:38:50.174665 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:50.174813 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:38:50.175023 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.175180 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:38:50.178351 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_fact)
[0m21:38:50.181930 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:38:50.182151 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:38:50.182305 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:50.188736 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.189032 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:38:50.189212 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:38:50.207657 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.208458 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:38:50.209897 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:38:50.210075 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:38:50.212173 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m21:38:50.213726 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:38:50.213909 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:38:50.214063 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:50.219528 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.219769 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:38:50.219939 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:38:50.235327 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.238741 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:38:50.239008 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:38:50.239184 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:38:50.245315 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m21:38:50.247197 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:38:50.248396 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:38:50.248620 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:50.255338 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.255626 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:38:50.255801 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:38:50.271557 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.275479 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:38:50.275737 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:38:50.275905 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:38:50.278297 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m21:38:50.280093 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:38:50.280278 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:38:50.280426 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:50.286252 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.286507 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:38:50.286678 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:38:50.304318 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.305201 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:38:50.305428 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:38:50.305586 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:38:50.308963 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:50.309153 [debug] [MainThread]: On master: BEGIN
[0m21:38:50.309303 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:38:50.315419 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:50.315698 [debug] [MainThread]: On master: COMMIT
[0m21:38:50.315865 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:50.316015 [debug] [MainThread]: On master: COMMIT
[0m21:38:50.316223 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:50.316414 [debug] [MainThread]: On master: Close
[0m21:38:50.318150 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:38:50.318416 [info ] [MainThread]: 
[0m21:38:50.321672 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m21:38:50.321974 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_product ................................... [RUN]
[0m21:38:50.322375 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_product)
[0m21:38:50.322594 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m21:38:50.328440 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m21:38:50.329467 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 21:38:50.322736 => 21:38:50.329266
[0m21:38:50.329781 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m21:38:50.350264 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m21:38:50.350967 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:50.351189 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m21:38:50.351374 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:38:50.357813 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:50.358117 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:50.358356 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

SELECT
    product_id,
    name AS product_name,
    productcode AS product_code,
    description AS product_description,
    isactive AS is_active,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    family AS product_family,
    externaldatasourceid AS external_datasource_id,
    externalid AS external_id,
    displayurl AS display_url,
    quantityunitofmeasure AS quantity_unit_of_measure,
    stockkeepingunit AS stock_keeping_unit,
    type AS product_type,
    productclass AS product_class,
    sourceproductid AS source_product_id,
    sellerid AS seller_id
FROM
    "dbt"."staging"."stg_salesforce__product_2"
WHERE
    isdeleted = FALSE
    --isactive =1 ?
ORDER BY
    product_name;
    );
  
  
[0m21:38:50.358919 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 21:38:50.329949 => 21:38:50.358813
[0m21:38:50.359128 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: ROLLBACK
[0m21:38:50.363022 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_product'
[0m21:38:50.363244 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m21:38:50.365109 [debug] [Thread-1  ]: Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Parser Error: syntax error at or near ";"
[0m21:38:50.365539 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_product .......................... [[31mERROR[0m in 0.04s]
[0m21:38:50.365886 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m21:38:50.366676 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:50.366849 [debug] [MainThread]: On master: BEGIN
[0m21:38:50.366995 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:38:50.372774 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:50.373031 [debug] [MainThread]: On master: COMMIT
[0m21:38:50.373195 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:50.373348 [debug] [MainThread]: On master: COMMIT
[0m21:38:50.373550 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:50.373750 [debug] [MainThread]: On master: Close
[0m21:38:50.375198 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:38:50.375370 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_product' was properly closed.
[0m21:38:50.375547 [info ] [MainThread]: 
[0m21:38:50.375728 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m21:38:50.376057 [debug] [MainThread]: Command end result
[0m21:38:50.383431 [info ] [MainThread]: 
[0m21:38:50.383738 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:38:50.383906 [info ] [MainThread]: 
[0m21:38:50.384242 [error] [MainThread]:   Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Parser Error: syntax error at or near ";"
[0m21:38:50.384470 [info ] [MainThread]: 
[0m21:38:50.384670 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:38:50.385169 [debug] [MainThread]: Command `dbt run` failed at 21:38:50.385092 after 0.42 seconds
[0m21:38:50.385423 [debug] [MainThread]: Flushing usage events


============================== 21:38:58.329733 | 11f06eb1-aafb-412e-a571-8498f66b0419 ==============================
[0m21:38:58.329733 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:38:58.332615 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_product.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:38:58.332949 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:38:58.414342 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:38:58.432247 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:38:58.467308 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:38:58.467600 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:38:58.468534 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m21:38:58.482916 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:38:58.484558 [info ] [MainThread]: 
[0m21:38:58.485065 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:38:58.485653 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:38:58.494809 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:38:58.496069 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:38:58.496352 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:38:58.530348 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.532678 [debug] [ThreadPool]: On list_dbt: Close
[0m21:38:58.535122 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:38:58.535656 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:38:58.538479 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:58.538673 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:38:58.538827 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:58.545218 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.545492 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:58.545656 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:38:58.545912 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.546427 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:38:58.546591 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:58.546737 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:38:58.546945 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.547104 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:38:58.549866 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m21:38:58.553320 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:38:58.553533 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:38:58.553692 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:58.559890 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.560226 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:38:58.560472 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:38:58.576490 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.579994 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:38:58.581393 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:38:58.581602 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:38:58.583845 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m21:38:58.586720 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:38:58.586926 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:38:58.587087 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:58.592768 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.593040 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:38:58.593238 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:38:58.612209 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.613190 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:38:58.613436 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:38:58.613596 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:38:58.616089 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m21:38:58.618478 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:38:58.618661 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:38:58.618808 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:58.624743 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.624994 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:38:58.625164 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:38:58.640324 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.643594 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:38:58.643820 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:38:58.643978 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:38:58.646314 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m21:38:58.648042 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:38:58.648223 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:38:58.648370 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:58.654090 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.654337 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:38:58.654509 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:38:58.672411 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.673125 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:38:58.673356 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:38:58.673515 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:38:58.675593 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:58.675793 [debug] [MainThread]: On master: BEGIN
[0m21:38:58.675953 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:38:58.681709 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:58.681963 [debug] [MainThread]: On master: COMMIT
[0m21:38:58.682126 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:58.682276 [debug] [MainThread]: On master: COMMIT
[0m21:38:58.682490 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:58.682651 [debug] [MainThread]: On master: Close
[0m21:38:58.684089 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:38:58.684294 [info ] [MainThread]: 
[0m21:38:58.687390 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m21:38:58.687666 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_product ................................... [RUN]
[0m21:38:58.688023 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_product)
[0m21:38:58.688211 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m21:38:58.693812 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m21:38:58.694459 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 21:38:58.688343 => 21:38:58.694349
[0m21:38:58.694680 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m21:38:58.713827 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m21:38:58.714305 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:58.714510 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m21:38:58.714693 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:38:58.720643 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:58.720945 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:58.721187 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

SELECT
    product_id,
    name AS product_name,
    productcode AS product_code,
    description AS product_description,
    isactive AS is_active,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    family AS product_family,
    externaldatasourceid AS external_datasource_id,
    externalid AS external_id,
    displayurl AS display_url,
    quantityunitofmeasure AS quantity_unit_of_measure,
    stockkeepingunit AS stock_keeping_unit,
    type AS product_type,
    productclass AS product_class,
    sourceproductid AS source_product_id,
    sellerid AS seller_id
FROM
    "dbt"."staging"."stg_salesforce__product_2"
WHERE
    isdeleted = FALSE
    --isactive =1 ?
ORDER BY
    product_name
    );
  
  
[0m21:38:58.723166 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:58.727088 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:58.727331 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m21:38:58.727693 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:58.737080 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m21:38:58.737308 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:58.737497 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m21:38:58.738360 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:58.741330 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:58.741567 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m21:38:58.741852 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:58.742598 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 21:38:58.694819 => 21:38:58.742506
[0m21:38:58.742800 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m21:38:58.759210 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_product .............................. [[32mOK[0m in 0.07s]
[0m21:38:58.759621 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m21:38:58.760467 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:58.760671 [debug] [MainThread]: On master: BEGIN
[0m21:38:58.760828 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:38:58.766599 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:58.766833 [debug] [MainThread]: On master: COMMIT
[0m21:38:58.766993 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:58.767141 [debug] [MainThread]: On master: COMMIT
[0m21:38:58.767336 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:58.767732 [debug] [MainThread]: On master: Close
[0m21:38:58.769643 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:38:58.769931 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_product' was properly closed.
[0m21:38:58.770127 [info ] [MainThread]: 
[0m21:38:58.770331 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m21:38:58.770696 [debug] [MainThread]: Command end result
[0m21:38:58.778596 [info ] [MainThread]: 
[0m21:38:58.778876 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:38:58.779074 [info ] [MainThread]: 
[0m21:38:58.779268 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:38:58.779656 [debug] [MainThread]: Command `dbt run` succeeded at 21:38:58.779605 after 0.47 seconds
[0m21:38:58.779866 [debug] [MainThread]: Flushing usage events


============================== 21:40:11.828890 | 528fe3d3-49e3-4538-a7ee-5dfb34aa9dfe ==============================
[0m21:40:11.828890 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:40:11.831932 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_pricebook.sql', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m21:40:11.832192 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:40:11.915443 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:40:11.933690 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:40:11.975229 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:40:11.975527 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:40:11.976549 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m21:40:11.990723 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:40:11.992199 [info ] [MainThread]: 
[0m21:40:11.992660 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:40:11.993394 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:40:12.001099 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:40:12.001401 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:40:12.001586 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:40:12.015381 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.016263 [debug] [ThreadPool]: On list_dbt: Close
[0m21:40:12.018252 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:40:12.018781 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:40:12.021770 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:12.021980 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:40:12.022143 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:12.028242 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.028519 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:12.028690 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:40:12.028948 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.029478 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:40:12.029650 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:12.029799 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:40:12.030075 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.030259 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:40:12.033365 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m21:40:12.036772 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:40:12.037016 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:40:12.037177 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:12.043437 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.043721 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:40:12.043901 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:40:12.059725 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.063292 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:40:12.063985 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:40:12.064186 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:40:12.066382 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:40:12.068895 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:40:12.069112 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:40:12.069275 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:12.075286 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.075544 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:40:12.075717 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:40:12.095313 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.096135 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:40:12.096373 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:40:12.096537 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:40:12.098959 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m21:40:12.100633 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:40:12.100850 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:40:12.101019 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:12.108842 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.109191 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:40:12.109391 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:40:12.128505 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.132769 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:40:12.133126 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:40:12.133305 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:40:12.135923 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m21:40:12.137716 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:40:12.138340 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:40:12.138585 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:12.144820 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.145120 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:40:12.145321 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:40:12.164902 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.166038 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:40:12.166348 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:40:12.166516 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:40:12.169742 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:12.169948 [debug] [MainThread]: On master: BEGIN
[0m21:40:12.170109 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:40:12.176284 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:12.176556 [debug] [MainThread]: On master: COMMIT
[0m21:40:12.176719 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:12.176884 [debug] [MainThread]: On master: COMMIT
[0m21:40:12.177094 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:12.177256 [debug] [MainThread]: On master: Close
[0m21:40:12.179006 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:40:12.179328 [info ] [MainThread]: 
[0m21:40:12.182215 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m21:40:12.182552 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_pricebook ................................. [RUN]
[0m21:40:12.182988 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_pricebook)
[0m21:40:12.183201 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m21:40:12.188803 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:12.189887 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 21:40:12.183343 => 21:40:12.189691
[0m21:40:12.190173 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m21:40:12.209324 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:12.210412 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:12.210686 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m21:40:12.210873 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:40:12.216869 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:40:12.217145 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:12.217368 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

SELECT
    pricebook_entry_id,
    pricebook2id AS pricebook_id,
    product2id AS product_id,
    unitprice AS unit_price,
    isactive AS is_active,
    usestandardprice AS use_standard_price,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."raw"."pricebook_entry"
WHERE
    isdeleted = FALSE  -- Exclude deleted entries
ORDER BY
    pricebook_entry_id
    );
  
  
[0m21:40:12.218070 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 21:40:12.190325 => 21:40:12.217965
[0m21:40:12.218294 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: ROLLBACK
[0m21:40:12.221990 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_pricebook'
[0m21:40:12.222237 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m21:40:12.224116 [debug] [Thread-1  ]: Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "pricebook_entry_id" not found in FROM clause!
  Candidate bindings: "pricebook_entry.pricebook2id"
  LINE 14:     pricebook_entry_id,
               ^
[0m21:40:12.224520 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_pricebook ........................ [[31mERROR[0m in 0.04s]
[0m21:40:12.224855 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m21:40:12.225621 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:12.225830 [debug] [MainThread]: On master: BEGIN
[0m21:40:12.225993 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:40:12.232103 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:12.232319 [debug] [MainThread]: On master: COMMIT
[0m21:40:12.232481 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:12.232642 [debug] [MainThread]: On master: COMMIT
[0m21:40:12.232889 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:12.233054 [debug] [MainThread]: On master: Close
[0m21:40:12.234650 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:40:12.234816 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_pricebook' was properly closed.
[0m21:40:12.234991 [info ] [MainThread]: 
[0m21:40:12.235176 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m21:40:12.235518 [debug] [MainThread]: Command end result
[0m21:40:12.243013 [info ] [MainThread]: 
[0m21:40:12.243287 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:40:12.243642 [info ] [MainThread]: 
[0m21:40:12.243829 [error] [MainThread]:   Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "pricebook_entry_id" not found in FROM clause!
  Candidate bindings: "pricebook_entry.pricebook2id"
  LINE 14:     pricebook_entry_id,
               ^
[0m21:40:12.244000 [info ] [MainThread]: 
[0m21:40:12.244182 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:40:12.244544 [debug] [MainThread]: Command `dbt run` failed at 21:40:12.244497 after 0.44 seconds
[0m21:40:12.244744 [debug] [MainThread]: Flushing usage events


============================== 21:40:41.873729 | bbb33c79-bb0b-46da-9890-82a54caa1eaa ==============================
[0m21:40:41.873729 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:40:41.876792 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_pricebook.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:40:41.877083 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:40:41.954443 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:40:41.972279 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:40:42.007500 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:40:42.007812 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:40:42.008755 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m21:40:42.024491 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:40:42.025931 [info ] [MainThread]: 
[0m21:40:42.026364 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:40:42.026909 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:40:42.033732 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:40:42.033963 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:40:42.034140 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:40:42.047530 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.048594 [debug] [ThreadPool]: On list_dbt: Close
[0m21:40:42.050587 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:40:42.051047 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:40:42.054071 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:42.054284 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:40:42.054442 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:42.060738 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.061007 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:42.061174 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:40:42.061427 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.061981 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:40:42.062152 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:42.062313 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:40:42.062519 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.062684 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:40:42.065443 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:40:42.068801 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:40:42.069017 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:40:42.069173 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:42.075042 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.075286 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:40:42.075467 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:40:42.093541 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.094507 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:40:42.095189 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:40:42.095372 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:40:42.097583 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m21:40:42.100158 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:40:42.100371 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:40:42.100526 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:42.106648 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.106918 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:40:42.107097 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:40:42.123059 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.126637 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:40:42.126931 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:40:42.127097 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:40:42.129521 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:40:42.131329 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:40:42.132534 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:40:42.132984 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:42.139186 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.139475 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:40:42.139656 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:40:42.155237 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.158762 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:40:42.159016 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:40:42.159193 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:40:42.161392 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:40:42.163819 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:40:42.164001 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:40:42.164152 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:42.169926 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.170180 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:40:42.170352 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:40:42.187976 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.189500 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:40:42.189749 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:40:42.189906 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:40:42.192356 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:42.192614 [debug] [MainThread]: On master: BEGIN
[0m21:40:42.192788 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:40:42.198384 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:42.198642 [debug] [MainThread]: On master: COMMIT
[0m21:40:42.198810 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:42.198960 [debug] [MainThread]: On master: COMMIT
[0m21:40:42.199169 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:42.199328 [debug] [MainThread]: On master: Close
[0m21:40:42.200907 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:40:42.201153 [info ] [MainThread]: 
[0m21:40:42.202590 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m21:40:42.202916 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_pricebook ................................. [RUN]
[0m21:40:42.203313 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_pricebook)
[0m21:40:42.203511 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m21:40:42.209105 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:42.209578 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 21:40:42.203650 => 21:40:42.209474
[0m21:40:42.209775 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m21:40:42.228883 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:42.229564 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:42.229770 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m21:40:42.229949 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:40:42.235892 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:40:42.238323 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:42.238553 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

SELECT
    pricebook_entry_id,
    pricebook2id AS pricebook_id,
    product2id AS product_id,
    unitprice AS unit_price,
    isactive AS is_active,
    usestandardprice AS use_standard_price,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."raw"."pricebook_entry"
WHERE
    isdeleted = FALSE  -- Exclude deleted entries
ORDER BY
    pricebook_entry_id
    );
  
  
[0m21:40:42.239291 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 21:40:42.209905 => 21:40:42.239180
[0m21:40:42.239509 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: ROLLBACK
[0m21:40:42.245904 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_pricebook'
[0m21:40:42.246130 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m21:40:42.248046 [debug] [Thread-1  ]: Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "pricebook_entry_id" not found in FROM clause!
  Candidate bindings: "pricebook_entry.pricebook2id"
  LINE 14:     pricebook_entry_id,
               ^
[0m21:40:42.248484 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_pricebook ........................ [[31mERROR[0m in 0.05s]
[0m21:40:42.248842 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m21:40:42.249598 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:42.250009 [debug] [MainThread]: On master: BEGIN
[0m21:40:42.250167 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:40:42.255958 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:42.256221 [debug] [MainThread]: On master: COMMIT
[0m21:40:42.256392 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:42.256549 [debug] [MainThread]: On master: COMMIT
[0m21:40:42.257086 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:42.257427 [debug] [MainThread]: On master: Close
[0m21:40:42.259169 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:40:42.259360 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_pricebook' was properly closed.
[0m21:40:42.259539 [info ] [MainThread]: 
[0m21:40:42.259734 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m21:40:42.260072 [debug] [MainThread]: Command end result
[0m21:40:42.267619 [info ] [MainThread]: 
[0m21:40:42.267911 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:40:42.268077 [info ] [MainThread]: 
[0m21:40:42.268244 [error] [MainThread]:   Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "pricebook_entry_id" not found in FROM clause!
  Candidate bindings: "pricebook_entry.pricebook2id"
  LINE 14:     pricebook_entry_id,
               ^
[0m21:40:42.268415 [info ] [MainThread]: 
[0m21:40:42.268600 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:40:42.268988 [debug] [MainThread]: Command `dbt run` failed at 21:40:42.268931 after 0.42 seconds
[0m21:40:42.269199 [debug] [MainThread]: Flushing usage events


============================== 21:41:03.599502 | 87100356-2f83-4fdb-8b48-44e74aa40c15 ==============================
[0m21:41:03.599502 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:41:03.602366 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_pricebook.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:41:03.602653 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:41:03.679645 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:41:03.698513 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:41:03.733574 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:41:03.734037 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_pricebook.sql
[0m21:41:03.763528 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:41:03.777926 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:41:03.779417 [info ] [MainThread]: 
[0m21:41:03.779869 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:41:03.780462 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:41:03.787681 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:41:03.787970 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:41:03.788168 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:41:03.801548 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.802669 [debug] [ThreadPool]: On list_dbt: Close
[0m21:41:03.804895 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:41:03.805417 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:41:03.808461 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:41:03.808744 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:41:03.809006 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:03.815021 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.815313 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:41:03.815487 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:41:03.815744 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.816272 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:41:03.816447 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:41:03.816599 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:41:03.816820 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.816981 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:41:03.820452 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:41:03.824095 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:41:03.824388 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:41:03.824586 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:03.831105 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.831367 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:41:03.831554 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:41:03.850098 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.851260 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:41:03.851978 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:41:03.852278 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:41:03.854874 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m21:41:03.857814 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:41:03.858215 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:41:03.858456 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:03.864340 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.864630 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:41:03.864823 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:41:03.888168 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.891948 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:41:03.892320 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:41:03.892535 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:41:03.894955 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:41:03.897662 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:41:03.897912 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:41:03.898076 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:03.904984 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.905316 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:41:03.905517 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:41:03.924121 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.927886 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:41:03.928239 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:41:03.928420 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:41:03.930871 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:41:03.933285 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:41:03.933483 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:41:03.933641 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:03.939817 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.940047 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:41:03.940227 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:41:03.958161 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.958963 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:41:03.959325 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:41:03.959502 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:41:03.961746 [debug] [MainThread]: Using duckdb connection "master"
[0m21:41:03.962026 [debug] [MainThread]: On master: BEGIN
[0m21:41:03.962197 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:41:03.967939 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:41:03.968208 [debug] [MainThread]: On master: COMMIT
[0m21:41:03.968375 [debug] [MainThread]: Using duckdb connection "master"
[0m21:41:03.968528 [debug] [MainThread]: On master: COMMIT
[0m21:41:03.968724 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:41:03.968881 [debug] [MainThread]: On master: Close
[0m21:41:03.970356 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:41:03.970574 [info ] [MainThread]: 
[0m21:41:03.972993 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m21:41:03.973303 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_pricebook ................................. [RUN]
[0m21:41:03.973672 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_pricebook)
[0m21:41:03.973871 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m21:41:03.979134 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:03.979649 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 21:41:03.974013 => 21:41:03.979544
[0m21:41:03.979847 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m21:41:03.998441 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:03.998930 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:03.999134 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m21:41:03.999314 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:41:04.005208 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:41:04.005598 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:04.005830 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

SELECT
    pricebook_entry_id,
    pricebook2id AS pricebook_id,
    product2id AS product_id,
    unitprice AS unit_price,
    isactive AS is_active,
    usestandardprice AS use_standard_price,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."staging"."stg_salesforce__pricebook_entry"
WHERE
    isdeleted = FALSE  -- Exclude deleted entries
ORDER BY
    pricebook_entry_id
    );
  
  
[0m21:41:04.007727 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:41:04.012317 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:04.012590 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m21:41:04.013008 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:41:04.051715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m21:41:04.052096 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:04.052305 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m21:41:04.053285 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:41:04.057324 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:04.057645 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m21:41:04.058021 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:41:04.058878 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 21:41:03.979984 => 21:41:04.058780
[0m21:41:04.059102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m21:41:04.081501 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_pricebook ............................ [[32mOK[0m in 0.11s]
[0m21:41:04.081922 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m21:41:04.082738 [debug] [MainThread]: Using duckdb connection "master"
[0m21:41:04.083016 [debug] [MainThread]: On master: BEGIN
[0m21:41:04.083187 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:41:04.089925 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:41:04.090268 [debug] [MainThread]: On master: COMMIT
[0m21:41:04.090440 [debug] [MainThread]: Using duckdb connection "master"
[0m21:41:04.090593 [debug] [MainThread]: On master: COMMIT
[0m21:41:04.090809 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:41:04.090984 [debug] [MainThread]: On master: Close
[0m21:41:04.092878 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:41:04.093196 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_pricebook' was properly closed.
[0m21:41:04.093403 [info ] [MainThread]: 
[0m21:41:04.093653 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.31 seconds (0.31s).
[0m21:41:04.094006 [debug] [MainThread]: Command end result
[0m21:41:04.101547 [info ] [MainThread]: 
[0m21:41:04.101932 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:41:04.102131 [info ] [MainThread]: 
[0m21:41:04.102309 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:41:04.102628 [debug] [MainThread]: Command `dbt run` succeeded at 21:41:04.102580 after 0.53 seconds
[0m21:41:04.102819 [debug] [MainThread]: Flushing usage events


============================== 21:52:42.314340 | e33c42b5-3b06-4985-8cfd-e79c1f9e99f8 ==============================
[0m21:52:42.314340 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:52:42.317549 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity.sql', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:52:42.317814 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:52:42.402661 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:52:42.423050 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:52:42.469973 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:52:42.470491 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_opportunity.sql
[0m21:52:42.489272 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  expected token 'end of print statement', got 'WHERE'
    line 50
      WHERE
[0m21:52:42.489795 [debug] [MainThread]: Command `dbt run` failed at 21:52:42.489715 after 0.20 seconds
[0m21:52:42.490025 [debug] [MainThread]: Flushing usage events


============================== 21:52:55.993051 | ce54ddaf-9857-46c1-bd72-bdb1e3c2ba3e ==============================
[0m21:52:55.993051 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:52:55.995952 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:52:55.996244 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:52:56.073773 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:52:56.091859 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:52:56.128590 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:52:56.128903 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:52:56.129893 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m21:52:56.146952 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:52:56.148544 [info ] [MainThread]: 
[0m21:52:56.149013 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:52:56.149614 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:52:56.157296 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:52:56.157626 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:52:56.157804 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:52:56.173651 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.174852 [debug] [ThreadPool]: On list_dbt: Close
[0m21:52:56.176877 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:52:56.177265 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:52:56.180165 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:52:56.180369 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:52:56.180523 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:56.186914 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.187210 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:52:56.187381 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:52:56.187623 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.188153 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:52:56.188324 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:52:56.188473 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:52:56.188687 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.188847 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:52:56.192058 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m21:52:56.195792 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:52:56.196082 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:52:56.196261 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:56.202466 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.202739 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:52:56.202908 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:52:56.218619 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.222195 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:52:56.224618 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:52:56.224826 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:52:56.227125 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m21:52:56.229009 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:52:56.229191 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:52:56.229351 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:56.235007 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.235244 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:52:56.235421 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:52:56.252965 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.253683 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:52:56.253903 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:52:56.254059 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:52:56.256421 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m21:52:56.258055 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:52:56.258252 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:52:56.258411 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:56.264685 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.264876 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:52:56.265046 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:52:56.282637 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.283605 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:52:56.283961 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:52:56.284144 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:52:56.286722 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:52:56.288256 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:52:56.289537 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:52:56.289759 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:56.297639 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.297909 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:52:56.298093 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:52:56.314413 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.318072 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:52:56.318430 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:52:56.318594 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:52:56.322564 [debug] [MainThread]: Using duckdb connection "master"
[0m21:52:56.322766 [debug] [MainThread]: On master: BEGIN
[0m21:52:56.322925 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:52:56.329150 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:52:56.329411 [debug] [MainThread]: On master: COMMIT
[0m21:52:56.329575 [debug] [MainThread]: Using duckdb connection "master"
[0m21:52:56.329724 [debug] [MainThread]: On master: COMMIT
[0m21:52:56.329927 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:52:56.330085 [debug] [MainThread]: On master: Close
[0m21:52:56.331652 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:52:56.331888 [info ] [MainThread]: 
[0m21:52:56.336242 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m21:52:56.336591 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_opportunity ............................... [RUN]
[0m21:52:56.337076 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_opportunity)
[0m21:52:56.337343 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m21:52:56.342637 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.344260 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 21:52:56.337488 => 21:52:56.344086
[0m21:52:56.344522 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m21:52:56.363577 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.364256 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.364473 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m21:52:56.364657 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:52:56.370749 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:52:56.371020 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.371267 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

SELECT
    opportunity_id,
    accountid AS account_id,
    isprivate AS is_private,
    name AS opportunity_name,
    description AS opportunity_description,
    stagename AS stage_name,
    stagesortorder AS stage_sort_order,
    amount AS opportunity_amount,
    probability AS opportunity_probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,
    closedate AS close_date,
    type AS opportunity_type,
    nextstep AS next_step,
    leadsource AS lead_source,
    isclosed AS is_closed,
    iswon AS is_won,
    forecastcategory AS forecast_category,
    forecastcategoryname AS forecast_category_name,
    campaignid AS campaign_id,
    hasopportunitylineitem AS has_opportunity_line_item,
    pricebook2id AS pricebook_id,
    ownerid AS owner_id,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    lastactivitydate AS last_activity_date,
    laststagechangedate AS last_stage_change_date,
    fiscalyear AS fiscal_year,
    fiscalquarter AS fiscal_quarter,
    contactid AS contact_id,
    primarypartneraccountid AS primary_partner_account_id,
    contractid AS contract_id,
    lastamountchangedhistoryid AS last_amount_changed_history_id,
    lastclosedatechangedhistoryid AS last_close_date_changed_history_id,
    deliveryinstallationstatus__c AS delivery_installation_status,
    trackingnumber__c AS tracking_number,
    ordernumber__c AS order_number,
    currentgenerators__c AS current_generators,
    maincompetitors__c AS main_competitors
FROM
     "dbt"."staging"."stg_salesforce__opportunity"
WHERE
    isdeleted = FALSE
ORDER BY
    opportunity_name
    );
  
  
[0m21:52:56.375107 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:52:56.379406 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.379723 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m21:52:56.380192 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:52:56.390431 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m21:52:56.390826 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.391130 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m21:52:56.392525 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:52:56.396363 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.396665 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m21:52:56.396962 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:52:56.397747 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 21:52:56.344663 => 21:52:56.397647
[0m21:52:56.397946 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m21:52:56.431704 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_opportunity .......................... [[32mOK[0m in 0.09s]
[0m21:52:56.432301 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m21:52:56.433224 [debug] [MainThread]: Using duckdb connection "master"
[0m21:52:56.433411 [debug] [MainThread]: On master: BEGIN
[0m21:52:56.433565 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:52:56.439581 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:52:56.439868 [debug] [MainThread]: On master: COMMIT
[0m21:52:56.440039 [debug] [MainThread]: Using duckdb connection "master"
[0m21:52:56.440190 [debug] [MainThread]: On master: COMMIT
[0m21:52:56.440397 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:52:56.440555 [debug] [MainThread]: On master: Close
[0m21:52:56.442341 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:52:56.442542 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_opportunity' was properly closed.
[0m21:52:56.442729 [info ] [MainThread]: 
[0m21:52:56.442916 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m21:52:56.443257 [debug] [MainThread]: Command end result
[0m21:52:56.450321 [info ] [MainThread]: 
[0m21:52:56.450581 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:52:56.450752 [info ] [MainThread]: 
[0m21:52:56.450945 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:52:56.451318 [debug] [MainThread]: Command `dbt run` succeeded at 21:52:56.451272 after 0.48 seconds
[0m21:52:56.451529 [debug] [MainThread]: Flushing usage events


============================== 21:53:20.674395 | f3d21fdd-7fcb-4732-843b-11aed2d56e6d ==============================
[0m21:53:20.674395 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:53:20.677795 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity_sstage.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:53:20.678087 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:53:20.795534 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:53:20.821011 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:53:20.895734 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:53:20.896030 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:53:20.896973 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:53:20.937344 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:53:20.938373 [warn ] [MainThread]: The selection criterion 'models/dimensions/dim_opportunity_sstage.sql' does not match any nodes
[0m21:53:20.939098 [info ] [MainThread]: 
[0m21:53:20.939294 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:53:20.939557 [debug] [MainThread]: Command end result
[0m21:53:20.946603 [debug] [MainThread]: Command `dbt run` succeeded at 21:53:20.946509 after 0.30 seconds
[0m21:53:20.946861 [debug] [MainThread]: Flushing usage events


============================== 21:53:44.437558 | 058a3c58-ac54-4b2c-9b23-05ccc49124bc ==============================
[0m21:53:44.437558 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:53:44.440572 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity_stage.sql', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m21:53:44.440833 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:53:44.525440 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:53:44.545223 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:53:44.592840 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:53:44.593129 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:53:44.594101 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m21:53:44.608577 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:53:44.610223 [info ] [MainThread]: 
[0m21:53:44.610704 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:53:44.611306 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:53:44.618881 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:53:44.619174 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:53:44.619351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:53:44.633727 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.634831 [debug] [ThreadPool]: On list_dbt: Close
[0m21:53:44.636845 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:53:44.637299 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:53:44.640299 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:53:44.640504 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:53:44.640663 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:44.647018 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.647304 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:53:44.647489 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:53:44.647730 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.648249 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:53:44.648415 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:53:44.648562 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:53:44.648772 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.648935 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:53:44.652071 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m21:53:44.655823 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:53:44.656056 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:53:44.656274 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:44.662081 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.662332 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:53:44.662524 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:53:44.678509 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.682228 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:53:44.685430 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:53:44.685613 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:53:44.687833 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m21:53:44.690275 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:53:44.690476 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:53:44.690625 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:44.696823 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.697089 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:53:44.697258 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:53:44.712586 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.716541 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:53:44.716908 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:53:44.717096 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:53:44.719359 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m21:53:44.721328 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:53:44.721529 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:53:44.721679 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:44.728093 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.728372 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:53:44.728553 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:53:44.747843 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.748754 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:53:44.748993 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:53:44.749154 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:53:44.751932 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m21:53:44.753679 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:53:44.753905 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:53:44.754059 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:44.760223 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.760492 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:53:44.760664 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:53:44.778381 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.779283 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:53:44.779512 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:53:44.779672 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:53:44.782846 [debug] [MainThread]: Using duckdb connection "master"
[0m21:53:44.783038 [debug] [MainThread]: On master: BEGIN
[0m21:53:44.783188 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:53:44.788976 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:53:44.789227 [debug] [MainThread]: On master: COMMIT
[0m21:53:44.789387 [debug] [MainThread]: Using duckdb connection "master"
[0m21:53:44.789533 [debug] [MainThread]: On master: COMMIT
[0m21:53:44.789772 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:53:44.789956 [debug] [MainThread]: On master: Close
[0m21:53:44.791440 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:53:44.791690 [info ] [MainThread]: 
[0m21:53:44.794954 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m21:53:44.795237 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_opportunity_stage ......................... [RUN]
[0m21:53:44.795607 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m21:53:44.795801 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m21:53:44.801482 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.802149 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 21:53:44.795937 => 21:53:44.802025
[0m21:53:44.802355 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m21:53:44.821353 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.822022 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.822227 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m21:53:44.822419 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:44.828256 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:53:44.828549 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.828763 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        stagename AS stage_name,
        stagesortorder AS stage_sort_order
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY stage_sort_order) AS stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
FROM source
    );
  
  
[0m21:53:44.831447 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:53:44.835392 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.835647 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m21:53:44.836042 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:53:44.845489 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m21:53:44.845722 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.845906 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m21:53:44.846551 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:53:44.849410 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.849632 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m21:53:44.849911 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:53:44.850654 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 21:53:44.802491 => 21:53:44.850559
[0m21:53:44.850864 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m21:53:44.864412 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_opportunity_stage .................... [[32mOK[0m in 0.07s]
[0m21:53:44.864836 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m21:53:44.865717 [debug] [MainThread]: Using duckdb connection "master"
[0m21:53:44.865979 [debug] [MainThread]: On master: BEGIN
[0m21:53:44.866149 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:53:44.872533 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:53:44.872844 [debug] [MainThread]: On master: COMMIT
[0m21:53:44.873023 [debug] [MainThread]: Using duckdb connection "master"
[0m21:53:44.873188 [debug] [MainThread]: On master: COMMIT
[0m21:53:44.873395 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:53:44.873557 [debug] [MainThread]: On master: Close
[0m21:53:44.875061 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:53:44.875260 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_opportunity_stage' was properly closed.
[0m21:53:44.875659 [info ] [MainThread]: 
[0m21:53:44.875948 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m21:53:44.876404 [debug] [MainThread]: Command end result
[0m21:53:44.885224 [info ] [MainThread]: 
[0m21:53:44.885567 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:53:44.885757 [info ] [MainThread]: 
[0m21:53:44.885962 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:53:44.886371 [debug] [MainThread]: Command `dbt run` succeeded at 21:53:44.886316 after 0.48 seconds
[0m21:53:44.886594 [debug] [MainThread]: Flushing usage events


============================== 21:56:57.313289 | 0c785e82-2234-4ef9-882e-2d927414686b ==============================
[0m21:56:57.313289 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:56:57.316386 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_lead.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:56:57.316639 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:56:57.399851 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:56:57.418228 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:56:57.455659 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:56:57.455974 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:56:57.456977 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m21:56:57.472214 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:56:57.473998 [info ] [MainThread]: 
[0m21:56:57.474485 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:56:57.475165 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:56:57.483452 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:56:57.483817 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:56:57.484000 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:56:57.490297 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:56:57.490544 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m21:56:57.490721 [info ] [MainThread]: 
[0m21:56:57.490922 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m21:56:57.491209 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 29952) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m21:56:57.491634 [debug] [MainThread]: Command `dbt run` failed at 21:56:57.491581 after 0.20 seconds
[0m21:56:57.491842 [debug] [MainThread]: Flushing usage events


============================== 21:57:08.821139 | 1ff3fc2d-702d-4e0d-8931-990d14eb953a ==============================
[0m21:57:08.821139 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:57:08.824157 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_lead.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:57:08.824442 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:57:08.901052 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:57:08.921358 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:57:08.971660 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:57:08.971967 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:57:08.972954 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:57:08.986958 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:57:08.988348 [info ] [MainThread]: 
[0m21:57:08.988766 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:57:08.989303 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:57:08.997020 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:57:08.997369 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:57:08.997558 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:57:09.003534 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:57:09.003792 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m21:57:09.003978 [info ] [MainThread]: 
[0m21:57:09.004355 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m21:57:09.004683 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 29952) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m21:57:09.005061 [debug] [MainThread]: Command `dbt run` failed at 21:57:09.005014 after 0.21 seconds
[0m21:57:09.005251 [debug] [MainThread]: Flushing usage events


============================== 21:57:25.224394 | 926796f0-75e8-4bf8-afb6-e3e92489db5e ==============================
[0m21:57:25.224394 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:57:25.236392 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_lead.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:57:25.237144 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:57:25.367917 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:57:25.387592 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:57:25.461206 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:57:25.461496 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:57:25.462453 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:57:25.477086 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:57:25.478571 [info ] [MainThread]: 
[0m21:57:25.479031 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:57:25.479690 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:57:25.487311 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:57:25.487651 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:57:25.488226 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:57:25.500389 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.501700 [debug] [ThreadPool]: On list_dbt: Close
[0m21:57:25.509003 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:57:25.511703 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:57:25.524521 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:25.526772 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:57:25.527069 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:25.540296 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.540601 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:25.540774 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:57:25.541008 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.541521 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:57:25.541689 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:25.541835 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:57:25.542036 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.542195 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:57:25.545230 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m21:57:25.549070 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:57:25.549378 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:57:25.549543 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:25.556011 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.556344 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:57:25.556518 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:57:25.573463 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.577618 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:57:25.578408 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:57:25.578692 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:57:25.581219 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m21:57:25.583746 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:57:25.583977 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:57:25.584150 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:25.591180 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.591470 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:57:25.591646 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:57:25.610010 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.611159 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:57:25.611519 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:57:25.611699 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:57:25.614193 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m21:57:25.616662 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:57:25.616865 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:57:25.617019 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:25.623387 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.623637 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:57:25.623825 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:57:25.642315 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.643086 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:57:25.643316 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:57:25.643471 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:57:25.646192 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m21:57:25.647889 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:57:25.648092 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:57:25.648250 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:25.654573 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.654764 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:57:25.654927 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:57:25.670080 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.673397 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:57:25.673614 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:57:25.673784 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:57:25.676418 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:25.676600 [debug] [MainThread]: On master: BEGIN
[0m21:57:25.676746 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:57:25.682875 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:25.683136 [debug] [MainThread]: On master: COMMIT
[0m21:57:25.683297 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:25.683445 [debug] [MainThread]: On master: COMMIT
[0m21:57:25.683650 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:25.683807 [debug] [MainThread]: On master: Close
[0m21:57:25.685347 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:57:25.685552 [info ] [MainThread]: 
[0m21:57:25.689169 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m21:57:25.689544 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_lead ...................................... [RUN]
[0m21:57:25.689943 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_lead)
[0m21:57:25.690140 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m21:57:25.695958 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.697635 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 21:57:25.690271 => 21:57:25.697449
[0m21:57:25.697919 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m21:57:25.717312 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.718155 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.718381 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m21:57:25.718596 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:25.724933 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:25.725226 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.725492 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

SELECT
    lead_id,
    masterrecordid AS master_record_id,
    salutation AS lead_salutation,
    firstname AS first_name,
    lastname AS last_name,
    title AS lead_title,
    company AS lead_company,
    street AS lead_street,
    city AS lead_city,
    state AS lead_state,
    postalcode AS lead_postal_code,
    country AS lead_country,
    latitude AS lead_latitude,
    longitude AS lead_longitude,
    geocodeaccuracy AS geocode_accuracy,
    phone AS lead_phone,
    mobilephone AS lead_mobile_phone,
    fax AS lead_fax,
    email AS lead_email,
    website AS lead_website,
    description AS lead_description,
    leadsource AS lead_source,
    status AS lead_status,
    industry AS lead_industry,
    rating AS lead_rating,
    annualrevenue AS annual_revenue,
    numberofemployees AS number_of_employees,
    ownerid AS owner_id,
    hasoptedoutofemail AS has_opted_out_of_email,
    isconverted AS is_converted,
    converteddate AS converted_date,
    convertedaccountid AS converted_account_id,
    convertedcontactid AS converted_contact_id,
    convertedopportunityid AS converted_opportunity_id,
    isunreadbyowner AS is_unread_by_owner,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    donotcall AS do_not_call,
    hasoptedoutoffax AS has_opted_out_of_fax,
    lasttransferdate AS last_transfer_date,
    jigsaw AS jigsaw_id,
    jigsawcontactid AS jigsaw_contact_id,
    cleanstatus AS clean_status,
    companydunsnumber AS company_duns_number,
    dandbcompanyid AS dandb_company_id,
    emailbouncedreason AS email_bounced_reason,
    emailbounceddate AS email_bounced_date,
    individualid AS individual_id,
    pronouns AS lead_pronouns,
    genderidentity AS gender_identity,
    siccode__c AS sic_code,
    productinterest__c AS product_interest,
    primary__c AS is_primary,
    currentgenerators__c AS current_generators,
    numberoflocations__c AS number_of_locations
FROM
    "dbt"."staging"."stg_salesforce__lead"
WHERE
    isdeleted = FALSE  -- Exclude deleted leads
ORDER BY
    last_name, first_name
    );
  
  
[0m21:57:25.730108 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:25.734252 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.734554 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m21:57:25.734932 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:25.745050 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m21:57:25.745366 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.745562 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m21:57:25.747306 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:25.750399 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.750622 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m21:57:25.750927 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:25.751751 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 21:57:25.698089 => 21:57:25.751648
[0m21:57:25.751960 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m21:57:25.783079 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_lead ................................. [[32mOK[0m in 0.09s]
[0m21:57:25.783513 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m21:57:25.784310 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:25.784531 [debug] [MainThread]: On master: BEGIN
[0m21:57:25.784708 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:57:25.791101 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:25.791386 [debug] [MainThread]: On master: COMMIT
[0m21:57:25.791551 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:25.791704 [debug] [MainThread]: On master: COMMIT
[0m21:57:25.791909 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:25.792077 [debug] [MainThread]: On master: Close
[0m21:57:25.793904 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:57:25.794177 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_lead' was properly closed.
[0m21:57:25.794379 [info ] [MainThread]: 
[0m21:57:25.794591 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m21:57:25.794965 [debug] [MainThread]: Command end result
[0m21:57:25.803211 [info ] [MainThread]: 
[0m21:57:25.803503 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:57:25.803673 [info ] [MainThread]: 
[0m21:57:25.803860 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:57:25.804261 [debug] [MainThread]: Command `dbt run` succeeded at 21:57:25.804205 after 0.61 seconds
[0m21:57:25.804464 [debug] [MainThread]: Flushing usage events


============================== 21:57:42.577042 | 21e5a38d-08de-48c1-a030-0e03878b5007 ==============================
[0m21:57:42.577042 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:57:42.579434 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m21:57:42.579707 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:57:42.656063 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:57:42.674410 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:57:42.711681 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:57:42.711977 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:57:42.712897 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:57:42.726006 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:57:42.727421 [info ] [MainThread]: 
[0m21:57:42.727849 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:57:42.728570 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:57:42.736383 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:57:42.736856 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:57:42.737096 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:57:42.745435 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.746376 [debug] [ThreadPool]: On list_dbt: Close
[0m21:57:42.748375 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:57:42.748881 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:57:42.751885 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:42.752092 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:57:42.752249 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:42.759103 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.759313 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:42.759477 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:57:42.759728 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.760217 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:57:42.760382 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:42.760529 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:57:42.760735 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.760891 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:57:42.763872 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:57:42.767346 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:57:42.767544 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:57:42.767703 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:42.774318 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.774600 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:57:42.774808 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:57:42.792753 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.793757 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:57:42.794194 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:57:42.794364 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:57:42.796582 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:57:42.799052 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:57:42.799264 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:57:42.799423 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:42.805390 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.805644 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:57:42.805814 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:57:42.820976 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.824247 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:57:42.824477 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:57:42.824635 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:57:42.827042 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:57:42.829885 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:57:42.830124 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:57:42.830278 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:42.836289 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.836559 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:57:42.836738 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:57:42.856851 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.857730 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:57:42.857971 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:57:42.858134 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:57:42.860698 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m21:57:42.862262 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:57:42.862448 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:57:42.862604 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:42.869184 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.869424 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:57:42.869610 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:57:42.886192 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.889744 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:57:42.889979 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:57:42.890141 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:57:42.894514 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:42.894785 [debug] [MainThread]: On master: BEGIN
[0m21:57:42.894969 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:57:42.901890 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:42.902143 [debug] [MainThread]: On master: COMMIT
[0m21:57:42.902312 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:42.902468 [debug] [MainThread]: On master: COMMIT
[0m21:57:42.902674 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:42.902838 [debug] [MainThread]: On master: Close
[0m21:57:42.904619 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:57:42.904867 [info ] [MainThread]: 
[0m21:57:42.906215 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m21:57:42.906514 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_contact ................................... [RUN]
[0m21:57:42.906884 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_contact)
[0m21:57:42.907083 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m21:57:42.912501 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.914010 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 21:57:42.907221 => 21:57:42.913900
[0m21:57:42.914215 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m21:57:42.933193 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.933781 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.933979 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m21:57:42.934383 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:42.940876 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:42.941187 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.941445 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    contact_id,
    masterrecordid AS master_record_id,
    accountid AS account_id,
    reportstoid AS reports_to_id,
    ownerid AS owner_id,
    jigsawcontactid AS jigsaw_contact_id,
    individualid AS individual_id,

    /* Dates */
    birthdate AS birth_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    lastcurequestdate AS last_cu_request_date,
    lastcuupdatedate AS last_cu_update_date,
    emailbounceddate AS email_bounced_date,

    /* Dimensions */
    salutation,
    firstname AS first_name,
    lastname AS last_name,
    otherstreet AS other_street,
    othercity AS other_city,
    otherstate AS other_state,
    otherpostalcode AS other_postal_code,
    othercountry AS other_country,
    otherlatitude AS other_latitude,
    otherlongitude AS other_longitude,
    othergeocodeaccuracy AS other_geocode_accuracy,
    mailingstreet AS mailing_street,
    mailingcity AS mailing_city,
    mailingstate AS mailing_state,
    mailingpostalcode AS mailing_postal_code,
    mailingcountry AS mailing_country,
    mailinglatitude AS mailing_latitude,
    mailinglongitude AS mailing_longitude,
    mailinggeocodeaccuracy AS mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone AS mobile_phone,
    homephone AS home_phone,
    otherphone AS other_phone,
    assistantphone AS assistant_phone,
    email,
    title,
    department,
    assistantname AS assistant_name,
    leadsource AS lead_source,
    description,
    pronouns,
    genderidentity AS gender_identity,
    cleanstatus AS clean_status,
    emailbouncedreason AS email_bounced_reason,
    level__c AS level,
    languages__c AS languages,

    /* Metrics */
    hasoptedoutofemail AS has_opted_out_of_email,
    hasoptedoutoffax AS has_opted_out_of_fax,
    donotcall AS do_not_call

FROM "dbt"."staging"."stg_salesforce__contact"
WHERE isdeleted = FALSE
    );
  
  
[0m21:57:42.945433 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:42.949638 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.949942 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m21:57:42.950341 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:42.960396 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m21:57:42.960702 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.960904 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m21:57:42.962347 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:42.966137 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.966403 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m21:57:42.966723 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:42.967531 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 21:57:42.914350 => 21:57:42.967437
[0m21:57:42.967734 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m21:57:42.986963 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_contact .............................. [[32mOK[0m in 0.08s]
[0m21:57:42.987403 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m21:57:42.988154 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:42.988346 [debug] [MainThread]: On master: BEGIN
[0m21:57:42.988501 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:57:42.995350 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:42.995710 [debug] [MainThread]: On master: COMMIT
[0m21:57:42.995903 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:42.996083 [debug] [MainThread]: On master: COMMIT
[0m21:57:42.996325 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:42.996524 [debug] [MainThread]: On master: Close
[0m21:57:42.998351 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:57:42.998627 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m21:57:42.998815 [info ] [MainThread]: 
[0m21:57:42.999020 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m21:57:42.999378 [debug] [MainThread]: Command end result
[0m21:57:43.007010 [info ] [MainThread]: 
[0m21:57:43.007354 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:57:43.007531 [info ] [MainThread]: 
[0m21:57:43.007721 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:57:43.008117 [debug] [MainThread]: Command `dbt run` succeeded at 21:57:43.008059 after 0.45 seconds
[0m21:57:43.008326 [debug] [MainThread]: Flushing usage events


============================== 22:02:30.318193 | 001af2e4-d0ef-44a8-a24e-15d648d24bae ==============================
[0m22:02:30.318193 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:02:30.322038 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_account.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:02:30.322397 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:02:30.410194 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:02:30.429858 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:02:30.465990 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:02:30.466308 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:02:30.467283 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:02:30.481862 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:02:30.483477 [info ] [MainThread]: 
[0m22:02:30.484009 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:02:30.484716 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:02:30.492035 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:02:30.492300 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:02:30.492477 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:02:30.506807 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.507908 [debug] [ThreadPool]: On list_dbt: Close
[0m22:02:30.509988 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:02:30.510448 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:02:30.513445 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:30.513659 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:02:30.513823 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:30.520722 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.521007 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:30.521177 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:02:30.521434 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.521970 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:02:30.522138 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:30.522289 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:02:30.522496 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.522662 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:02:30.525684 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m22:02:30.529287 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:02:30.529535 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:02:30.529704 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:30.536348 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.536613 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:02:30.536786 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:02:30.554928 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.555895 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:02:30.556519 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:02:30.556766 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:02:30.559200 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:02:30.561758 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:02:30.562009 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:02:30.562173 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:30.568609 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.568877 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:02:30.569063 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:02:30.589214 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.590020 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:02:30.590263 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:02:30.590431 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:02:30.592817 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:02:30.594690 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:02:30.594954 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:02:30.595131 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:30.601863 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.602167 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:02:30.602358 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:02:30.618326 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.621832 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:02:30.622064 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:02:30.622220 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:02:30.624691 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:02:30.626979 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:02:30.627197 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:02:30.627356 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:30.633891 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.634172 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:02:30.634351 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:02:30.649873 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.653655 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:02:30.653912 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:02:30.654083 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:02:30.657979 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:30.658192 [debug] [MainThread]: On master: BEGIN
[0m22:02:30.658352 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:02:30.664747 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:30.665014 [debug] [MainThread]: On master: COMMIT
[0m22:02:30.665183 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:30.665334 [debug] [MainThread]: On master: COMMIT
[0m22:02:30.665532 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:30.665692 [debug] [MainThread]: On master: Close
[0m22:02:30.667216 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:02:30.667466 [info ] [MainThread]: 
[0m22:02:30.668855 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m22:02:30.669139 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_account ................................... [RUN]
[0m22:02:30.669544 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_account)
[0m22:02:30.669769 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m22:02:30.675168 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m22:02:30.676781 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 22:02:30.669922 => 22:02:30.676627
[0m22:02:30.677031 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m22:02:30.696360 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m22:02:30.697121 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:02:30.697436 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m22:02:30.697632 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:02:30.704106 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:30.704338 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:02:30.704605 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

SELECT
    account_id,
    masterrecordid AS master_record_id,
    name AS account_name,
    type AS account_type,
    parentid AS parent_account_id,
    billingstreet AS billing_street,
    billingcity AS billing_city,
    billingstate AS billing_state,
    billingpostalcode AS billing_postal_code,
    billingcountry AS billing_country,
    billinglatitude AS billing_latitude,
    billinglongitude AS billing_longitude,
    billinggeocodeaccuracy AS billing_geocode_accuracy,
    shippingstreet AS shipping_street,
    shippingcity AS shipping_city,
    shippingstate AS shipping_state,
    shippingpostalcode AS shipping_postal_code,
    shippingcountry AS shipping_country,
    shippinglatitude AS shipping_latitude,
    shippinglongitude AS shipping_longitude,
    shippinggeocodeaccuracy AS shipping_geocode_accuracy,
    phone AS account_phone,
    fax AS account_fax,
    accountnumber AS account_number,
    website AS account_website,
    sic AS sic_code,
    industry AS account_industry,
    annualrevenue AS annual_revenue,
    numberofemployees AS number_of_employees,
    ownership AS account_ownership,
    tickersymbol AS ticker_symbol,
    description AS account_description,
    rating AS account_rating,
    site AS account_site,
    ownerid AS owner_id,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    jigsaw AS jigsaw_id,
    jigsawcompanyid AS jigsaw_company_id,
    cleanstatus AS clean_status,
    accountsource AS account_source,
    dunsnumber AS duns_number,
    tradestyle AS trade_style,
    naicscode AS naics_code,
    naicsdesc AS naics_description,
    yearstarted AS year_started,
    sicdesc AS sic_description,
    dandbcompanyid AS dandb_company_id,
    operatinghoursid AS operating_hours_id,
    customerpriority__c AS customer_priority,
    sla__c AS sla,
    active__c AS is_active,
    numberoflocations__c AS number_of_locations,
    upsellopportunity__c AS upsell_opportunity,
    slaserialnumber__c AS sla_serial_number,
    slaexpirationdate__c AS sla_expiration_date
FROM
    "dbt"."staging"."stg_salesforce__account"
WHERE
    isdeleted = FALSE  -- Exclude deleted accounts
ORDER BY
    account_name
    );
  
  
[0m22:02:30.709131 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:30.713435 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:02:30.713701 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m22:02:30.714159 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:30.724135 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m22:02:30.724411 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:02:30.724599 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m22:02:30.726076 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:30.729971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:02:30.730262 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m22:02:30.730595 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:30.731365 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 22:02:30.677205 => 22:02:30.731272
[0m22:02:30.731573 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m22:02:30.762560 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_account .............................. [[32mOK[0m in 0.09s]
[0m22:02:30.762995 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m22:02:30.763752 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:30.763941 [debug] [MainThread]: On master: BEGIN
[0m22:02:30.764101 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:02:30.770712 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:30.770974 [debug] [MainThread]: On master: COMMIT
[0m22:02:30.771436 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:30.771931 [debug] [MainThread]: On master: COMMIT
[0m22:02:30.772273 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:30.772609 [debug] [MainThread]: On master: Close
[0m22:02:30.774464 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:02:30.774661 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_account' was properly closed.
[0m22:02:30.774861 [info ] [MainThread]: 
[0m22:02:30.775055 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m22:02:30.775457 [debug] [MainThread]: Command end result
[0m22:02:30.782926 [info ] [MainThread]: 
[0m22:02:30.783185 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:02:30.783370 [info ] [MainThread]: 
[0m22:02:30.783554 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:02:30.783958 [debug] [MainThread]: Command `dbt run` succeeded at 22:02:30.783908 after 0.49 seconds
[0m22:02:30.784176 [debug] [MainThread]: Flushing usage events


============================== 22:02:42.556862 | 9a71764e-9f50-4fa4-bbaf-17f43f05b667 ==============================
[0m22:02:42.556862 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:02:42.559557 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_campaign.sql', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:02:42.559810 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:02:42.634517 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:02:42.652667 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:02:42.688998 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:02:42.689309 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:02:42.690299 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m22:02:42.704225 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:02:42.705759 [info ] [MainThread]: 
[0m22:02:42.706195 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:02:42.706772 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:02:42.714285 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:02:42.714554 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:02:42.714734 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:02:42.723421 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.724370 [debug] [ThreadPool]: On list_dbt: Close
[0m22:02:42.726372 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:02:42.726878 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:02:42.729893 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:42.730132 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:02:42.730295 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:42.736601 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.736816 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:42.736979 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:02:42.737221 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.737706 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:02:42.737878 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:42.738026 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:02:42.738234 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.738387 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:02:42.741607 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m22:02:42.744933 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:02:42.745195 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:02:42.745376 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:42.752364 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.752640 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:02:42.752815 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:02:42.768397 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.772015 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:02:42.772805 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:02:42.773003 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:02:42.775248 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:02:42.777426 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:02:42.777604 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:02:42.777780 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:42.784358 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.784604 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:02:42.784775 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:02:42.800309 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.804032 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:02:42.804289 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:02:42.804458 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:02:42.806888 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:02:42.808409 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:02:42.809398 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:02:42.809565 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:42.816657 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.816941 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:02:42.817117 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:02:42.835532 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.836601 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:02:42.836839 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:02:42.836996 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:02:42.839367 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:02:42.842511 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:02:42.842706 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:02:42.842873 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:42.849200 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.849439 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:02:42.849620 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:02:42.867727 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.868560 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:02:42.868797 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:02:42.868963 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:02:42.871563 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:42.871835 [debug] [MainThread]: On master: BEGIN
[0m22:02:42.872012 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:02:42.878647 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:42.878941 [debug] [MainThread]: On master: COMMIT
[0m22:02:42.879115 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:42.879263 [debug] [MainThread]: On master: COMMIT
[0m22:02:42.879492 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:42.879653 [debug] [MainThread]: On master: Close
[0m22:02:42.881307 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:02:42.881527 [info ] [MainThread]: 
[0m22:02:42.883686 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m22:02:42.883968 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_campaign .................................. [RUN]
[0m22:02:42.884322 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_campaign)
[0m22:02:42.884513 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m22:02:42.890142 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m22:02:42.891262 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 22:02:42.884646 => 22:02:42.891093
[0m22:02:42.891528 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m22:02:42.911358 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m22:02:42.911988 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:02:42.912193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m22:02:42.912374 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:02:42.919505 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:42.919887 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:02:42.920166 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    id AS campaign_id,
    parentid AS parent_campaign_id,
    ownerid AS owner_id,
    createdbyid AS created_by_id,
    lastmodifiedbyid AS last_modified_by_id,
    campaignmemberrecordtypeid AS campaign_member_record_type_id,

    /* Dates */
    startdate AS start_date,
    enddate AS end_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,

    /* Dimensions */
    name AS campaign_name,
    type AS campaign_type,
    status,
    description,
    isactive AS is_active,

    /* Metrics */
    expectedrevenue AS expected_revenue,
    budgetedcost AS budgeted_cost,
    actualcost AS actual_cost,
    expectedresponse AS expected_response,
    numbersent AS number_sent,
    numberofleads AS number_of_leads,
    numberofconvertedleads AS number_of_converted_leads,
    numberofcontacts AS number_of_contacts,
    numberofresponses AS number_of_responses,
    numberofopportunities AS number_of_opportunities,
    numberofwonopportunities AS number_of_won_opportunities,
    amountallopportunities AS amount_all_opportunities,
    amountwonopportunities AS amount_won_opportunities,
    hierarchynumberofleads AS hierarchy_number_of_leads,
    hierarchynumberofconvertedleads AS hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts AS hierarchy_number_of_contacts,
    hierarchynumberofresponses AS hierarchy_number_of_responses,
    hierarchynumberofopportunities AS hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities AS hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities AS hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities AS hierarchy_amount_won_opportunities,
    hierarchynumbersent AS hierarchy_number_sent,
    hierarchyexpectedrevenue AS hierarchy_expected_revenue,
    hierarchybudgetedcost AS hierarchy_budgeted_cost,
    hierarchyactualcost AS hierarchy_actual_cost

FROM "dbt"."staging"."stg_salesforce__campaign"
WHERE isdeleted = FALSE;
    );
  
  
[0m22:02:42.920814 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 22:02:42.891675 => 22:02:42.920693
[0m22:02:42.921050 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: ROLLBACK
[0m22:02:42.925575 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_campaign'
[0m22:02:42.925997 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m22:02:42.928881 [debug] [Thread-1  ]: Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Parser Error: syntax error at or near ";"
[0m22:02:42.929416 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_campaign ......................... [[31mERROR[0m in 0.04s]
[0m22:02:42.929776 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m22:02:42.930712 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:42.930893 [debug] [MainThread]: On master: BEGIN
[0m22:02:42.931045 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:02:42.937748 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:42.938020 [debug] [MainThread]: On master: COMMIT
[0m22:02:42.938186 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:42.938340 [debug] [MainThread]: On master: COMMIT
[0m22:02:42.938539 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:42.938701 [debug] [MainThread]: On master: Close
[0m22:02:42.940778 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:02:42.940964 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_campaign' was properly closed.
[0m22:02:42.941151 [info ] [MainThread]: 
[0m22:02:42.941345 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m22:02:42.941727 [debug] [MainThread]: Command end result
[0m22:02:42.949591 [info ] [MainThread]: 
[0m22:02:42.949874 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:02:42.950116 [info ] [MainThread]: 
[0m22:02:42.950439 [error] [MainThread]:   Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Parser Error: syntax error at or near ";"
[0m22:02:42.950638 [info ] [MainThread]: 
[0m22:02:42.950843 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:02:42.951266 [debug] [MainThread]: Command `dbt run` failed at 22:02:42.951206 after 0.42 seconds
[0m22:02:42.951497 [debug] [MainThread]: Flushing usage events


============================== 22:05:38.406616 | dfec6857-29dc-476a-b880-0da8c78d29a0 ==============================
[0m22:05:38.406616 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:05:38.410022 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:05:38.410299 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:05:38.495109 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:05:38.512870 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:05:38.548446 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:05:38.548759 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:05:38.549729 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:05:38.565125 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:05:38.566587 [info ] [MainThread]: 
[0m22:05:38.567032 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:05:38.567701 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:05:38.575735 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:05:38.576074 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:05:38.576247 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:05:38.592517 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.593612 [debug] [ThreadPool]: On list_dbt: Close
[0m22:05:38.595590 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:05:38.595964 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:05:38.598920 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:38.599130 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:05:38.599297 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:38.606107 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.606396 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:38.606569 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:05:38.606818 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.607336 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:05:38.607503 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:38.607653 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:05:38.607872 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.608028 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:05:38.611058 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_fact)
[0m22:05:38.614434 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:05:38.614629 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:05:38.614785 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:38.620950 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.621224 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:05:38.621404 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:05:38.640433 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.641314 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:05:38.644758 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:05:38.644947 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:05:38.647473 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:05:38.649264 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:05:38.649480 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:05:38.649641 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:38.656779 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.657065 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:05:38.657262 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:05:38.673577 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.677395 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:05:38.677640 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:05:38.677806 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:05:38.680545 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:05:38.682972 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:05:38.683166 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:05:38.683320 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:38.690146 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.690427 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:05:38.690614 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:05:38.706371 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.709844 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:05:38.710089 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:05:38.710251 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:05:38.713008 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:05:38.715552 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:05:38.715769 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:05:38.715925 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:38.722408 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.722673 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:05:38.722845 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:05:38.740699 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.741613 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:05:38.741842 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:05:38.741999 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:05:38.745985 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:38.746185 [debug] [MainThread]: On master: BEGIN
[0m22:05:38.746343 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:05:38.752574 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:38.752831 [debug] [MainThread]: On master: COMMIT
[0m22:05:38.753000 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:38.753152 [debug] [MainThread]: On master: COMMIT
[0m22:05:38.753350 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:38.753512 [debug] [MainThread]: On master: Close
[0m22:05:38.755110 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:05:38.755345 [info ] [MainThread]: 
[0m22:05:38.756882 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m22:05:38.757166 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_solution .................................. [RUN]
[0m22:05:38.757546 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_solution)
[0m22:05:38.757745 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m22:05:38.763110 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m22:05:38.764178 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 22:05:38.757882 => 22:05:38.763998
[0m22:05:38.764486 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m22:05:38.783716 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m22:05:38.784661 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:38.784996 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m22:05:38.785183 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:05:38.791509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:38.791772 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:38.791986 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

SELECT
    id AS solution_id,
    solutionnumber AS solution_number,
    solutionname AS solution_name,
    ispublished AS is_published,
    ispublishedinpublickb AS is_published_in_public_kb,
    status AS solution_status,
    isreviewed AS is_reviewed,
    solutionnote AS solution_note,
    caseid AS case_id,
    ownerid AS owner_id,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    timesused AS times_used,
    ishtml AS is_html
FROM
    "dbt"."staging"."stg_salesforce__solution"
WHERE
    isdeleted = FALSE
ORDER BY
    solution_name
    );
  
  
[0m22:05:38.792826 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 22:05:38.764654 => 22:05:38.792726
[0m22:05:38.793040 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: ROLLBACK
[0m22:05:38.796876 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_solution'
[0m22:05:38.797174 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m22:05:38.799842 [debug] [Thread-1  ]: Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__solution.caseid", "stg_salesforce__solution.ownerid"
  LINE 14:     id AS solution_id,
               ^
[0m22:05:38.800260 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_solution ......................... [[31mERROR[0m in 0.04s]
[0m22:05:38.800587 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m22:05:38.801420 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:38.801757 [debug] [MainThread]: On master: BEGIN
[0m22:05:38.801939 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:05:38.808923 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:38.809203 [debug] [MainThread]: On master: COMMIT
[0m22:05:38.809368 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:38.809520 [debug] [MainThread]: On master: COMMIT
[0m22:05:38.809725 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:38.809882 [debug] [MainThread]: On master: Close
[0m22:05:38.811416 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:05:38.811624 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_solution' was properly closed.
[0m22:05:38.811808 [info ] [MainThread]: 
[0m22:05:38.811999 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m22:05:38.812346 [debug] [MainThread]: Command end result
[0m22:05:38.819531 [info ] [MainThread]: 
[0m22:05:38.819772 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:05:38.819931 [info ] [MainThread]: 
[0m22:05:38.820146 [error] [MainThread]:   Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__solution.caseid", "stg_salesforce__solution.ownerid"
  LINE 14:     id AS solution_id,
               ^
[0m22:05:38.820527 [info ] [MainThread]: 
[0m22:05:38.820765 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:05:38.821124 [debug] [MainThread]: Command `dbt run` failed at 22:05:38.821076 after 0.43 seconds
[0m22:05:38.821329 [debug] [MainThread]: Flushing usage events


============================== 22:05:50.474338 | 7b8f473a-8ef2-4e59-af3d-697af720b93b ==============================
[0m22:05:50.474338 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:05:50.476826 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:05:50.477090 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:05:50.552857 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:05:50.571425 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:05:50.607579 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:05:50.607879 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:05:50.608923 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:05:50.622484 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:05:50.623888 [info ] [MainThread]: 
[0m22:05:50.624310 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:05:50.625086 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:05:50.634260 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:05:50.634662 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:05:50.634900 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:05:50.643754 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.644823 [debug] [ThreadPool]: On list_dbt: Close
[0m22:05:50.647683 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:05:50.648203 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:05:50.651458 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:50.651746 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:05:50.651919 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:50.659437 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.659755 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:50.659944 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:05:50.660241 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.660776 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:05:50.660951 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:50.661095 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:05:50.661312 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.661467 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:05:50.664732 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m22:05:50.668815 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:05:50.669060 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:05:50.669311 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:50.677238 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.677673 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:05:50.677980 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:05:50.694981 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.698955 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:05:50.699774 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:05:50.699980 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:05:50.702789 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:05:50.705233 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:05:50.705416 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:05:50.705574 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:50.712189 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.712431 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:05:50.712656 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:05:50.728889 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.732669 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:05:50.732915 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:05:50.733085 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:05:50.735539 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:05:50.737994 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:05:50.738211 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:05:50.738401 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:50.745429 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.745684 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:05:50.745858 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:05:50.764469 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.765260 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:05:50.765620 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:05:50.765797 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:05:50.768228 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:05:50.769843 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:05:50.770045 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:05:50.770194 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:50.777026 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.777319 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:05:50.777504 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:05:50.795750 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.796691 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:05:50.796927 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:05:50.797085 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:05:50.801548 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:50.801770 [debug] [MainThread]: On master: BEGIN
[0m22:05:50.801926 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:05:50.808314 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:50.808568 [debug] [MainThread]: On master: COMMIT
[0m22:05:50.808728 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:50.808875 [debug] [MainThread]: On master: COMMIT
[0m22:05:50.809073 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:50.809231 [debug] [MainThread]: On master: Close
[0m22:05:50.810826 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:05:50.811059 [info ] [MainThread]: 
[0m22:05:50.813282 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m22:05:50.813568 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_solution .................................. [RUN]
[0m22:05:50.813967 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_solution)
[0m22:05:50.814159 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m22:05:50.819714 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.820319 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 22:05:50.814294 => 22:05:50.820204
[0m22:05:50.820533 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m22:05:50.840289 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.840971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.841172 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m22:05:50.841351 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:05:50.847953 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.848261 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.848480 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

SELECT
    solution_id,
    solutionnumber AS solution_number,
    solutionname AS solution_name,
    ispublished AS is_published,
    ispublishedinpublickb AS is_published_in_public_kb,
    status AS solution_status,
    isreviewed AS is_reviewed,
    solutionnote AS solution_note,
    caseid AS case_id,
    ownerid AS owner_id,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    timesused AS times_used,
    ishtml AS is_html
FROM
    "dbt"."staging"."stg_salesforce__solution"
WHERE
    isdeleted = FALSE
ORDER BY
    solution_name
    );
  
  
[0m22:05:50.850572 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.854585 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.854837 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m22:05:50.855226 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.857095 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.857303 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m22:05:50.857625 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.868961 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m22:05:50.869214 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.869412 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m22:05:50.870300 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.873246 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.873459 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m22:05:50.873888 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.874681 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 22:05:50.820685 => 22:05:50.874580
[0m22:05:50.874893 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m22:05:50.900100 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_solution ............................. [[32mOK[0m in 0.09s]
[0m22:05:50.900507 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m22:05:50.901364 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:50.901540 [debug] [MainThread]: On master: BEGIN
[0m22:05:50.901695 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:05:50.908396 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:50.908663 [debug] [MainThread]: On master: COMMIT
[0m22:05:50.908835 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:50.908998 [debug] [MainThread]: On master: COMMIT
[0m22:05:50.909196 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:50.909355 [debug] [MainThread]: On master: Close
[0m22:05:50.911044 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:05:50.911282 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_solution' was properly closed.
[0m22:05:50.911481 [info ] [MainThread]: 
[0m22:05:50.911698 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m22:05:50.912042 [debug] [MainThread]: Command end result
[0m22:05:50.918798 [info ] [MainThread]: 
[0m22:05:50.919050 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:05:50.919218 [info ] [MainThread]: 
[0m22:05:50.919398 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:05:50.919887 [debug] [MainThread]: Command `dbt run` succeeded at 22:05:50.919787 after 0.46 seconds
[0m22:05:50.920204 [debug] [MainThread]: Flushing usage events


============================== 22:13:30.110293 | 5b992ca6-ea57-4c7b-8bbe-6ccd47d1e945 ==============================
[0m22:13:30.110293 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:13:30.113891 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_case_history.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:13:30.114186 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:13:30.200295 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:13:30.220723 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:13:30.262338 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:13:30.262703 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:13:30.263696 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:13:30.278795 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:13:30.280477 [info ] [MainThread]: 
[0m22:13:30.281009 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:13:30.281708 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:13:30.289077 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:13:30.289333 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:13:30.289504 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:13:30.304488 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.305401 [debug] [ThreadPool]: On list_dbt: Close
[0m22:13:30.307534 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:13:30.308031 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:13:30.311115 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:30.311331 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:13:30.311496 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:30.318555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.318841 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:30.319020 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:13:30.319290 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.319813 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:30.319993 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:30.320147 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:30.320668 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.320874 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:13:30.348233 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:13:30.351863 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:30.352063 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:13:30.352218 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:30.358501 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.358744 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:30.358916 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:13:30.374440 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.377943 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:13:30.378821 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:13:30.379059 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:13:30.382000 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:13:30.384698 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:30.384933 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:13:30.385105 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:30.391940 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.392270 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:30.392611 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:13:30.409217 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.412858 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:13:30.413102 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:13:30.413267 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:13:30.415828 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:13:30.418006 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:30.418187 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:13:30.418338 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:30.424902 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.425145 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:30.425312 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:13:30.443445 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.444207 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:13:30.444447 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:13:30.444610 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:13:30.446867 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:13:30.448445 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:30.448631 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:13:30.448778 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:30.454917 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.455182 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:30.455361 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:13:30.473167 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.474023 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:13:30.474240 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:13:30.474397 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:13:30.478173 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:30.478357 [debug] [MainThread]: On master: BEGIN
[0m22:13:30.478504 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:13:30.484612 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:30.484869 [debug] [MainThread]: On master: COMMIT
[0m22:13:30.485033 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:30.485183 [debug] [MainThread]: On master: COMMIT
[0m22:13:30.485381 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:30.485542 [debug] [MainThread]: On master: Close
[0m22:13:30.487069 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:13:30.487275 [info ] [MainThread]: 
[0m22:13:30.489547 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m22:13:30.489875 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case_history ...................... [RUN]
[0m22:13:30.490254 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_case_history)
[0m22:13:30.490461 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m22:13:30.496844 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m22:13:30.498295 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 22:13:30.490598 => 22:13:30.498180
[0m22:13:30.498502 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m22:13:30.528209 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case_history"
[0m22:13:30.528976 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:13:30.529198 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: BEGIN
[0m22:13:30.529398 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:13:30.535989 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:13:30.536290 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:13:30.536514 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case_history"
  
    as (
      

select
    case_history_id,
    h.status,
    h.priority,
    h.created_date,
    h.last_modified_date,
    c.account_id,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__case_history_2" h
left join "dbt"."fact"."fact_case" c
    on h.case_history_id = c.case_id
left join "dbt"."dim"."dim_account" a
    on c.account_id = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.owner_id = u.user_id;
    );
  
  
  
[0m22:13:30.537048 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 22:13:30.498637 => 22:13:30.536945
[0m22:13:30.537258 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: ROLLBACK
[0m22:13:30.540975 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case_history'
[0m22:13:30.541179 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: Close
[0m22:13:30.543129 [debug] [Thread-1  ]: Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Parser Error: syntax error at or near ";"
[0m22:13:30.543524 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case_history ............. [[31mERROR[0m in 0.05s]
[0m22:13:30.543870 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m22:13:30.544641 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:30.544823 [debug] [MainThread]: On master: BEGIN
[0m22:13:30.545000 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:13:30.551596 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:30.551962 [debug] [MainThread]: On master: COMMIT
[0m22:13:30.552264 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:30.552613 [debug] [MainThread]: On master: COMMIT
[0m22:13:30.552940 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:30.553122 [debug] [MainThread]: On master: Close
[0m22:13:30.554983 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:13:30.555227 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case_history' was properly closed.
[0m22:13:30.555415 [info ] [MainThread]: 
[0m22:13:30.555604 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m22:13:30.555973 [debug] [MainThread]: Command end result
[0m22:13:30.563874 [info ] [MainThread]: 
[0m22:13:30.564177 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:13:30.564341 [info ] [MainThread]: 
[0m22:13:30.564505 [error] [MainThread]:   Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Parser Error: syntax error at or near ";"
[0m22:13:30.564663 [info ] [MainThread]: 
[0m22:13:30.564852 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:13:30.565266 [debug] [MainThread]: Command `dbt run` failed at 22:13:30.565207 after 0.48 seconds
[0m22:13:30.565471 [debug] [MainThread]: Flushing usage events


============================== 22:13:37.673286 | 82436322-84de-4e14-a150-5416c0a7ddb2 ==============================
[0m22:13:37.673286 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:13:37.675561 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/facts/fact_case_history.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:13:37.675813 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:13:37.749610 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:13:37.767540 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:13:37.802357 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:13:37.802655 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:13:37.803599 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:13:37.818540 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:13:37.820054 [info ] [MainThread]: 
[0m22:13:37.820477 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:13:37.821031 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:13:37.828456 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:13:37.828710 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:13:37.828892 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:13:37.837703 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.838672 [debug] [ThreadPool]: On list_dbt: Close
[0m22:13:37.840886 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:13:37.841406 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:13:37.844500 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:37.844726 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:13:37.844890 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:37.851481 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.851752 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:37.851920 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:13:37.852193 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.852715 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:37.852964 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:37.853135 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:37.853398 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.853568 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:13:37.856502 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:13:37.860019 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:37.860309 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:13:37.860493 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:37.867229 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.867531 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:37.867711 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:13:37.883455 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.887085 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:13:37.887510 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:13:37.887692 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:13:37.889952 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:13:37.892265 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:37.892438 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:13:37.892586 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:37.898665 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.898925 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:37.899099 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:13:37.917578 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.918626 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:13:37.918884 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:13:37.919064 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:13:37.921571 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:13:37.924261 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:37.924474 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:13:37.924638 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:37.931664 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.931920 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:37.932125 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:13:37.951215 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.952379 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:13:37.952628 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:13:37.952796 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:13:37.955560 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:13:37.957295 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:37.957496 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:13:37.957665 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:37.964222 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.964477 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:37.964650 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:13:37.979870 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.983314 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:13:37.983542 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:13:37.983698 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:13:37.987301 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:37.987486 [debug] [MainThread]: On master: BEGIN
[0m22:13:37.987633 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:13:37.994391 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:37.994680 [debug] [MainThread]: On master: COMMIT
[0m22:13:37.994848 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:37.995000 [debug] [MainThread]: On master: COMMIT
[0m22:13:37.995206 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:37.995371 [debug] [MainThread]: On master: Close
[0m22:13:37.996988 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:13:37.997195 [info ] [MainThread]: 
[0m22:13:37.999483 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m22:13:37.999777 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case_history ...................... [RUN]
[0m22:13:38.000142 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_case_history)
[0m22:13:38.000337 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m22:13:38.006436 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m22:13:38.007016 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 22:13:38.000473 => 22:13:38.006895
[0m22:13:38.007220 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m22:13:38.036749 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case_history"
[0m22:13:38.037395 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:13:38.037604 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: BEGIN
[0m22:13:38.037783 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:13:38.044686 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:13:38.045007 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:13:38.045244 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case_history"
  
    as (
      

select
    case_history_id,
    h.status,
    h.priority,
    h.created_date,
    h.last_modified_date,
    c.account_id,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__case_history_2" h
left join "dbt"."fact"."fact_case" c
    on h.case_history_id = c.case_id
left join "dbt"."dim"."dim_account" a
    on c.account_id = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.owner_id = u.user_id
    );
  
  
  
[0m22:13:38.060176 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 22:13:38.007356 => 22:13:38.060033
[0m22:13:38.060442 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: ROLLBACK
[0m22:13:38.064184 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case_history'
[0m22:13:38.064412 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: Close
[0m22:13:38.066818 [debug] [Thread-1  ]: Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Catalog Error: Table with name fact_case does not exist!
  Did you mean "dbt.fact_case"?
  LINE 25: left join "dbt"."fact"."fact_case" c
                     ^
[0m22:13:38.067233 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case_history ............. [[31mERROR[0m in 0.07s]
[0m22:13:38.067581 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m22:13:38.068385 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:38.068560 [debug] [MainThread]: On master: BEGIN
[0m22:13:38.068716 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:13:38.075173 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:38.075452 [debug] [MainThread]: On master: COMMIT
[0m22:13:38.075614 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:38.075762 [debug] [MainThread]: On master: COMMIT
[0m22:13:38.075964 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:38.076123 [debug] [MainThread]: On master: Close
[0m22:13:38.077726 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:13:38.077955 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case_history' was properly closed.
[0m22:13:38.078144 [info ] [MainThread]: 
[0m22:13:38.078347 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:13:38.078699 [debug] [MainThread]: Command end result
[0m22:13:38.086645 [info ] [MainThread]: 
[0m22:13:38.086966 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:13:38.087148 [info ] [MainThread]: 
[0m22:13:38.087320 [error] [MainThread]:   Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Catalog Error: Table with name fact_case does not exist!
  Did you mean "dbt.fact_case"?
  LINE 25: left join "dbt"."fact"."fact_case" c
                     ^
[0m22:13:38.087489 [info ] [MainThread]: 
[0m22:13:38.087681 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:13:38.088112 [debug] [MainThread]: Command `dbt run` failed at 22:13:38.088047 after 0.43 seconds
[0m22:13:38.088314 [debug] [MainThread]: Flushing usage events


============================== 22:13:50.752708 | 733cfac9-751e-4d3d-a238-8d3d88f7bd98 ==============================
[0m22:13:50.752708 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:13:50.755173 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:13:50.755459 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:13:50.831339 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:13:50.850086 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:13:50.885520 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:13:50.885850 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:13:50.886935 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m22:13:50.901670 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:13:50.903217 [info ] [MainThread]: 
[0m22:13:50.903732 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:13:50.904391 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:13:50.912053 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:13:50.912343 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:13:50.912692 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:13:50.920792 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.921770 [debug] [ThreadPool]: On list_dbt: Close
[0m22:13:50.923992 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:13:50.924499 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:13:50.927641 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:50.927874 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:13:50.928046 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:50.935649 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.935945 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:50.936115 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:13:50.936371 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.936909 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:50.937082 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:50.937231 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:50.937456 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.937616 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:13:50.940857 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:13:50.944516 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:50.944750 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:13:50.944913 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:50.951643 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.951909 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:50.952096 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:13:50.967804 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.971291 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:13:50.971721 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:13:50.971890 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:13:50.974439 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:13:50.976710 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:50.976890 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:13:50.977040 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:50.983452 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.983702 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:50.983883 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:13:51.003425 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:51.004160 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:13:51.004397 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:13:51.004560 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:13:51.007042 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:13:51.008527 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:51.008708 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:13:51.008861 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:51.016023 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:51.016397 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:51.016633 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:13:51.032691 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:51.036348 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:13:51.036599 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:13:51.036760 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:13:51.039187 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:13:51.041750 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:51.041984 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:13:51.042137 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:51.048624 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:51.048866 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:51.049040 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:13:51.066947 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:51.067874 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:13:51.068114 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:13:51.068274 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:13:51.072313 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:51.072495 [debug] [MainThread]: On master: BEGIN
[0m22:13:51.072658 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:13:51.078841 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:51.079147 [debug] [MainThread]: On master: COMMIT
[0m22:13:51.079312 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:51.079468 [debug] [MainThread]: On master: COMMIT
[0m22:13:51.079682 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:51.079853 [debug] [MainThread]: On master: Close
[0m22:13:51.081778 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:13:51.081986 [info ] [MainThread]: 
[0m22:13:51.084381 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:13:51.084755 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:13:51.085209 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_case)
[0m22:13:51.085448 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:13:51.091583 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:13:51.092781 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:13:51.085602 => 22:13:51.092612
[0m22:13:51.093047 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:13:51.122982 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:13:51.123686 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:13:51.123930 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:13:51.124155 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:13:51.130984 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:13:51.131306 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:13:51.131530 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.created_date,
    c.last_modified_date,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.account_id = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.owner_id = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.created_date = d.date
where c.is_deleted = false
    );
  
  
  
[0m22:13:51.132579 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:13:51.093186 => 22:13:51.132473
[0m22:13:51.132788 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m22:13:51.136368 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m22:13:51.136588 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:13:51.138747 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "account_id"
  LINE 27:     on c.account_id = a.account_id
                  ^
[0m22:13:51.139189 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case ..................... [[31mERROR[0m in 0.05s]
[0m22:13:51.139547 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:13:51.140412 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:51.140661 [debug] [MainThread]: On master: BEGIN
[0m22:13:51.140823 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:13:51.147389 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:51.147697 [debug] [MainThread]: On master: COMMIT
[0m22:13:51.147867 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:51.148021 [debug] [MainThread]: On master: COMMIT
[0m22:13:51.148229 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:51.148393 [debug] [MainThread]: On master: Close
[0m22:13:51.150346 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:13:51.150604 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:13:51.150783 [info ] [MainThread]: 
[0m22:13:51.150988 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:13:51.151354 [debug] [MainThread]: Command end result
[0m22:13:51.159011 [info ] [MainThread]: 
[0m22:13:51.159341 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:13:51.159525 [info ] [MainThread]: 
[0m22:13:51.159700 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "account_id"
  LINE 27:     on c.account_id = a.account_id
                  ^
[0m22:13:51.159876 [info ] [MainThread]: 
[0m22:13:51.160187 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:13:51.160636 [debug] [MainThread]: Command `dbt run` failed at 22:13:51.160577 after 0.43 seconds
[0m22:13:51.160850 [debug] [MainThread]: Flushing usage events


============================== 22:15:06.220494 | 7efc0044-244c-4491-9354-af4295a555d5 ==============================
[0m22:15:06.220494 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:15:06.228958 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/opportunity.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:15:06.229245 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:15:08.073107 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:15:08.454975 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:15:08.543400 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:15:08.543704 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:15:08.544677 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:15:08.565720 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:15:08.566843 [warn ] [MainThread]: The selection criterion 'models/facts/opportunity.sql' does not match any nodes
[0m22:15:08.567622 [info ] [MainThread]: 
[0m22:15:08.567840 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:15:08.568132 [debug] [MainThread]: Command end result
[0m22:15:08.576388 [debug] [MainThread]: Command `dbt run` succeeded at 22:15:08.576253 after 2.39 seconds
[0m22:15:08.576711 [debug] [MainThread]: Flushing usage events


============================== 22:15:19.603660 | 4b646931-29bf-4a93-95a7-8fce904e6bb4 ==============================
[0m22:15:19.603660 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:15:19.606304 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact-opportunity.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:15:19.606568 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:15:19.697775 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:15:19.717537 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:15:19.765092 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:15:19.765404 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:15:19.766379 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:15:19.780509 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:15:19.781395 [warn ] [MainThread]: The selection criterion 'models/facts/fact-opportunity.sql' does not match any nodes
[0m22:15:19.782051 [info ] [MainThread]: 
[0m22:15:19.782248 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:15:19.782510 [debug] [MainThread]: Command end result
[0m22:15:19.789685 [debug] [MainThread]: Command `dbt run` succeeded at 22:15:19.789581 after 0.21 seconds
[0m22:15:19.789983 [debug] [MainThread]: Flushing usage events


============================== 22:15:24.636304 | 3ac96365-a212-4f2e-85ad-de155b1f4bc3 ==============================
[0m22:15:24.636304 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:15:24.638774 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:15:24.639021 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:15:24.713435 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:15:24.731785 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:15:24.767940 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:15:24.768275 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:15:24.769272 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:15:24.783145 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:15:24.784574 [info ] [MainThread]: 
[0m22:15:24.785035 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:15:24.785712 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:15:24.792991 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:15:24.793309 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:15:24.793488 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:15:24.808409 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.809375 [debug] [ThreadPool]: On list_dbt: Close
[0m22:15:24.811534 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:15:24.812014 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:15:24.815039 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:24.815276 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:15:24.815431 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:24.822176 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.822453 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:24.822634 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:15:24.822889 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.823393 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:15:24.823557 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:24.823702 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:15:24.823908 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.824064 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:15:24.827417 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:15:24.831083 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:15:24.831346 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:15:24.831503 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:24.838384 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.838672 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:15:24.838852 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:15:24.855010 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.858536 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:15:24.860211 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:15:24.860460 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:15:24.863219 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m22:15:24.865603 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:15:24.865801 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:15:24.865955 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:24.872290 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.872562 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:15:24.872762 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:15:24.892628 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.893438 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:15:24.893685 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:15:24.893855 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:15:24.896370 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:15:24.898055 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:15:24.898330 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:15:24.898493 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:24.904991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.905260 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:15:24.905430 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:15:24.923750 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.924768 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:15:24.925011 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:15:24.925170 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:15:24.927588 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:15:24.929990 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:15:24.930185 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:15:24.930335 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:24.936814 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.937070 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:15:24.937238 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:15:24.952469 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.956117 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:15:24.956380 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:15:24.956542 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:15:24.960356 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:24.960548 [debug] [MainThread]: On master: BEGIN
[0m22:15:24.960701 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:15:24.966754 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:24.966997 [debug] [MainThread]: On master: COMMIT
[0m22:15:24.967156 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:24.967308 [debug] [MainThread]: On master: COMMIT
[0m22:15:24.967505 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:24.967663 [debug] [MainThread]: On master: Close
[0m22:15:24.969363 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:15:24.969573 [info ] [MainThread]: 
[0m22:15:24.971992 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:15:24.972273 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:15:24.972632 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity)
[0m22:15:24.972823 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:15:24.978419 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:24.979625 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:15:24.972956 => 22:15:24.979458
[0m22:15:24.979895 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:15:25.009240 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:25.009818 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:25.010029 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:15:25.010214 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:15:25.016870 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:15:25.017209 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:25.017427 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

select
    o.opportunity_id,
    o.amount,
    o.probability,
    o.close_date,
    o.created_date,
    o.last_modified_date,
    a.account_name,
    u.username as owner_username,
    d.date_key as close_date_key
from "dbt"."staging"."stg_salesforce__opportunity" o
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
left join "dbt"."dim"."dim_date" d
    on o.close_date = d.date
where o.is_deleted = false
    );
  
  
  
[0m22:15:25.018518 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:15:24.980034 => 22:15:25.018412
[0m22:15:25.018731 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:15:25.022889 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:15:25.023198 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:15:25.025306 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Values list "o" does not have a column named "account_id"
  LINE 25:     on o.account_id = a.account_id
                  ^
[0m22:15:25.025717 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:15:25.026079 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:15:25.026974 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:25.027221 [debug] [MainThread]: On master: BEGIN
[0m22:15:25.027388 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:15:25.034240 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:25.034555 [debug] [MainThread]: On master: COMMIT
[0m22:15:25.034758 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:25.034911 [debug] [MainThread]: On master: COMMIT
[0m22:15:25.035140 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:25.035304 [debug] [MainThread]: On master: Close
[0m22:15:25.037166 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:15:25.037408 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:15:25.037593 [info ] [MainThread]: 
[0m22:15:25.037794 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:15:25.038159 [debug] [MainThread]: Command end result
[0m22:15:25.046229 [info ] [MainThread]: 
[0m22:15:25.046528 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:15:25.046691 [info ] [MainThread]: 
[0m22:15:25.046852 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Values list "o" does not have a column named "account_id"
  LINE 25:     on o.account_id = a.account_id
                  ^
[0m22:15:25.047011 [info ] [MainThread]: 
[0m22:15:25.047185 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:15:25.047553 [debug] [MainThread]: Command `dbt run` failed at 22:15:25.047502 after 0.43 seconds
[0m22:15:25.047756 [debug] [MainThread]: Flushing usage events


============================== 22:15:55.774723 | bcc294c5-a8a9-4ea4-9185-f990c131d39b ==============================
[0m22:15:55.774723 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:15:55.777704 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:15:55.777976 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:15:55.858555 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:15:55.877457 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:15:55.925293 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:15:55.925813 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m22:15:55.954569 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:15:55.968335 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:15:55.969806 [info ] [MainThread]: 
[0m22:15:55.970281 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:15:55.970846 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:15:55.978007 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:15:55.978321 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:15:55.978520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:15:55.996889 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:55.998078 [debug] [ThreadPool]: On list_dbt: Close
[0m22:15:56.000315 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:15:56.000827 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:15:56.004812 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:56.005110 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:15:56.005286 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:56.012714 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.013009 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:56.013178 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:15:56.013423 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.014143 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:15:56.014369 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:56.014530 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:15:56.014775 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.014950 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:15:56.018085 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:15:56.021685 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:15:56.021967 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:15:56.022134 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:56.028881 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.029113 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:15:56.029294 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:15:56.045543 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.049684 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:15:56.051375 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:15:56.051671 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:15:56.054633 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:15:56.058831 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:15:56.059206 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:15:56.059517 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:56.066333 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.066617 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:15:56.066797 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:15:56.084799 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.085799 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:15:56.086034 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:15:56.086199 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:15:56.088600 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:15:56.092022 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:15:56.092225 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:15:56.092379 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:56.098526 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.098788 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:15:56.098957 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:15:56.117801 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.118548 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:15:56.118772 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:15:56.118934 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:15:56.121890 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:15:56.123848 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:15:56.124117 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:15:56.124287 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:56.131040 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.131336 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:15:56.131518 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:15:56.147411 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.150995 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:15:56.151235 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:15:56.151396 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:15:56.154549 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:56.154781 [debug] [MainThread]: On master: BEGIN
[0m22:15:56.154954 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:15:56.161688 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:56.161997 [debug] [MainThread]: On master: COMMIT
[0m22:15:56.162173 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:56.162324 [debug] [MainThread]: On master: COMMIT
[0m22:15:56.162537 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:56.162695 [debug] [MainThread]: On master: Close
[0m22:15:56.164375 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:15:56.164591 [info ] [MainThread]: 
[0m22:15:56.166883 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:15:56.167322 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:15:56.167726 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m22:15:56.167929 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:15:56.173644 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:56.174172 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:15:56.168067 => 22:15:56.174065
[0m22:15:56.174371 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:15:56.204924 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:56.205542 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:56.205761 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:15:56.206040 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:15:56.213338 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:15:56.213659 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:56.213942 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

select
    o.opportunity_id,
    o.amount,
    o.probability,
    o.close_date,
    o.created_date,
    o.last_modified_date,
    a.account_name,
    u.username as owner_username,
    d.date_key as close_date_key
from "dbt"."dim"."dim_opportunity" o
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
left join "dbt"."dim"."dim_date" d
    on o.close_date = d.date
where o.is_deleted = false
    );
  
  
  
[0m22:15:56.229453 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:15:56.174504 => 22:15:56.229287
[0m22:15:56.229743 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:15:56.233949 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:15:56.234260 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:15:56.237174 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Catalog Error: Table with name dim_date does not exist!
  Did you mean "dbt.dim_date"?
  LINE 28: left join "dbt"."dim"."dim_date" d
                     ^
[0m22:15:56.237625 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.07s]
[0m22:15:56.238036 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:15:56.238943 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:56.239161 [debug] [MainThread]: On master: BEGIN
[0m22:15:56.239318 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:15:56.245801 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:56.246094 [debug] [MainThread]: On master: COMMIT
[0m22:15:56.246496 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:56.246751 [debug] [MainThread]: On master: COMMIT
[0m22:15:56.247032 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:56.247209 [debug] [MainThread]: On master: Close
[0m22:15:56.249005 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:15:56.249245 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:15:56.249432 [info ] [MainThread]: 
[0m22:15:56.249624 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m22:15:56.249974 [debug] [MainThread]: Command end result
[0m22:15:56.283921 [info ] [MainThread]: 
[0m22:15:56.284242 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:15:56.284419 [info ] [MainThread]: 
[0m22:15:56.284596 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Catalog Error: Table with name dim_date does not exist!
  Did you mean "dbt.dim_date"?
  LINE 28: left join "dbt"."dim"."dim_date" d
                     ^
[0m22:15:56.284767 [info ] [MainThread]: 
[0m22:15:56.284941 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:15:56.285280 [debug] [MainThread]: Command `dbt run` failed at 22:15:56.285229 after 0.53 seconds
[0m22:15:56.285494 [debug] [MainThread]: Flushing usage events


============================== 22:16:25.375455 | 0f4504a3-b249-4f99-b34b-fdef200cb7f1 ==============================
[0m22:16:25.375455 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:16:25.378514 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_date.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:16:25.378788 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:16:25.460025 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:16:25.479674 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:16:25.527335 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:16:25.527648 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:16:25.528647 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:16:25.542792 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:16:25.544156 [info ] [MainThread]: 
[0m22:16:25.544578 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:16:25.545267 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:16:25.552739 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:16:25.553041 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:16:25.553239 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:16:25.568009 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.569063 [debug] [ThreadPool]: On list_dbt: Close
[0m22:16:25.571225 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:16:25.571658 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:16:25.574606 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:16:25.574818 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:16:25.574980 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:25.581610 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.581844 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:16:25.582014 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:16:25.582256 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.582728 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:16:25.582896 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:16:25.583052 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:16:25.583265 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.583432 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:16:25.586980 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_fact)
[0m22:16:25.591079 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:16:25.591392 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:16:25.591560 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:25.598178 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.598384 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:16:25.598551 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:16:25.616573 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.617309 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:16:25.620961 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:16:25.621145 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:16:25.623472 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:16:25.625224 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:16:25.625465 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:16:25.625634 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:25.632451 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.632724 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:16:25.632905 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:16:25.648389 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.651952 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:16:25.652223 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:16:25.652408 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:16:25.654727 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:16:25.657073 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:16:25.657255 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:16:25.657407 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:25.664562 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.664903 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:16:25.665107 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:16:25.683994 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.685085 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:16:25.685328 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:16:25.685493 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:16:25.687944 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:16:25.689541 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:16:25.689751 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:16:25.689921 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:25.697822 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.698057 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:16:25.698231 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:16:25.713722 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.718057 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:16:25.718299 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:16:25.718459 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:16:25.721649 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:25.721850 [debug] [MainThread]: On master: BEGIN
[0m22:16:25.722010 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:16:25.728451 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:25.728711 [debug] [MainThread]: On master: COMMIT
[0m22:16:25.728876 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:25.729037 [debug] [MainThread]: On master: COMMIT
[0m22:16:25.729242 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:25.729409 [debug] [MainThread]: On master: Close
[0m22:16:25.730956 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:16:25.731170 [info ] [MainThread]: 
[0m22:16:25.733177 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:16:25.733453 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_date ...................................... [RUN]
[0m22:16:25.733834 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_date)
[0m22:16:25.734044 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:16:25.759518 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.759850 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:16:25.760042 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:16:25.766437 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.766725 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.766925 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:16:25.767310 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.807772 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:16:25.810090 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:16:25.734188 => 22:16:25.809974
[0m22:16:25.810308 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:16:25.828258 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m22:16:25.828716 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.829137 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

with date_range as (
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1

)

select
    *,
    case when date_day = current_date then true else false end as is_current_date,

    case
        when
            extract(year from date_day) = extract(year from current_date)
            and extract(month from date_day) = extract(month from current_date)
        then true
        else false
    end as is_current_month,

    case
        when
            extract(year from date_day) = extract(year from current_date)
            and extract(quarter from date_day) = extract(quarter from current_date)
        then true
        else false
    end as is_current_quarter,

    case
        when extract(year from date_day) = extract(year from current_date)
        then true
        else false
    end as is_current_year,

    concat(month_name, ' ', year_number) as year_qualified_month_name
from date_range
    );
  
  
[0m22:16:25.909230 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.913219 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.913457 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m22:16:25.913879 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.923120 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:16:25.923343 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.923532 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:16:25.928330 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.931286 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.931512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m22:16:25.931818 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.932565 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:16:25.810452 => 22:16:25.932470
[0m22:16:25.932771 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:16:25.969075 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_date ................................. [[32mOK[0m in 0.24s]
[0m22:16:25.969511 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:16:25.970270 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:25.970469 [debug] [MainThread]: On master: BEGIN
[0m22:16:25.970632 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:16:25.977843 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:25.978142 [debug] [MainThread]: On master: COMMIT
[0m22:16:25.978333 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:25.978502 [debug] [MainThread]: On master: COMMIT
[0m22:16:25.978719 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:25.978886 [debug] [MainThread]: On master: Close
[0m22:16:25.980645 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:16:25.980909 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_date' was properly closed.
[0m22:16:25.981100 [info ] [MainThread]: 
[0m22:16:25.981311 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.44 seconds (0.44s).
[0m22:16:25.981674 [debug] [MainThread]: Command end result
[0m22:16:26.015741 [info ] [MainThread]: 
[0m22:16:26.016038 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:16:26.016213 [info ] [MainThread]: 
[0m22:16:26.016395 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:16:26.016717 [debug] [MainThread]: Command `dbt run` succeeded at 22:16:26.016668 after 0.66 seconds
[0m22:16:26.016908 [debug] [MainThread]: Flushing usage events


============================== 22:16:29.675769 | 2649339f-bd28-439e-9aa0-e99d6238b3d9 ==============================
[0m22:16:29.675769 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:16:29.678158 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:16:29.678400 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:16:29.753353 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:16:29.772552 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:16:29.808335 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:16:29.808663 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:16:29.809691 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:16:29.824995 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:16:29.826536 [info ] [MainThread]: 
[0m22:16:29.827197 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:16:29.827965 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:16:29.835540 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:16:29.835808 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:16:29.835987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:16:29.844714 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.845661 [debug] [ThreadPool]: On list_dbt: Close
[0m22:16:29.848005 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:16:29.848548 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:16:29.851761 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:16:29.851998 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:16:29.852175 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:29.859133 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.859390 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:16:29.859557 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:16:29.859801 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.860312 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:16:29.860477 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:16:29.860623 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:16:29.860838 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.860993 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:16:29.864103 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m22:16:29.867696 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:16:29.867912 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:16:29.868073 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:29.875292 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.875607 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:16:29.875785 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:16:29.895132 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.895981 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:16:29.896408 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:16:29.896572 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:16:29.899033 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:16:29.900781 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:16:29.901016 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:16:29.901189 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:29.907705 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.907969 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:16:29.908145 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:16:29.924230 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.927892 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:16:29.928182 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:16:29.928349 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:16:29.930684 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:16:29.933403 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:16:29.933608 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:16:29.933765 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:29.940563 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.940853 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:16:29.941033 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:16:29.959209 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.960144 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:16:29.960369 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:16:29.960531 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:16:29.962898 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:16:29.965319 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:16:29.965510 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:16:29.965676 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:29.972041 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.972442 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:16:29.972669 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:16:29.988798 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.992269 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:16:29.992499 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:16:29.992659 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:16:29.996924 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:29.997209 [debug] [MainThread]: On master: BEGIN
[0m22:16:29.997390 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:16:30.004002 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:30.004275 [debug] [MainThread]: On master: COMMIT
[0m22:16:30.004435 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:30.004590 [debug] [MainThread]: On master: COMMIT
[0m22:16:30.004795 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:30.004985 [debug] [MainThread]: On master: Close
[0m22:16:30.006727 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:16:30.006978 [info ] [MainThread]: 
[0m22:16:30.008151 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:16:30.008453 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:16:30.008880 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m22:16:30.009131 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:16:30.014789 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:16:30.015350 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:16:30.009271 => 22:16:30.015237
[0m22:16:30.015564 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:16:30.045091 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:16:30.045784 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:16:30.046000 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:16:30.046197 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:16:30.052659 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:30.052959 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:16:30.053167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

select
    o.opportunity_id,
    o.amount,
    o.probability,
    o.close_date,
    o.created_date,
    o.last_modified_date,
    a.account_name,
    u.username as owner_username,
    d.date_key as close_date_key
from "dbt"."dim"."dim_opportunity" o
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
left join "dbt"."dim"."dim_date" d
    on o.close_date = d.date
where o.is_deleted = false
    );
  
  
  
[0m22:16:30.054008 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:16:30.015712 => 22:16:30.053886
[0m22:16:30.054211 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:16:30.057915 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:16:30.058140 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:16:30.060133 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 29:     on o.close_date = d.date
                                 ^
[0m22:16:30.060542 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:16:30.060890 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:16:30.061685 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:30.061872 [debug] [MainThread]: On master: BEGIN
[0m22:16:30.062026 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:16:30.068724 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:30.069026 [debug] [MainThread]: On master: COMMIT
[0m22:16:30.069204 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:30.069360 [debug] [MainThread]: On master: COMMIT
[0m22:16:30.069574 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:30.069739 [debug] [MainThread]: On master: Close
[0m22:16:30.071593 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:16:30.071866 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:16:30.072065 [info ] [MainThread]: 
[0m22:16:30.072278 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:16:30.072646 [debug] [MainThread]: Command end result
[0m22:16:30.080784 [info ] [MainThread]: 
[0m22:16:30.081207 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:16:30.081463 [info ] [MainThread]: 
[0m22:16:30.081637 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 29:     on o.close_date = d.date
                                 ^
[0m22:16:30.081802 [info ] [MainThread]: 
[0m22:16:30.081980 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:16:30.082357 [debug] [MainThread]: Command `dbt run` failed at 22:16:30.082307 after 0.42 seconds
[0m22:16:30.082567 [debug] [MainThread]: Flushing usage events


============================== 22:19:32.078516 | baca8b82-a742-4cc4-9b06-d59abfd69865 ==============================
[0m22:19:32.078516 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:19:32.081731 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:19:32.082017 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:19:32.168954 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:19:32.187080 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:19:32.231133 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:19:32.231448 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:19:32.232408 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:19:32.249930 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:19:32.251488 [info ] [MainThread]: 
[0m22:19:32.251973 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:19:32.252551 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:19:32.260445 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:19:32.260727 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:19:32.260903 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:19:32.277161 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.278188 [debug] [ThreadPool]: On list_dbt: Close
[0m22:19:32.280387 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:19:32.280840 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:19:32.283846 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:32.284079 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:19:32.284240 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:32.292744 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.293055 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:32.293238 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:19:32.293586 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.294170 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:19:32.294359 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:32.294524 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:19:32.294752 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.294925 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:19:32.298491 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:19:32.302067 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:19:32.302335 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:19:32.302537 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:32.309770 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.310070 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:19:32.310244 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:19:32.325775 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.329380 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:19:32.329985 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:19:32.330160 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:19:32.333059 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:19:32.335766 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:19:32.336002 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:19:32.336385 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:32.343624 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.343918 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:19:32.344104 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:19:32.360930 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.364747 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:19:32.364983 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:19:32.365141 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:19:32.367993 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:19:32.370355 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:19:32.370541 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:19:32.370697 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:32.377462 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.377731 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:19:32.377908 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:19:32.396744 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.397713 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:19:32.397938 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:19:32.398096 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:19:32.400570 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:19:32.404107 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:19:32.404367 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:19:32.404529 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:32.411355 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.411603 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:19:32.411778 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:19:32.430069 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.430835 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:19:32.431074 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:19:32.431232 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:19:32.433701 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:32.433891 [debug] [MainThread]: On master: BEGIN
[0m22:19:32.434040 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:19:32.440681 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:32.440939 [debug] [MainThread]: On master: COMMIT
[0m22:19:32.441098 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:32.441244 [debug] [MainThread]: On master: COMMIT
[0m22:19:32.441444 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:32.441604 [debug] [MainThread]: On master: Close
[0m22:19:32.443408 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:19:32.443678 [info ] [MainThread]: 
[0m22:19:32.445221 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:19:32.445578 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:19:32.446027 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_opportunity)
[0m22:19:32.446252 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:19:32.454169 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:32.455283 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:19:32.446415 => 22:19:32.455081
[0m22:19:32.455581 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:19:32.484785 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:32.485335 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:32.485546 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:19:32.485725 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:19:32.492373 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:19:32.492671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:32.492890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.amount,
        o.probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.username as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date
    WHERE o.is_deleted = false

    
)

SELECT * FROM latest_data;
    );
  
  
  
[0m22:19:32.493396 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:19:32.455723 => 22:19:32.493295
[0m22:19:32.493600 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:19:32.497247 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:19:32.497449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:19:32.499515 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:19:32.499933 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:19:32.500278 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:19:32.501060 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:32.501253 [debug] [MainThread]: On master: BEGIN
[0m22:19:32.501399 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:19:32.508029 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:32.508538 [debug] [MainThread]: On master: COMMIT
[0m22:19:32.508739 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:32.508907 [debug] [MainThread]: On master: COMMIT
[0m22:19:32.509159 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:32.509356 [debug] [MainThread]: On master: Close
[0m22:19:32.511157 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:19:32.511421 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:19:32.511623 [info ] [MainThread]: 
[0m22:19:32.511827 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:19:32.512213 [debug] [MainThread]: Command end result
[0m22:19:32.519689 [info ] [MainThread]: 
[0m22:19:32.520033 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:19:32.520224 [info ] [MainThread]: 
[0m22:19:32.520399 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:19:32.520568 [info ] [MainThread]: 
[0m22:19:32.520755 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:19:32.521232 [debug] [MainThread]: Command `dbt run` failed at 22:19:32.521134 after 0.46 seconds
[0m22:19:32.521530 [debug] [MainThread]: Flushing usage events


============================== 22:19:40.323557 | 7cd3c86b-acc6-41d1-a4d4-0e7479b29b5d ==============================
[0m22:19:40.323557 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:19:40.326119 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:19:40.326380 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:19:40.404079 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:19:40.422883 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:19:40.458434 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:19:40.458735 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:19:40.459666 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:19:40.473225 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:19:40.474626 [info ] [MainThread]: 
[0m22:19:40.475068 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:19:40.475745 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:19:40.483260 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:19:40.483542 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:19:40.483715 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:19:40.492437 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.493362 [debug] [ThreadPool]: On list_dbt: Close
[0m22:19:40.495565 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:19:40.496053 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:19:40.498923 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:40.499122 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:19:40.499281 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:40.506710 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.507018 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:40.507194 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:19:40.507469 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.508028 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:19:40.508273 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:40.508436 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:19:40.508669 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.508836 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:19:40.511849 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:19:40.515405 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:19:40.515621 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:19:40.515774 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:40.522874 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.523148 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:19:40.523327 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:19:40.541361 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.542369 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:19:40.543117 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:19:40.543319 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:19:40.545917 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:19:40.548539 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:19:40.548749 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:19:40.548902 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:40.555945 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.556219 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:19:40.556391 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:19:40.575700 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.576537 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:19:40.576771 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:19:40.576929 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:19:40.579338 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:19:40.581091 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:19:40.581343 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:19:40.581503 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:40.588500 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.588793 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:19:40.588984 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:19:40.604732 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.608090 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:19:40.608330 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:19:40.608490 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:19:40.610854 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:19:40.613407 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:19:40.613594 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:19:40.613744 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:40.620160 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.620411 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:19:40.620588 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:19:40.637479 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.641829 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:19:40.642080 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:19:40.642240 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:19:40.646353 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:40.646648 [debug] [MainThread]: On master: BEGIN
[0m22:19:40.646827 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:19:40.653889 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:40.654183 [debug] [MainThread]: On master: COMMIT
[0m22:19:40.654343 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:40.654492 [debug] [MainThread]: On master: COMMIT
[0m22:19:40.654695 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:40.654857 [debug] [MainThread]: On master: Close
[0m22:19:40.656539 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:19:40.656745 [info ] [MainThread]: 
[0m22:19:40.658339 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:19:40.658707 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:19:40.659134 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m22:19:40.659338 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:19:40.667160 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:40.667680 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:19:40.659470 => 22:19:40.667577
[0m22:19:40.667886 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:19:40.697052 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:40.697732 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:40.697948 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:19:40.698137 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:19:40.704863 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:19:40.705156 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:40.705366 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.amount,
        o.probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.username as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date
    WHERE o.is_deleted = false

    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:19:40.706187 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:19:40.668016 => 22:19:40.706081
[0m22:19:40.706390 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:19:40.710087 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:19:40.710310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:19:40.712255 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 30:         ON o.close_date = d.date
                                     ^
[0m22:19:40.712648 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:19:40.712985 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:19:40.713745 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:40.713917 [debug] [MainThread]: On master: BEGIN
[0m22:19:40.714061 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:19:40.720752 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:40.721149 [debug] [MainThread]: On master: COMMIT
[0m22:19:40.721373 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:40.721591 [debug] [MainThread]: On master: COMMIT
[0m22:19:40.721897 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:40.722083 [debug] [MainThread]: On master: Close
[0m22:19:40.724122 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:19:40.724353 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:19:40.724541 [info ] [MainThread]: 
[0m22:19:40.724728 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:19:40.725063 [debug] [MainThread]: Command end result
[0m22:19:40.732815 [info ] [MainThread]: 
[0m22:19:40.733139 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:19:40.733460 [info ] [MainThread]: 
[0m22:19:40.733664 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 30:         ON o.close_date = d.date
                                     ^
[0m22:19:40.733833 [info ] [MainThread]: 
[0m22:19:40.734006 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:19:40.734373 [debug] [MainThread]: Command `dbt run` failed at 22:19:40.734323 after 0.43 seconds
[0m22:19:40.734571 [debug] [MainThread]: Flushing usage events


============================== 22:20:11.402543 | 41446852-1e43-45d3-b12c-b916cd445400 ==============================
[0m22:20:11.402543 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:20:11.405551 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:20:11.405811 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:20:11.482596 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:20:11.500310 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:20:11.535621 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:20:11.535926 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:20:11.536888 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m22:20:11.553857 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:20:11.555362 [info ] [MainThread]: 
[0m22:20:11.555792 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:20:11.556448 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:20:11.563742 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:20:11.564013 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:20:11.564194 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:20:11.579840 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.580735 [debug] [ThreadPool]: On list_dbt: Close
[0m22:20:11.582828 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:20:11.583314 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:20:11.586173 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:11.586380 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:20:11.586539 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:11.593532 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.593776 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:11.593945 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:20:11.594193 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.594709 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:20:11.594874 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:11.595025 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:20:11.595238 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.595613 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:20:11.599326 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:20:11.602992 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:20:11.603255 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:20:11.603423 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:11.610192 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.610450 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:20:11.610623 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:20:11.626051 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.629759 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:20:11.630392 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:20:11.630565 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:20:11.633184 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:20:11.635716 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:20:11.635926 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:20:11.636082 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:11.675769 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.677624 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:20:11.677831 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:20:11.694972 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.698486 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:20:11.698724 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:20:11.698879 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:20:11.701302 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:20:11.702886 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:20:11.703795 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:20:11.703952 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:11.710288 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.710551 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:20:11.710721 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:20:11.729044 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.730091 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:20:11.730337 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:20:11.730495 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:20:11.732854 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:20:11.734478 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:20:11.734667 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:20:11.734822 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:11.742749 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.742968 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:20:11.743138 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:20:11.761012 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.761716 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:20:11.761951 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:20:11.762107 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:20:11.764160 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:11.764364 [debug] [MainThread]: On master: BEGIN
[0m22:20:11.764527 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:20:11.770416 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:11.770667 [debug] [MainThread]: On master: COMMIT
[0m22:20:11.770829 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:11.770977 [debug] [MainThread]: On master: COMMIT
[0m22:20:11.771177 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:11.771336 [debug] [MainThread]: On master: Close
[0m22:20:11.772847 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:20:11.773069 [info ] [MainThread]: 
[0m22:20:11.775403 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:20:11.775682 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:20:11.776040 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_opportunity)
[0m22:20:11.776231 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:20:11.784129 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:11.784706 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:20:11.776366 => 22:20:11.784590
[0m22:20:11.784910 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:20:11.815085 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:11.815718 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:11.815930 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:20:11.816130 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:11.823027 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:20:11.823344 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:11.823561 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.amount,
        o.probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.username as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here
    WHERE o.is_deleted = false

    
)

SELECT * FROM latest_data;
    );
  
  
  
[0m22:20:11.824099 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:20:11.785044 => 22:20:11.824000
[0m22:20:11.824302 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:20:11.827971 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:20:11.828226 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:20:11.830258 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:20:11.830672 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:20:11.831018 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:20:11.831838 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:11.832014 [debug] [MainThread]: On master: BEGIN
[0m22:20:11.832171 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:20:11.838554 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:11.838880 [debug] [MainThread]: On master: COMMIT
[0m22:20:11.839108 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:11.839273 [debug] [MainThread]: On master: COMMIT
[0m22:20:11.839498 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:11.839659 [debug] [MainThread]: On master: Close
[0m22:20:11.841487 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:20:11.841777 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:20:11.841982 [info ] [MainThread]: 
[0m22:20:11.842244 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m22:20:11.842650 [debug] [MainThread]: Command end result
[0m22:20:11.850381 [info ] [MainThread]: 
[0m22:20:11.850746 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:20:11.850948 [info ] [MainThread]: 
[0m22:20:11.851125 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:20:11.851300 [info ] [MainThread]: 
[0m22:20:11.851484 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:20:11.851901 [debug] [MainThread]: Command `dbt run` failed at 22:20:11.851849 after 0.47 seconds
[0m22:20:11.852120 [debug] [MainThread]: Flushing usage events


============================== 22:20:18.135840 | 73c1150a-1133-4167-9e3a-f9adf004690e ==============================
[0m22:20:18.135840 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:20:18.138601 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:20:18.138854 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:20:18.221994 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:20:18.240326 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:20:18.276826 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:20:18.277353 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m22:20:18.310284 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m22:20:18.323158 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:20:18.324697 [info ] [MainThread]: 
[0m22:20:18.325170 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:20:18.325784 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:20:18.333447 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:20:18.334278 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:20:18.334714 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:20:18.344205 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.345291 [debug] [ThreadPool]: On list_dbt: Close
[0m22:20:18.347759 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:20:18.348362 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:20:18.351416 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:18.351655 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:20:18.351838 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:18.358902 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.359202 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:18.359385 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:20:18.359642 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.360173 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:20:18.360351 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:18.360507 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:20:18.360733 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.360903 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:20:18.363775 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m22:20:18.367235 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:20:18.367496 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:20:18.368258 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:18.374748 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.375040 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:20:18.375232 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:20:18.394422 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.395277 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:20:18.396270 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:20:18.396560 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:20:18.399295 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:20:18.400973 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:20:18.401173 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:20:18.401322 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:18.408497 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.408812 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:20:18.409025 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:20:18.426003 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.430443 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:20:18.430825 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:20:18.431000 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:20:18.433858 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:20:18.436589 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:20:18.436799 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:20:18.436950 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:18.443991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.444197 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:20:18.444373 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:20:18.462605 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.463523 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:20:18.463758 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:20:18.463920 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:20:18.466476 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:20:18.468223 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:20:18.469376 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:20:18.469539 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:18.475907 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.476160 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:20:18.476330 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:20:18.491582 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.495250 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:20:18.495502 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:20:18.495664 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:20:18.498874 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:18.499084 [debug] [MainThread]: On master: BEGIN
[0m22:20:18.499250 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:20:18.506450 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:18.506749 [debug] [MainThread]: On master: COMMIT
[0m22:20:18.506912 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:18.507067 [debug] [MainThread]: On master: COMMIT
[0m22:20:18.507272 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:18.507430 [debug] [MainThread]: On master: Close
[0m22:20:18.509254 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:20:18.509544 [info ] [MainThread]: 
[0m22:20:18.512253 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:20:18.512608 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:20:18.513017 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m22:20:18.513214 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:20:18.519396 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:18.519943 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:20:18.513350 => 22:20:18.519833
[0m22:20:18.520149 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:20:18.577087 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:18.577763 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:18.577973 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:20:18.578167 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:18.584676 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:20:18.584933 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:18.585147 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.amount,
        o.probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.username as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here
    WHERE o.is_deleted = false

    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:20:18.586130 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:20:18.520281 => 22:20:18.586024
[0m22:20:18.586361 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:20:18.590273 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:20:18.590521 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:20:18.592726 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "o" does not have a column named "is_deleted"
  LINE 31:     WHERE o.is_deleted = false
                     ^
[0m22:20:18.593189 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.08s]
[0m22:20:18.593523 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:20:18.594319 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:18.594575 [debug] [MainThread]: On master: BEGIN
[0m22:20:18.594738 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:20:18.601938 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:18.602232 [debug] [MainThread]: On master: COMMIT
[0m22:20:18.602467 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:18.602698 [debug] [MainThread]: On master: COMMIT
[0m22:20:18.602961 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:18.603137 [debug] [MainThread]: On master: Close
[0m22:20:18.604845 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:20:18.605083 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:20:18.605277 [info ] [MainThread]: 
[0m22:20:18.605453 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m22:20:18.605796 [debug] [MainThread]: Command end result
[0m22:20:18.612633 [info ] [MainThread]: 
[0m22:20:18.612883 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:20:18.613044 [info ] [MainThread]: 
[0m22:20:18.613223 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "o" does not have a column named "is_deleted"
  LINE 31:     WHERE o.is_deleted = false
                     ^
[0m22:20:18.613466 [info ] [MainThread]: 
[0m22:20:18.613812 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:20:18.614202 [debug] [MainThread]: Command `dbt run` failed at 22:20:18.614147 after 0.50 seconds
[0m22:20:18.614415 [debug] [MainThread]: Flushing usage events


============================== 22:22:01.641134 | c84f3c9c-f9ba-4161-a8ea-9c5c7ed2747c ==============================
[0m22:22:01.641134 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:22:01.644339 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:22:01.644584 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:22:01.728819 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:22:01.749912 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:22:01.799921 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:22:01.800207 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:22:01.801179 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:22:01.825995 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:22:01.839187 [info ] [MainThread]: 
[0m22:22:01.848252 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:22:01.848956 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:22:01.860533 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:22:01.860871 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:22:01.861209 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:22:01.876761 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.877850 [debug] [ThreadPool]: On list_dbt: Close
[0m22:22:01.879932 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:22:01.880349 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:22:01.883368 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:01.883570 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:22:01.883731 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:01.890319 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.890584 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:01.890759 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:22:01.891020 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.891536 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:22:01.891705 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:01.891849 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:22:01.892056 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.892217 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:22:01.895170 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:22:01.898535 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:22:01.898729 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:22:01.898878 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:01.904891 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.905138 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:22:01.905311 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:22:01.920949 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.924511 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:22:01.925262 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:22:01.925484 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:22:01.927779 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:22:01.929353 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:22:01.930387 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:22:01.930568 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:01.937425 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.937723 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:22:01.937917 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:22:01.958376 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.959270 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:22:01.959525 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:22:01.959694 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:22:01.962068 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:22:01.963783 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:22:01.964090 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:22:01.964259 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:01.971345 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.971634 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:22:01.971818 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:22:01.991098 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.992019 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:22:01.992250 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:22:01.992409 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:22:01.995018 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:22:01.998066 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:22:01.998304 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:22:01.998474 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:02.004651 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:02.004903 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:22:02.005078 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:22:02.020932 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:02.024347 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:22:02.024582 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:22:02.024734 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:22:02.028854 [debug] [MainThread]: Using duckdb connection "master"
[0m22:22:02.029113 [debug] [MainThread]: On master: BEGIN
[0m22:22:02.029276 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:22:02.036237 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:22:02.036511 [debug] [MainThread]: On master: COMMIT
[0m22:22:02.036685 [debug] [MainThread]: Using duckdb connection "master"
[0m22:22:02.036838 [debug] [MainThread]: On master: COMMIT
[0m22:22:02.037047 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:22:02.037208 [debug] [MainThread]: On master: Close
[0m22:22:02.038770 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:22:02.038977 [info ] [MainThread]: 
[0m22:22:02.040552 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:22:02.040866 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:22:02.041253 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m22:22:02.041448 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:22:02.049164 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:22:02.050355 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:22:02.041586 => 22:22:02.050168
[0m22:22:02.050729 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:22:02.079964 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:22:02.080634 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:22:02.080840 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:22:02.081018 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:22:02.087719 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:22:02.088009 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:22:02.088225 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.amount,
        o.probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.username as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here

    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:22:02.089207 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:22:02.050970 => 22:22:02.089102
[0m22:22:02.089418 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:22:02.093119 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:22:02.093354 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:22:02.095336 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "o" does not have a column named "amount"
  LINE 16:         o.amount,
                   ^
[0m22:22:02.095756 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:22:02.096100 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:22:02.096844 [debug] [MainThread]: Using duckdb connection "master"
[0m22:22:02.097026 [debug] [MainThread]: On master: BEGIN
[0m22:22:02.097196 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:22:02.103859 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:22:02.104145 [debug] [MainThread]: On master: COMMIT
[0m22:22:02.104309 [debug] [MainThread]: Using duckdb connection "master"
[0m22:22:02.104464 [debug] [MainThread]: On master: COMMIT
[0m22:22:02.104663 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:22:02.104824 [debug] [MainThread]: On master: Close
[0m22:22:02.106404 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:22:02.106611 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:22:02.106985 [info ] [MainThread]: 
[0m22:22:02.107209 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:22:02.107582 [debug] [MainThread]: Command end result
[0m22:22:02.115395 [info ] [MainThread]: 
[0m22:22:02.115671 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:22:02.115840 [info ] [MainThread]: 
[0m22:22:02.116005 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "o" does not have a column named "amount"
  LINE 16:         o.amount,
                   ^
[0m22:22:02.116168 [info ] [MainThread]: 
[0m22:22:02.116343 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:22:02.116713 [debug] [MainThread]: Command `dbt run` failed at 22:22:02.116663 after 0.50 seconds
[0m22:22:02.117061 [debug] [MainThread]: Flushing usage events


============================== 22:22:59.719705 | 9b3d8d86-cf7a-4ec8-bf10-48aaa854e30c ==============================
[0m22:22:59.719705 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:22:59.723041 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:22:59.723342 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:22:59.807093 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:22:59.827271 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:22:59.863188 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:22:59.863473 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:22:59.864460 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:22:59.882029 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:22:59.883660 [info ] [MainThread]: 
[0m22:22:59.884161 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:22:59.884950 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:22:59.892007 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:22:59.892289 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:22:59.892483 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:22:59.908851 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.909934 [debug] [ThreadPool]: On list_dbt: Close
[0m22:22:59.912185 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:22:59.912629 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:22:59.915634 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:59.915844 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:22:59.916006 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:59.922813 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.923081 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:59.923252 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:22:59.923494 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.924013 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:22:59.924178 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:59.924326 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:22:59.924542 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.924703 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:22:59.927760 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:22:59.931383 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:22:59.931590 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:22:59.931743 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:59.938492 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.938759 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:22:59.938933 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:22:59.957196 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.958233 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:22:59.960258 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:22:59.960439 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:22:59.963101 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:22:59.965825 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:22:59.966038 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:22:59.966194 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:59.973128 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.973444 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:22:59.973638 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:22:59.992530 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.993399 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:22:59.993639 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:22:59.993799 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:22:59.997369 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:22:59.999110 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:22:59.999326 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:22:59.999486 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:23:00.006631 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:23:00.006917 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:23:00.007095 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:23:00.023191 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:23:00.026814 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:23:00.027065 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:23:00.027229 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:23:00.029816 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:23:00.032217 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:23:00.032420 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:23:00.032570 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:23:00.039432 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:23:00.039723 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:23:00.039908 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:23:00.055364 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:23:00.058998 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:23:00.059256 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:23:00.059427 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:23:00.063423 [debug] [MainThread]: Using duckdb connection "master"
[0m22:23:00.063636 [debug] [MainThread]: On master: BEGIN
[0m22:23:00.063793 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:23:00.070228 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:23:00.070486 [debug] [MainThread]: On master: COMMIT
[0m22:23:00.070647 [debug] [MainThread]: Using duckdb connection "master"
[0m22:23:00.070795 [debug] [MainThread]: On master: COMMIT
[0m22:23:00.071001 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:23:00.071158 [debug] [MainThread]: On master: Close
[0m22:23:00.072700 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:23:00.072910 [info ] [MainThread]: 
[0m22:23:00.075610 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:23:00.075894 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:23:00.076272 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity)
[0m22:23:00.076472 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:23:00.084461 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:23:00.085525 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:23:00.076609 => 22:23:00.085349
[0m22:23:00.085815 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:23:00.115896 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:23:00.116511 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:23:00.116853 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:23:00.117071 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:23:00.123532 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:23:00.123854 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:23:00.124092 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here

    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:23:00.125024 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:23:00.085974 => 22:23:00.124920
[0m22:23:00.125239 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:23:00.129574 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:23:00.129781 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:23:00.131897 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date_key"
  LINE 23:         d.date_key as close_date_key
                   ^
[0m22:23:00.132329 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.06s]
[0m22:23:00.132659 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:23:00.133463 [debug] [MainThread]: Using duckdb connection "master"
[0m22:23:00.133715 [debug] [MainThread]: On master: BEGIN
[0m22:23:00.133876 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:23:00.140544 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:23:00.141014 [debug] [MainThread]: On master: COMMIT
[0m22:23:00.141216 [debug] [MainThread]: Using duckdb connection "master"
[0m22:23:00.141373 [debug] [MainThread]: On master: COMMIT
[0m22:23:00.141575 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:23:00.141774 [debug] [MainThread]: On master: Close
[0m22:23:00.143710 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:23:00.143880 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:23:00.144060 [info ] [MainThread]: 
[0m22:23:00.144243 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:23:00.144575 [debug] [MainThread]: Command end result
[0m22:23:00.152212 [info ] [MainThread]: 
[0m22:23:00.152552 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:23:00.152731 [info ] [MainThread]: 
[0m22:23:00.152909 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date_key"
  LINE 23:         d.date_key as close_date_key
                   ^
[0m22:23:00.153078 [info ] [MainThread]: 
[0m22:23:00.153262 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:23:00.153632 [debug] [MainThread]: Command `dbt run` failed at 22:23:00.153585 after 0.46 seconds
[0m22:23:00.153839 [debug] [MainThread]: Flushing usage events


============================== 22:24:17.916404 | ad35447f-57f5-453b-b979-34ed2ce9a0f7 ==============================
[0m22:24:17.916404 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:24:17.919568 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_date.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:24:17.919835 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:24:18.005303 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:24:18.024356 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:24:18.059487 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:24:18.059768 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:24:18.060770 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:24:18.076115 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:24:18.077718 [info ] [MainThread]: 
[0m22:24:18.078203 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:24:18.078878 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:24:18.086427 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:24:18.086687 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:24:18.086855 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:18.104079 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.105114 [debug] [ThreadPool]: On list_dbt: Close
[0m22:24:18.107587 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:24:18.108089 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:24:18.111038 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:18.111236 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:24:18.111395 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:18.120514 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.120760 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:18.120922 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:24:18.121164 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.121669 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:24:18.121837 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:18.121985 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:24:18.122193 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.122361 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:24:18.125539 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m22:24:18.129172 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:18.129435 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:24:18.129635 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:18.136810 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.137092 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:18.137269 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:24:18.153063 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.156749 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:24:18.157438 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:24:18.157635 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:24:18.159964 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:24:18.162427 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:18.162653 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:24:18.162802 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:18.169538 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.169832 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:18.170019 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:24:18.190138 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.191216 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:24:18.191455 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:24:18.191620 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:24:18.194124 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:24:18.196544 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:18.196728 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:24:18.196885 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:18.203836 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.204090 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:18.204261 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:24:18.220891 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.226130 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:24:18.226375 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:24:18.226542 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:24:18.231349 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:24:18.234550 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:18.234809 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:24:18.234968 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:18.242286 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.242565 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:18.242741 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:24:18.261292 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.262054 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:24:18.262312 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:24:18.262484 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:24:18.264988 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:18.265188 [debug] [MainThread]: On master: BEGIN
[0m22:24:18.265336 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:18.271810 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:18.272072 [debug] [MainThread]: On master: COMMIT
[0m22:24:18.272227 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:18.272373 [debug] [MainThread]: On master: COMMIT
[0m22:24:18.272571 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:18.272729 [debug] [MainThread]: On master: Close
[0m22:24:18.274269 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:18.274471 [info ] [MainThread]: 
[0m22:24:18.277197 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:24:18.277475 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_date ...................................... [RUN]
[0m22:24:18.277836 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_date)
[0m22:24:18.278026 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:24:18.304152 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:18.304512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:24:18.304708 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:18.311575 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:18.311863 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:18.312059 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:24:18.312430 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:18.353689 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:24:18.355962 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:24:18.278158 => 22:24:18.355827
[0m22:24:18.356197 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:24:18.374489 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m22:24:18.374969 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:18.375367 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day;
    );
  
  
[0m22:24:18.376238 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:24:18.356344 => 22:24:18.376145
[0m22:24:18.376458 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: ROLLBACK
[0m22:24:18.380149 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_date'
[0m22:24:18.380360 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:24:18.382393 [debug] [Thread-1  ]: Runtime Error in model dim_date (models/dimensions/dim_date.sql)
  Parser Error: syntax error at or near ";"
[0m22:24:18.382774 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_date ............................. [[31mERROR[0m in 0.10s]
[0m22:24:18.383111 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:24:18.383893 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:18.384107 [debug] [MainThread]: On master: BEGIN
[0m22:24:18.384286 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:18.390697 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:18.391030 [debug] [MainThread]: On master: COMMIT
[0m22:24:18.391213 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:18.391375 [debug] [MainThread]: On master: COMMIT
[0m22:24:18.391638 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:18.391813 [debug] [MainThread]: On master: Close
[0m22:24:18.396497 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:18.397733 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_date' was properly closed.
[0m22:24:18.399854 [info ] [MainThread]: 
[0m22:24:18.400498 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m22:24:18.406943 [debug] [MainThread]: Command end result
[0m22:24:18.434483 [info ] [MainThread]: 
[0m22:24:18.435034 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:24:18.435476 [info ] [MainThread]: 
[0m22:24:18.435727 [error] [MainThread]:   Runtime Error in model dim_date (models/dimensions/dim_date.sql)
  Parser Error: syntax error at or near ";"
[0m22:24:18.435905 [info ] [MainThread]: 
[0m22:24:18.436107 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:24:18.436585 [debug] [MainThread]: Command `dbt run` failed at 22:24:18.436511 after 0.55 seconds
[0m22:24:18.436835 [debug] [MainThread]: Flushing usage events


============================== 22:24:24.624780 | 82cc520c-2307-43e8-9861-f4c541fdadf7 ==============================
[0m22:24:24.624780 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:24:24.628024 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_date.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:24:24.628332 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:24:24.710393 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:24:24.729610 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:24:24.765262 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:24:24.765768 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_date.sql
[0m22:24:24.853460 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:24:24.866584 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:24:24.867947 [info ] [MainThread]: 
[0m22:24:24.868370 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:24:24.869083 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:24:24.873401 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:24:24.873644 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:24:24.873829 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:24.884583 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.885685 [debug] [ThreadPool]: On list_dbt: Close
[0m22:24:24.887992 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:24:24.888464 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:24:24.891318 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:24.891507 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:24:24.891662 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:24.898471 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.898764 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:24.898933 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:24:24.899184 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.899714 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:24:24.899888 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:24.900035 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:24:24.900379 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.900650 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:24:24.904130 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m22:24:24.907619 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:24.907851 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:24:24.908011 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:24.914804 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.915096 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:24.915272 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:24:24.932084 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.936104 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:24:24.937523 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:24:24.937871 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:24:24.940156 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:24:24.941691 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:24.942776 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:24:24.943071 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:24.950097 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.950377 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:24.950563 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:24:24.972359 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.973379 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:24:24.973629 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:24:24.973800 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:24:24.976282 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:24:25.006410 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:25.006868 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:24:25.007070 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:25.014501 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:25.014805 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:25.014979 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:24:25.033998 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:25.034879 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:24:25.035145 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:24:25.035319 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:24:25.037771 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:24:25.039507 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:25.039784 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:24:25.040005 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:25.046683 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:25.046888 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:25.047064 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:24:25.064165 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:25.068270 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:24:25.068607 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:24:25.068783 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:24:25.072396 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:25.072663 [debug] [MainThread]: On master: BEGIN
[0m22:24:25.072824 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:25.081571 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:25.081855 [debug] [MainThread]: On master: COMMIT
[0m22:24:25.082023 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:25.082182 [debug] [MainThread]: On master: COMMIT
[0m22:24:25.082389 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:25.082558 [debug] [MainThread]: On master: Close
[0m22:24:25.085018 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:25.085221 [info ] [MainThread]: 
[0m22:24:25.086776 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:24:25.087146 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_date ...................................... [RUN]
[0m22:24:25.087560 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_date)
[0m22:24:25.087770 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:24:25.099946 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.100312 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:24:25.100501 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:25.107247 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.107572 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.107768 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:24:25.108154 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.118908 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:24:25.119454 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:24:25.087908 => 22:24:25.119351
[0m22:24:25.119765 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:24:25.138508 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m22:24:25.139120 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.139527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m22:24:25.188466 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.192524 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.192776 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m22:24:25.193227 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.195071 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.195284 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m22:24:25.195617 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.207317 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:24:25.207740 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.208078 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:24:25.212401 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.215929 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.216253 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m22:24:25.217066 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.217950 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:24:25.119905 => 22:24:25.217849
[0m22:24:25.218299 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:24:25.260914 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_date ................................. [[32mOK[0m in 0.17s]
[0m22:24:25.261333 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:24:25.262214 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:25.262418 [debug] [MainThread]: On master: BEGIN
[0m22:24:25.262581 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:25.269333 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:25.269629 [debug] [MainThread]: On master: COMMIT
[0m22:24:25.269810 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:25.269978 [debug] [MainThread]: On master: COMMIT
[0m22:24:25.270230 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:25.270410 [debug] [MainThread]: On master: Close
[0m22:24:25.272306 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:25.272570 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_date' was properly closed.
[0m22:24:25.272774 [info ] [MainThread]: 
[0m22:24:25.272969 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.40 seconds (0.40s).
[0m22:24:25.273422 [debug] [MainThread]: Command end result
[0m22:24:25.282059 [info ] [MainThread]: 
[0m22:24:25.282374 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:24:25.282554 [info ] [MainThread]: 
[0m22:24:25.282739 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:24:25.283081 [debug] [MainThread]: Command `dbt run` succeeded at 22:24:25.283029 after 0.69 seconds
[0m22:24:25.283278 [debug] [MainThread]: Flushing usage events


============================== 22:24:30.255554 | cc0e5a11-c6a5-406c-966d-98c1b6222f62 ==============================
[0m22:24:30.255554 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:24:30.258034 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:24:30.258281 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:24:30.364511 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:24:30.382989 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:24:30.418080 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:24:30.418361 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:24:30.419288 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m22:24:30.432493 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:24:30.433917 [info ] [MainThread]: 
[0m22:24:30.434356 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:24:30.435104 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:24:30.442348 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:24:30.442641 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:24:30.442817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:30.451042 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.452009 [debug] [ThreadPool]: On list_dbt: Close
[0m22:24:30.454194 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:24:30.454697 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:24:30.457863 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:30.458111 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:24:30.458273 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:30.465215 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.465469 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:30.465633 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:24:30.465886 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.466395 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:30.466570 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:30.466717 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:30.466924 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.467083 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:24:30.470383 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:24:30.473898 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:30.474103 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:24:30.474258 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:30.481366 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.481641 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:30.481818 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:24:30.497799 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.501289 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:24:30.501711 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:24:30.501884 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:24:30.504117 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:24:30.506417 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:30.506598 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:24:30.506746 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:30.512913 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.513157 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:30.513343 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:24:30.529043 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.532584 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:24:30.532900 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:24:30.533062 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:24:30.535449 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:24:30.538011 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:30.538291 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:24:30.538505 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:30.545243 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.545524 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:30.545703 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:24:30.563822 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.564940 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:24:30.565232 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:24:30.565412 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:24:30.567842 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:24:30.571085 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:30.571288 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:24:30.571448 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:30.578220 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.578486 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:30.578661 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:24:30.596589 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.597291 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:24:30.597518 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:24:30.597675 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:24:30.600184 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:30.600393 [debug] [MainThread]: On master: BEGIN
[0m22:24:30.600548 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:30.606916 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:30.607177 [debug] [MainThread]: On master: COMMIT
[0m22:24:30.607336 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:30.607491 [debug] [MainThread]: On master: COMMIT
[0m22:24:30.607692 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:30.607851 [debug] [MainThread]: On master: Close
[0m22:24:30.609643 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:30.609913 [info ] [MainThread]: 
[0m22:24:30.611010 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:24:30.611356 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:24:30.611782 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_opportunity)
[0m22:24:30.611983 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:24:30.619937 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:24:30.621573 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:24:30.612121 => 22:24:30.621414
[0m22:24:30.621790 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:24:30.650999 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:24:30.651638 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:24:30.651843 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:24:30.652019 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:30.658800 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:30.659097 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:24:30.659307 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here

    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:24:30.662906 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:30.673115 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:24:30.673392 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:24:30.673584 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:24:30.674303 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:30.674785 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:24:30.621930 => 22:24:30.674697
[0m22:24:30.674989 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:24:30.689712 [info ] [Thread-1  ]: 1 of 1 OK created sql incremental model fact.fact_opportunity .................. [[32mOK[0m in 0.08s]
[0m22:24:30.690137 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:24:30.690902 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:30.691085 [debug] [MainThread]: On master: BEGIN
[0m22:24:30.691236 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:30.697812 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:30.698089 [debug] [MainThread]: On master: COMMIT
[0m22:24:30.698258 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:30.698420 [debug] [MainThread]: On master: COMMIT
[0m22:24:30.698719 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:30.698909 [debug] [MainThread]: On master: Close
[0m22:24:30.700719 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:30.700991 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:24:30.701179 [info ] [MainThread]: 
[0m22:24:30.701375 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m22:24:30.701825 [debug] [MainThread]: Command end result
[0m22:24:30.709400 [info ] [MainThread]: 
[0m22:24:30.709688 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:24:30.709866 [info ] [MainThread]: 
[0m22:24:30.710058 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:24:30.710511 [debug] [MainThread]: Command `dbt run` succeeded at 22:24:30.710445 after 0.47 seconds
[0m22:24:30.710738 [debug] [MainThread]: Flushing usage events


============================== 22:24:46.561706 | 1a7050c5-8a81-46a0-a0ac-841542910fab ==============================
[0m22:24:46.561706 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:24:46.565399 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:24:46.565935 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:24:46.642187 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:24:46.662842 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:24:46.709344 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:24:46.709652 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:24:46.710605 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:24:46.725442 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:24:46.726850 [info ] [MainThread]: 
[0m22:24:46.727284 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:24:46.728002 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:24:46.735351 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:24:46.735627 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:24:46.735802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:46.751690 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.752726 [debug] [ThreadPool]: On list_dbt: Close
[0m22:24:46.754854 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:24:46.755404 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:24:46.758387 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:46.758605 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:24:46.758758 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.766054 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.766359 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:46.766527 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:24:46.766767 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.767295 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:46.767468 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:46.767613 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:46.767827 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.767992 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:24:46.771284 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:24:46.775100 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:46.775423 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:24:46.775673 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.783027 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.783335 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:46.783532 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:24:46.801409 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.802572 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:24:46.803249 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:24:46.803470 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:24:46.806359 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:24:46.809216 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:46.809438 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:24:46.809596 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.816541 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.816804 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:46.816976 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:24:46.832615 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.836265 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:24:46.836616 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:24:46.836798 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:24:46.839165 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:24:46.840739 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:46.840931 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:24:46.841090 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.848626 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.848875 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:46.849053 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:24:46.864543 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.867877 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:24:46.868111 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:24:46.868271 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:24:46.870756 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:24:46.873165 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:46.873375 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:24:46.873529 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.880242 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.880500 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:46.880669 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:24:46.896243 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.900549 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:24:46.900855 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:24:46.901039 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:24:46.904025 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:46.904209 [debug] [MainThread]: On master: BEGIN
[0m22:24:46.904363 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:46.911180 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:46.911436 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.911601 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:46.911753 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.911950 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:46.912110 [debug] [MainThread]: On master: Close
[0m22:24:46.914245 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:46.914472 [info ] [MainThread]: 
[0m22:24:46.916104 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:46.916470 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:24:46.918280 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:24:46.918518 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:46.924606 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:46.925813 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:24:46.918678 => 22:24:46.925639
[0m22:24:46.926100 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:46.954606 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:46.955183 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:46.955390 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:24:46.955572 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:46.962236 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:46.962600 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:46.962865 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id;
    );
  
  
  
[0m22:24:46.963396 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:24:46.926245 => 22:24:46.963289
[0m22:24:46.963605 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:24:46.967661 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:24:46.967968 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:24:46.970050 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Parser Error: syntax error at or near ";"
[0m22:24:46.970484 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:24:46.970843 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:46.971651 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:46.971860 [debug] [MainThread]: On master: BEGIN
[0m22:24:46.972021 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:46.978963 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:46.979252 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.979417 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:46.979570 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.979777 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:46.979945 [debug] [MainThread]: On master: Close
[0m22:24:46.981770 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:46.981944 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:24:46.982120 [info ] [MainThread]: 
[0m22:24:46.982305 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:24:46.982636 [debug] [MainThread]: Command end result
[0m22:24:46.990008 [info ] [MainThread]: 
[0m22:24:46.990274 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:24:46.990441 [info ] [MainThread]: 
[0m22:24:46.990596 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Parser Error: syntax error at or near ";"
[0m22:24:46.990758 [info ] [MainThread]: 
[0m22:24:46.990943 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:24:46.991342 [debug] [MainThread]: Command `dbt run` failed at 22:24:46.991287 after 0.45 seconds
[0m22:24:46.991546 [debug] [MainThread]: Flushing usage events


============================== 22:24:52.267034 | a4fc9014-414b-461f-a27c-8eece2e72c51 ==============================
[0m22:24:52.267034 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:24:52.269428 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:24:52.269680 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:24:52.354065 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:24:52.374040 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:24:52.417458 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:24:52.417931 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m22:24:52.446319 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m22:24:52.460130 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:24:52.461766 [info ] [MainThread]: 
[0m22:24:52.462264 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:24:52.462882 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:24:52.470335 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:24:52.470663 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:24:52.470840 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:52.479238 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.480202 [debug] [ThreadPool]: On list_dbt: Close
[0m22:24:52.482519 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:24:52.483038 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:24:52.485933 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:52.486127 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:24:52.486288 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:52.493369 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.493678 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:52.493852 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:24:52.494109 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.494618 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:52.494793 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:52.494942 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:52.495153 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.495318 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:24:52.498480 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m22:24:52.502108 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:52.502318 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:24:52.502473 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:52.509638 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.509903 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:52.510078 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:24:52.526187 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.529854 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:24:52.531310 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:24:52.531515 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:24:52.534030 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:24:52.535937 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:52.536159 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:24:52.536315 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:52.542526 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.542806 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:52.542998 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:24:52.559282 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.563047 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:24:52.563298 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:24:52.563463 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:24:52.566114 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:24:52.568430 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:52.569436 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:24:52.569612 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:52.576248 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.576497 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:52.576672 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:24:52.595124 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.596096 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:24:52.596347 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:24:52.596518 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:24:52.598884 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:24:52.600404 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:52.601558 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:24:52.601717 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:52.608611 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.608876 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:52.609053 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:24:52.624343 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.627739 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:24:52.627974 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:24:52.628131 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:24:52.631241 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:52.631443 [debug] [MainThread]: On master: BEGIN
[0m22:24:52.631597 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:52.637670 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:52.637919 [debug] [MainThread]: On master: COMMIT
[0m22:24:52.638081 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:52.638232 [debug] [MainThread]: On master: COMMIT
[0m22:24:52.638439 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:52.638602 [debug] [MainThread]: On master: Close
[0m22:24:52.640235 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:52.640442 [info ] [MainThread]: 
[0m22:24:52.642048 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:52.642339 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:24:52.642712 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:24:52.642910 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:52.648723 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:52.649306 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:24:52.643047 => 22:24:52.649190
[0m22:24:52.649513 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:52.679023 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:52.679680 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:52.679898 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:24:52.680088 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:52.686978 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:52.687231 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:52.687448 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:24:52.688308 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:24:52.649651 => 22:24:52.688210
[0m22:24:52.688514 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:24:52.692245 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:24:52.692562 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:24:52.694673 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:24:52.695124 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:24:52.695483 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:52.696277 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:52.696482 [debug] [MainThread]: On master: BEGIN
[0m22:24:52.696646 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:52.703194 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:52.703456 [debug] [MainThread]: On master: COMMIT
[0m22:24:52.703617 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:52.703770 [debug] [MainThread]: On master: COMMIT
[0m22:24:52.703971 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:52.704132 [debug] [MainThread]: On master: Close
[0m22:24:52.705888 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:52.706176 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:24:52.706367 [info ] [MainThread]: 
[0m22:24:52.706575 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m22:24:52.706955 [debug] [MainThread]: Command end result
[0m22:24:52.739248 [info ] [MainThread]: 
[0m22:24:52.739541 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:24:52.739770 [info ] [MainThread]: 
[0m22:24:52.740079 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:24:52.740275 [info ] [MainThread]: 
[0m22:24:52.740464 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:24:52.740831 [debug] [MainThread]: Command `dbt run` failed at 22:24:52.740776 after 0.49 seconds
[0m22:24:52.741040 [debug] [MainThread]: Flushing usage events


============================== 22:25:23.765101 | c842815c-afb6-4159-8971-7429760f0a16 ==============================
[0m22:25:23.765101 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:25:23.768395 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:25:23.768686 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:25:23.844763 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:25:23.863346 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:25:23.898905 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:25:23.899188 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:25:23.900139 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:25:23.915319 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:25:23.916916 [info ] [MainThread]: 
[0m22:25:23.917397 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:25:23.918055 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:25:23.925556 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:25:23.925873 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:25:23.926045 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:25:23.942270 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.943461 [debug] [ThreadPool]: On list_dbt: Close
[0m22:25:23.945712 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:25:23.946122 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:25:23.948962 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:25:23.949235 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:25:23.949396 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:23.956499 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.956750 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:25:23.956926 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:25:23.957187 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.957699 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:25:23.957864 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:25:23.958013 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:25:23.958220 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.958377 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:25:23.961733 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:25:23.965258 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:25:23.965474 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:25:23.965627 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:23.972637 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.972930 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:25:23.973121 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:25:23.991186 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.992132 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:25:23.993480 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:25:23.993673 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:25:23.996183 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:25:23.998607 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:25:23.998792 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:25:23.998941 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:24.005837 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.006130 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:25:24.006311 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:25:24.022333 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.025846 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:25:24.026090 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:25:24.026254 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:25:24.028840 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:25:24.031416 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:25:24.031631 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:25:24.031791 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:24.038423 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.038671 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:25:24.038846 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:25:24.054549 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.058037 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:25:24.058276 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:25:24.058440 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:25:24.061024 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:25:24.062806 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:25:24.063016 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:25:24.063166 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:24.069747 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.069995 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:25:24.070171 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:25:24.085624 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.088989 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:25:24.089215 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:25:24.089369 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:25:24.093256 [debug] [MainThread]: Using duckdb connection "master"
[0m22:25:24.093437 [debug] [MainThread]: On master: BEGIN
[0m22:25:24.093582 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:25:24.099599 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:25:24.099842 [debug] [MainThread]: On master: COMMIT
[0m22:25:24.099998 [debug] [MainThread]: Using duckdb connection "master"
[0m22:25:24.100142 [debug] [MainThread]: On master: COMMIT
[0m22:25:24.100341 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:25:24.100501 [debug] [MainThread]: On master: Close
[0m22:25:24.102118 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:25:24.102333 [info ] [MainThread]: 
[0m22:25:24.104662 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:25:24.104952 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:25:24.105333 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:25:24.105530 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:25:24.111231 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:25:24.111776 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:25:24.105673 => 22:25:24.111660
[0m22:25:24.111976 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:25:24.141334 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:25:24.141881 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:25:24.142088 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:25:24.142269 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:25:24.148936 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:25:24.149255 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:25:24.149490 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:25:24.150377 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:25:24.112108 => 22:25:24.150264
[0m22:25:24.150603 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:25:24.154263 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:25:24.154511 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:25:24.156707 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:25:24.157143 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:25:24.157496 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:25:24.158277 [debug] [MainThread]: Using duckdb connection "master"
[0m22:25:24.158470 [debug] [MainThread]: On master: BEGIN
[0m22:25:24.158623 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:25:24.165636 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:25:24.165911 [debug] [MainThread]: On master: COMMIT
[0m22:25:24.166075 [debug] [MainThread]: Using duckdb connection "master"
[0m22:25:24.166225 [debug] [MainThread]: On master: COMMIT
[0m22:25:24.166421 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:25:24.166584 [debug] [MainThread]: On master: Close
[0m22:25:24.168687 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:25:24.169040 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:25:24.169286 [info ] [MainThread]: 
[0m22:25:24.169614 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:25:24.169999 [debug] [MainThread]: Command end result
[0m22:25:24.178215 [info ] [MainThread]: 
[0m22:25:24.178498 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:25:24.178664 [info ] [MainThread]: 
[0m22:25:24.178831 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:25:24.178994 [info ] [MainThread]: 
[0m22:25:24.179168 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:25:24.179513 [debug] [MainThread]: Command `dbt run` failed at 22:25:24.179464 after 0.44 seconds
[0m22:25:24.179713 [debug] [MainThread]: Flushing usage events


============================== 22:26:40.708348 | 81526282-a5a3-4b43-8049-eabc2f93b027 ==============================
[0m22:26:40.708348 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:26:40.711428 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select models/staging/stg_salesforce__opportunity_history.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:26:40.711692 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:26:40.795601 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:26:40.815768 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:26:40.863519 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:26:40.863849 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:26:40.864870 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:26:40.879090 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:26:40.880733 [info ] [MainThread]: 
[0m22:26:40.881222 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:26:40.881905 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:26:40.889593 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:26:40.889900 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:26:40.890081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:26:40.906228 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.907277 [debug] [ThreadPool]: On list_dbt: Close
[0m22:26:40.909487 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m22:26:40.909947 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m22:26:40.912855 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:26:40.913073 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m22:26:40.913255 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:40.920186 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.920474 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:26:40.920643 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m22:26:40.920896 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.921419 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m22:26:40.921604 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:26:40.921753 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m22:26:40.921965 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.922124 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m22:26:40.925341 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_staging)
[0m22:26:40.928816 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:40.929091 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:26:40.929282 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:40.936038 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.936323 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:40.936500 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:26:40.955330 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.956369 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:26:40.958268 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:26:40.958468 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:26:40.960996 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:26:40.963687 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:40.963912 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:26:40.964068 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:40.970968 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.971236 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:40.971414 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:26:40.987424 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.991321 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:26:40.991672 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:26:40.991852 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:26:40.994807 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:26:40.996658 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:40.996879 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:26:40.997037 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:41.004269 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:41.004532 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:41.004720 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:26:41.021822 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:41.025631 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:26:41.025997 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:26:41.026166 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:26:41.028887 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:26:41.031274 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:41.031477 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:26:41.031626 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:41.038554 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:41.038799 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:41.038968 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:26:41.054722 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:41.059161 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:26:41.059438 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:26:41.059628 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:26:41.062872 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:41.063105 [debug] [MainThread]: On master: BEGIN
[0m22:26:41.063280 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:26:41.070157 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:41.070436 [debug] [MainThread]: On master: COMMIT
[0m22:26:41.070605 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:41.070756 [debug] [MainThread]: On master: COMMIT
[0m22:26:41.070951 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:41.071117 [debug] [MainThread]: On master: Close
[0m22:26:41.072731 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:26:41.072943 [info ] [MainThread]: 
[0m22:26:41.075793 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:26:41.076086 [info ] [Thread-1  ]: 1 of 1 START sql view model staging.stg_salesforce__opportunity_history ........ [RUN]
[0m22:26:41.076459 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m22:26:41.076663 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:26:41.082046 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.083442 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 22:26:41.076796 => 22:26:41.083243
[0m22:26:41.083732 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:26:41.102188 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.103081 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.103318 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m22:26:41.103512 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:26:41.110127 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.110427 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.110649 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m22:26:41.111286 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.115350 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.115602 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m22:26:41.115972 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.117803 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.118006 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m22:26:41.118301 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.127852 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m22:26:41.128105 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.128299 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m22:26:41.128961 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.131842 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.132101 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m22:26:41.132510 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.133288 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 22:26:41.083877 => 22:26:41.133190
[0m22:26:41.133500 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m22:26:41.162888 [info ] [Thread-1  ]: 1 of 1 OK created sql view model staging.stg_salesforce__opportunity_history ... [[32mOK[0m in 0.09s]
[0m22:26:41.163310 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:26:41.164073 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:41.164268 [debug] [MainThread]: On master: BEGIN
[0m22:26:41.164422 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:26:41.170999 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:41.171281 [debug] [MainThread]: On master: COMMIT
[0m22:26:41.171447 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:41.171599 [debug] [MainThread]: On master: COMMIT
[0m22:26:41.171796 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:41.171957 [debug] [MainThread]: On master: Close
[0m22:26:41.173668 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:26:41.174001 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.stg_salesforce__opportunity_history' was properly closed.
[0m22:26:41.174253 [info ] [MainThread]: 
[0m22:26:41.174467 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m22:26:41.174846 [debug] [MainThread]: Command end result
[0m22:26:41.182197 [info ] [MainThread]: 
[0m22:26:41.182501 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:26:41.182673 [info ] [MainThread]: 
[0m22:26:41.182865 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:26:41.183242 [debug] [MainThread]: Command `dbt run` succeeded at 22:26:41.183193 after 0.50 seconds
[0m22:26:41.183476 [debug] [MainThread]: Flushing usage events


============================== 22:26:46.030295 | e00cd055-a8a9-4601-ae97-67b9cd4ee0b0 ==============================
[0m22:26:46.030295 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:26:46.032692 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:26:46.032948 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:26:46.107277 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:26:46.125682 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:26:46.162507 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:26:46.162785 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:26:46.163742 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:26:46.177923 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:26:46.179558 [info ] [MainThread]: 
[0m22:26:46.180049 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:26:46.180689 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:26:46.188064 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:26:46.188350 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:26:46.188525 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:26:46.197443 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.198466 [debug] [ThreadPool]: On list_dbt: Close
[0m22:26:46.200794 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:26:46.201316 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:26:46.204321 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:46.204558 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:26:46.204725 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:46.211781 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.212056 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:46.212230 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:26:46.212484 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.212992 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:26:46.213163 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:46.213311 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:26:46.213519 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.213682 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:26:46.216780 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:26:46.220278 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:46.220495 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:26:46.220656 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:46.228081 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.228417 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:46.228617 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:26:46.246004 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.250055 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:26:46.250528 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:26:46.250722 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:26:46.253470 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:26:46.256260 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:46.256531 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:26:46.256705 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:46.263934 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.264277 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:46.264472 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:26:46.282678 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.283859 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:26:46.284106 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:26:46.284273 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:26:46.286789 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:26:46.289244 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:46.289435 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:26:46.289589 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:46.296283 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.296536 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:46.296714 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:26:46.312643 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.316380 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:26:46.316635 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:26:46.316805 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:26:46.319180 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:26:46.320850 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:46.321048 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:26:46.321201 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:46.328095 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.328362 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:46.328540 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:26:46.344006 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.347491 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:26:46.347794 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:26:46.347966 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:26:46.352203 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:46.352453 [debug] [MainThread]: On master: BEGIN
[0m22:26:46.352611 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:26:46.358943 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:46.359262 [debug] [MainThread]: On master: COMMIT
[0m22:26:46.359432 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:46.359584 [debug] [MainThread]: On master: COMMIT
[0m22:26:46.359820 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:46.359977 [debug] [MainThread]: On master: Close
[0m22:26:46.361607 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:26:46.361829 [info ] [MainThread]: 
[0m22:26:46.362970 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:46.363246 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:26:46.363625 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:26:46.363825 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:46.369567 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:46.371559 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:26:46.363962 => 22:26:46.371374
[0m22:26:46.371824 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:46.401603 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:46.402261 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:46.402461 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:26:46.402643 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:26:46.409601 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:46.409911 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:46.410128 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:26:46.411004 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:26:46.371973 => 22:26:46.410894
[0m22:26:46.411213 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:26:46.415470 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:26:46.415669 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:26:46.417652 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:26:46.418085 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:26:46.418449 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:46.419216 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:46.419460 [debug] [MainThread]: On master: BEGIN
[0m22:26:46.419623 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:26:46.426111 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:46.426480 [debug] [MainThread]: On master: COMMIT
[0m22:26:46.426753 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:46.426974 [debug] [MainThread]: On master: COMMIT
[0m22:26:46.427299 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:46.427585 [debug] [MainThread]: On master: Close
[0m22:26:46.429378 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:26:46.429612 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:26:46.429797 [info ] [MainThread]: 
[0m22:26:46.429990 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:26:46.430346 [debug] [MainThread]: Command end result
[0m22:26:46.438785 [info ] [MainThread]: 
[0m22:26:46.439134 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:26:46.439334 [info ] [MainThread]: 
[0m22:26:46.439518 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:26:46.439697 [info ] [MainThread]: 
[0m22:26:46.439900 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:26:46.440351 [debug] [MainThread]: Command `dbt run` failed at 22:26:46.440296 after 0.43 seconds
[0m22:26:46.440568 [debug] [MainThread]: Flushing usage events


============================== 22:26:59.347372 | 0f108e18-f8d7-4251-8b9d-d8a47906ba1b ==============================
[0m22:26:59.347372 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:26:59.350552 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:26:59.350856 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:26:59.428555 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:26:59.448819 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:26:59.497600 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:26:59.498100 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m22:26:59.526087 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:26:59.539571 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:26:59.541249 [info ] [MainThread]: 
[0m22:26:59.541736 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:26:59.542418 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:26:59.549641 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:26:59.549916 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:26:59.550094 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:26:59.565501 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.566568 [debug] [ThreadPool]: On list_dbt: Close
[0m22:26:59.568965 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:26:59.569393 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:26:59.572385 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:59.572595 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:26:59.572755 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:59.579782 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.580059 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:59.580226 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:26:59.580474 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.580998 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:26:59.581169 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:59.581316 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:26:59.581536 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.581698 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:26:59.584636 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m22:26:59.588092 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:59.588361 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:26:59.588517 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:59.594837 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.595094 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:59.595264 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:26:59.611334 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.614857 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:26:59.616842 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:26:59.617101 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:26:59.619938 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:26:59.621760 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:59.621985 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:26:59.622156 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:59.629216 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.629500 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:59.629678 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:26:59.649351 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.650582 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:26:59.650873 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:26:59.651041 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:26:59.653625 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:26:59.656931 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:59.657155 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:26:59.657317 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:59.664211 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.664466 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:59.664646 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:26:59.680143 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.683826 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:26:59.684084 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:26:59.684252 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:26:59.686554 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:26:59.688772 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:59.688953 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:26:59.689106 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:59.695489 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.695747 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:59.695925 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:26:59.711024 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.714464 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:26:59.714695 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:26:59.714851 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:26:59.717801 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:59.717977 [debug] [MainThread]: On master: BEGIN
[0m22:26:59.718124 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:26:59.724242 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:59.724502 [debug] [MainThread]: On master: COMMIT
[0m22:26:59.724668 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:59.724828 [debug] [MainThread]: On master: COMMIT
[0m22:26:59.725035 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:59.725195 [debug] [MainThread]: On master: Close
[0m22:26:59.726785 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:26:59.726992 [info ] [MainThread]: 
[0m22:26:59.729510 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:59.729792 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:26:59.730156 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:26:59.730351 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:59.736057 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:59.736627 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:26:59.730488 => 22:26:59.736514
[0m22:26:59.736837 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:59.767105 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:59.767767 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:59.767980 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:26:59.768170 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:26:59.775055 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:59.775357 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:59.775572 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:26:59.776481 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:26:59.736972 => 22:26:59.776371
[0m22:26:59.776699 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:26:59.780281 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:26:59.780535 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:26:59.782545 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:26:59.782967 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:26:59.783327 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:59.784084 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:59.784265 [debug] [MainThread]: On master: BEGIN
[0m22:26:59.784415 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:26:59.790897 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:59.791335 [debug] [MainThread]: On master: COMMIT
[0m22:26:59.791564 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:59.791746 [debug] [MainThread]: On master: COMMIT
[0m22:26:59.792059 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:59.792245 [debug] [MainThread]: On master: Close
[0m22:26:59.794301 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:26:59.794588 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:26:59.794791 [info ] [MainThread]: 
[0m22:26:59.795209 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:26:59.795763 [debug] [MainThread]: Command end result
[0m22:26:59.857056 [info ] [MainThread]: 
[0m22:26:59.857366 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:26:59.857545 [info ] [MainThread]: 
[0m22:26:59.857764 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:26:59.857974 [info ] [MainThread]: 
[0m22:26:59.858167 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:26:59.858520 [debug] [MainThread]: Command `dbt run` failed at 22:26:59.858466 after 0.54 seconds
[0m22:26:59.858723 [debug] [MainThread]: Flushing usage events


============================== 22:27:44.942210 | a425a544-ef67-4d70-a805-ac82a24a5160 ==============================
[0m22:27:44.942210 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:27:44.945111 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dim/dim_opportunity.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:27:44.945366 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:27:45.021633 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:27:45.041471 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:27:45.088319 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:27:45.088610 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:27:45.089553 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:27:45.104365 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:27:45.105361 [warn ] [MainThread]: The selection criterion 'models/dim/dim_opportunity.sql' does not match any nodes
[0m22:27:45.106039 [info ] [MainThread]: 
[0m22:27:45.106232 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:27:45.106517 [debug] [MainThread]: Command end result
[0m22:27:45.113965 [debug] [MainThread]: Command `dbt run` succeeded at 22:27:45.113867 after 0.20 seconds
[0m22:27:45.114245 [debug] [MainThread]: Flushing usage events


============================== 22:27:53.969018 | 561587ce-a144-4787-9072-995849635f82 ==============================
[0m22:27:53.969018 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:27:53.972046 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity.sql', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:27:53.972301 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:27:54.056326 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:27:54.075123 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:27:54.118924 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:27:54.119233 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:27:54.120257 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:27:54.134210 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:27:54.135602 [info ] [MainThread]: 
[0m22:27:54.136020 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:27:54.136555 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:27:54.143845 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:27:54.144126 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:27:54.144309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:27:54.162820 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.163948 [debug] [ThreadPool]: On list_dbt: Close
[0m22:27:54.166112 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:27:54.166501 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:27:54.169412 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:27:54.169601 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:27:54.169753 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:54.176647 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.176929 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:27:54.177098 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:27:54.177347 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.177852 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:27:54.178014 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:27:54.178159 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:27:54.178371 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.178530 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:27:54.181630 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m22:27:54.185222 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:27:54.185419 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:27:54.185574 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:54.192741 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.192996 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:27:54.193179 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:27:54.211183 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.212135 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:27:54.214601 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:27:54.214769 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:27:54.217282 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:27:54.219694 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:27:54.219867 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:27:54.220013 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:54.226823 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.227119 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:27:54.227301 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:27:54.246430 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.251052 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:27:54.251323 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:27:54.251488 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:27:54.254396 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:27:54.256847 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:27:54.257033 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:27:54.257188 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:54.263799 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.264047 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:27:54.264222 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:27:54.279686 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.282969 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:27:54.283262 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:27:54.283514 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:27:54.285790 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:27:54.287408 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:27:54.287600 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:27:54.287755 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:54.294353 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.294597 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:27:54.294767 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:27:54.310210 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.313672 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:27:54.313899 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:27:54.314057 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:27:54.317854 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:54.318031 [debug] [MainThread]: On master: BEGIN
[0m22:27:54.318175 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:27:54.324465 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:54.324713 [debug] [MainThread]: On master: COMMIT
[0m22:27:54.324875 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:54.325029 [debug] [MainThread]: On master: COMMIT
[0m22:27:54.325224 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:54.325382 [debug] [MainThread]: On master: Close
[0m22:27:54.326923 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:27:54.327145 [info ] [MainThread]: 
[0m22:27:54.328804 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m22:27:54.329082 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_opportunity ............................... [RUN]
[0m22:27:54.329459 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_opportunity)
[0m22:27:54.329651 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m22:27:54.335384 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.336677 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 22:27:54.329787 => 22:27:54.336560
[0m22:27:54.336889 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m22:27:54.356210 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.356829 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.357036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m22:27:54.357220 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:27:54.363933 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.364192 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.364439 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m22:27:54.368291 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.372095 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.372342 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m22:27:54.372749 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.374522 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.374719 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m22:27:54.375038 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.385954 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m22:27:54.386197 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.386377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m22:27:54.387732 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.390658 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.390879 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m22:27:54.391367 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.392135 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 22:27:54.337024 => 22:27:54.392037
[0m22:27:54.392333 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m22:27:54.426347 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_opportunity .......................... [[32mOK[0m in 0.10s]
[0m22:27:54.426777 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m22:27:54.427595 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:54.427787 [debug] [MainThread]: On master: BEGIN
[0m22:27:54.427960 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:27:54.434642 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:54.434966 [debug] [MainThread]: On master: COMMIT
[0m22:27:54.435141 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:54.435318 [debug] [MainThread]: On master: COMMIT
[0m22:27:54.435585 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:54.435823 [debug] [MainThread]: On master: Close
[0m22:27:54.437710 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:27:54.437903 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_opportunity' was properly closed.
[0m22:27:54.438088 [info ] [MainThread]: 
[0m22:27:54.438282 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.30 seconds (0.30s).
[0m22:27:54.438645 [debug] [MainThread]: Command end result
[0m22:27:54.445845 [info ] [MainThread]: 
[0m22:27:54.446118 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:27:54.446290 [info ] [MainThread]: 
[0m22:27:54.446478 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:27:54.446865 [debug] [MainThread]: Command `dbt run` succeeded at 22:27:54.446815 after 0.50 seconds
[0m22:27:54.447072 [debug] [MainThread]: Flushing usage events


============================== 22:27:59.455544 | 82ae3549-6468-4506-a2e0-8766ed27f9dd ==============================
[0m22:27:59.455544 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:27:59.457901 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:27:59.458163 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:27:59.533846 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:27:59.551920 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:27:59.589161 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:27:59.589450 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:27:59.590416 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:27:59.604773 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:27:59.606350 [info ] [MainThread]: 
[0m22:27:59.606830 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:27:59.607536 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:27:59.615229 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:27:59.615580 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:27:59.615767 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:27:59.624605 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.625547 [debug] [ThreadPool]: On list_dbt: Close
[0m22:27:59.627631 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:27:59.628149 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:27:59.631322 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:27:59.631564 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:27:59.631723 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:59.638586 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.638850 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:27:59.639021 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:27:59.639273 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.639762 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:27:59.639925 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:27:59.640071 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:27:59.640289 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.640452 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:27:59.643433 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:27:59.646923 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:27:59.647126 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:27:59.647279 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:59.654222 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.654506 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:27:59.654692 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:27:59.670113 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.673662 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:27:59.674105 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:27:59.674281 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:27:59.676761 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:27:59.678988 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:27:59.679165 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:27:59.679309 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:59.685567 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.685828 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:27:59.686000 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:27:59.704412 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.705491 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:27:59.705757 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:27:59.705934 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:27:59.709097 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:27:59.711707 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:27:59.711911 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:27:59.712073 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:59.718817 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.719091 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:27:59.719273 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:27:59.734970 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.738451 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:27:59.738700 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:27:59.738863 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:27:59.741353 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:27:59.742914 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:27:59.743110 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:27:59.743270 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:59.750496 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.750726 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:27:59.750904 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:27:59.766129 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.769482 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:27:59.769710 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:27:59.769868 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:27:59.773733 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:59.773958 [debug] [MainThread]: On master: BEGIN
[0m22:27:59.774115 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:27:59.780354 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:59.780605 [debug] [MainThread]: On master: COMMIT
[0m22:27:59.780761 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:59.780908 [debug] [MainThread]: On master: COMMIT
[0m22:27:59.781128 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:59.781285 [debug] [MainThread]: On master: Close
[0m22:27:59.783187 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:27:59.783456 [info ] [MainThread]: 
[0m22:27:59.784641 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:27:59.784964 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:27:59.785416 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:27:59.785628 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:27:59.791459 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:27:59.793003 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:27:59.785763 => 22:27:59.792827
[0m22:27:59.793262 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:27:59.822856 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:27:59.823533 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:27:59.823747 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:27:59.823931 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:27:59.830546 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:59.830853 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:27:59.831072 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:27:59.832028 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:27:59.793408 => 22:27:59.831927
[0m22:27:59.832244 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:27:59.835852 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:27:59.836090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:27:59.838275 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:27:59.838731 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:27:59.839076 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:27:59.839888 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:59.840104 [debug] [MainThread]: On master: BEGIN
[0m22:27:59.840269 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:27:59.846813 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:59.847080 [debug] [MainThread]: On master: COMMIT
[0m22:27:59.847246 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:59.847420 [debug] [MainThread]: On master: COMMIT
[0m22:27:59.847638 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:59.847798 [debug] [MainThread]: On master: Close
[0m22:27:59.849700 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:27:59.850050 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:27:59.850312 [info ] [MainThread]: 
[0m22:27:59.850522 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m22:27:59.850890 [debug] [MainThread]: Command end result
[0m22:27:59.858645 [info ] [MainThread]: 
[0m22:27:59.858932 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:27:59.859102 [info ] [MainThread]: 
[0m22:27:59.859270 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:27:59.859434 [info ] [MainThread]: 
[0m22:27:59.859612 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:27:59.859993 [debug] [MainThread]: Command `dbt run` failed at 22:27:59.859932 after 0.42 seconds
[0m22:27:59.860192 [debug] [MainThread]: Flushing usage events


============================== 22:28:43.341582 | f6368a7d-8755-42cb-b805-4bb412ded9df ==============================
[0m22:28:43.341582 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:28:43.344813 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/fact_opportunity.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:28:43.345085 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:28:43.430266 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:28:43.450753 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:28:43.502343 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:28:43.502666 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:28:43.503697 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:28:43.519464 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:28:43.520485 [warn ] [MainThread]: The selection criterion 'models/dimensions/fact_opportunity.sql' does not match any nodes
[0m22:28:43.521205 [info ] [MainThread]: 
[0m22:28:43.521415 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:28:43.521801 [debug] [MainThread]: Command end result
[0m22:28:43.529791 [debug] [MainThread]: Command `dbt run` succeeded at 22:28:43.529686 after 0.21 seconds
[0m22:28:43.530091 [debug] [MainThread]: Flushing usage events


============================== 22:28:51.869377 | 3e23db34-bec6-485a-8dce-26d6be658111 ==============================
[0m22:28:51.869377 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:28:51.872012 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:28:51.872268 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:28:51.948866 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:28:51.967471 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:28:52.011756 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:28:52.012082 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:28:52.013155 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:28:52.027750 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:28:52.029402 [info ] [MainThread]: 
[0m22:28:52.029859 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:28:52.030516 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:28:52.037982 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:28:52.038270 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:28:52.038450 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:28:52.054328 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.055402 [debug] [ThreadPool]: On list_dbt: Close
[0m22:28:52.057536 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:28:52.057981 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:28:52.060879 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:28:52.061114 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:28:52.061268 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:52.068350 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.068587 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:28:52.068799 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:28:52.069201 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.069806 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:28:52.070003 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:28:52.070159 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:28:52.070413 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.070575 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:28:52.074099 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:28:52.078058 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:28:52.078290 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:28:52.078452 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:52.088641 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.088912 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:28:52.089084 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:28:52.106597 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.110796 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:28:52.111378 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:28:52.111555 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:28:52.114128 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:28:52.116864 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:28:52.117103 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:28:52.117264 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:52.124710 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.125070 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:28:52.125274 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:28:52.145752 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.146816 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:28:52.147059 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:28:52.147230 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:28:52.149993 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:28:52.152785 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:28:52.152978 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:28:52.153131 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:52.160145 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.160411 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:28:52.160588 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:28:52.176876 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.181457 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:28:52.181698 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:28:52.181857 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:28:52.184369 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:28:52.186019 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:28:52.186387 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:28:52.186574 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:52.193446 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.193753 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:28:52.193948 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:28:52.209579 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.213277 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:28:52.213539 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:28:52.213706 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:28:52.217871 [debug] [MainThread]: Using duckdb connection "master"
[0m22:28:52.218063 [debug] [MainThread]: On master: BEGIN
[0m22:28:52.218214 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:28:52.224590 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:28:52.224847 [debug] [MainThread]: On master: COMMIT
[0m22:28:52.225006 [debug] [MainThread]: Using duckdb connection "master"
[0m22:28:52.225153 [debug] [MainThread]: On master: COMMIT
[0m22:28:52.225349 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:28:52.225514 [debug] [MainThread]: On master: Close
[0m22:28:52.227177 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:28:52.227407 [info ] [MainThread]: 
[0m22:28:52.228962 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:28:52.229240 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:28:52.229602 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity)
[0m22:28:52.229796 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:28:52.238530 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:28:52.239594 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:28:52.229929 => 22:28:52.239429
[0m22:28:52.239877 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:28:52.268958 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:28:52.269350 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821222852255418"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here

    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:28:52.269580 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:28:52.277724 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:28:52.240030 => 22:28:52.277539
[0m22:28:52.278020 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:28:52.283271 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Not implemented Error: Cannot perform non-inner join on subquery!
[0m22:28:52.283717 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:28:52.284064 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:28:52.284941 [debug] [MainThread]: Using duckdb connection "master"
[0m22:28:52.285177 [debug] [MainThread]: On master: BEGIN
[0m22:28:52.285368 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:28:52.291789 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:28:52.292074 [debug] [MainThread]: On master: COMMIT
[0m22:28:52.292245 [debug] [MainThread]: Using duckdb connection "master"
[0m22:28:52.292399 [debug] [MainThread]: On master: COMMIT
[0m22:28:52.292616 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:28:52.292777 [debug] [MainThread]: On master: Close
[0m22:28:52.294584 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:28:52.294884 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:28:52.295142 [info ] [MainThread]: 
[0m22:28:52.295367 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m22:28:52.295739 [debug] [MainThread]: Command end result
[0m22:28:52.303181 [info ] [MainThread]: 
[0m22:28:52.303540 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:28:52.303724 [info ] [MainThread]: 
[0m22:28:52.303885 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Not implemented Error: Cannot perform non-inner join on subquery!
[0m22:28:52.304053 [info ] [MainThread]: 
[0m22:28:52.304232 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:28:52.304659 [debug] [MainThread]: Command `dbt run` failed at 22:28:52.304605 after 0.46 seconds
[0m22:28:52.305015 [debug] [MainThread]: Flushing usage events


============================== 22:29:53.216856 | 96456032-53b3-4d1f-b5cd-2a3005dd98fa ==============================
[0m22:29:53.216856 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:29:53.220064 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:29:53.220337 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:29:53.308516 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:29:53.327497 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:29:53.397046 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:29:53.397570 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m22:29:53.429869 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m22:29:53.445498 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:29:53.447200 [info ] [MainThread]: 
[0m22:29:53.447844 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:29:53.448478 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:29:53.455903 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:29:53.456245 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:29:53.456435 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:29:53.481178 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.482190 [debug] [ThreadPool]: On list_dbt: Close
[0m22:29:53.484396 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:29:53.484825 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:29:53.487737 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:53.487931 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:29:53.488087 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:53.494815 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.495084 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:53.495245 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:29:53.495489 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.496031 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:29:53.496202 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:53.496351 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:29:53.496556 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.496718 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:29:53.499764 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:29:53.503171 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:29:53.503417 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:29:53.504310 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:53.511129 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.511410 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:29:53.511593 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:29:53.527995 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.531770 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:29:53.532419 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:29:53.532606 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:29:53.535321 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:29:53.537789 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:29:53.537989 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:29:53.538140 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:53.544643 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.544903 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:29:53.545071 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:29:53.563443 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.564447 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:29:53.564690 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:29:53.564852 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:29:53.567271 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:29:53.569719 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:29:53.569909 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:29:53.570072 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:53.576586 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.576846 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:29:53.577015 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:29:53.592400 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.595797 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:29:53.596025 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:29:53.596179 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:29:53.598773 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m22:29:53.601195 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:29:53.601407 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:29:53.601564 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:53.608228 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.608443 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:29:53.608615 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:29:53.625365 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.628988 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:29:53.629238 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:29:53.629406 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:29:53.632382 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:53.632650 [debug] [MainThread]: On master: BEGIN
[0m22:29:53.632834 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:29:53.639395 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:53.639641 [debug] [MainThread]: On master: COMMIT
[0m22:29:53.639805 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:53.639956 [debug] [MainThread]: On master: COMMIT
[0m22:29:53.640160 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:53.640322 [debug] [MainThread]: On master: Close
[0m22:29:53.642294 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:29:53.642576 [info ] [MainThread]: 
[0m22:29:53.645018 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:29:53.645339 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:29:53.645757 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_opportunity)
[0m22:29:53.645956 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:29:53.653735 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:53.654743 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:29:53.646100 => 22:29:53.654572
[0m22:29:53.655027 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:29:53.707835 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:53.708206 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821222953695300"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data;
    );
  
  
  
[0m22:29:53.708432 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:29:53.715250 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:29:53.655208 => 22:29:53.715092
[0m22:29:53.715517 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:29:53.722037 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:29:53.722463 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.08s]
[0m22:29:53.722813 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:29:53.723577 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:53.723768 [debug] [MainThread]: On master: BEGIN
[0m22:29:53.723921 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:29:53.730721 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:53.731008 [debug] [MainThread]: On master: COMMIT
[0m22:29:53.731168 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:53.731319 [debug] [MainThread]: On master: COMMIT
[0m22:29:53.731527 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:53.731686 [debug] [MainThread]: On master: Close
[0m22:29:53.733416 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:29:53.733613 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:29:53.733794 [info ] [MainThread]: 
[0m22:29:53.733975 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m22:29:53.734308 [debug] [MainThread]: Command end result
[0m22:29:53.741268 [info ] [MainThread]: 
[0m22:29:53.741648 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:29:53.741916 [info ] [MainThread]: 
[0m22:29:53.742096 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:29:53.742268 [info ] [MainThread]: 
[0m22:29:53.742447 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:29:53.742789 [debug] [MainThread]: Command `dbt run` failed at 22:29:53.742739 after 0.55 seconds
[0m22:29:53.742993 [debug] [MainThread]: Flushing usage events


============================== 22:29:59.249932 | 35fabaae-e0b8-49fb-a668-feb7d019f103 ==============================
[0m22:29:59.249932 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:29:59.252606 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:29:59.252867 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:29:59.331372 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:29:59.350718 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:29:59.387404 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:29:59.387979 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m22:29:59.419524 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:29:59.432814 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:29:59.434246 [info ] [MainThread]: 
[0m22:29:59.434706 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:29:59.435377 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:29:59.442359 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:29:59.442670 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:29:59.442852 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:29:59.451442 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.452420 [debug] [ThreadPool]: On list_dbt: Close
[0m22:29:59.454811 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:29:59.455400 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:29:59.458254 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:59.458458 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:29:59.458614 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:59.465211 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.465499 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:59.465674 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:29:59.465928 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.466458 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:29:59.466629 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:59.466778 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:29:59.466989 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.467152 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:29:59.470455 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:29:59.474281 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:29:59.475025 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:29:59.475186 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:59.481505 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.481775 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:29:59.481950 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:29:59.497806 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.501389 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:29:59.502829 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:29:59.503010 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:29:59.505428 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:29:59.507670 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:29:59.507858 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:29:59.508012 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:59.515451 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.515751 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:29:59.515946 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:29:59.532623 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.536303 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:29:59.536685 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:29:59.536872 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:29:59.539543 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:29:59.541314 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:29:59.541521 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:29:59.541687 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:59.548619 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.548885 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:29:59.549074 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:29:59.566063 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.569964 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:29:59.570205 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:29:59.570365 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:29:59.573010 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:29:59.575416 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:29:59.575619 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:29:59.575776 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:59.582374 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.582625 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:29:59.582801 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:29:59.601178 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.602118 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:29:59.602349 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:29:59.602506 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:29:59.605640 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:59.605817 [debug] [MainThread]: On master: BEGIN
[0m22:29:59.605963 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:29:59.612186 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:59.612432 [debug] [MainThread]: On master: COMMIT
[0m22:29:59.612592 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:59.612749 [debug] [MainThread]: On master: COMMIT
[0m22:29:59.612945 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:59.613103 [debug] [MainThread]: On master: Close
[0m22:29:59.614762 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:29:59.615008 [info ] [MainThread]: 
[0m22:29:59.617166 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:29:59.617462 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:29:59.617834 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_opportunity)
[0m22:29:59.618032 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:29:59.625163 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.625749 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:29:59.618170 => 22:29:59.625638
[0m22:29:59.625958 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:29:59.678572 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.678966 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821222959666127"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:29:59.679198 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:29:59.690131 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.694581 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.694831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:29:59.695116 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.695305 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.695508 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240821222959666127'
      
      
      order by ordinal_position

  
[0m22:29:59.722912 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.726103 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.726354 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:29:59.740251 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.745289 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.745613 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:29:59.759573 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.768623 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.769233 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.769476 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240821222959666127"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240821222959666127"
    )
  
[0m22:29:59.771268 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.808539 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:29:59.809438 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.809973 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:29:59.811066 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.811990 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:29:59.626098 => 22:29:59.811726
[0m22:29:59.812343 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:29:59.815307 [info ] [Thread-1  ]: 1 of 1 OK created sql incremental model fact.fact_opportunity .................. [[32mOK[0m in 0.20s]
[0m22:29:59.815728 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:29:59.816594 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:59.816870 [debug] [MainThread]: On master: BEGIN
[0m22:29:59.817030 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:29:59.824180 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:59.824504 [debug] [MainThread]: On master: COMMIT
[0m22:29:59.824694 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:59.824990 [debug] [MainThread]: On master: COMMIT
[0m22:29:59.825245 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:59.825436 [debug] [MainThread]: On master: Close
[0m22:29:59.827540 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:29:59.827825 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:29:59.828020 [info ] [MainThread]: 
[0m22:29:59.828218 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.39 seconds (0.39s).
[0m22:29:59.828642 [debug] [MainThread]: Command end result
[0m22:29:59.836579 [info ] [MainThread]: 
[0m22:29:59.836874 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:29:59.837051 [info ] [MainThread]: 
[0m22:29:59.837232 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:29:59.837569 [debug] [MainThread]: Command `dbt run` succeeded at 22:29:59.837520 after 0.61 seconds
[0m22:29:59.837765 [debug] [MainThread]: Flushing usage events


============================== 22:30:24.595388 | eb85a8e9-c3e2-4489-b7e4-0f35d1189317 ==============================
[0m22:30:24.595388 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:30:24.598200 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:30:24.598490 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:30:24.673075 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:30:24.690467 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:30:24.725771 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:30:24.726077 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:30:24.727026 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:30:24.741091 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:30:24.742535 [info ] [MainThread]: 
[0m22:30:24.742953 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:30:24.743597 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:30:24.750822 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:30:24.751130 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:30:24.751308 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:30:24.759926 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.760898 [debug] [ThreadPool]: On list_dbt: Close
[0m22:30:24.763006 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:30:24.763467 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:30:24.766512 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:30:24.766717 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:30:24.766866 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:24.773801 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.774103 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:30:24.774274 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:30:24.774522 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.775038 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:30:24.775203 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:30:24.775350 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:30:24.775557 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.775716 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:30:24.778839 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:30:24.782780 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:30:24.783023 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:30:24.783178 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:24.789617 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.789849 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:30:24.790023 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:30:24.806334 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.809971 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:30:24.811314 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:30:24.811485 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:30:24.814041 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m22:30:24.815519 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:30:24.815703 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:30:24.815854 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:24.823559 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.823801 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:30:24.823972 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:30:24.839600 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.843544 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:30:24.843851 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:30:24.844014 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:30:24.846304 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:30:24.847819 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:30:24.848247 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:30:24.848490 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:24.855529 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.855755 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:30:24.855926 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:30:24.877496 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.879112 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:30:24.879401 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:30:24.879579 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:30:24.882723 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:30:24.885565 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:30:24.885797 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:30:24.885965 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:24.892838 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.893096 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:30:24.893274 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:30:24.908858 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.912489 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:30:24.912723 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:30:24.912882 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:30:24.916728 [debug] [MainThread]: Using duckdb connection "master"
[0m22:30:24.916908 [debug] [MainThread]: On master: BEGIN
[0m22:30:24.917059 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:30:24.923419 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:30:24.923689 [debug] [MainThread]: On master: COMMIT
[0m22:30:24.923853 [debug] [MainThread]: Using duckdb connection "master"
[0m22:30:24.924008 [debug] [MainThread]: On master: COMMIT
[0m22:30:24.924212 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:30:24.924371 [debug] [MainThread]: On master: Close
[0m22:30:24.926270 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:30:24.926565 [info ] [MainThread]: 
[0m22:30:24.928687 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:30:24.928989 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:30:24.929388 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_case)
[0m22:30:24.929600 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:30:24.935598 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:30:24.937075 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:30:24.929750 => 22:30:24.936924
[0m22:30:24.937325 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:30:24.966333 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:30:24.966882 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:30:24.967091 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:30:24.967270 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:30:24.973638 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:30:24.973918 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:30:24.974138 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.created_date,
    c.last_modified_date,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.account_id = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.owner_id = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.created_date = d.date
where c.is_deleted = false
    );
  
  
  
[0m22:30:24.975238 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:30:24.937466 => 22:30:24.975099
[0m22:30:24.975484 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m22:30:24.979095 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m22:30:24.979336 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:30:24.981580 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "account_id"
  LINE 27:     on c.account_id = a.account_id
                  ^
[0m22:30:24.981990 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case ..................... [[31mERROR[0m in 0.05s]
[0m22:30:24.982319 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:30:24.983166 [debug] [MainThread]: Using duckdb connection "master"
[0m22:30:24.983367 [debug] [MainThread]: On master: BEGIN
[0m22:30:24.983520 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:30:24.990579 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:30:24.990876 [debug] [MainThread]: On master: COMMIT
[0m22:30:24.991042 [debug] [MainThread]: Using duckdb connection "master"
[0m22:30:24.991192 [debug] [MainThread]: On master: COMMIT
[0m22:30:24.991470 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:30:24.991745 [debug] [MainThread]: On master: Close
[0m22:30:24.993605 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:30:24.993796 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:30:24.993974 [info ] [MainThread]: 
[0m22:30:24.994162 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:30:24.994501 [debug] [MainThread]: Command end result
[0m22:30:25.001752 [info ] [MainThread]: 
[0m22:30:25.002026 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:30:25.002194 [info ] [MainThread]: 
[0m22:30:25.002357 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "account_id"
  LINE 27:     on c.account_id = a.account_id
                  ^
[0m22:30:25.002530 [info ] [MainThread]: 
[0m22:30:25.002716 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:30:25.003097 [debug] [MainThread]: Command `dbt run` failed at 22:30:25.003049 after 0.42 seconds
[0m22:30:25.003299 [debug] [MainThread]: Flushing usage events


============================== 22:31:08.594212 | 640a9ac1-3a90-4e29-af5d-aefd1d846f23 ==============================
[0m22:31:08.594212 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:31:08.597199 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:31:08.597470 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:31:08.673535 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:31:08.691627 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:31:08.726659 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:31:08.726949 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:31:08.727920 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m22:31:08.742130 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:31:08.743522 [info ] [MainThread]: 
[0m22:31:08.743940 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:31:08.744597 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:31:08.751574 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:31:08.751864 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:31:08.752037 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:31:08.768362 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.769585 [debug] [ThreadPool]: On list_dbt: Close
[0m22:31:08.772269 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:31:08.772751 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:31:08.775687 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:08.775876 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:31:08.776032 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:08.783592 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.783864 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:08.784026 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:31:08.784265 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.784781 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:31:08.784945 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:08.785095 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:31:08.785299 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.785457 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:31:08.788836 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:31:08.792740 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:31:08.793055 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:31:08.793320 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:08.799899 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.800178 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:31:08.800361 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:31:08.816239 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.819860 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:31:08.821333 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:31:08.821541 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:31:08.823911 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:31:08.826172 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:31:08.826366 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:31:08.826513 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:08.832873 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.833140 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:31:08.833325 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:31:08.852722 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.853756 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:31:08.853990 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:31:08.854147 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:31:08.856559 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:31:08.859241 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:31:08.859481 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:31:08.859644 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:08.866686 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.866935 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:31:08.867106 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:31:08.882446 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.885845 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:31:08.886089 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:31:08.886252 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:31:08.888904 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:31:08.890541 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:31:08.890733 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:31:08.890885 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:08.897578 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.897828 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:31:08.898000 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:31:08.913318 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.917516 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:31:08.917756 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:31:08.917914 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:31:08.920954 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:08.921143 [debug] [MainThread]: On master: BEGIN
[0m22:31:08.921296 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:31:08.927426 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:08.927672 [debug] [MainThread]: On master: COMMIT
[0m22:31:08.927838 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:08.927985 [debug] [MainThread]: On master: COMMIT
[0m22:31:08.928181 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:08.928334 [debug] [MainThread]: On master: Close
[0m22:31:08.929906 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:31:08.930128 [info ] [MainThread]: 
[0m22:31:08.932392 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:31:08.932668 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:31:08.933022 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_case)
[0m22:31:08.933214 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:31:08.939509 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:31:08.940123 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:31:08.933349 => 22:31:08.940015
[0m22:31:08.940328 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:31:08.970354 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:31:08.970994 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:31:08.971201 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:31:08.971389 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:31:08.978313 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:31:08.978595 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:31:08.978805 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.created_date,
    c.last_modified_date,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.accountid = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.ownerid = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.created_date = d.date
where c.is_deleted = false
    );
  
  
  
[0m22:31:08.979952 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:31:08.940474 => 22:31:08.979838
[0m22:31:08.980161 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m22:31:08.984009 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m22:31:08.984325 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:31:08.986538 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "contact_id"
  LINE 29:     on c.contact_id = ct.contact_id
                  ^
[0m22:31:08.986995 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case ..................... [[31mERROR[0m in 0.05s]
[0m22:31:08.987343 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:31:08.988164 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:08.988384 [debug] [MainThread]: On master: BEGIN
[0m22:31:08.988538 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:31:08.995505 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:08.995751 [debug] [MainThread]: On master: COMMIT
[0m22:31:08.995910 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:08.996060 [debug] [MainThread]: On master: COMMIT
[0m22:31:08.996255 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:08.996410 [debug] [MainThread]: On master: Close
[0m22:31:08.998275 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:31:08.998579 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:31:08.998771 [info ] [MainThread]: 
[0m22:31:08.998985 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:31:08.999364 [debug] [MainThread]: Command end result
[0m22:31:09.007862 [info ] [MainThread]: 
[0m22:31:09.008251 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:31:09.008437 [info ] [MainThread]: 
[0m22:31:09.008601 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "contact_id"
  LINE 29:     on c.contact_id = ct.contact_id
                  ^
[0m22:31:09.008763 [info ] [MainThread]: 
[0m22:31:09.008951 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:31:09.009431 [debug] [MainThread]: Command `dbt run` failed at 22:31:09.009346 after 0.44 seconds
[0m22:31:09.009718 [debug] [MainThread]: Flushing usage events


============================== 22:31:57.961928 | 4d144952-48c7-4b81-acfd-7a16510ccc39 ==============================
[0m22:31:57.961928 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:31:57.965015 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:31:57.965282 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:31:58.049038 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:31:58.066939 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:31:58.102537 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:31:58.102844 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:31:58.103902 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:31:58.119472 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:31:58.121141 [info ] [MainThread]: 
[0m22:31:58.121719 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:31:58.122412 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:31:58.129902 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:31:58.130184 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:31:58.130359 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:31:58.146404 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.147298 [debug] [ThreadPool]: On list_dbt: Close
[0m22:31:58.149656 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:31:58.150187 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:31:58.153198 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:58.153403 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:31:58.153556 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:58.160469 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.160780 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:58.160961 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:31:58.161226 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.161864 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:31:58.162155 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:58.162328 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:31:58.162606 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.162794 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:31:58.166257 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:31:58.170025 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:31:58.170278 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:31:58.170439 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:58.177530 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.177814 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:31:58.177991 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:31:58.193567 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.197020 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:31:58.198371 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:31:58.198652 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:31:58.201072 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:31:58.203531 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:31:58.203717 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:31:58.203866 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:58.210668 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.210933 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:31:58.211113 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:31:58.230248 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.231313 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:31:58.231573 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:31:58.231751 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:31:58.234372 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:31:58.236943 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:31:58.237142 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:31:58.237288 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:58.244192 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.244450 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:31:58.244621 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:31:58.260210 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.263794 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:31:58.264033 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:31:58.264201 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:31:58.266727 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:31:58.269701 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:31:58.269889 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:31:58.270048 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:58.276792 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.277044 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:31:58.277217 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:31:58.293097 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.296625 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:31:58.296868 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:31:58.297029 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:31:58.299605 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:58.299803 [debug] [MainThread]: On master: BEGIN
[0m22:31:58.299957 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:31:58.306472 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:58.306730 [debug] [MainThread]: On master: COMMIT
[0m22:31:58.306894 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:58.307045 [debug] [MainThread]: On master: COMMIT
[0m22:31:58.307252 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:58.307408 [debug] [MainThread]: On master: Close
[0m22:31:58.309002 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:31:58.309207 [info ] [MainThread]: 
[0m22:31:58.311728 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:31:58.312004 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:31:58.312368 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_case)
[0m22:31:58.312563 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:31:58.318837 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:31:58.319740 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:31:58.312696 => 22:31:58.319565
[0m22:31:58.320012 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:31:58.349538 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:31:58.350175 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:31:58.350381 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:31:58.350560 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:31:58.357150 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:31:58.357442 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:31:58.357650 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.createddate,
    c.lastmodifieddate,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.accountid = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contactid = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.ownerid = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.createddate = d.date
where c.isdeleted = false
    );
  
  
  
[0m22:31:58.358836 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:31:58.320152 => 22:31:58.358732
[0m22:31:58.359041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m22:31:58.362863 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m22:31:58.363100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:31:58.365152 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 33:     on c.createddate = d.date
                                  ^
[0m22:31:58.365573 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case ..................... [[31mERROR[0m in 0.05s]
[0m22:31:58.365922 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:31:58.366807 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:58.367058 [debug] [MainThread]: On master: BEGIN
[0m22:31:58.367228 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:31:58.373693 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:58.373987 [debug] [MainThread]: On master: COMMIT
[0m22:31:58.374186 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:58.374345 [debug] [MainThread]: On master: COMMIT
[0m22:31:58.374544 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:58.374705 [debug] [MainThread]: On master: Close
[0m22:31:58.376583 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:31:58.376846 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:31:58.377045 [info ] [MainThread]: 
[0m22:31:58.377250 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:31:58.377612 [debug] [MainThread]: Command end result
[0m22:31:58.385610 [info ] [MainThread]: 
[0m22:31:58.385915 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:31:58.386095 [info ] [MainThread]: 
[0m22:31:58.386266 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 33:     on c.createddate = d.date
                                  ^
[0m22:31:58.386433 [info ] [MainThread]: 
[0m22:31:58.386621 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:31:58.387015 [debug] [MainThread]: Command `dbt run` failed at 22:31:58.386959 after 0.45 seconds
[0m22:31:58.387232 [debug] [MainThread]: Flushing usage events


============================== 22:32:26.160937 | 5705276b-49cf-4779-a8cd-343aab949b4e ==============================
[0m22:32:26.160937 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:32:26.163806 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:32:26.164070 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:32:26.239680 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:32:26.258350 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:32:26.293485 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:32:26.293782 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:32:26.294694 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:32:26.309919 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:32:26.311607 [info ] [MainThread]: 
[0m22:32:26.312158 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:32:26.312922 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:32:26.320483 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:32:26.320775 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:32:26.320953 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:32:26.337290 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.338314 [debug] [ThreadPool]: On list_dbt: Close
[0m22:32:26.340459 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:32:26.340879 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:32:26.343846 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:26.344046 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:32:26.344203 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:26.350885 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.351163 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:26.351335 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:32:26.351577 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.352079 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:26.352250 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:26.352398 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:26.352610 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.352774 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:32:26.355803 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:32:26.359300 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:26.359515 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:32:26.359682 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:26.366400 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.366675 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:26.366851 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:32:26.385034 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.386041 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:32:26.386751 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:32:26.386983 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:32:26.389712 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:32:26.392243 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:26.392428 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:32:26.392583 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:26.399235 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.399626 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:26.399826 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:32:26.416292 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.420218 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:32:26.420569 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:32:26.420781 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:32:26.423548 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:32:26.426099 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:26.426324 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:32:26.426478 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:26.433076 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.433325 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:26.433500 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:32:26.449268 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.453118 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:32:26.453425 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:32:26.453600 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:32:26.455954 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:32:26.458966 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:26.459182 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:32:26.459344 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:26.465780 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.466037 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:26.466215 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:32:26.481763 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.485235 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:32:26.485462 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:32:26.485622 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:32:26.488246 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:26.488461 [debug] [MainThread]: On master: BEGIN
[0m22:32:26.488625 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:32:26.495216 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:26.495482 [debug] [MainThread]: On master: COMMIT
[0m22:32:26.495646 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:26.495798 [debug] [MainThread]: On master: COMMIT
[0m22:32:26.496001 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:26.496175 [debug] [MainThread]: On master: Close
[0m22:32:26.497741 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:32:26.497978 [info ] [MainThread]: 
[0m22:32:26.500041 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:32:26.500321 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:32:26.500694 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_case)
[0m22:32:26.500886 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:32:26.506757 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:32:26.507306 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:32:26.501016 => 22:32:26.507199
[0m22:32:26.507510 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:32:26.536935 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:32:26.537534 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:26.537740 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:32:26.537925 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:26.544739 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:26.545030 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:26.545245 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.createddate,
    c.lastmodifieddate,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.accountid = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contactid = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.ownerid = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.createddate = d.date_day
where c.isdeleted = false
    );
  
  
  
[0m22:32:26.546485 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:32:26.507644 => 22:32:26.546381
[0m22:32:26.546696 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m22:32:26.550727 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m22:32:26.550930 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:32:26.552945 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Table "u" does not have a column named "username"
  LINE 23:     u.username as owner_username,
               ^
[0m22:32:26.553349 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case ..................... [[31mERROR[0m in 0.05s]
[0m22:32:26.553701 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:32:26.554449 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:26.554628 [debug] [MainThread]: On master: BEGIN
[0m22:32:26.554778 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:32:26.561316 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:26.561564 [debug] [MainThread]: On master: COMMIT
[0m22:32:26.561726 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:26.561878 [debug] [MainThread]: On master: COMMIT
[0m22:32:26.562086 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:26.562244 [debug] [MainThread]: On master: Close
[0m22:32:26.563925 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:32:26.564119 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:32:26.564310 [info ] [MainThread]: 
[0m22:32:26.564499 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:32:26.564838 [debug] [MainThread]: Command end result
[0m22:32:26.572152 [info ] [MainThread]: 
[0m22:32:26.572423 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:32:26.572792 [info ] [MainThread]: 
[0m22:32:26.573065 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Table "u" does not have a column named "username"
  LINE 23:     u.username as owner_username,
               ^
[0m22:32:26.573268 [info ] [MainThread]: 
[0m22:32:26.573466 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:32:26.573852 [debug] [MainThread]: Command `dbt run` failed at 22:32:26.573799 after 0.43 seconds
[0m22:32:26.574059 [debug] [MainThread]: Flushing usage events


============================== 22:32:42.960984 | 4db7511c-f3c6-4a8c-86a3-16390f540927 ==============================
[0m22:32:42.960984 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:32:42.964361 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:32:42.964706 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:32:43.046854 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:32:43.066770 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:32:43.101226 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:32:43.101533 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:32:43.102496 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m22:32:43.118492 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:32:43.120046 [info ] [MainThread]: 
[0m22:32:43.120565 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:32:43.121226 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:32:43.129372 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:32:43.129687 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:32:43.129883 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:32:43.146561 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.147571 [debug] [ThreadPool]: On list_dbt: Close
[0m22:32:43.149608 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:32:43.149986 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:32:43.153043 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:43.153338 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:32:43.153503 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:43.160421 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.160645 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:43.160808 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:32:43.161056 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.161540 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:43.161702 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:43.161846 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:43.162050 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.162207 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:32:43.165321 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:32:43.168624 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:43.168844 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:32:43.169005 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:43.175863 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.176163 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:43.176338 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:32:43.191618 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.195157 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:32:43.198923 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:32:43.199100 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:32:43.201352 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:32:43.203607 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:43.203781 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:32:43.203930 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:43.211284 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.211545 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:43.211718 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:32:43.227618 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.231201 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:32:43.231430 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:32:43.231588 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:32:43.234029 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:32:43.235694 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:43.235867 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:32:43.236012 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:43.242304 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.242559 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:43.242729 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:32:43.261603 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.262663 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:32:43.262914 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:32:43.263080 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:32:43.265737 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:32:43.268418 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:43.268625 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:32:43.268795 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:43.275610 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.275883 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:43.276059 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:32:43.292233 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.295725 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:32:43.295995 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:32:43.296167 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:32:43.300143 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:43.300363 [debug] [MainThread]: On master: BEGIN
[0m22:32:43.300526 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:32:43.306972 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:43.307244 [debug] [MainThread]: On master: COMMIT
[0m22:32:43.307404 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:43.307552 [debug] [MainThread]: On master: COMMIT
[0m22:32:43.307754 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:43.307911 [debug] [MainThread]: On master: Close
[0m22:32:43.309580 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:32:43.309799 [info ] [MainThread]: 
[0m22:32:43.311257 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:32:43.311557 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:32:43.311959 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_case)
[0m22:32:43.312162 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:32:43.318261 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:32:43.318838 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:32:43.312306 => 22:32:43.318726
[0m22:32:43.319045 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:32:43.348408 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:32:43.349070 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:43.349281 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:32:43.349463 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:43.356260 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:43.356495 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:43.356710 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.createddate,
    c.lastmodifieddate,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.user_name as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.accountid = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contactid = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.ownerid = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.createddate = d.date_day
where c.isdeleted = false
    );
  
  
  
[0m22:32:43.361398 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:43.371171 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: COMMIT
[0m22:32:43.371428 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:43.371614 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: COMMIT
[0m22:32:43.372432 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:43.372907 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:32:43.319186 => 22:32:43.372815
[0m22:32:43.373108 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:32:43.403656 [info ] [Thread-1  ]: 1 of 1 OK created sql incremental model fact.fact_case ......................... [[32mOK[0m in 0.09s]
[0m22:32:43.404109 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:32:43.404909 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:43.405112 [debug] [MainThread]: On master: BEGIN
[0m22:32:43.405270 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:32:43.412444 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:43.412739 [debug] [MainThread]: On master: COMMIT
[0m22:32:43.412920 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:43.413080 [debug] [MainThread]: On master: COMMIT
[0m22:32:43.413286 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:43.413458 [debug] [MainThread]: On master: Close
[0m22:32:43.415352 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:32:43.415613 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:32:43.415798 [info ] [MainThread]: 
[0m22:32:43.415991 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.30 seconds (0.30s).
[0m22:32:43.416429 [debug] [MainThread]: Command end result
[0m22:32:43.423659 [info ] [MainThread]: 
[0m22:32:43.424046 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:32:43.424232 [info ] [MainThread]: 
[0m22:32:43.424427 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:32:43.425093 [debug] [MainThread]: Command `dbt run` succeeded at 22:32:43.424986 after 0.49 seconds
[0m22:32:43.425522 [debug] [MainThread]: Flushing usage events


============================== 22:32:50.484547 | 7e1350ab-00aa-47cd-8d12-3450257b134c ==============================
[0m22:32:50.484547 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:32:50.487123 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:32:50.489151 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:32:50.562510 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:32:50.580875 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:32:50.619822 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:32:50.620150 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:32:50.621095 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:32:50.635053 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:32:50.636847 [info ] [MainThread]: 
[0m22:32:50.637343 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:32:50.638876 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:32:50.646001 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:32:50.646279 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:32:50.646469 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:32:50.654833 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.655728 [debug] [ThreadPool]: On list_dbt: Close
[0m22:32:50.659559 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:32:50.659956 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:32:50.660213 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.667665 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.668462 [debug] [ThreadPool]: On list_dbt: Close
[0m22:32:50.671971 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:32:50.672231 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:32:50.672402 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.679454 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.680161 [debug] [ThreadPool]: On list_dbt: Close
[0m22:32:50.682322 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m22:32:50.682818 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m22:32:50.686007 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:32:50.686292 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m22:32:50.686480 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.694718 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.695041 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:32:50.695322 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m22:32:50.696092 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.696678 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m22:32:50.696874 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:32:50.697041 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m22:32:50.697329 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.697504 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m22:32:50.699986 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now create_dbt_dim)
[0m22:32:50.700529 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:32:50.702321 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:32:50.702584 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:32:50.702759 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.709817 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.710103 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:32:50.710378 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:32:50.710953 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.711529 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:32:50.711825 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:32:50.712105 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:32:50.712421 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.712615 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:32:50.714949 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now create_dbt_fact)
[0m22:32:50.715566 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:32:50.717869 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:50.718149 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:32:50.718371 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.726799 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.727257 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:50.727525 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:32:50.727923 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.728640 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:50.728884 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:50.729085 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:50.729392 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.730269 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:32:50.734431 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:32:50.740109 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:50.740383 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:32:50.740543 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.747593 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.747944 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:50.748147 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:32:50.766059 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.766945 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:32:50.767663 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:32:50.767841 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:32:50.770246 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:32:50.772682 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:50.772887 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:32:50.773051 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.779642 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.779902 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:50.780079 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:32:50.795430 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.798847 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:32:50.799077 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:32:50.799252 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:32:50.801708 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:32:50.803904 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:50.804078 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:32:50.804223 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.810810 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.811067 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:50.811240 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:32:50.826664 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.830077 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:32:50.830303 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:32:50.830463 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:32:50.832747 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:32:50.834887 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:50.835058 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:32:50.835205 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.841638 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.841903 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:50.842088 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:32:50.857423 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.860784 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:32:50.861016 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:32:50.861171 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:32:50.863652 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:50.863857 [debug] [MainThread]: On master: BEGIN
[0m22:32:50.864012 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:32:50.870216 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:50.870480 [debug] [MainThread]: On master: COMMIT
[0m22:32:50.870638 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:50.870789 [debug] [MainThread]: On master: COMMIT
[0m22:32:50.870994 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:50.871160 [debug] [MainThread]: On master: Close
[0m22:32:50.872764 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:32:50.872972 [info ] [MainThread]: 
[0m22:32:50.875417 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:32:50.875697 [info ] [Thread-1  ]: 1 of 32 START sql table model dim.dim_date ..................................... [RUN]
[0m22:32:50.876055 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_date)
[0m22:32:50.876243 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:32:50.902734 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:50.903079 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:32:50.903274 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:50.910055 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:50.910379 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:50.910640 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:32:50.911082 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:50.951562 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:32:50.954341 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:32:50.876379 => 22:32:50.954222
[0m22:32:50.954561 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:32:50.996244 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m22:32:50.996892 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:50.997293 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m22:32:51.044101 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.048086 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:51.048334 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m22:32:51.048765 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.050523 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:51.050726 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m22:32:51.051040 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.061335 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:32:51.061552 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:51.061751 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:32:51.064625 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.067629 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:51.067851 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m22:32:51.068362 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.069106 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:32:50.954704 => 22:32:51.069011
[0m22:32:51.069310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:32:51.097160 [info ] [Thread-1  ]: 1 of 32 OK created sql table model dim.dim_date ................................ [[32mOK[0m in 0.22s]
[0m22:32:51.097579 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:32:51.097834 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m22:32:51.098113 [info ] [Thread-1  ]: 2 of 32 START sql table model fact.fact_product_sales .......................... [RUN]
[0m22:32:51.098468 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.fact_product_sales)
[0m22:32:51.098666 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m22:32:51.100840 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m22:32:51.101916 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 22:32:51.098798 => 22:32:51.101802
[0m22:32:51.102128 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m22:32:51.105622 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_product_sales"
[0m22:32:51.106158 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m22:32:51.106360 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: BEGIN
[0m22:32:51.106541 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.112920 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.113211 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m22:32:51.113430 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_product_sales"} */

  
    
    

    create  table
      "dbt"."fact"."fact_product_sales__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."pricebook_entry"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS product_sales_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
    product2id AS product_fk,                                      -- Foreign Key to dim_product
    pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
    unitprice AS unit_price,
    isactive AS is_active,
    createddate AS product_sales_created_at,
    lastmodifieddate AS product_sales_last_modified_date
FROM source
    );
  
  
[0m22:32:51.114394 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 22:32:51.102262 => 22:32:51.114224
[0m22:32:51.114638 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: ROLLBACK
[0m22:32:51.118456 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_product_sales'
[0m22:32:51.118799 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: Close
[0m22:32:51.120901 [debug] [Thread-1  ]: Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m22:32:51.121343 [error] [Thread-1  ]: 2 of 32 ERROR creating sql table model fact.fact_product_sales ................. [[31mERROR[0m in 0.02s]
[0m22:32:51.121689 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m22:32:51.121914 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:32:51.122276 [info ] [Thread-1  ]: 3 of 32 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m22:32:51.122757 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m22:32:51.122988 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m22:32:51.125142 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.125851 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 22:32:51.123125 => 22:32:51.125739
[0m22:32:51.126059 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m22:32:51.136506 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.137279 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.137573 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m22:32:51.137756 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.144153 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.144460 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.144715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m22:32:51.145533 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.147903 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.148246 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m22:32:51.148720 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.150865 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.151109 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m22:32:51.151441 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.152450 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m22:32:51.152669 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.152847 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m22:32:51.153468 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.155790 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.156011 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m22:32:51.156378 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.157098 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 22:32:51.126191 => 22:32:51.157006
[0m22:32:51.157299 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m22:32:51.170848 [info ] [Thread-1  ]: 3 of 32 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.05s]
[0m22:32:51.171267 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:32:51.171513 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:32:51.171839 [info ] [Thread-1  ]: 4 of 32 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m22:32:51.172210 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m22:32:51.172417 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:32:51.174482 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.174958 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 22:32:51.172558 => 22:32:51.174858
[0m22:32:51.175176 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:32:51.178036 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.178671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.178914 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m22:32:51.179092 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.185757 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.186043 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.186279 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m22:32:51.187041 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.189193 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.189404 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m22:32:51.189731 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.191436 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.191641 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m22:32:51.191929 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.192764 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m22:32:51.192955 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.193130 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m22:32:51.193830 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.196432 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.196677 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m22:32:51.197106 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.197948 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 22:32:51.175318 => 22:32:51.197839
[0m22:32:51.198206 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m22:32:51.211796 [info ] [Thread-1  ]: 4 of 32 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.04s]
[0m22:32:51.212232 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:32:51.212481 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:32:51.212722 [info ] [Thread-1  ]: 5 of 32 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m22:32:51.213164 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m22:32:51.213367 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m22:32:51.215458 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.215930 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 22:32:51.213501 => 22:32:51.215829
[0m22:32:51.216127 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m22:32:51.224763 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.225412 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.226651 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m22:32:51.231317 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.259164 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.259536 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.259840 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m22:32:51.260706 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.263093 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.263344 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m22:32:51.263703 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.265448 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.265653 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m22:32:51.265943 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.266845 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m22:32:51.267036 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.267208 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m22:32:51.267756 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.269329 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.269531 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m22:32:51.269956 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.270745 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 22:32:51.216256 => 22:32:51.270649
[0m22:32:51.270956 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m22:32:51.284873 [info ] [Thread-1  ]: 5 of 32 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.07s]
[0m22:32:51.285293 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:32:51.285533 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:32:51.285905 [info ] [Thread-1  ]: 6 of 32 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m22:32:51.286395 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m22:32:51.286608 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:32:51.289399 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.290199 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 22:32:51.286761 => 22:32:51.290081
[0m22:32:51.290417 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:32:51.293163 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.293636 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.293836 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m22:32:51.294019 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.300953 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.301254 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.301464 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m22:32:51.301910 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.304084 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.304296 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m22:32:51.304601 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.306291 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.306516 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m22:32:51.306854 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.307793 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m22:32:51.308159 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.308367 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m22:32:51.308986 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.310809 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.311014 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m22:32:51.311385 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.312186 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 22:32:51.290548 => 22:32:51.312086
[0m22:32:51.312448 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m22:32:51.326613 [info ] [Thread-1  ]: 6 of 32 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.04s]
[0m22:32:51.327027 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:32:51.327308 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:32:51.327665 [info ] [Thread-1  ]: 7 of 32 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m22:32:51.328036 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m22:32:51.328231 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:32:51.330314 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.330808 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 22:32:51.328370 => 22:32:51.330709
[0m22:32:51.331010 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:32:51.334518 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.335020 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.335223 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m22:32:51.335407 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.342527 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.342834 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.343101 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m22:32:51.343914 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.346343 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.346582 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m22:32:51.346941 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.348754 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.348981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m22:32:51.349290 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.350170 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m22:32:51.350365 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.350538 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m22:32:51.351154 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.353104 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.353372 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m22:32:51.353823 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.354672 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 22:32:51.331138 => 22:32:51.354568
[0m22:32:51.354880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m22:32:51.373364 [info ] [Thread-1  ]: 7 of 32 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.05s]
[0m22:32:51.373793 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:32:51.374042 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:32:51.374369 [info ] [Thread-1  ]: 8 of 32 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m22:32:51.374737 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m22:32:51.374931 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:32:51.377030 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.377587 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 22:32:51.375065 => 22:32:51.377476
[0m22:32:51.377810 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:32:51.381463 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.382037 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.382243 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m22:32:51.382424 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.388966 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.389260 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.389492 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m22:32:51.390310 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.392700 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.393013 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m22:32:51.393476 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.395513 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.395777 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m22:32:51.396100 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.396998 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m22:32:51.397188 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.397365 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m22:32:51.398029 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.399971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.400196 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m22:32:51.400610 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.401537 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 22:32:51.377946 => 22:32:51.401406
[0m22:32:51.401816 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m22:32:51.415080 [info ] [Thread-1  ]: 8 of 32 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.04s]
[0m22:32:51.415507 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:32:51.415757 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:32:51.416086 [info ] [Thread-1  ]: 9 of 32 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m22:32:51.416462 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m22:32:51.416656 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:32:51.418693 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.419177 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 22:32:51.416793 => 22:32:51.419070
[0m22:32:51.419378 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:32:51.422076 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.422645 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.422875 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m22:32:51.423073 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.429930 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.430229 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.430464 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m22:32:51.431233 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.434339 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.434664 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m22:32:51.435119 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.437142 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.437359 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m22:32:51.437675 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.438582 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m22:32:51.438884 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.439089 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m22:32:51.439804 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.441620 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.441850 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m22:32:51.442240 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.443011 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 22:32:51.419505 => 22:32:51.442906
[0m22:32:51.443220 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m22:32:51.457071 [info ] [Thread-1  ]: 9 of 32 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.04s]
[0m22:32:51.457491 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:32:51.457740 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:32:51.458082 [info ] [Thread-1  ]: 10 of 32 START sql view model staging.stg_salesforce__opportunity_history ...... [RUN]
[0m22:32:51.458442 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m22:32:51.458641 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:32:51.460650 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.461143 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 22:32:51.458777 => 22:32:51.461037
[0m22:32:51.461342 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:32:51.464026 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.464524 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.464781 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m22:32:51.464963 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.471585 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.471873 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.472090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m22:32:51.472655 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.475538 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.475765 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m22:32:51.476098 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.477796 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.478018 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m22:32:51.478320 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.479161 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m22:32:51.479354 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.479529 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m22:32:51.480173 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.481946 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.482180 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m22:32:51.482572 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.483478 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 22:32:51.461470 => 22:32:51.483372
[0m22:32:51.483714 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m22:32:51.497251 [info ] [Thread-1  ]: 10 of 32 OK created sql view model staging.stg_salesforce__opportunity_history . [[32mOK[0m in 0.04s]
[0m22:32:51.497730 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:32:51.497986 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:32:51.498327 [info ] [Thread-1  ]: 11 of 32 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m22:32:51.498758 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m22:32:51.498994 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:32:51.501107 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.501615 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 22:32:51.499134 => 22:32:51.501504
[0m22:32:51.501835 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:32:51.504609 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.505051 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.505253 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m22:32:51.505442 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.512270 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.512608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.512825 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m22:32:51.513459 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.515750 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.515990 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m22:32:51.516333 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.518747 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.518956 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m22:32:51.519259 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.520112 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m22:32:51.520302 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.520478 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m22:32:51.521080 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.523043 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.523281 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m22:32:51.523691 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.524491 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 22:32:51.501975 => 22:32:51.524385
[0m22:32:51.524710 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m22:32:51.538778 [info ] [Thread-1  ]: 11 of 32 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.04s]
[0m22:32:51.539253 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:32:51.539512 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:32:51.539852 [info ] [Thread-1  ]: 12 of 32 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m22:32:51.540261 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m22:32:51.540460 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:32:51.542476 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.543374 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 22:32:51.540591 => 22:32:51.543272
[0m22:32:51.543580 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:32:51.546461 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.546935 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.547182 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m22:32:51.547364 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.553830 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.554156 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.554385 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m22:32:51.555063 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.557552 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.557814 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m22:32:51.558144 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.560543 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.560765 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m22:32:51.561064 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.561932 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m22:32:51.562135 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.562310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m22:32:51.562891 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.564709 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.564937 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m22:32:51.565321 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.566219 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 22:32:51.543718 => 22:32:51.566120
[0m22:32:51.566441 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m22:32:51.580090 [info ] [Thread-1  ]: 12 of 32 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.04s]
[0m22:32:51.580566 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:32:51.580832 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:32:51.581081 [info ] [Thread-1  ]: 13 of 32 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m22:32:51.581575 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m22:32:51.581802 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:32:51.583813 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.584319 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 22:32:51.581946 => 22:32:51.584213
[0m22:32:51.584526 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:32:51.587262 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.588054 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.588380 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m22:32:51.588641 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.594987 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.595284 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.595495 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m22:32:51.595972 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.598185 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.598538 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m22:32:51.598953 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.601083 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.601300 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m22:32:51.601633 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.602646 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m22:32:51.602860 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.603047 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m22:32:51.603608 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.605924 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.606248 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m22:32:51.606668 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.607504 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 22:32:51.584670 => 22:32:51.607401
[0m22:32:51.607728 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m22:32:51.621999 [info ] [Thread-1  ]: 13 of 32 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.04s]
[0m22:32:51.622429 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:32:51.622671 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:32:51.622997 [info ] [Thread-1  ]: 14 of 32 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m22:32:51.623368 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m22:32:51.623564 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:32:51.625659 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.626340 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 22:32:51.623696 => 22:32:51.626238
[0m22:32:51.626544 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:32:51.629289 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.629796 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.629990 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m22:32:51.630170 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.636668 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.636965 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.637175 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m22:32:51.637683 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.640238 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.640549 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m22:32:51.640948 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.642823 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.643036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m22:32:51.643364 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.644227 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m22:32:51.644422 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.644599 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m22:32:51.645133 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.646676 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.646880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m22:32:51.647249 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.648886 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 22:32:51.626676 => 22:32:51.648785
[0m22:32:51.649106 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m22:32:51.661887 [info ] [Thread-1  ]: 14 of 32 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.04s]
[0m22:32:51.662276 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:32:51.662508 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:32:51.662949 [info ] [Thread-1  ]: 15 of 32 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m22:32:51.663430 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m22:32:51.663649 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m22:32:51.665800 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.666394 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 22:32:51.663860 => 22:32:51.666261
[0m22:32:51.666637 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m22:32:51.669468 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.670157 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.670438 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m22:32:51.670752 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.677096 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.677391 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.677645 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m22:32:51.678831 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.681322 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.681600 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m22:32:51.681986 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.683742 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.683949 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m22:32:51.684254 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.685194 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m22:32:51.685403 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.685579 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m22:32:51.686198 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.687749 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.687942 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m22:32:51.688284 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.689024 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 22:32:51.666778 => 22:32:51.688931
[0m22:32:51.689310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m22:32:51.712758 [info ] [Thread-1  ]: 15 of 32 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.05s]
[0m22:32:51.713221 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:32:51.713487 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:32:51.713783 [info ] [Thread-1  ]: 16 of 32 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m22:32:51.714176 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m22:32:51.714393 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:32:51.717306 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.717773 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 22:32:51.714543 => 22:32:51.717675
[0m22:32:51.717972 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:32:51.720805 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.721346 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.721549 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m22:32:51.721731 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.728495 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.728717 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.728927 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m22:32:51.729459 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.731603 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.731831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m22:32:51.732144 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.733933 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.734271 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m22:32:51.734714 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.735705 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m22:32:51.735918 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.736101 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m22:32:51.736749 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.738621 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.738845 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m22:32:51.739248 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.740071 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 22:32:51.718106 => 22:32:51.739973
[0m22:32:51.740293 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m22:32:51.753972 [info ] [Thread-1  ]: 16 of 32 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.04s]
[0m22:32:51.754377 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:32:51.754622 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m22:32:51.754947 [info ] [Thread-1  ]: 17 of 32 START sql table model dim.dim_account ................................. [RUN]
[0m22:32:51.755302 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m22:32:51.755511 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m22:32:51.757791 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m22:32:51.758966 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 22:32:51.755649 => 22:32:51.758853
[0m22:32:51.759182 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m22:32:51.788813 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m22:32:51.789383 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.789576 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m22:32:51.789755 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.796422 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.796714 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.797008 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

select
    account_id,
    masterrecordid as master_record_id,
    name as account_name,
    type as account_type,
    parentid as parent_account_id,
    billingstreet as billing_street,
    billingcity as billing_city,
    billingstate as billing_state,
    billingpostalcode as billing_postal_code,
    billingcountry as billing_country,
    billinglatitude as billing_latitude,
    billinglongitude as billing_longitude,
    billinggeocodeaccuracy as billing_geocode_accuracy,
    shippingstreet as shipping_street,
    shippingcity as shipping_city,
    shippingstate as shipping_state,
    shippingpostalcode as shipping_postal_code,
    shippingcountry as shipping_country,
    shippinglatitude as shipping_latitude,
    shippinglongitude as shipping_longitude,
    shippinggeocodeaccuracy as shipping_geocode_accuracy,
    phone as account_phone,
    fax as account_fax,
    accountnumber as account_number,
    website as account_website,
    sic as sic_code,
    industry as account_industry,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownership as account_ownership,
    tickersymbol as ticker_symbol,
    description as account_description,
    rating as account_rating,
    site as account_site,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    jigsaw as jigsaw_id,
    jigsawcompanyid as jigsaw_company_id,
    cleanstatus as clean_status,
    accountsource as account_source,
    dunsnumber as duns_number,
    tradestyle as trade_style,
    naicscode as naics_code,
    naicsdesc as naics_description,
    yearstarted as year_started,
    sicdesc as sic_description,
    dandbcompanyid as dandb_company_id,
    operatinghoursid as operating_hours_id,
    customerpriority__c as customer_priority,
    sla__c as sla,
    active__c as is_active,
    numberoflocations__c as number_of_locations,
    upsellopportunity__c as upsell_opportunity,
    slaserialnumber__c as sla_serial_number,
    slaexpirationdate__c as sla_expiration_date
from "dbt"."staging"."stg_salesforce__account"
where isdeleted = false  -- Exclude deleted accounts
order by account_name
    );
  
  
[0m22:32:51.801460 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.803591 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.803810 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account" rename to "dim_account__dbt_backup"
[0m22:32:51.804194 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.805964 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.806178 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m22:32:51.806506 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.807499 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m22:32:51.807699 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.807884 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m22:32:51.809382 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.810948 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.811143 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m22:32:51.811637 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.812340 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 22:32:51.759319 => 22:32:51.812251
[0m22:32:51.812545 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m22:32:51.833848 [info ] [Thread-1  ]: 17 of 32 OK created sql table model dim.dim_account ............................ [[32mOK[0m in 0.08s]
[0m22:32:51.834256 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m22:32:51.834488 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m22:32:51.834871 [info ] [Thread-1  ]: 18 of 32 START sql table model dim.dim_campaign ................................ [RUN]
[0m22:32:51.835353 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m22:32:51.835579 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m22:32:51.837928 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.838857 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 22:32:51.835723 => 22:32:51.838717
[0m22:32:51.839082 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m22:32:51.842742 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.843248 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.843452 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m22:32:51.843627 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.849976 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.850266 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.850628 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

select
    /* IDs */
    campaign_id,
    parentid as parent_campaign_id,
    ownerid as owner_id,
    createdbyid as created_by_id,
    lastmodifiedbyid as last_modified_by_id,
    campaignmemberrecordtypeid as campaign_member_record_type_id,

    /* Dates */
    startdate as start_date,
    enddate as end_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,

    /* Dimensions */
    name as campaign_name,
    type as campaign_type,
    status,
    description,
    isactive as is_active,

    /* Metrics */
    expectedrevenue as expected_revenue,
    budgetedcost as budgeted_cost,
    actualcost as actual_cost,
    expectedresponse as expected_response,
    numbersent as number_sent,
    numberofleads as number_of_leads,
    numberofconvertedleads as number_of_converted_leads,
    numberofcontacts as number_of_contacts,
    numberofresponses as number_of_responses,
    numberofopportunities as number_of_opportunities,
    numberofwonopportunities as number_of_won_opportunities,
    amountallopportunities as amount_all_opportunities,
    amountwonopportunities as amount_won_opportunities,
    hierarchynumberofleads as hierarchy_number_of_leads,
    hierarchynumberofconvertedleads as hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts as hierarchy_number_of_contacts,
    hierarchynumberofresponses as hierarchy_number_of_responses,
    hierarchynumberofopportunities as hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities as hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities as hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities as hierarchy_amount_won_opportunities,
    hierarchynumbersent as hierarchy_number_sent,
    hierarchyexpectedrevenue as hierarchy_expected_revenue,
    hierarchybudgetedcost as hierarchy_budgeted_cost,
    hierarchyactualcost as hierarchy_actual_cost

from "dbt"."staging"."stg_salesforce__campaign"
where isdeleted = false
    );
  
  
[0m22:32:51.854239 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.856325 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.856544 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m22:32:51.856909 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.857775 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m22:32:51.857968 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.858141 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m22:32:51.859332 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.861256 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.861505 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."dim"."dim_campaign__dbt_backup" cascade
[0m22:32:51.861835 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.862626 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 22:32:51.839223 => 22:32:51.862528
[0m22:32:51.862834 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m22:32:51.884922 [info ] [Thread-1  ]: 18 of 32 OK created sql table model dim.dim_campaign ........................... [[32mOK[0m in 0.05s]
[0m22:32:51.885356 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m22:32:51.885602 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m22:32:51.885876 [info ] [Thread-1  ]: 19 of 32 START sql table model dim.dim_case_status ............................. [RUN]
[0m22:32:51.886249 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m22:32:51.886437 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m22:32:51.888639 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.896548 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 22:32:51.886572 => 22:32:51.896380
[0m22:32:51.896802 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m22:32:51.899455 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.900222 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.900487 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m22:32:51.900671 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.907487 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.907766 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.907981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."dim"."dim_case_status__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct status as status_name, status as status_description  -- Adjust as needed, typically a description field should be separate
        from "dbt"."staging"."stg_salesforce__case_history_2"
    )

select
    row_number() over (order by status_name) as status_id,  -- Surrogate Key
    status_name,
    status_description
from source
    );
  
  
[0m22:32:51.909906 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.912198 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.912452 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m22:32:51.912863 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.913892 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m22:32:51.914171 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.914387 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m22:32:51.914948 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.917369 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.917598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."dim"."dim_case_status__dbt_backup" cascade
[0m22:32:51.917988 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.918851 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 22:32:51.896941 => 22:32:51.918749
[0m22:32:51.919078 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m22:32:51.934671 [info ] [Thread-1  ]: 19 of 32 OK created sql table model dim.dim_case_status ........................ [[32mOK[0m in 0.05s]
[0m22:32:51.935113 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m22:32:51.935359 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m22:32:51.935626 [info ] [Thread-1  ]: 20 of 32 START sql table model dim.dim_contact ................................. [RUN]
[0m22:32:51.935974 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m22:32:51.936164 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m22:32:51.938380 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.938885 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 22:32:51.936296 => 22:32:51.938781
[0m22:32:51.939089 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m22:32:51.941719 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.942157 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.942350 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m22:32:51.942523 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.949131 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.949405 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.949659 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

select
    /* IDs */
    contact_id,
    masterrecordid as master_record_id,
    accountid as account_id,
    reportstoid as reports_to_id,
    ownerid as owner_id,
    jigsawcontactid as jigsaw_contact_id,
    individualid as individual_id,

    /* Dates */
    birthdate as birth_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    lastcurequestdate as last_cu_request_date,
    lastcuupdatedate as last_cu_update_date,
    emailbounceddate as email_bounced_date,

    /* Dimensions */
    salutation,
    firstname as first_name,
    lastname as last_name,
    otherstreet as other_street,
    othercity as other_city,
    otherstate as other_state,
    otherpostalcode as other_postal_code,
    othercountry as other_country,
    otherlatitude as other_latitude,
    otherlongitude as other_longitude,
    othergeocodeaccuracy as other_geocode_accuracy,
    mailingstreet as mailing_street,
    mailingcity as mailing_city,
    mailingstate as mailing_state,
    mailingpostalcode as mailing_postal_code,
    mailingcountry as mailing_country,
    mailinglatitude as mailing_latitude,
    mailinglongitude as mailing_longitude,
    mailinggeocodeaccuracy as mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone as mobile_phone,
    homephone as home_phone,
    otherphone as other_phone,
    assistantphone as assistant_phone,
    email,
    title,
    department,
    assistantname as assistant_name,
    leadsource as lead_source,
    description,
    pronouns,
    genderidentity as gender_identity,
    cleanstatus as clean_status,
    emailbouncedreason as email_bounced_reason,
    level__c as level,
    languages__c as languages,

    /* Metrics */
    hasoptedoutofemail as has_opted_out_of_email,
    hasoptedoutoffax as has_opted_out_of_fax,
    donotcall as do_not_call

from "dbt"."staging"."stg_salesforce__contact"
where isdeleted = false
    );
  
  
[0m22:32:51.953561 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.955920 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.956173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact" rename to "dim_contact__dbt_backup"
[0m22:32:51.956520 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.958499 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.958710 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m22:32:51.959035 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.959974 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m22:32:51.960162 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.960331 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m22:32:51.961607 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.963455 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.963676 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m22:32:51.964335 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.965160 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 22:32:51.939228 => 22:32:51.965063
[0m22:32:51.965391 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m22:32:51.989655 [info ] [Thread-1  ]: 20 of 32 OK created sql table model dim.dim_contact ............................ [[32mOK[0m in 0.05s]
[0m22:32:51.990090 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m22:32:51.990335 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m22:32:51.990606 [info ] [Thread-1  ]: 21 of 32 START sql table model dim.dim_lead .................................... [RUN]
[0m22:32:51.990969 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m22:32:51.991160 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m22:32:51.994450 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m22:32:51.994952 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 22:32:51.991291 => 22:32:51.994844
[0m22:32:51.995151 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m22:32:51.998273 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m22:32:51.998718 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:51.998901 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m22:32:51.999071 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.005617 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.005897 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:52.006167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

select
    lead_id,
    masterrecordid as master_record_id,
    salutation as lead_salutation,
    firstname as first_name,
    lastname as last_name,
    title as lead_title,
    company as lead_company,
    street as lead_street,
    city as lead_city,
    state as lead_state,
    postalcode as lead_postal_code,
    country as lead_country,
    latitude as lead_latitude,
    longitude as lead_longitude,
    geocodeaccuracy as geocode_accuracy,
    phone as lead_phone,
    mobilephone as lead_mobile_phone,
    fax as lead_fax,
    email as lead_email,
    website as lead_website,
    description as lead_description,
    leadsource as lead_source,
    status as lead_status,
    industry as lead_industry,
    rating as lead_rating,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownerid as owner_id,
    hasoptedoutofemail as has_opted_out_of_email,
    isconverted as is_converted,
    converteddate as converted_date,
    convertedaccountid as converted_account_id,
    convertedcontactid as converted_contact_id,
    convertedopportunityid as converted_opportunity_id,
    isunreadbyowner as is_unread_by_owner,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    donotcall as do_not_call,
    hasoptedoutoffax as has_opted_out_of_fax,
    lasttransferdate as last_transfer_date,
    jigsaw as jigsaw_id,
    jigsawcontactid as jigsaw_contact_id,
    cleanstatus as clean_status,
    companydunsnumber as company_duns_number,
    dandbcompanyid as dandb_company_id,
    emailbouncedreason as email_bounced_reason,
    emailbounceddate as email_bounced_date,
    individualid as individual_id,
    pronouns as lead_pronouns,
    genderidentity as gender_identity,
    siccode__c as sic_code,
    productinterest__c as product_interest,
    primary__c as is_primary,
    currentgenerators__c as current_generators,
    numberoflocations__c as number_of_locations
from "dbt"."staging"."stg_salesforce__lead"
where isdeleted = false  -- Exclude deleted leads
order by last_name, first_name
    );
  
  
[0m22:32:52.010300 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.012722 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:52.012981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead" rename to "dim_lead__dbt_backup"
[0m22:32:52.013440 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.015529 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:52.015760 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m22:32:52.016107 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.017102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m22:32:52.017299 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:52.017473 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m22:32:52.018738 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.020618 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:52.020868 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m22:32:52.021436 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.022267 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 22:32:51.995281 => 22:32:52.022165
[0m22:32:52.022486 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m22:32:52.043938 [info ] [Thread-1  ]: 21 of 32 OK created sql table model dim.dim_lead ............................... [[32mOK[0m in 0.05s]
[0m22:32:52.044388 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m22:32:52.044640 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m22:32:52.044923 [info ] [Thread-1  ]: 22 of 32 START sql table model dim.dim_opportunity ............................. [RUN]
[0m22:32:52.045289 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m22:32:52.045510 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m22:32:52.048822 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.049399 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 22:32:52.045653 => 22:32:52.049283
[0m22:32:52.049616 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m22:32:52.052464 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.052968 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.053171 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m22:32:52.053347 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.060359 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.060644 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.060893 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m22:32:52.064151 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.066577 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.066873 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m22:32:52.067340 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.069351 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.069568 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m22:32:52.069896 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.070858 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m22:32:52.071053 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.071311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m22:32:52.072827 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.074732 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.074968 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m22:32:52.075508 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.076397 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 22:32:52.049758 => 22:32:52.076294
[0m22:32:52.076613 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m22:32:52.097125 [info ] [Thread-1  ]: 22 of 32 OK created sql table model dim.dim_opportunity ........................ [[32mOK[0m in 0.05s]
[0m22:32:52.097592 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m22:32:52.097842 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:32:52.098163 [info ] [Thread-1  ]: 23 of 32 START sql table model dim.dim_opportunity_stage ....................... [RUN]
[0m22:32:52.098667 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m22:32:52.098928 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:32:52.101167 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.104210 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 22:32:52.099073 => 22:32:52.104065
[0m22:32:52.104432 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:32:52.108869 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.109508 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.109753 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m22:32:52.109944 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.117262 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.117589 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.117810 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct stagename as stage_name, stagesortorder as stage_sort_order
        from "dbt"."staging"."stg_salesforce__opportunity"
    )

select
    row_number() over (order by stage_sort_order) as stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
from source
    );
  
  
[0m22:32:52.121939 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.124311 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.124565 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m22:32:52.124926 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.126890 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.127152 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m22:32:52.127458 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.128449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m22:32:52.128647 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.128822 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m22:32:52.129493 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.131556 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.131820 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m22:32:52.132280 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.133172 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 22:32:52.104574 => 22:32:52.133017
[0m22:32:52.133420 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m22:32:52.148800 [info ] [Thread-1  ]: 23 of 32 OK created sql table model dim.dim_opportunity_stage .................. [[32mOK[0m in 0.05s]
[0m22:32:52.149187 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:32:52.149417 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m22:32:52.149786 [info ] [Thread-1  ]: 24 of 32 START sql table model dim.dim_pricebook ............................... [RUN]
[0m22:32:52.150305 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m22:32:52.150527 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m22:32:52.152847 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.155040 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 22:32:52.150670 => 22:32:52.154811
[0m22:32:52.155351 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m22:32:52.159667 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.160383 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.160611 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m22:32:52.160793 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.168218 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.168614 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.168987 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

select
    pricebook_entry_id,
    pricebook2id as pricebook_id,
    product2id as product_id,
    unitprice as unit_price,
    isactive as is_active,
    usestandardprice as use_standard_price,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__pricebook_entry"
where isdeleted = false  -- Exclude deleted entries
order by pricebook_entry_id
    );
  
  
[0m22:32:52.170474 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.172494 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.172718 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook" rename to "dim_pricebook__dbt_backup"
[0m22:32:52.173116 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.175356 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.175611 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m22:32:52.175974 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.177036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m22:32:52.177250 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.177434 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m22:32:52.178187 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.179850 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.180059 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m22:32:52.180493 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.181398 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 22:32:52.155523 => 22:32:52.181304
[0m22:32:52.181608 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m22:32:52.197488 [info ] [Thread-1  ]: 24 of 32 OK created sql table model dim.dim_pricebook .......................... [[32mOK[0m in 0.05s]
[0m22:32:52.197957 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m22:32:52.198212 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m22:32:52.198521 [info ] [Thread-1  ]: 25 of 32 START sql table model dim.dim_product ................................. [RUN]
[0m22:32:52.198940 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m22:32:52.199161 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m22:32:52.201420 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m22:32:52.201910 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 22:32:52.199313 => 22:32:52.201808
[0m22:32:52.202112 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m22:32:52.204822 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m22:32:52.205517 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.205751 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m22:32:52.206002 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.212455 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.212746 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.212966 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    productcode as product_code,
    description as product_description,
    isactive as is_active,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    family as product_family,
    externaldatasourceid as external_datasource_id,
    externalid as external_id,
    displayurl as display_url,
    quantityunitofmeasure as quantity_unit_of_measure,
    stockkeepingunit as stock_keeping_unit,
    type as product_type,
    productclass as product_class,
    sourceproductid as source_product_id,
    sellerid as seller_id
from "dbt"."staging"."stg_salesforce__product_2"
where isdeleted = false
-- isactive =1 ?
order by product_name
    );
  
  
[0m22:32:52.215218 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.218422 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.218678 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product" rename to "dim_product__dbt_backup"
[0m22:32:52.219102 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.220995 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.221216 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m22:32:52.221551 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.222656 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m22:32:52.222892 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.223087 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m22:32:52.223979 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.225756 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.225976 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m22:32:52.226424 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.227198 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 22:32:52.202249 => 22:32:52.227099
[0m22:32:52.227410 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m22:32:52.245175 [info ] [Thread-1  ]: 25 of 32 OK created sql table model dim.dim_product ............................ [[32mOK[0m in 0.05s]
[0m22:32:52.245625 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m22:32:52.245889 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m22:32:52.246162 [info ] [Thread-1  ]: 26 of 32 START sql table model dim.dim_record_type ............................. [RUN]
[0m22:32:52.246529 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m22:32:52.246724 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m22:32:52.249072 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.249877 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 22:32:52.246860 => 22:32:52.249776
[0m22:32:52.250080 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m22:32:52.253255 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.253887 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.254145 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m22:32:52.254335 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.261162 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.261449 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.261805 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

select
    record_type_id,
    name as record_type_name,
    modulenamespace as module_namespace,
    description as record_type_description,
    businessprocessid as business_process_id,
    sobjecttype as sobject_type,
    isactive as is_active,
    createdbyid as created_by_id,
    createddate as created_date,
    lastmodifiedbyid as last_modified_by_id,
    lastmodifieddate as last_modified_date,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__record_type"
where isdeleted = false
order by record_type_name
    );
  
  
[0m22:32:52.263745 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.265792 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.266013 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m22:32:52.266346 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.268835 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.269037 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m22:32:52.269403 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.270641 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m22:32:52.270857 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.271030 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m22:32:52.271727 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.273663 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.273880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m22:32:52.274388 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.275166 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 22:32:52.250231 => 22:32:52.275065
[0m22:32:52.275375 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m22:32:52.292026 [info ] [Thread-1  ]: 26 of 32 OK created sql table model dim.dim_record_type ........................ [[32mOK[0m in 0.05s]
[0m22:32:52.292466 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m22:32:52.292708 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m22:32:52.293016 [info ] [Thread-1  ]: 27 of 32 START sql table model dim.dim_solution ................................ [RUN]
[0m22:32:52.293428 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m22:32:52.293649 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m22:32:52.295943 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.296402 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 22:32:52.293799 => 22:32:52.296301
[0m22:32:52.296603 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m22:32:52.299739 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.300214 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.300612 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m22:32:52.300857 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.308641 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.308941 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.309152 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solutionnumber as solution_number,
    solutionname as solution_name,
    ispublished as is_published,
    ispublishedinpublickb as is_published_in_public_kb,
    status as solution_status,
    isreviewed as is_reviewed,
    solutionnote as solution_note,
    caseid as case_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    timesused as times_used,
    ishtml as is_html
from "dbt"."staging"."stg_salesforce__solution"
where isdeleted = false
order by solution_name
    );
  
  
[0m22:32:52.310883 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.313038 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.313273 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m22:32:52.313628 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.316004 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.316214 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m22:32:52.316522 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.317444 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m22:32:52.317628 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.317803 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m22:32:52.318621 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.320161 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.320361 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m22:32:52.320833 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.321821 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 22:32:52.296734 => 22:32:52.321718
[0m22:32:52.322060 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m22:32:52.340291 [info ] [Thread-1  ]: 27 of 32 OK created sql table model dim.dim_solution ........................... [[32mOK[0m in 0.05s]
[0m22:32:52.340745 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m22:32:52.340993 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m22:32:52.341290 [info ] [Thread-1  ]: 28 of 32 START sql table model dim.dim_user .................................... [RUN]
[0m22:32:52.341699 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m22:32:52.341917 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m22:32:52.344315 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m22:32:52.344983 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 22:32:52.342061 => 22:32:52.344884
[0m22:32:52.345184 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m22:32:52.348019 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m22:32:52.348569 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.348890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m22:32:52.349142 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.356116 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.356411 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.356640 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m22:32:52.360509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.362973 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.363262 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user" rename to "dim_user__dbt_backup"
[0m22:32:52.363718 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.365757 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.366002 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m22:32:52.366391 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.367406 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m22:32:52.367595 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.367765 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m22:32:52.368655 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.371460 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.371674 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m22:32:52.372182 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.373108 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 22:32:52.345324 => 22:32:52.373003
[0m22:32:52.373333 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m22:32:52.392690 [info ] [Thread-1  ]: 28 of 32 OK created sql table model dim.dim_user ............................... [[32mOK[0m in 0.05s]
[0m22:32:52.393102 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m22:32:52.393644 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:32:52.393925 [info ] [Thread-1  ]: 29 of 32 START sql incremental model fact.fact_case ............................ [RUN]
[0m22:32:52.394288 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_case)
[0m22:32:52.394490 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:32:52.397055 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:32:52.397648 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:32:52.394622 => 22:32:52.397553
[0m22:32:52.397843 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:32:52.417896 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.418229 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

    
  
    
    

    create temporary table
      "fact_case__dbt_tmp20240821223252414129"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.createddate,
    c.lastmodifieddate,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.user_name as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.accountid = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contactid = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.ownerid = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.createddate = d.date_day
where c.isdeleted = false
    );
  
  
  
[0m22:32:52.419324 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.432480 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.437067 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.437339 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:32:52.437678 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.437877 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.438136 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_case__dbt_tmp20240821223252414129'
      
      
      order by ordinal_position

  
[0m22:32:52.467586 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.471793 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.472071 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_case'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:32:52.487329 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.493675 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.493976 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_case'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:32:52.508987 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.518617 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:32:52.519211 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.519476 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

        
            delete from "dbt"."fact"."fact_case"
            where (
                case_id) in (
                select (case_id)
                from "fact_case__dbt_tmp20240821223252414129"
            );

        
    

    insert into "dbt"."fact"."fact_case" ("case_id", "status", "priority", "origin", "createddate", "lastmodifieddate", "account_name", "contact_first_name", "contact_last_name", "owner_username", "created_date_key")
    (
        select "case_id", "status", "priority", "origin", "createddate", "lastmodifieddate", "account_name", "contact_first_name", "contact_last_name", "owner_username", "created_date_key"
        from "fact_case__dbt_tmp20240821223252414129"
    )
  
[0m22:32:52.522104 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.523050 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: COMMIT
[0m22:32:52.523258 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.523451 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: COMMIT
[0m22:32:52.524260 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.524689 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:32:52.397980 => 22:32:52.524600
[0m22:32:52.524880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:32:52.542951 [info ] [Thread-1  ]: 29 of 32 OK created sql incremental model fact.fact_case ....................... [[32mOK[0m in 0.15s]
[0m22:32:52.543416 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:32:52.543662 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:32:52.544026 [info ] [Thread-1  ]: 30 of 32 START sql incremental model fact.fact_opportunity ..................... [RUN]
[0m22:32:52.544441 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.fact_opportunity)
[0m22:32:52.544653 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:32:52.549347 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.550230 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:32:52.544788 => 22:32:52.550061
[0m22:32:52.550517 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:32:52.553695 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.553977 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821223252552570"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:32:52.554195 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.563672 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.565949 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.566190 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:32:52.566482 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.566711 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.566929 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240821223252552570'
      
      
      order by ordinal_position

  
[0m22:32:52.596010 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.598372 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.598625 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:32:52.612551 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.615453 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.615686 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:32:52.630142 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.632111 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.632752 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.633070 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240821223252552570"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240821223252552570"
    )
  
[0m22:32:52.634496 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.635602 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:32:52.635828 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.636019 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:32:52.636342 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.636795 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:32:52.550735 => 22:32:52.636701
[0m22:32:52.636992 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:32:52.639770 [info ] [Thread-1  ]: 30 of 32 OK created sql incremental model fact.fact_opportunity ................ [[32mOK[0m in 0.10s]
[0m22:32:52.640175 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:32:52.640420 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m22:32:52.640770 [info ] [Thread-1  ]: 31 of 32 START sql incremental model fact.fact_case_history .................... [RUN]
[0m22:32:52.641196 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_case_history)
[0m22:32:52.641416 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m22:32:52.644176 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m22:32:52.644743 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 22:32:52.641560 => 22:32:52.644625
[0m22:32:52.644966 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m22:32:52.647797 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case_history"
[0m22:32:52.648343 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:32:52.648547 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: BEGIN
[0m22:32:52.648730 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.655633 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.655927 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:32:52.656155 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case_history"
  
    as (
      

select
    case_history_id,
    h.status,
    h.priority,
    h.created_date,
    h.last_modified_date,
    c.account_id,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__case_history_2" h
left join "dbt"."fact"."fact_case" c
    on h.case_history_id = c.case_id
left join "dbt"."dim"."dim_account" a
    on c.account_id = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.owner_id = u.user_id
    );
  
  
  
[0m22:32:52.656901 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 22:32:52.645110 => 22:32:52.656800
[0m22:32:52.657106 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: ROLLBACK
[0m22:32:52.657640 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case_history'
[0m22:32:52.657823 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: Close
[0m22:32:52.660537 [debug] [Thread-1  ]: Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Binder Error: Table "c" does not have a column named "account_id"
  LINE 28:     on c.account_id = a.account_id
                  ^
[0m22:32:52.661067 [error] [Thread-1  ]: 31 of 32 ERROR creating sql incremental model fact.fact_case_history ........... [[31mERROR[0m in 0.02s]
[0m22:32:52.661422 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m22:32:52.661668 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:32:52.661907 [info ] [Thread-1  ]: 32 of 32 START sql incremental model fact.fact_opportunity_history ............. [RUN]
[0m22:32:52.662316 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:32:52.662611 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:32:52.665172 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:32:52.665737 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:32:52.662778 => 22:32:52.665625
[0m22:32:52.665940 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:32:52.669038 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:32:52.669612 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:32:52.669812 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:32:52.669992 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.676894 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.677262 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:32:52.677522 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:32:52.678485 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:32:52.666074 => 22:32:52.678369
[0m22:32:52.678713 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:32:52.679251 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:32:52.679454 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:32:52.681753 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:32:52.682210 [error] [Thread-1  ]: 32 of 32 ERROR creating sql incremental model fact.fact_opportunity_history .... [[31mERROR[0m in 0.02s]
[0m22:32:52.682554 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:32:52.683403 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:52.683621 [debug] [MainThread]: On master: BEGIN
[0m22:32:52.683791 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:32:52.690697 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:52.690962 [debug] [MainThread]: On master: COMMIT
[0m22:32:52.691121 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:52.691269 [debug] [MainThread]: On master: COMMIT
[0m22:32:52.691467 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:52.691625 [debug] [MainThread]: On master: Close
[0m22:32:52.693617 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:32:52.694000 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:32:52.694353 [info ] [MainThread]: 
[0m22:32:52.694618 [info ] [MainThread]: Finished running 14 table models, 14 view models, 4 incremental models in 0 hours 0 minutes and 2.06 seconds (2.06s).
[0m22:32:52.697067 [debug] [MainThread]: Command end result
[0m22:32:52.706573 [info ] [MainThread]: 
[0m22:32:52.706876 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m22:32:52.707048 [info ] [MainThread]: 
[0m22:32:52.707237 [error] [MainThread]:   Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m22:32:52.707424 [info ] [MainThread]: 
[0m22:32:52.707585 [error] [MainThread]:   Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Binder Error: Table "c" does not have a column named "account_id"
  LINE 28:     on c.account_id = a.account_id
                  ^
[0m22:32:52.707749 [info ] [MainThread]: 
[0m22:32:52.707904 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:32:52.708084 [info ] [MainThread]: 
[0m22:32:52.708369 [info ] [MainThread]: Done. PASS=29 WARN=0 ERROR=3 SKIP=0 TOTAL=32
[0m22:32:52.708773 [debug] [MainThread]: Command `dbt run` failed at 22:32:52.708717 after 2.24 seconds
[0m22:32:52.708976 [debug] [MainThread]: Flushing usage events


============================== 22:33:31.551890 | e997c118-97c2-4988-9ac5-3c03fd28b728 ==============================
[0m22:33:31.551890 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:33:31.554941 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt test', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:33:31.555209 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:33:31.637117 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:33:31.657109 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:33:31.704926 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:33:31.705235 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:33:31.706212 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:33:31.720748 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:33:31.722052 [info ] [MainThread]: 
[0m22:33:31.722248 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:33:31.722514 [debug] [MainThread]: Command end result
[0m22:33:31.729777 [debug] [MainThread]: Command `dbt test` succeeded at 22:33:31.729663 after 0.20 seconds
[0m22:33:31.730057 [debug] [MainThread]: Flushing usage events


============================== 22:33:41.117768 | 5ecd156b-4f54-4fc0-bf99-140341fc8039 ==============================
[0m22:33:41.117768 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:33:41.120624 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'False'}
[0m22:33:41.120898 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:33:41.198140 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:33:41.216177 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:33:41.262236 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:33:41.262552 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:33:41.263534 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:33:41.278657 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:33:41.279942 [info ] [MainThread]: 
[0m22:33:41.280140 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:33:41.280404 [debug] [MainThread]: Command end result
[0m22:33:41.287188 [debug] [MainThread]: Command `dbt test` succeeded at 22:33:41.287117 after 0.20 seconds
[0m22:33:41.287422 [debug] [MainThread]: Flushing usage events


============================== 22:35:01.848626 | 0a7ad9a5-3c07-49a3-bf3f-fa713255c0f5 ==============================
[0m22:35:01.848626 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:35:01.851903 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt test', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:35:01.852168 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:35:01.936300 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:35:01.956998 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:35:02.008585 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:35:02.008920 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:35:02.009911 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:35:02.024200 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:35:02.025532 [info ] [MainThread]: 
[0m22:35:02.025753 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:35:02.026030 [debug] [MainThread]: Command end result
[0m22:35:02.032686 [debug] [MainThread]: Command `dbt test` succeeded at 22:35:02.032598 after 0.20 seconds
[0m22:35:02.032949 [debug] [MainThread]: Flushing usage events


============================== 22:35:09.294044 | 90d3cc0c-6659-4911-a59f-a1f49d499be2 ==============================
[0m22:35:09.294044 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:35:09.296533 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:35:09.296800 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:35:09.370852 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:35:09.388244 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:35:09.424662 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:35:09.424968 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:35:09.425954 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:35:09.441395 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:35:09.443202 [info ] [MainThread]: 
[0m22:35:09.443650 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:35:09.445034 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt_main'
[0m22:35:09.451816 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:35:09.452105 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:35:09.452274 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:35:09.468078 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.468317 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:35:09.468490 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:35:09.484539 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.488352 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:35:09.489099 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:35:09.489386 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:35:09.491829 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:35:09.494298 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:35:09.494493 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:35:09.494646 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:09.502358 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.502641 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:35:09.502823 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:35:09.519247 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.523377 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:35:09.523711 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:35:09.523889 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:35:09.526746 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:35:09.529567 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:35:09.529764 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:35:09.529912 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:09.537206 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.537494 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:35:09.537682 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:35:09.557360 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.558535 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:35:09.558880 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:35:09.559069 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:35:09.561922 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:35:09.564533 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:35:09.564725 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:35:09.564882 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:09.571748 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.572005 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:35:09.572176 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:35:09.587710 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.591149 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:35:09.591390 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:35:09.591547 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:35:09.594306 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:35:09.594564 [info ] [MainThread]: 
[0m22:35:09.597430 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:35:09.597793 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_date)
[0m22:35:09.597990 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:35:09.625183 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:35:09.625550 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:35:09.625736 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:35:09.632644 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:35:09.632938 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:35:09.633136 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:35:09.633509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:35:09.674401 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:35:09.675526 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:35:09.598132 => 22:35:09.675339
[0m22:35:09.675807 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:35:09.676069 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:35:09.675955 => 22:35:09.675976
[0m22:35:09.676276 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: ROLLBACK
[0m22:35:09.676518 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_date'
[0m22:35:09.676705 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:35:09.678618 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:35:09.678893 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m22:35:09.679360 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.fact_product_sales)
[0m22:35:09.679622 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m22:35:09.682097 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m22:35:09.683050 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 22:35:09.679768 => 22:35:09.682930
[0m22:35:09.683268 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m22:35:09.683501 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 22:35:09.683408 => 22:35:09.683415
[0m22:35:09.684048 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m22:35:09.684337 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:09.684774 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m22:35:09.685000 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:09.687733 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:35:09.688570 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 22:35:09.685146 => 22:35:09.688407
[0m22:35:09.688840 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:09.689094 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 22:35:09.688996 => 22:35:09.689003
[0m22:35:09.689615 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:09.689862 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:09.690421 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m22:35:09.690829 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:09.693207 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:35:09.694277 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 22:35:09.691036 => 22:35:09.694143
[0m22:35:09.694524 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:09.694776 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 22:35:09.694675 => 22:35:09.694683
[0m22:35:09.695426 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:09.695713 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:09.696121 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m22:35:09.696345 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:09.699070 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:35:09.699676 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 22:35:09.696764 => 22:35:09.699556
[0m22:35:09.699903 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:09.700273 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 22:35:09.700148 => 22:35:09.700157
[0m22:35:09.700839 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:09.701109 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:09.701499 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m22:35:09.701715 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:09.703881 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:35:09.705255 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 22:35:09.701854 => 22:35:09.705078
[0m22:35:09.705558 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:09.705817 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 22:35:09.705713 => 22:35:09.705722
[0m22:35:09.706361 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:09.706603 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:09.706951 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m22:35:09.707367 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:09.709615 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:35:09.710150 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 22:35:09.707559 => 22:35:09.710040
[0m22:35:09.710363 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:09.710593 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 22:35:09.710503 => 22:35:09.710509
[0m22:35:09.711064 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:09.711295 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:09.711632 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m22:35:09.711852 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:09.714016 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:35:09.714531 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 22:35:09.712168 => 22:35:09.714423
[0m22:35:09.714749 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:09.714981 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 22:35:09.714892 => 22:35:09.714897
[0m22:35:09.715469 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:09.715699 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:09.716034 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m22:35:09.716243 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:09.744983 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:35:09.745696 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 22:35:09.716452 => 22:35:09.745585
[0m22:35:09.745908 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:09.746135 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 22:35:09.746045 => 22:35:09.746051
[0m22:35:09.746600 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:09.746823 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:09.747274 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m22:35:09.747518 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:09.749297 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:35:09.749748 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 22:35:09.747663 => 22:35:09.749637
[0m22:35:09.749954 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:09.750181 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 22:35:09.750090 => 22:35:09.750094
[0m22:35:09.750664 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:09.750881 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:09.751217 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m22:35:09.751407 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:09.753072 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:35:09.753461 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 22:35:09.751532 => 22:35:09.753364
[0m22:35:09.753653 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:09.753867 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 22:35:09.753783 => 22:35:09.753787
[0m22:35:09.754280 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:09.754489 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:09.754847 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m22:35:09.755036 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:09.756699 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:35:09.757056 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 22:35:09.755164 => 22:35:09.756963
[0m22:35:09.757246 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:09.757456 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 22:35:09.757375 => 22:35:09.757379
[0m22:35:09.757863 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:09.758085 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:09.758430 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m22:35:09.758660 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:09.760330 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:35:09.760700 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 22:35:09.758793 => 22:35:09.760607
[0m22:35:09.760892 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:09.761109 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 22:35:09.761025 => 22:35:09.761029
[0m22:35:09.761522 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:09.761727 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:09.762080 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m22:35:09.762282 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:09.763973 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:35:09.764516 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 22:35:09.762412 => 22:35:09.764420
[0m22:35:09.764704 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:09.764925 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 22:35:09.764840 => 22:35:09.764844
[0m22:35:09.765338 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:09.765543 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:09.765888 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m22:35:09.766081 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:09.768622 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:35:09.768997 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 22:35:09.766208 => 22:35:09.768899
[0m22:35:09.769189 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:09.769401 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 22:35:09.769320 => 22:35:09.769324
[0m22:35:09.769852 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:09.770076 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:09.770405 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m22:35:09.770592 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:09.772277 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:35:09.772655 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 22:35:09.770718 => 22:35:09.772557
[0m22:35:09.772856 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:09.773069 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 22:35:09.772987 => 22:35:09.772991
[0m22:35:09.773482 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:09.773698 [debug] [Thread-1  ]: Began running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:09.774059 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now seed.elastic_dbt_interview.dbt_project_evaluator_exceptions)
[0m22:35:09.774265 [debug] [Thread-1  ]: Began compiling node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:09.775487 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (compile): 22:35:09.774400 => 22:35:09.775392
[0m22:35:09.775677 [debug] [Thread-1  ]: Began executing node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:09.775891 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (execute): 22:35:09.775808 => 22:35:09.775812
[0m22:35:09.776304 [debug] [Thread-1  ]: Finished running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:09.776508 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m22:35:09.776827 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly seed.elastic_dbt_interview.dbt_project_evaluator_exceptions, now model.elastic_dbt_interview.dim_account)
[0m22:35:09.777011 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m22:35:09.778947 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m22:35:09.779613 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 22:35:09.777136 => 22:35:09.779517
[0m22:35:09.779806 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m22:35:09.780025 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 22:35:09.779940 => 22:35:09.779944
[0m22:35:09.780442 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m22:35:09.780663 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m22:35:09.781021 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m22:35:09.781222 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m22:35:09.783242 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m22:35:09.783652 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 22:35:09.781369 => 22:35:09.783560
[0m22:35:09.783840 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m22:35:09.784053 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 22:35:09.783972 => 22:35:09.783977
[0m22:35:09.784473 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m22:35:09.784680 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m22:35:09.785001 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m22:35:09.785184 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m22:35:09.787004 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m22:35:09.787644 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 22:35:09.785308 => 22:35:09.787548
[0m22:35:09.787849 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m22:35:09.788068 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 22:35:09.787984 => 22:35:09.787988
[0m22:35:09.788481 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m22:35:09.788684 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m22:35:09.788998 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m22:35:09.789181 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m22:35:09.791791 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m22:35:09.792175 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 22:35:09.789307 => 22:35:09.792083
[0m22:35:09.792368 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m22:35:09.792581 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 22:35:09.792499 => 22:35:09.792503
[0m22:35:09.792992 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m22:35:09.793196 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m22:35:09.793520 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m22:35:09.793707 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m22:35:09.795628 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m22:35:09.795987 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 22:35:09.793833 => 22:35:09.795897
[0m22:35:09.796190 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m22:35:09.796417 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 22:35:09.796330 => 22:35:09.796334
[0m22:35:09.796842 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m22:35:09.797046 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m22:35:09.797363 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m22:35:09.797545 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m22:35:09.799426 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:35:09.799803 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 22:35:09.797672 => 22:35:09.799705
[0m22:35:09.799990 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m22:35:09.800202 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 22:35:09.800120 => 22:35:09.800124
[0m22:35:09.800609 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m22:35:09.800814 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:09.801132 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m22:35:09.801316 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:09.803094 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:35:09.803478 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 22:35:09.801440 => 22:35:09.803387
[0m22:35:09.803668 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:09.803896 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 22:35:09.803814 => 22:35:09.803818
[0m22:35:09.804303 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:09.804506 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m22:35:09.804819 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m22:35:09.805001 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m22:35:09.806782 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m22:35:09.807133 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 22:35:09.805124 => 22:35:09.807043
[0m22:35:09.807320 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m22:35:09.807536 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 22:35:09.807453 => 22:35:09.807457
[0m22:35:09.807943 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m22:35:09.808143 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m22:35:09.808462 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m22:35:09.808654 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m22:35:09.810500 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m22:35:09.810865 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 22:35:09.808795 => 22:35:09.810777
[0m22:35:09.811051 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m22:35:09.811261 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 22:35:09.811180 => 22:35:09.811184
[0m22:35:09.811672 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m22:35:09.811878 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m22:35:09.812192 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m22:35:09.812375 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m22:35:09.814850 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m22:35:09.815234 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 22:35:09.812497 => 22:35:09.815134
[0m22:35:09.815425 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m22:35:09.815640 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 22:35:09.815557 => 22:35:09.815561
[0m22:35:09.816057 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m22:35:09.816261 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m22:35:09.816576 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m22:35:09.816763 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m22:35:09.818603 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m22:35:09.818971 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 22:35:09.816889 => 22:35:09.818882
[0m22:35:09.819160 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m22:35:09.819373 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 22:35:09.819292 => 22:35:09.819295
[0m22:35:09.819810 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m22:35:09.820033 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m22:35:09.820356 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m22:35:09.820537 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m22:35:09.822425 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m22:35:09.822787 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 22:35:09.820660 => 22:35:09.822696
[0m22:35:09.822975 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m22:35:09.823184 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 22:35:09.823104 => 22:35:09.823108
[0m22:35:09.823588 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m22:35:09.823961 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:35:09.824363 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_case)
[0m22:35:09.824566 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:35:09.826802 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:35:09.827182 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:35:09.824696 => 22:35:09.827090
[0m22:35:09.827375 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:35:09.827592 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:35:09.827507 => 22:35:09.827511
[0m22:35:09.828004 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:35:09.828209 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:35:09.828557 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.fact_opportunity)
[0m22:35:09.828740 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:35:09.833974 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:35:09.834424 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:35:09.828864 => 22:35:09.834323
[0m22:35:09.834619 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:35:09.834835 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:35:09.834751 => 22:35:09.834756
[0m22:35:09.835254 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:35:09.835466 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m22:35:09.835818 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_case_history)
[0m22:35:09.836006 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m22:35:09.838901 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m22:35:09.839305 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 22:35:09.836133 => 22:35:09.839212
[0m22:35:09.839503 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m22:35:09.839718 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 22:35:09.839636 => 22:35:09.839640
[0m22:35:09.840133 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m22:35:09.840340 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:09.840656 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:35:09.840840 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:09.842963 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:35:09.843332 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:35:09.840966 => 22:35:09.843240
[0m22:35:09.843522 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:09.843738 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:35:09.843654 => 22:35:09.843658
[0m22:35:09.844148 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:09.844678 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:35:09.844865 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:35:09.847010 [debug] [MainThread]: Command end result
[0m22:35:09.854837 [debug] [MainThread]: Command `dbt compile` succeeded at 22:35:09.854761 after 0.58 seconds
[0m22:35:09.855063 [debug] [MainThread]: Flushing usage events


============================== 22:35:12.526581 | a6c36d18-17a7-4ff2-aedc-89847ac81036 ==============================
[0m22:35:12.526581 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:35:12.528876 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt compile', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:35:12.529120 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:35:12.603443 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:35:12.621280 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:35:12.658970 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:35:12.659293 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:35:12.660287 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:35:12.674009 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:35:12.675751 [info ] [MainThread]: 
[0m22:35:12.676186 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:35:12.677852 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt_fact'
[0m22:35:12.684460 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:35:12.684735 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:35:12.684904 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:35:12.693167 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.693499 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:35:12.693699 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:35:12.710137 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.714098 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:35:12.714660 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:35:12.714840 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:35:12.717409 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:35:12.719294 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:35:12.719505 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:35:12.719665 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:12.727006 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.727282 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:35:12.727464 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:35:12.743041 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.746509 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:35:12.746750 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:35:12.747007 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:35:12.757577 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:35:12.760413 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:35:12.760741 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:35:12.760973 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:12.774534 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.774846 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:35:12.775047 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:35:12.791547 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.795689 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:35:12.796033 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:35:12.796210 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:35:12.798807 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:35:12.801212 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:35:12.801412 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:35:12.801564 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:12.808703 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.808970 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:35:12.809148 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:35:12.827725 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.828824 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:35:12.829057 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:35:12.829216 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:35:12.832715 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:35:12.832932 [info ] [MainThread]: 
[0m22:35:12.834205 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:35:12.834562 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_date)
[0m22:35:12.834755 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:35:12.861618 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:35:12.861975 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:35:12.862173 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:35:12.869056 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:35:12.869338 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:35:12.869536 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:35:12.869908 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:35:12.912082 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:35:12.912778 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:35:12.834889 => 22:35:12.912653
[0m22:35:12.912996 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:35:12.913244 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:35:12.913137 => 22:35:12.913159
[0m22:35:12.913449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: ROLLBACK
[0m22:35:12.913681 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_date'
[0m22:35:12.913858 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:35:12.915867 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:35:12.916167 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m22:35:12.916517 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.fact_product_sales)
[0m22:35:12.916733 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m22:35:12.919051 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m22:35:12.919573 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 22:35:12.916870 => 22:35:12.919449
[0m22:35:12.919805 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m22:35:12.920023 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 22:35:12.919942 => 22:35:12.919946
[0m22:35:12.920513 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m22:35:12.920739 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:12.921054 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m22:35:12.921250 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:12.923788 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:35:12.924276 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 22:35:12.921387 => 22:35:12.924174
[0m22:35:12.924478 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:12.924700 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 22:35:12.924610 => 22:35:12.924618
[0m22:35:12.925135 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:12.925367 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:12.925758 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m22:35:12.925950 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:12.927746 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:35:12.928138 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 22:35:12.926076 => 22:35:12.928045
[0m22:35:12.928335 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:12.928546 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 22:35:12.928464 => 22:35:12.928468
[0m22:35:12.928962 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:12.929170 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:12.929526 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m22:35:12.929717 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:12.931608 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:35:12.932068 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 22:35:12.929873 => 22:35:12.931955
[0m22:35:12.932280 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:12.932499 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 22:35:12.932415 => 22:35:12.932419
[0m22:35:12.932932 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:12.933146 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:12.933504 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m22:35:12.933697 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:12.935399 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:35:12.935782 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 22:35:12.933826 => 22:35:12.935681
[0m22:35:12.935978 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:12.936223 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 22:35:12.936134 => 22:35:12.936138
[0m22:35:12.936679 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:12.936901 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:12.937289 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m22:35:12.937522 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:12.939334 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:35:12.939738 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 22:35:12.937660 => 22:35:12.939634
[0m22:35:12.939935 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:12.940149 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 22:35:12.940066 => 22:35:12.940070
[0m22:35:12.940576 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:12.940782 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:12.941139 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m22:35:12.941331 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:12.943099 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:35:12.943486 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 22:35:12.941455 => 22:35:12.943390
[0m22:35:12.943678 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:12.943893 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 22:35:12.943807 => 22:35:12.943810
[0m22:35:12.944305 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:12.944512 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:12.944856 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m22:35:12.945044 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:12.971274 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:35:12.971892 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 22:35:12.945172 => 22:35:12.971787
[0m22:35:12.972115 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:12.972351 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 22:35:12.972262 => 22:35:12.972267
[0m22:35:12.972822 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:12.973048 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:12.973367 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m22:35:12.973644 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:12.975388 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:35:12.975822 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 22:35:12.973786 => 22:35:12.975727
[0m22:35:12.976015 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:12.976234 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 22:35:12.976146 => 22:35:12.976150
[0m22:35:12.976708 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:12.976923 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:12.977248 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m22:35:12.977434 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:12.979111 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:35:12.979474 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 22:35:12.977559 => 22:35:12.979377
[0m22:35:12.979658 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:12.979877 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 22:35:12.979791 => 22:35:12.979795
[0m22:35:12.980315 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:12.980531 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:12.980892 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m22:35:12.981103 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:12.982808 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:35:12.983205 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 22:35:12.981255 => 22:35:12.983111
[0m22:35:12.983400 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:12.983621 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 22:35:12.983537 => 22:35:12.983541
[0m22:35:12.984045 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:12.984275 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:12.984655 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m22:35:12.984851 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:12.986539 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:35:12.986927 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 22:35:12.984987 => 22:35:12.986824
[0m22:35:12.987117 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:12.987332 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 22:35:12.987248 => 22:35:12.987252
[0m22:35:12.987744 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:12.987950 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:12.988300 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m22:35:12.988488 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:12.990148 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:35:12.990522 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 22:35:12.988615 => 22:35:12.990425
[0m22:35:12.990708 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:12.990926 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 22:35:12.990843 => 22:35:12.990846
[0m22:35:12.991339 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:12.991539 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:12.991880 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m22:35:12.992066 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:12.994544 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:35:12.994920 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 22:35:12.992189 => 22:35:12.994830
[0m22:35:12.995125 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:12.995353 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 22:35:12.995267 => 22:35:12.995271
[0m22:35:12.995783 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:12.995991 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:12.996316 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m22:35:12.996499 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:12.998153 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:35:12.998530 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 22:35:12.996624 => 22:35:12.998430
[0m22:35:12.998719 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:12.998930 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 22:35:12.998847 => 22:35:12.998851
[0m22:35:12.999351 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:12.999571 [debug] [Thread-1  ]: Began running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:12.999915 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now seed.elastic_dbt_interview.dbt_project_evaluator_exceptions)
[0m22:35:13.000099 [debug] [Thread-1  ]: Began compiling node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:13.001317 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (compile): 22:35:13.000227 => 22:35:13.001224
[0m22:35:13.001505 [debug] [Thread-1  ]: Began executing node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:13.001713 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (execute): 22:35:13.001630 => 22:35:13.001633
[0m22:35:13.002119 [debug] [Thread-1  ]: Finished running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:13.002318 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m22:35:13.002638 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly seed.elastic_dbt_interview.dbt_project_evaluator_exceptions, now model.elastic_dbt_interview.dim_account)
[0m22:35:13.002815 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m22:35:13.004748 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m22:35:13.005145 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 22:35:13.002939 => 22:35:13.005048
[0m22:35:13.005332 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m22:35:13.005545 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 22:35:13.005464 => 22:35:13.005468
[0m22:35:13.005958 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m22:35:13.006178 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m22:35:13.006500 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m22:35:13.006710 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m22:35:13.008639 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m22:35:13.009028 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 22:35:13.006841 => 22:35:13.008930
[0m22:35:13.009214 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m22:35:13.009424 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 22:35:13.009344 => 22:35:13.009348
[0m22:35:13.009846 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m22:35:13.010050 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m22:35:13.010368 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m22:35:13.010555 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m22:35:13.012351 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m22:35:13.012712 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 22:35:13.010680 => 22:35:13.012612
[0m22:35:13.012905 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m22:35:13.013115 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 22:35:13.013034 => 22:35:13.013037
[0m22:35:13.013521 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m22:35:13.013720 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m22:35:13.014052 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m22:35:13.014243 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m22:35:13.016912 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m22:35:13.017360 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 22:35:13.014368 => 22:35:13.017268
[0m22:35:13.017554 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m22:35:13.017798 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 22:35:13.017719 => 22:35:13.017723
[0m22:35:13.018220 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m22:35:13.018429 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m22:35:13.018758 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m22:35:13.018939 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m22:35:13.020887 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m22:35:13.021256 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 22:35:13.019063 => 22:35:13.021167
[0m22:35:13.021462 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m22:35:13.021684 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 22:35:13.021599 => 22:35:13.021603
[0m22:35:13.022122 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m22:35:13.022328 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m22:35:13.022647 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m22:35:13.022831 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m22:35:13.024716 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:35:13.025102 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 22:35:13.022954 => 22:35:13.025007
[0m22:35:13.025309 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m22:35:13.025522 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 22:35:13.025441 => 22:35:13.025445
[0m22:35:13.025936 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m22:35:13.026142 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:13.026460 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m22:35:13.026647 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:13.028429 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:35:13.028813 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 22:35:13.026773 => 22:35:13.028723
[0m22:35:13.029004 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:13.029218 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 22:35:13.029135 => 22:35:13.029138
[0m22:35:13.029627 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:13.029827 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m22:35:13.030140 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m22:35:13.030322 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m22:35:13.032120 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m22:35:13.032466 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 22:35:13.030445 => 22:35:13.032378
[0m22:35:13.032655 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m22:35:13.032869 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 22:35:13.032789 => 22:35:13.032792
[0m22:35:13.033276 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m22:35:13.033479 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m22:35:13.033799 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m22:35:13.033999 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m22:35:13.035823 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m22:35:13.036189 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 22:35:13.034130 => 22:35:13.036096
[0m22:35:13.036373 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m22:35:13.036583 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 22:35:13.036500 => 22:35:13.036504
[0m22:35:13.036991 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m22:35:13.037196 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m22:35:13.037521 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m22:35:13.037708 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m22:35:13.040189 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m22:35:13.040569 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 22:35:13.037833 => 22:35:13.040468
[0m22:35:13.040759 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m22:35:13.040979 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 22:35:13.040894 => 22:35:13.040898
[0m22:35:13.041390 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m22:35:13.041592 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m22:35:13.041910 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m22:35:13.042094 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m22:35:13.043911 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m22:35:13.044247 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 22:35:13.042219 => 22:35:13.044160
[0m22:35:13.044434 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m22:35:13.044645 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 22:35:13.044565 => 22:35:13.044569
[0m22:35:13.045059 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m22:35:13.045283 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m22:35:13.045607 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m22:35:13.045789 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m22:35:13.047670 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m22:35:13.048040 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 22:35:13.045914 => 22:35:13.047953
[0m22:35:13.048225 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m22:35:13.048434 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 22:35:13.048356 => 22:35:13.048360
[0m22:35:13.048841 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m22:35:13.049179 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:35:13.049527 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_case)
[0m22:35:13.049732 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:35:13.051974 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:35:13.052348 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:35:13.049861 => 22:35:13.052257
[0m22:35:13.052539 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:35:13.052753 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:35:13.052671 => 22:35:13.052675
[0m22:35:13.053162 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:35:13.053367 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:35:13.053712 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.fact_opportunity)
[0m22:35:13.053898 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:35:13.059077 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:35:13.059505 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:35:13.054024 => 22:35:13.059400
[0m22:35:13.059701 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:35:13.059918 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:35:13.059833 => 22:35:13.059837
[0m22:35:13.060334 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:35:13.060542 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m22:35:13.060888 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_case_history)
[0m22:35:13.061072 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m22:35:13.063915 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m22:35:13.064273 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 22:35:13.061196 => 22:35:13.064180
[0m22:35:13.064466 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m22:35:13.064677 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 22:35:13.064597 => 22:35:13.064600
[0m22:35:13.065089 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m22:35:13.065296 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:13.065630 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:35:13.065817 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:13.067920 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:35:13.068287 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:35:13.065940 => 22:35:13.068194
[0m22:35:13.068483 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:13.068696 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:35:13.068613 => 22:35:13.068617
[0m22:35:13.069109 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:13.069604 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:35:13.069782 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:35:13.071889 [debug] [MainThread]: Command end result
[0m22:35:13.080020 [debug] [MainThread]: Command `dbt compile` succeeded at 22:35:13.079945 after 0.57 seconds
[0m22:35:13.080233 [debug] [MainThread]: Flushing usage events


============================== 22:35:15.147930 | a5482c8a-b0f3-4931-ba48-865b5979c1ee ==============================
[0m22:35:15.147930 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:35:15.150577 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt test', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:35:15.150829 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:35:15.225234 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:35:15.244419 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:35:15.280409 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:35:15.280716 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:35:15.281666 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m22:35:15.295847 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:35:15.297170 [info ] [MainThread]: 
[0m22:35:15.297381 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:35:15.297656 [debug] [MainThread]: Command end result
[0m22:35:15.304904 [debug] [MainThread]: Command `dbt test` succeeded at 22:35:15.304798 after 0.17 seconds
[0m22:35:15.305260 [debug] [MainThread]: Flushing usage events


============================== 22:37:32.787385 | faa2669b-b68e-467f-b5c6-83c539ab0e98 ==============================
[0m22:37:32.787385 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:37:32.790587 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt test', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:37:32.790836 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:37:32.875316 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:37:32.896143 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:37:32.914822 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error
    Syntax error near line 25
    ------------------------------
    22 |         tests:
    23 |           - not_null
    24 | 
    25 |   - name: dim_account
    26 |     description: "Dimension table containing account data."
    27 |     columns:
    28 |       - name: account_id
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 1, column 1
    did not find expected key
      in "<unicode string>", line 25, column 3
[0m22:37:32.915514 [debug] [MainThread]: Command `dbt test` failed at 22:37:32.915424 after 0.16 seconds
[0m22:37:32.915807 [debug] [MainThread]: Flushing usage events


============================== 22:37:51.978348 | ccb220f5-d970-447a-9b16-9795cbaf7597 ==============================
[0m22:37:51.978348 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:37:51.980739 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:37:51.981005 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:37:52.063336 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:37:52.081492 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:37:52.097047 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error
    Syntax error near line 25
    ------------------------------
    22 |         tests:
    23 |           - not_null
    24 | 
    25 |   - name: dim_account
    26 |     description: "Dimension table containing account data."
    27 |     columns:
    28 |       - name: account_id
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 1, column 1
    did not find expected key
      in "<unicode string>", line 25, column 3
[0m22:37:52.097629 [debug] [MainThread]: Command `dbt test` failed at 22:37:52.097556 after 0.14 seconds
[0m22:37:52.097884 [debug] [MainThread]: Flushing usage events


============================== 22:38:46.477058 | 1a3c332c-fca4-43aa-9805-05a6270fec2e ==============================
[0m22:38:46.477058 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:38:46.480398 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt test', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:38:46.480653 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:38:46.562662 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:38:46.582155 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:38:46.600433 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error
    Syntax error near line 39
    ------------------------------
    36 |     columns:
    37 |       - name: account_id
    38 |         description
    
    Raw Error:
    ------------------------------
    while scanning a simple key
      in "<unicode string>", line 38, column 9
    could not find expected ':'
      in "<unicode string>", line 39, column 1
[0m22:38:46.600956 [debug] [MainThread]: Command `dbt test` failed at 22:38:46.600887 after 0.14 seconds
[0m22:38:46.601185 [debug] [MainThread]: Flushing usage events


============================== 22:39:24.032634 | 7c7598e6-2fb8-4a1b-bde2-4227efd2d53d ==============================
[0m22:39:24.032634 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:39:24.035611 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt test', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:39:24.035940 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:39:24.119138 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:39:24.138247 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:39:24.154234 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error
    Syntax error near line 65
    ------------------------------
    62 |     columns:
    63 |       - name: account_id
    64 |         description
    
    Raw Error:
    ------------------------------
    while scanning a simple key
      in "<unicode string>", line 64, column 9
    could not find expected ':'
      in "<unicode string>", line 65, column 1
[0m22:39:24.154782 [debug] [MainThread]: Command `dbt test` failed at 22:39:24.154710 after 0.15 seconds
[0m22:39:24.155022 [debug] [MainThread]: Flushing usage events
{"data": {"log_version": 3, "version": "=1.6.18"}, "info": {"category": "", "code": "A001", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "info", "msg": "Running with dbt=1.6.18", "name": "MainReportVersion", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.752659Z"}}
{"data": {"args": {"cache_selected_only": "False", "debug": "False", "fail_fast": "False", "indirect_selection": "eager", "introspect": "True", "invocation_command": "dbt --log-format json ls --select elastic_dbt_interview --resource-type model --resource-type seed --resource-type snapshot --resource-type analysis --project-dir /Users/sunjay.nair/Documents/GitHub/dbt_sunjay --output json", "log_cache_events": "False", "log_format": "json", "log_path": "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs", "no_print": "None", "partial_parse": "True", "printer_width": "80", "profiles_dir": ".", "quiet": "False", "send_anonymous_usage_stats": "False", "static_parser": "True", "target_path": "None", "use_colors": "True", "use_experimental_parser": "False", "version_check": "True", "warn_error": "None", "warn_error_options": "WarnErrorOptions(include=[], exclude=[])", "write_json": "True"}}, "info": {"category": "", "code": "A002", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "debug", "msg": "running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'json', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt --log-format json ls --select elastic_dbt_interview --resource-type model --resource-type seed --resource-type snapshot --resource-type analysis --project-dir /Users/sunjay.nair/Documents/GitHub/dbt_sunjay --output json', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}", "name": "MainReportArgs", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.756084Z"}}
{"data": {}, "info": {"category": "", "code": "D013", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "warn", "msg": "[\u001b[33mWARNING\u001b[0m]: Deprecated functionality\n\nUser config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.", "name": "ProjectFlagsMovedDeprecation", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.756443Z"}}
{"data": {"adapter_name": "duckdb", "adapter_version": "=1.6.2"}, "info": {"category": "", "code": "E034", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "info", "msg": "Registered adapter: duckdb=1.6.2", "name": "AdapterRegistered", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.836935Z"}}
{"data": {"checksum": "7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189", "profile": "", "target": "", "vars": "{}", "version": "1.6.18"}, "info": {"category": "", "code": "I025", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "debug", "msg": "checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18", "name": "StateCheckVarsHash", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.855189Z"}}
{"data": {"exc": "Parsing Error\n  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error\n    Syntax error near line 65\n    ------------------------------\n    62 |     columns:\n    63 |       - name: account_id\n    64 |         description\n    \n    Raw Error:\n    ------------------------------\n    while scanning a simple key\n      in \"<unicode string>\", line 64, column 9\n    could not find expected ':'\n      in \"<unicode string>\", line 65, column 1"}, "info": {"category": "", "code": "Z002", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "error", "msg": "Encountered an error:\nParsing Error\n  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error\n    Syntax error near line 65\n    ------------------------------\n    62 |     columns:\n    63 |       - name: account_id\n    64 |         description\n    \n    Raw Error:\n    ------------------------------\n    while scanning a simple key\n      in \"<unicode string>\", line 64, column 9\n    could not find expected ':'\n      in \"<unicode string>\", line 65, column 1", "name": "MainEncounteredError", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.874671Z"}}
{"data": {"command": "dbt ls", "completed_at": "2024-08-21T20:40:12.875195Z", "elapsed": 0.14122829, "success": false}, "info": {"category": "", "code": "Q039", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "debug", "msg": "Command `dbt ls` failed at 22:40:12.875195 after 0.14 seconds", "name": "CommandCompleted", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.875270Z"}}
{"data": {}, "info": {"category": "", "code": "Z042", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "debug", "msg": "Flushing usage events", "name": "FlushEvents", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.875607Z"}}
