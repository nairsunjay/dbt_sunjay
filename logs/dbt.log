

============================== 04:50:32.589806 | 1d90fe44-dcf0-463a-be01-15ef1c8374e8 ==============================
[0m04:50:32.589806 [info ] [MainThread]: Running with dbt=1.6.17
[0m04:50:32.590483 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/workspaces/dbt_interview/transformation/logs', 'debug': 'False', 'profiles_dir': '/workspaces/dbt_interview/transformation', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt build', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m04:50:32.779020 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m04:50:32.779880 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 3 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m04:50:32.780595 [debug] [MainThread]: Command `dbt build` failed at 04:50:32.780470 after 0.21 seconds
[0m04:50:32.781057 [debug] [MainThread]: Flushing usage events
[0m04:51:41.994142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f31796623d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f317991fa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3179608ed0>]}


============================== 04:51:41.996798 | e328dc54-c758-4d42-a042-4dbcb6114912 ==============================
[0m04:51:41.996798 [info ] [MainThread]: Running with dbt=1.6.17
[0m04:51:41.997453 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/workspaces/dbt_interview/transformation', 'version_check': 'True', 'debug': 'False', 'log_path': '/workspaces/dbt_interview/transformation/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m04:51:42.025814 [error] [MainThread]: Encountered an error:
Runtime Error
  at path []: Additional properties are not allowed ('config' was unexpected)

Error encountered in /workspaces/dbt_interview/transformation/dbt_project.yml
[0m04:51:42.026554 [debug] [MainThread]: Command `dbt deps` failed at 04:51:42.026432 after 0.04 seconds
[0m04:51:42.026978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f317934e0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3179347590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f317df31a90>]}
[0m04:51:42.027455 [debug] [MainThread]: Flushing usage events
[0m04:52:07.448252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7aba3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7769390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7a3f690>]}


============================== 04:52:07.450857 | 5b036ffc-3e60-4c54-bf2e-d049e72b8b7a ==============================
[0m04:52:07.450857 [info ] [MainThread]: Running with dbt=1.6.17
[0m04:52:07.451487 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/workspaces/dbt_interview/transformation', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/workspaces/dbt_interview/transformation/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt deps', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:52:07.495493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5b036ffc-3e60-4c54-bf2e-d049e72b8b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea74baed0>]}
[0m04:52:07.497190 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-lnnf05nj'
[0m04:52:07.497738 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m04:52:07.647205 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m04:52:07.647906 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json
[0m04:52:07.717532 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json 200
[0m04:52:07.720140 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m04:52:07.789779 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m04:52:07.795391 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m04:52:07.865049 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m04:52:07.883051 [info ] [MainThread]: Installing dbt-labs/codegen
[0m04:52:08.120758 [info ] [MainThread]: Installed from version 0.11.0
[0m04:52:08.121290 [info ] [MainThread]: Updated version available: 0.12.1
[0m04:52:08.121818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5b036ffc-3e60-4c54-bf2e-d049e72b8b7a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea77c6850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea74b9dd0>]}
[0m04:52:08.122294 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m04:52:08.388774 [info ] [MainThread]: Installed from version 1.2.0
[0m04:52:08.389270 [info ] [MainThread]: Up to date!
[0m04:52:08.389732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5b036ffc-3e60-4c54-bf2e-d049e72b8b7a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7a752d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7a74110>]}
[0m04:52:08.390167 [info ] [MainThread]: Installing calogica/dbt_date
[0m04:52:08.591973 [info ] [MainThread]: Installed from version 0.9.2
[0m04:52:08.592604 [info ] [MainThread]: Updated version available: 0.10.1
[0m04:52:08.593074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5b036ffc-3e60-4c54-bf2e-d049e72b8b7a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea794f790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea74ba9d0>]}
[0m04:52:08.593519 [info ] [MainThread]: 
[0m04:52:08.593941 [info ] [MainThread]: Updates available for packages: ['dbt-labs/codegen', 'calogica/dbt_date']                 
Update your versions in packages.yml, then run dbt deps
[0m04:52:08.594986 [debug] [MainThread]: Command `dbt deps` succeeded at 04:52:08.594869 after 1.16 seconds
[0m04:52:08.595381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7aab190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdea7ec2550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdeac035690>]}
[0m04:52:08.595898 [debug] [MainThread]: Flushing usage events
[0m04:52:12.676279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b6fb0250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33bbb35710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b753f550>]}


============================== 04:52:12.678873 | ca662291-5622-4965-bcf1-6dd532afcfe9 ==============================
[0m04:52:12.678873 [info ] [MainThread]: Running with dbt=1.6.17
[0m04:52:12.679501 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/workspaces/dbt_interview/transformation', 'log_path': '/workspaces/dbt_interview/transformation/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m04:52:12.786174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b7249dd0>]}
[0m04:52:12.804360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b6fb3090>]}
[0m04:52:12.805018 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m04:52:12.832039 [debug] [MainThread]: checksum: 5068fe71dce20d17d52958fa9cd93ea8f20705f4dc29725ea3e5886531e90d9e, vars: {}, profile: , target: , version: 1.6.17
[0m04:52:12.832844 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m04:52:12.833314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b6ff4810>]}
[0m04:52:14.379046 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m04:52:14.385150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33abde3b90>]}
[0m04:52:14.403560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9c34750>]}
[0m04:52:14.404054 [info ] [MainThread]: Found 14 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m04:52:14.406830 [info ] [MainThread]: 
[0m04:52:14.407675 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m04:52:14.409480 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m04:52:14.422743 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m04:52:14.423147 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m04:52:14.423520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:52:15.607834 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m04:52:15.609415 [debug] [ThreadPool]: On list_dbt: Close
[0m04:52:15.615038 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m04:52:15.615530 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m04:52:15.616001 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:15.625396 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.626746 [debug] [ThreadPool]: On list_dbt: Close
[0m04:52:15.629509 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m04:52:15.630264 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m04:52:15.635155 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m04:52:15.635543 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m04:52:15.635914 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:15.644312 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.644698 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m04:52:15.645119 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m04:52:15.646452 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.647383 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m04:52:15.647770 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m04:52:15.648193 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m04:52:15.649012 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.649415 [debug] [ThreadPool]: On create_dbt_main: Close
[0m04:52:15.652486 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now create_dbt_staging)
[0m04:52:15.653307 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m04:52:15.655763 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m04:52:15.656178 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m04:52:15.656540 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:15.664929 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.665318 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m04:52:15.665688 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m04:52:15.666315 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.667222 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m04:52:15.667608 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m04:52:15.667975 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m04:52:15.668625 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.669026 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m04:52:15.673260 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_main)
[0m04:52:15.680093 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m04:52:15.680481 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m04:52:15.680863 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:15.689643 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.690033 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m04:52:15.690417 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m04:52:15.717200 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.718456 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m04:52:15.719374 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m04:52:15.719750 [debug] [ThreadPool]: On list_dbt_main: Close
[0m04:52:15.723045 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m04:52:15.725502 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m04:52:15.725890 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m04:52:15.726248 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:15.734821 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.735197 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m04:52:15.735584 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m04:52:15.759912 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:52:15.761510 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m04:52:15.762004 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m04:52:15.762440 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m04:52:15.767609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9b742d0>]}
[0m04:52:15.768100 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:15.768506 [debug] [MainThread]: On master: BEGIN
[0m04:52:15.768865 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:52:15.778117 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m04:52:15.778509 [debug] [MainThread]: On master: COMMIT
[0m04:52:15.778875 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:15.779242 [debug] [MainThread]: On master: COMMIT
[0m04:52:15.779854 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m04:52:15.780251 [debug] [MainThread]: On master: Close
[0m04:52:15.782516 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:52:15.782993 [info ] [MainThread]: 
[0m04:52:15.790679 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m04:52:15.791255 [info ] [Thread-1 (]: 1 of 15 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m04:52:15.792142 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.elastic_dbt_interview.stg_salesforce__account'
[0m04:52:15.792635 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m04:52:15.801907 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.803618 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 04:52:15.793010 => 04:52:15.803399
[0m04:52:15.804061 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m04:52:15.840151 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.841877 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.842385 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m04:52:15.842803 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m04:52:15.853929 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.854375 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.854871 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m04:52:15.856302 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.863417 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.863864 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m04:52:15.864718 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.868197 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.868653 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m04:52:15.869786 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.888132 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m04:52:15.888586 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.889008 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m04:52:15.894034 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.899532 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m04:52:15.899972 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m04:52:15.903717 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.905308 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 04:52:15.804519 => 04:52:15.905115
[0m04:52:15.905744 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m04:52:15.929563 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b3d21a90>]}
[0m04:52:15.930211 [info ] [Thread-1 (]: 1 of 15 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.14s]
[0m04:52:15.930910 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m04:52:15.931425 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m04:52:15.932045 [info ] [Thread-1 (]: 2 of 15 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m04:52:15.932858 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m04:52:15.933319 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m04:52:15.938111 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.938852 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 04:52:15.933664 => 04:52:15.938603
[0m04:52:15.939305 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m04:52:15.943897 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.944642 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.945078 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m04:52:15.945523 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:15.954618 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.955081 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.955550 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m04:52:15.956777 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.960266 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.960701 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m04:52:15.961513 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.964800 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.965250 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m04:52:15.966171 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.967999 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m04:52:15.968434 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.968852 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m04:52:15.972613 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.975594 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m04:52:15.976037 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m04:52:15.980639 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:15.982069 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 04:52:15.939620 => 04:52:15.981878
[0m04:52:15.982512 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m04:52:16.001623 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9b72310>]}
[0m04:52:16.002301 [info ] [Thread-1 (]: 2 of 15 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.07s]
[0m04:52:16.003018 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m04:52:16.003551 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m04:52:16.004185 [info ] [Thread-1 (]: 3 of 15 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m04:52:16.005009 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m04:52:16.005495 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m04:52:16.008856 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.009526 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 04:52:16.005812 => 04:52:16.009318
[0m04:52:16.009948 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m04:52:16.014793 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.015402 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.015908 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m04:52:16.016337 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.025334 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.025767 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.026218 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m04:52:16.027394 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.030819 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.031278 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m04:52:16.032069 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.035359 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.035819 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m04:52:16.036588 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.038403 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m04:52:16.038830 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.039247 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m04:52:16.043802 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.046743 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m04:52:16.047202 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m04:52:16.050754 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.052174 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 04:52:16.010258 => 04:52:16.051987
[0m04:52:16.052662 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m04:52:16.072675 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b6fc2c10>]}
[0m04:52:16.073300 [info ] [Thread-1 (]: 3 of 15 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.07s]
[0m04:52:16.073987 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m04:52:16.074500 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m04:52:16.075124 [info ] [Thread-1 (]: 4 of 15 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m04:52:16.075949 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m04:52:16.076387 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m04:52:16.081440 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.082110 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 04:52:16.076703 => 04:52:16.081900
[0m04:52:16.082585 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m04:52:16.087439 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.088034 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.088500 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m04:52:16.088918 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.097938 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.098389 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.098838 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m04:52:16.099718 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.103318 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.103761 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m04:52:16.104534 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.107794 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.108233 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m04:52:16.109032 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.110844 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m04:52:16.111299 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.111716 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m04:52:16.116410 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.119367 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m04:52:16.119834 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m04:52:16.123414 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.124994 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 04:52:16.082899 => 04:52:16.124779
[0m04:52:16.125436 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m04:52:16.144940 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9b6aad0>]}
[0m04:52:16.145571 [info ] [Thread-1 (]: 4 of 15 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.07s]
[0m04:52:16.146301 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m04:52:16.146811 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m04:52:16.147440 [info ] [Thread-1 (]: 5 of 15 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m04:52:16.148250 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m04:52:16.148684 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m04:52:16.152013 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.152750 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 04:52:16.148995 => 04:52:16.152513
[0m04:52:16.153179 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m04:52:16.157666 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.158260 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.158694 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m04:52:16.159102 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.167564 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.168001 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.168522 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m04:52:16.169815 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.173426 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.173863 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m04:52:16.174673 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.178066 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.178536 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m04:52:16.179325 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.181162 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m04:52:16.181638 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.182060 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m04:52:16.185922 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.190640 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m04:52:16.191090 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m04:52:16.195769 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.197198 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 04:52:16.153487 => 04:52:16.197007
[0m04:52:16.197681 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m04:52:16.219898 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9b7fed0>]}
[0m04:52:16.220513 [info ] [Thread-1 (]: 5 of 15 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.07s]
[0m04:52:16.221215 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m04:52:16.221728 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m04:52:16.222409 [info ] [Thread-1 (]: 6 of 15 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m04:52:16.223201 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m04:52:16.223622 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m04:52:16.227209 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.227899 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 04:52:16.223987 => 04:52:16.227616
[0m04:52:16.228325 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m04:52:16.233427 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.234030 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.234454 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m04:52:16.234903 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.244430 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.244869 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.245339 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m04:52:16.246656 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.250120 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.250562 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m04:52:16.251380 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.254638 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.255081 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m04:52:16.255915 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.257778 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m04:52:16.258305 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.258723 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m04:52:16.263613 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.266452 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m04:52:16.266886 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m04:52:16.270462 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.271977 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 04:52:16.228638 => 04:52:16.271721
[0m04:52:16.272522 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m04:52:16.291137 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a99f8bd0>]}
[0m04:52:16.291751 [info ] [Thread-1 (]: 6 of 15 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.07s]
[0m04:52:16.292516 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m04:52:16.293063 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m04:52:16.293711 [info ] [Thread-1 (]: 7 of 15 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m04:52:16.294487 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m04:52:16.294949 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m04:52:16.298266 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.298938 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 04:52:16.295264 => 04:52:16.298726
[0m04:52:16.299359 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m04:52:16.304145 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.304751 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.305181 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m04:52:16.305602 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.314169 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.314628 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.315112 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m04:52:16.316319 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.319769 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.320270 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m04:52:16.321040 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.324299 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.324741 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m04:52:16.325525 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.327283 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m04:52:16.327721 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.328159 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m04:52:16.332695 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.337486 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m04:52:16.337922 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m04:52:16.341465 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.343082 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 04:52:16.299690 => 04:52:16.342891
[0m04:52:16.343545 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m04:52:16.364640 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9ab4210>]}
[0m04:52:16.365237 [info ] [Thread-1 (]: 7 of 15 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.07s]
[0m04:52:16.365938 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m04:52:16.366455 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m04:52:16.367092 [info ] [Thread-1 (]: 8 of 15 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m04:52:16.367894 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m04:52:16.368347 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m04:52:16.371514 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.372181 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 04:52:16.368671 => 04:52:16.371972
[0m04:52:16.372689 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m04:52:16.377315 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.377965 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.378392 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m04:52:16.378808 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.387765 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.388257 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.388723 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m04:52:16.389701 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.393246 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.393687 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m04:52:16.394452 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.397853 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.398305 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m04:52:16.399070 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.400894 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m04:52:16.401338 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.401781 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m04:52:16.405666 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.408534 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m04:52:16.408976 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m04:52:16.413532 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.414972 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 04:52:16.373002 => 04:52:16.414782
[0m04:52:16.415398 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m04:52:16.433948 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9a7ed10>]}
[0m04:52:16.434571 [info ] [Thread-1 (]: 8 of 15 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.07s]
[0m04:52:16.435302 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m04:52:16.435815 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m04:52:16.436470 [info ] [Thread-1 (]: 9 of 15 START sql view model staging.stg_salesforce__pricebook_entry ........... [RUN]
[0m04:52:16.437253 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m04:52:16.437690 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m04:52:16.440828 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.441586 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 04:52:16.438009 => 04:52:16.441293
[0m04:52:16.442055 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m04:52:16.446913 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.447472 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.447893 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m04:52:16.448312 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.456816 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.457269 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.457708 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m04:52:16.458666 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.462171 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.462674 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m04:52:16.463453 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.468463 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.468903 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m04:52:16.469704 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.471517 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m04:52:16.471968 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.472407 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m04:52:16.476210 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.479082 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m04:52:16.479541 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m04:52:16.484269 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.485671 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 04:52:16.442432 => 04:52:16.485481
[0m04:52:16.486106 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m04:52:16.507829 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9ad37d0>]}
[0m04:52:16.508488 [info ] [Thread-1 (]: 9 of 15 OK created sql view model staging.stg_salesforce__pricebook_entry ...... [[32mOK[0m in 0.07s]
[0m04:52:16.509178 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m04:52:16.509679 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m04:52:16.510283 [info ] [Thread-1 (]: 10 of 15 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m04:52:16.511079 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m04:52:16.511518 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m04:52:16.514813 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.515546 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 04:52:16.511834 => 04:52:16.515328
[0m04:52:16.515977 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m04:52:16.520574 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.521187 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.521608 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m04:52:16.522054 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.531343 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.531776 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.532274 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m04:52:16.533312 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.536709 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.537145 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m04:52:16.537908 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.541258 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.541740 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m04:52:16.542530 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.544298 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m04:52:16.544727 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.545180 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m04:52:16.549866 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.552917 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m04:52:16.553394 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m04:52:16.556987 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.558489 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 04:52:16.516299 => 04:52:16.558304
[0m04:52:16.558918 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m04:52:16.580154 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9a619d0>]}
[0m04:52:16.580755 [info ] [Thread-1 (]: 10 of 15 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.07s]
[0m04:52:16.581496 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m04:52:16.582021 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m04:52:16.582610 [info ] [Thread-1 (]: 11 of 15 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m04:52:16.583392 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m04:52:16.583824 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m04:52:16.587016 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.587751 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 04:52:16.584190 => 04:52:16.587471
[0m04:52:16.588182 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m04:52:16.592840 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.593453 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.593876 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m04:52:16.594305 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.602888 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.603354 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.603803 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m04:52:16.604730 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.608201 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.608644 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m04:52:16.609425 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.614710 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.615150 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m04:52:16.615974 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.617739 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m04:52:16.618210 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.618639 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m04:52:16.622371 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.625178 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m04:52:16.625664 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m04:52:16.630206 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.631677 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 04:52:16.588492 => 04:52:16.631490
[0m04:52:16.632101 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m04:52:16.652804 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9a59050>]}
[0m04:52:16.653440 [info ] [Thread-1 (]: 11 of 15 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.07s]
[0m04:52:16.654135 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m04:52:16.654637 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m04:52:16.655266 [info ] [Thread-1 (]: 12 of 15 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m04:52:16.656029 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m04:52:16.656458 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m04:52:16.659687 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.660419 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 04:52:16.656776 => 04:52:16.660151
[0m04:52:16.660903 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m04:52:16.665809 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.666434 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.666858 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m04:52:16.667298 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.676430 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.676879 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.677321 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m04:52:16.678313 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.681876 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.682403 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m04:52:16.683185 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.686512 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.686966 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m04:52:16.687808 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.689694 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m04:52:16.690132 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.690561 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m04:52:16.695395 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.698325 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m04:52:16.698761 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m04:52:16.702427 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.703902 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 04:52:16.661223 => 04:52:16.703709
[0m04:52:16.704384 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m04:52:16.723990 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a990b110>]}
[0m04:52:16.724619 [info ] [Thread-1 (]: 12 of 15 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.07s]
[0m04:52:16.725352 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m04:52:16.725875 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m04:52:16.726497 [info ] [Thread-1 (]: 13 of 15 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m04:52:16.727295 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m04:52:16.727739 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m04:52:16.731325 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.732006 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 04:52:16.728091 => 04:52:16.731799
[0m04:52:16.732462 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m04:52:16.737124 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.737752 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.738175 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m04:52:16.738586 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.747861 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.748352 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.748867 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m04:52:16.750356 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.756135 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.756591 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m04:52:16.757409 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.760673 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.761124 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m04:52:16.761896 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.763788 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m04:52:16.764244 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.764663 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m04:52:16.769463 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.772403 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m04:52:16.772851 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m04:52:16.776488 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.778032 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 04:52:16.732774 => 04:52:16.777843
[0m04:52:16.778460 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m04:52:16.802584 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9a0e290>]}
[0m04:52:16.803230 [info ] [Thread-1 (]: 13 of 15 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.08s]
[0m04:52:16.803926 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m04:52:16.804438 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m04:52:16.805058 [info ] [Thread-1 (]: 14 of 15 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m04:52:16.805859 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m04:52:16.806393 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m04:52:16.809732 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.810383 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 04:52:16.806747 => 04:52:16.810167
[0m04:52:16.810844 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m04:52:16.815484 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.816088 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.816512 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m04:52:16.816931 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.826533 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.826983 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.827432 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m04:52:16.828454 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.832100 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.832616 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m04:52:16.833410 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.836813 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.837252 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m04:52:16.838118 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.839886 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m04:52:16.840321 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.840738 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m04:52:16.845398 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.848307 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m04:52:16.848743 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m04:52:16.852365 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.853841 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 04:52:16.811159 => 04:52:16.853637
[0m04:52:16.854311 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m04:52:16.873657 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9935710>]}
[0m04:52:16.874279 [info ] [Thread-1 (]: 14 of 15 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.07s]
[0m04:52:16.874976 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m04:52:16.875537 [debug] [Thread-1 (]: Began running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m04:52:16.876176 [info ] [Thread-1 (]: 15 of 15 START seed file main.dbt_project_evaluator_exceptions ................. [RUN]
[0m04:52:16.876975 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now seed.elastic_dbt_interview.dbt_project_evaluator_exceptions)
[0m04:52:16.877454 [debug] [Thread-1 (]: Began compiling node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m04:52:16.877979 [debug] [Thread-1 (]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (compile): 04:52:16.877771 => 04:52:16.877773
[0m04:52:16.878415 [debug] [Thread-1 (]: Began executing node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m04:52:16.904355 [debug] [Thread-1 (]: Using duckdb connection "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"
[0m04:52:16.904805 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: BEGIN
[0m04:52:16.905263 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:52:16.914055 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.914508 [debug] [Thread-1 (]: Using duckdb connection "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"
[0m04:52:16.914939 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"} */

    create table "dbt"."main"."dbt_project_evaluator_exceptions" ("fct_name" text,"column_name" text,"id_to_exclude" text,"comment" text)
  
[0m04:52:16.915755 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.929961 [debug] [Thread-1 (]: Using duckdb connection "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"
[0m04:52:16.930414 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: 
          COPY "dbt"."main"."dbt_project_evaluator_exceptions" FROM '/workspaces/dbt_interview/transformation/seeds/dbt_project_evaluator_exceptions.csv' (FORMAT CSV, HEADER TRUE)
        ...
[0m04:52:16.934540 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.940480 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"
[0m04:52:16.946246 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: COMMIT
[0m04:52:16.946714 [debug] [Thread-1 (]: Using duckdb connection "seed.elastic_dbt_interview.dbt_project_evaluator_exceptions"
[0m04:52:16.947149 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: COMMIT
[0m04:52:16.951838 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m04:52:16.952786 [debug] [Thread-1 (]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (execute): 04:52:16.878726 => 04:52:16.952597
[0m04:52:16.953221 [debug] [Thread-1 (]: On seed.elastic_dbt_interview.dbt_project_evaluator_exceptions: Close
[0m04:52:16.975466 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca662291-5622-4965-bcf1-6dd532afcfe9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b3c87810>]}
[0m04:52:16.976073 [info ] [Thread-1 (]: 15 of 15 OK loaded seed file main.dbt_project_evaluator_exceptions ............. [[32mINSERT 2[0m in 0.10s]
[0m04:52:16.976843 [debug] [Thread-1 (]: Finished running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m04:52:16.978786 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:16.979178 [debug] [MainThread]: On master: BEGIN
[0m04:52:16.979543 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m04:52:16.988646 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m04:52:16.989042 [debug] [MainThread]: On master: COMMIT
[0m04:52:16.989436 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:16.989809 [debug] [MainThread]: On master: COMMIT
[0m04:52:16.990441 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m04:52:16.990870 [debug] [MainThread]: On master: Close
[0m04:52:16.993359 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:52:16.993726 [debug] [MainThread]: Connection 'list_dbt_staging' was properly closed.
[0m04:52:16.994082 [debug] [MainThread]: Connection 'seed.elastic_dbt_interview.dbt_project_evaluator_exceptions' was properly closed.
[0m04:52:16.994520 [info ] [MainThread]: 
[0m04:52:16.995096 [info ] [MainThread]: Finished running 14 view models, 1 seed in 0 hours 0 minutes and 2.59 seconds (2.59s).
[0m04:52:16.997954 [debug] [MainThread]: Command end result
[0m04:52:17.012491 [info ] [MainThread]: 
[0m04:52:17.012984 [info ] [MainThread]: [32mCompleted successfully[0m
[0m04:52:17.013453 [info ] [MainThread]: 
[0m04:52:17.013912 [info ] [MainThread]: Done. PASS=15 WARN=0 ERROR=0 SKIP=0 TOTAL=15
[0m04:52:17.014600 [debug] [MainThread]: Command `dbt build` succeeded at 04:52:17.014490 after 4.35 seconds
[0m04:52:17.015022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b744ee50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b753f6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33b7ae3950>]}
[0m04:52:17.015455 [debug] [MainThread]: Flushing usage events
[0m03:15:51.095218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13b74dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13ebaad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13b75050>]}


============================== 03:15:51.099332 | 4ab9fbc2-941e-4162-8d34-9c144dfe19e2 ==============================
[0m03:15:51.099332 [info ] [MainThread]: Running with dbt=1.6.17
[0m03:15:51.100140 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/workspaces/dbt_interview/transformation/logs', 'profiles_dir': '/workspaces/dbt_interview/transformation', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m03:15:51.286310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc138dbc50>]}
[0m03:15:51.304893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13d5b690>]}
[0m03:15:51.305800 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m03:15:51.333500 [debug] [MainThread]: checksum: 5068fe71dce20d17d52958fa9cd93ea8f20705f4dc29725ea3e5886531e90d9e, vars: {}, profile: , target: , version: 1.6.17
[0m03:15:51.415710 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m03:15:51.416211 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m03:15:51.417493 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m03:15:51.424534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13f4d110>]}
[0m03:15:51.444201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc08742e50>]}
[0m03:15:51.445091 [info ] [MainThread]: Found 14 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m03:15:51.445659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13d45550>]}
[0m03:15:51.448531 [info ] [MainThread]: 
[0m03:15:51.449441 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m03:15:51.451285 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m03:15:51.465504 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m03:15:51.465950 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m03:15:51.466317 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:15:51.491481 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.493009 [debug] [ThreadPool]: On list_dbt: Close
[0m03:15:51.496709 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m03:15:51.497517 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m03:15:51.502621 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m03:15:51.503005 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m03:15:51.503357 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:15:51.513325 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.513901 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m03:15:51.514506 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m03:15:51.516107 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.517123 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m03:15:51.517550 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m03:15:51.517916 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m03:15:51.518829 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.519259 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m03:15:51.523746 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_main)
[0m03:15:51.530309 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m03:15:51.530688 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m03:15:51.531034 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:15:51.540956 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.541419 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m03:15:51.541959 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m03:15:51.569043 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.573946 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m03:15:51.574986 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m03:15:51.575415 [debug] [ThreadPool]: On list_dbt_main: Close
[0m03:15:51.579277 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m03:15:51.582102 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m03:15:51.582470 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m03:15:51.582832 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:15:51.592038 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.592426 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m03:15:51.592789 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m03:15:51.618044 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m03:15:51.619746 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m03:15:51.620275 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m03:15:51.620619 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m03:15:51.626184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc1481f590>]}
[0m03:15:51.626698 [debug] [MainThread]: Using duckdb connection "master"
[0m03:15:51.627073 [debug] [MainThread]: On master: BEGIN
[0m03:15:51.627459 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:15:51.637843 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m03:15:51.638235 [debug] [MainThread]: On master: COMMIT
[0m03:15:51.638593 [debug] [MainThread]: Using duckdb connection "master"
[0m03:15:51.638986 [debug] [MainThread]: On master: COMMIT
[0m03:15:51.639597 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m03:15:51.639973 [debug] [MainThread]: On master: Close
[0m03:15:51.642611 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:15:51.643053 [info ] [MainThread]: 
[0m03:15:51.646022 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m03:15:51.646552 [info ] [Thread-1 (]: 1 of 14 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m03:15:51.647718 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.elastic_dbt_interview.stg_salesforce__account'
[0m03:15:51.648146 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m03:15:51.658585 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.659307 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 03:15:51.648446 => 03:15:51.659089
[0m03:15:51.659753 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m03:15:51.697589 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.698295 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.698713 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m03:15:51.699139 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m03:15:51.713642 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.714093 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.714629 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m03:15:51.716109 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.723958 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.724395 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m03:15:51.725276 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.728986 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.730572 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m03:15:51.732335 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.754310 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m03:15:51.754797 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.755256 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m03:15:51.760345 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.766547 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m03:15:51.767005 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m03:15:51.772594 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.774162 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 03:15:51.660046 => 03:15:51.773962
[0m03:15:51.774621 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m03:15:51.798531 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc087604d0>]}
[0m03:15:51.799236 [info ] [Thread-1 (]: 1 of 14 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.15s]
[0m03:15:51.799943 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m03:15:51.800449 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m03:15:51.801172 [info ] [Thread-1 (]: 2 of 14 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m03:15:51.802130 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m03:15:51.802616 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m03:15:51.806789 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.807789 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 03:15:51.803025 => 03:15:51.807350
[0m03:15:51.808267 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m03:15:51.814570 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.815416 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.815859 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m03:15:51.816259 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:51.826658 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.827119 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.827760 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m03:15:51.829308 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.833603 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.834030 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m03:15:51.834866 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.840572 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.841206 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m03:15:51.842017 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.843960 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m03:15:51.844461 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.844925 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m03:15:51.850048 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.853162 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m03:15:51.853666 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m03:15:51.857566 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.859744 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 03:15:51.808593 => 03:15:51.859387
[0m03:15:51.860398 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m03:15:51.888979 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc0832c4d0>]}
[0m03:15:51.889764 [info ] [Thread-1 (]: 2 of 14 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.09s]
[0m03:15:51.890528 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m03:15:51.891098 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m03:15:51.891733 [info ] [Thread-1 (]: 3 of 14 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m03:15:51.892606 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m03:15:51.893081 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m03:15:51.897389 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.898221 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 03:15:51.893471 => 03:15:51.897976
[0m03:15:51.898640 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m03:15:51.903954 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.904603 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.905027 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m03:15:51.905437 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:51.918827 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.919315 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.919767 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m03:15:51.920971 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.925235 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.925680 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m03:15:51.926563 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.930310 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.930755 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m03:15:51.931518 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.933379 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m03:15:51.933820 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.934220 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m03:15:51.938021 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.941349 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m03:15:51.941778 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m03:15:51.946515 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.948057 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 03:15:51.898936 => 03:15:51.947852
[0m03:15:51.948498 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m03:15:51.969300 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc138ff490>]}
[0m03:15:51.969968 [info ] [Thread-1 (]: 3 of 14 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.08s]
[0m03:15:51.970695 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m03:15:51.971236 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m03:15:51.971897 [info ] [Thread-1 (]: 4 of 14 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m03:15:51.972955 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m03:15:51.973517 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m03:15:51.977199 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:51.977999 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 03:15:51.973834 => 03:15:51.977781
[0m03:15:51.978425 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m03:15:51.983568 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:51.984248 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:51.984661 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m03:15:51.985056 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:51.994956 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:51.995410 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:51.995840 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m03:15:51.996799 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.000713 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:52.001170 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m03:15:52.001952 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.049308 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:52.049818 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m03:15:52.050837 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.052881 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m03:15:52.053324 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:52.053747 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m03:15:52.057598 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.060561 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m03:15:52.061033 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m03:15:52.065794 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.067276 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 03:15:51.978785 => 03:15:52.067042
[0m03:15:52.067778 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m03:15:52.089056 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc15569ad0>]}
[0m03:15:52.089689 [info ] [Thread-1 (]: 4 of 14 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.12s]
[0m03:15:52.090391 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m03:15:52.090907 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m03:15:52.091547 [info ] [Thread-1 (]: 5 of 14 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m03:15:52.092308 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m03:15:52.092748 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m03:15:52.096361 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.097116 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 03:15:52.093045 => 03:15:52.096837
[0m03:15:52.097578 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m03:15:52.102718 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.103301 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.103720 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m03:15:52.104117 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.116323 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.117054 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.117572 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m03:15:52.119168 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.122936 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.123382 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m03:15:52.124221 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.127748 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.128174 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m03:15:52.128951 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.130894 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m03:15:52.131307 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.131703 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m03:15:52.136548 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.139489 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m03:15:52.139952 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m03:15:52.143715 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.145454 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 03:15:52.097921 => 03:15:52.145167
[0m03:15:52.145884 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m03:15:52.168709 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc086f8990>]}
[0m03:15:52.169380 [info ] [Thread-1 (]: 5 of 14 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.08s]
[0m03:15:52.170084 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m03:15:52.170594 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m03:15:52.171254 [info ] [Thread-1 (]: 6 of 14 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m03:15:52.172036 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m03:15:52.172478 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m03:15:52.176280 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.176980 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 03:15:52.172776 => 03:15:52.176766
[0m03:15:52.177473 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m03:15:52.183671 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.184333 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.184821 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m03:15:52.185302 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.196016 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.196443 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.196924 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m03:15:52.198333 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.204992 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.205569 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m03:15:52.206413 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.209925 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.210348 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m03:15:52.211315 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.213162 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m03:15:52.213582 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.214005 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m03:15:52.218941 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.222125 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m03:15:52.222666 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m03:15:52.226372 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.227897 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 03:15:52.177962 => 03:15:52.227682
[0m03:15:52.228388 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m03:15:52.251328 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13bed310>]}
[0m03:15:52.252024 [info ] [Thread-1 (]: 6 of 14 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.08s]
[0m03:15:52.252819 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m03:15:52.253333 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m03:15:52.254039 [info ] [Thread-1 (]: 7 of 14 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m03:15:52.254832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m03:15:52.255256 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m03:15:52.259385 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.260091 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 03:15:52.255573 => 03:15:52.259849
[0m03:15:52.260504 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m03:15:52.266521 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.267494 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.268240 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m03:15:52.268941 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.283823 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.284351 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.284927 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m03:15:52.286226 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.290758 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.291265 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m03:15:52.292149 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.296335 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.296831 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m03:15:52.297625 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.299644 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m03:15:52.300094 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.300500 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m03:15:52.304493 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.307584 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m03:15:52.308014 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m03:15:52.312671 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.314178 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 03:15:52.260878 => 03:15:52.313948
[0m03:15:52.314615 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m03:15:52.336861 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc139351d0>]}
[0m03:15:52.337583 [info ] [Thread-1 (]: 7 of 14 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.08s]
[0m03:15:52.338322 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m03:15:52.338932 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m03:15:52.339573 [info ] [Thread-1 (]: 8 of 14 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m03:15:52.340393 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m03:15:52.340827 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m03:15:52.344433 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.345186 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 03:15:52.341155 => 03:15:52.344886
[0m03:15:52.345654 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m03:15:52.350699 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.351324 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.351732 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m03:15:52.352132 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.363054 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.363533 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.363979 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m03:15:52.365003 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.371655 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.372088 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m03:15:52.372939 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.376464 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.376898 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m03:15:52.377937 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.379965 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m03:15:52.380385 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.380818 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m03:15:52.385579 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.388521 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m03:15:52.388950 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m03:15:52.392677 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.394615 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 03:15:52.345949 => 03:15:52.394296
[0m03:15:52.395228 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m03:15:52.418910 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc083b1610>]}
[0m03:15:52.419589 [info ] [Thread-1 (]: 8 of 14 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.08s]
[0m03:15:52.420286 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m03:15:52.420870 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m03:15:52.421519 [info ] [Thread-1 (]: 9 of 14 START sql view model staging.stg_salesforce__pricebook_entry ........... [RUN]
[0m03:15:52.422318 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m03:15:52.422748 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m03:15:52.426381 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.427038 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 03:15:52.423051 => 03:15:52.426824
[0m03:15:52.427539 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m03:15:52.432992 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.433698 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.434140 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m03:15:52.434551 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.445042 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.445488 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.445939 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m03:15:52.446900 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.450742 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.451171 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m03:15:52.451972 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.456152 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.456862 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m03:15:52.457745 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.459631 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m03:15:52.460108 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.460513 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m03:15:52.465606 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.468570 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m03:15:52.469031 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m03:15:52.472879 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.474427 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 03:15:52.427868 => 03:15:52.474225
[0m03:15:52.474889 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m03:15:52.498870 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc0862c750>]}
[0m03:15:52.499787 [info ] [Thread-1 (]: 9 of 14 OK created sql view model staging.stg_salesforce__pricebook_entry ...... [[32mOK[0m in 0.08s]
[0m03:15:52.500524 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m03:15:52.501184 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m03:15:52.501846 [info ] [Thread-1 (]: 10 of 14 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m03:15:52.502708 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m03:15:52.503144 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m03:15:52.506758 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.507694 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 03:15:52.503450 => 03:15:52.507373
[0m03:15:52.508216 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m03:15:52.516602 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.517184 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.517648 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m03:15:52.518047 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.528393 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.528824 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.529260 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m03:15:52.530329 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.534884 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.535322 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m03:15:52.536124 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.539697 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.540185 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m03:15:52.541064 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.542950 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m03:15:52.543391 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.543813 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m03:15:52.548297 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.551426 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m03:15:52.551906 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m03:15:52.556747 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.558317 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 03:15:52.508513 => 03:15:52.558086
[0m03:15:52.558778 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m03:15:52.580347 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc08748810>]}
[0m03:15:52.580983 [info ] [Thread-1 (]: 10 of 14 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.08s]
[0m03:15:52.581667 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m03:15:52.582204 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m03:15:52.582924 [info ] [Thread-1 (]: 11 of 14 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m03:15:52.584021 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m03:15:52.584544 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m03:15:52.588914 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.589754 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 03:15:52.584956 => 03:15:52.589471
[0m03:15:52.590265 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m03:15:52.595387 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.596018 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.596422 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m03:15:52.596893 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.607086 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.607544 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.608011 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m03:15:52.608998 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.612998 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.613431 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m03:15:52.614219 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.617740 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.618197 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m03:15:52.619006 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.621610 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m03:15:52.622117 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.622678 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m03:15:52.626653 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.629824 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m03:15:52.630276 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m03:15:52.634933 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.636455 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 03:15:52.590571 => 03:15:52.636255
[0m03:15:52.636952 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m03:15:52.658930 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13ef0650>]}
[0m03:15:52.659609 [info ] [Thread-1 (]: 11 of 14 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.08s]
[0m03:15:52.660318 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m03:15:52.660827 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m03:15:52.661561 [info ] [Thread-1 (]: 12 of 14 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m03:15:52.662390 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m03:15:52.662847 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m03:15:52.666378 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.667076 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 03:15:52.663168 => 03:15:52.666858
[0m03:15:52.667566 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m03:15:52.675067 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.675672 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.676139 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m03:15:52.676580 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.686560 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.686978 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.687404 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m03:15:52.688861 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.692762 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.693202 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m03:15:52.693991 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.697438 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.697943 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m03:15:52.698704 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.700574 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m03:15:52.700993 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.701439 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m03:15:52.706267 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.709488 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m03:15:52.709925 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m03:15:52.713696 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.715856 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 03:15:52.667877 => 03:15:52.715597
[0m03:15:52.716443 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m03:15:52.738066 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc0850b150>]}
[0m03:15:52.738701 [info ] [Thread-1 (]: 12 of 14 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.08s]
[0m03:15:52.739484 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m03:15:52.740002 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m03:15:52.740624 [info ] [Thread-1 (]: 13 of 14 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m03:15:52.741444 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m03:15:52.741867 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m03:15:52.745754 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.746463 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 03:15:52.742164 => 03:15:52.746217
[0m03:15:52.746972 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m03:15:52.752050 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.752734 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.753250 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m03:15:52.753726 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.764014 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.764493 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.764972 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m03:15:52.766499 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.770346 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.770792 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m03:15:52.771578 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.774917 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.775342 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m03:15:52.776209 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.778335 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m03:15:52.778785 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.779652 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m03:15:52.785638 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.788768 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m03:15:52.789204 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m03:15:52.792898 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.794398 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 03:15:52.747270 => 03:15:52.794169
[0m03:15:52.794998 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m03:15:52.826946 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc084f4390>]}
[0m03:15:52.827801 [info ] [Thread-1 (]: 13 of 14 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.09s]
[0m03:15:52.828576 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m03:15:52.829291 [debug] [Thread-1 (]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m03:15:52.830058 [info ] [Thread-1 (]: 14 of 14 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m03:15:52.831281 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m03:15:52.831888 [debug] [Thread-1 (]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m03:15:52.835928 [debug] [Thread-1 (]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.836674 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 03:15:52.832191 => 03:15:52.836439
[0m03:15:52.837124 [debug] [Thread-1 (]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m03:15:52.845007 [debug] [Thread-1 (]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.845585 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.846123 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m03:15:52.846527 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:15:52.856879 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.857456 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.858044 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m03:15:52.859291 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.863428 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.863880 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m03:15:52.864673 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.868126 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.868604 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m03:15:52.869476 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.871726 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m03:15:52.872268 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.872813 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m03:15:52.876734 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.880610 [debug] [Thread-1 (]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m03:15:52.881064 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.17", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m03:15:52.885762 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m03:15:52.887333 [debug] [Thread-1 (]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 03:15:52.837434 => 03:15:52.887125
[0m03:15:52.887809 [debug] [Thread-1 (]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m03:15:52.910149 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ab9fbc2-941e-4162-8d34-9c144dfe19e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc08359050>]}
[0m03:15:52.911249 [info ] [Thread-1 (]: 14 of 14 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.08s]
[0m03:15:52.912318 [debug] [Thread-1 (]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m03:15:52.914451 [debug] [MainThread]: Using duckdb connection "master"
[0m03:15:52.914826 [debug] [MainThread]: On master: BEGIN
[0m03:15:52.915204 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m03:15:52.925278 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m03:15:52.925666 [debug] [MainThread]: On master: COMMIT
[0m03:15:52.926026 [debug] [MainThread]: Using duckdb connection "master"
[0m03:15:52.926419 [debug] [MainThread]: On master: COMMIT
[0m03:15:52.927198 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m03:15:52.927686 [debug] [MainThread]: On master: Close
[0m03:15:52.930780 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:15:52.931129 [debug] [MainThread]: Connection 'list_dbt_staging' was properly closed.
[0m03:15:52.931464 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.stg_salesforce__user_role' was properly closed.
[0m03:15:52.931947 [info ] [MainThread]: 
[0m03:15:52.932404 [info ] [MainThread]: Finished running 14 view models in 0 hours 0 minutes and 1.48 seconds (1.48s).
[0m03:15:52.934819 [debug] [MainThread]: Command end result
[0m03:15:52.951930 [info ] [MainThread]: 
[0m03:15:52.952480 [info ] [MainThread]: [32mCompleted successfully[0m
[0m03:15:52.952952 [info ] [MainThread]: 
[0m03:15:52.953419 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m03:15:52.954102 [debug] [MainThread]: Command `dbt run` succeeded at 03:15:52.953991 after 1.87 seconds
[0m03:15:52.954509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc15385110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc1448f090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc18489650>]}
[0m03:15:52.954979 [debug] [MainThread]: Flushing usage events


============================== 17:40:00.944557 | 9dd7b6f5-5b7c-4f38-94e2-ce83778e330e ==============================
[0m17:40:00.944557 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:40:00.947698 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'False'}
[0m17:40:00.947952 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:40:01.107348 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:40:01.129394 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:40:01.138183 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m17:40:01.911145 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.elastic_dbt_interview.dim_opportunity' (models/dimensions/dim_opportunity.sql) depends on a node named 'stg_opportunity' which was not found
[0m17:40:01.911669 [debug] [MainThread]: Command `dbt run` failed at 17:40:01.911606 after 0.99 seconds
[0m17:40:01.911878 [debug] [MainThread]: Flushing usage events


============================== 17:40:36.304666 | d1bed3e9-a3d4-41ab-8f0d-2303aa609775 ==============================
[0m17:40:36.304666 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:40:36.308190 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt compile', 'send_anonymous_usage_stats': 'False'}
[0m17:40:36.308495 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:40:36.407523 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:40:36.428440 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:40:36.437429 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m17:40:37.206107 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.elastic_dbt_interview.dim_opportunity' (models/dimensions/dim_opportunity.sql) depends on a node named 'stg_opportunity' which was not found
[0m17:40:37.206607 [debug] [MainThread]: Command `dbt compile` failed at 17:40:37.206545 after 0.92 seconds
[0m17:40:37.206807 [debug] [MainThread]: Flushing usage events


============================== 17:41:01.960693 | c03a14de-c8d5-4ee7-acbc-9b95b8bcf0e6 ==============================
[0m17:41:01.960693 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:41:01.963845 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt compile', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m17:41:01.964095 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:41:02.045501 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:41:02.091448 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:41:02.100062 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m17:41:02.888593 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m17:41:02.905056 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m17:41:02.906984 [info ] [MainThread]: 
[0m17:41:02.907597 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:41:02.909065 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt_main'
[0m17:41:02.915362 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:41:02.915607 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m17:41:02.915769 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:41:02.932050 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:02.932365 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:41:02.932555 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m17:41:02.958375 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:02.963053 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m17:41:02.963720 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m17:41:02.963903 [debug] [ThreadPool]: On list_dbt_main: Close
[0m17:41:02.966397 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m17:41:02.968588 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:41:02.968770 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m17:41:02.968926 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:02.974707 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:02.974961 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:41:02.975134 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m17:41:02.993450 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:02.994502 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m17:41:02.994738 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m17:41:02.994898 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m17:41:02.998854 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:41:02.999279 [info ] [MainThread]: 
[0m17:41:03.002667 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:03.003085 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_campaign_performance)
[0m17:41:03.003290 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:03.009382 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:41:03.011409 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 17:41:03.003428 => 17:41:03.011219
[0m17:41:03.011686 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:03.011940 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 17:41:03.011824 => 17:41:03.011831
[0m17:41:03.012451 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:03.012687 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m17:41:03.013001 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_case_history)
[0m17:41:03.013197 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m17:41:03.015451 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m17:41:03.016117 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 17:41:03.013324 => 17:41:03.016018
[0m17:41:03.016316 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m17:41:03.016531 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 17:41:03.016450 => 17:41:03.016454
[0m17:41:03.016957 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m17:41:03.017168 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m17:41:03.017491 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_product_sales)
[0m17:41:03.017681 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m17:41:03.020294 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m17:41:03.021162 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 17:41:03.017810 => 17:41:03.021068
[0m17:41:03.021363 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m17:41:03.021577 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 17:41:03.021492 => 17:41:03.021496
[0m17:41:03.021992 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m17:41:03.022204 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:03.022535 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m17:41:03.022728 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:03.024531 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:03.025374 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 17:41:03.022864 => 17:41:03.025283
[0m17:41:03.025564 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:03.025772 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 17:41:03.025692 => 17:41:03.025695
[0m17:41:03.026182 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:03.026442 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:03.026864 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m17:41:03.027086 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:03.028882 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:03.029443 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 17:41:03.027228 => 17:41:03.029342
[0m17:41:03.029635 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:03.029847 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 17:41:03.029762 => 17:41:03.029766
[0m17:41:03.030266 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:03.030473 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:03.030819 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m17:41:03.031007 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:03.032736 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:03.033286 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 17:41:03.031145 => 17:41:03.033193
[0m17:41:03.033475 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:03.033694 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 17:41:03.033603 => 17:41:03.033606
[0m17:41:03.034101 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:03.034316 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:03.034658 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m17:41:03.034846 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:03.036508 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:03.037003 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 17:41:03.034975 => 17:41:03.036914
[0m17:41:03.037194 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:03.037433 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 17:41:03.037350 => 17:41:03.037353
[0m17:41:03.037868 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:03.038076 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:03.038409 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m17:41:03.038593 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:03.040357 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:03.040741 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 17:41:03.038728 => 17:41:03.040637
[0m17:41:03.040930 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:03.041144 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 17:41:03.041061 => 17:41:03.041066
[0m17:41:03.041546 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:03.041753 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:03.042088 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m17:41:03.042273 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:03.044757 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:03.045207 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 17:41:03.042396 => 17:41:03.045095
[0m17:41:03.045406 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:03.045626 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 17:41:03.045539 => 17:41:03.045543
[0m17:41:03.046047 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:03.046261 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:03.046632 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m17:41:03.046821 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:03.048521 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:03.048944 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 17:41:03.046944 => 17:41:03.048838
[0m17:41:03.049142 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:03.049351 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 17:41:03.049269 => 17:41:03.049272
[0m17:41:03.049759 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:03.049963 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:03.050443 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m17:41:03.050718 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:03.052703 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:03.053240 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 17:41:03.050863 => 17:41:03.053139
[0m17:41:03.053436 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:03.053658 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 17:41:03.053568 => 17:41:03.053573
[0m17:41:03.054129 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:03.054351 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:03.054804 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m17:41:03.055016 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:03.056785 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:03.057183 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 17:41:03.055148 => 17:41:03.057092
[0m17:41:03.057370 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:03.057581 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 17:41:03.057498 => 17:41:03.057501
[0m17:41:03.057997 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:03.058210 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:03.058669 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m17:41:03.058910 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:03.060885 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:03.061371 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 17:41:03.059051 => 17:41:03.061262
[0m17:41:03.061593 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:03.061816 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 17:41:03.061731 => 17:41:03.061736
[0m17:41:03.062257 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:03.062471 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:03.062827 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m17:41:03.063014 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:03.064840 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:03.065498 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 17:41:03.063141 => 17:41:03.065375
[0m17:41:03.065729 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:03.065938 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 17:41:03.065855 => 17:41:03.065859
[0m17:41:03.066368 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:03.066594 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:03.067021 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m17:41:03.067217 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:03.069717 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:03.070188 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 17:41:03.067347 => 17:41:03.070089
[0m17:41:03.070383 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:03.070595 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 17:41:03.070508 => 17:41:03.070512
[0m17:41:03.071008 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:03.071214 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:03.071572 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m17:41:03.071757 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:03.097725 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:03.098433 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 17:41:03.071881 => 17:41:03.098325
[0m17:41:03.098639 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:03.098856 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 17:41:03.098772 => 17:41:03.098776
[0m17:41:03.099316 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:03.099534 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:03.099845 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m17:41:03.100028 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:03.101720 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:03.102141 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 17:41:03.100156 => 17:41:03.102046
[0m17:41:03.102335 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:03.102543 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 17:41:03.102463 => 17:41:03.102466
[0m17:41:03.102946 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:03.103152 [debug] [Thread-1  ]: Began running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m17:41:03.103486 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now seed.elastic_dbt_interview.dbt_project_evaluator_exceptions)
[0m17:41:03.103671 [debug] [Thread-1  ]: Began compiling node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m17:41:03.104867 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (compile): 17:41:03.103797 => 17:41:03.104774
[0m17:41:03.105053 [debug] [Thread-1  ]: Began executing node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m17:41:03.105262 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (execute): 17:41:03.105179 => 17:41:03.105182
[0m17:41:03.105664 [debug] [Thread-1  ]: Finished running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m17:41:03.105869 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m17:41:03.106180 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly seed.elastic_dbt_interview.dbt_project_evaluator_exceptions, now model.elastic_dbt_interview.dim_account)
[0m17:41:03.106366 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m17:41:03.108091 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m17:41:03.108978 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 17:41:03.106490 => 17:41:03.108879
[0m17:41:03.109175 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m17:41:03.109395 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 17:41:03.109311 => 17:41:03.109314
[0m17:41:03.109809 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m17:41:03.110012 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m17:41:03.110338 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m17:41:03.110517 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m17:41:03.112387 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m17:41:03.112776 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 17:41:03.110650 => 17:41:03.112685
[0m17:41:03.112963 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m17:41:03.113174 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 17:41:03.113096 => 17:41:03.113100
[0m17:41:03.113581 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m17:41:03.113790 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m17:41:03.114108 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.fact_case)
[0m17:41:03.114291 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m17:41:03.116744 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m17:41:03.117136 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 17:41:03.114415 => 17:41:03.117047
[0m17:41:03.117324 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m17:41:03.117537 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 17:41:03.117456 => 17:41:03.117460
[0m17:41:03.117957 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m17:41:03.118158 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m17:41:03.118465 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.dim_case_status)
[0m17:41:03.118646 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m17:41:03.120465 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m17:41:03.120867 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 17:41:03.118771 => 17:41:03.120778
[0m17:41:03.121054 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m17:41:03.121282 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 17:41:03.121193 => 17:41:03.121197
[0m17:41:03.121699 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m17:41:03.121904 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m17:41:03.122220 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m17:41:03.122402 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m17:41:03.124147 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m17:41:03.124528 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 17:41:03.122541 => 17:41:03.124440
[0m17:41:03.124714 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m17:41:03.124924 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 17:41:03.124845 => 17:41:03.124849
[0m17:41:03.125336 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m17:41:03.125554 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m17:41:03.125877 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m17:41:03.126057 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m17:41:03.127771 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m17:41:03.128125 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 17:41:03.126184 => 17:41:03.128040
[0m17:41:03.128318 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m17:41:03.128528 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 17:41:03.128448 => 17:41:03.128451
[0m17:41:03.128929 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m17:41:03.129132 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m17:41:03.129443 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m17:41:03.129627 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m17:41:03.131585 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:41:03.131972 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 17:41:03.129756 => 17:41:03.131882
[0m17:41:03.132163 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m17:41:03.132373 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 17:41:03.132294 => 17:41:03.132298
[0m17:41:03.132772 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m17:41:03.132976 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:03.133302 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m17:41:03.133509 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:03.135321 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:03.135705 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 17:41:03.133644 => 17:41:03.135619
[0m17:41:03.135891 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:03.136101 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 17:41:03.136022 => 17:41:03.136026
[0m17:41:03.136503 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:03.136702 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m17:41:03.137016 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.fact_opportunity)
[0m17:41:03.137215 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m17:41:03.139795 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m17:41:03.140177 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 17:41:03.137352 => 17:41:03.140084
[0m17:41:03.140373 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m17:41:03.140589 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 17:41:03.140508 => 17:41:03.140511
[0m17:41:03.140994 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m17:41:03.141197 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:03.141509 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_opportunity_history)
[0m17:41:03.141696 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:03.143486 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:41:03.144202 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 17:41:03.141824 => 17:41:03.144111
[0m17:41:03.144391 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:03.144603 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 17:41:03.144521 => 17:41:03.144524
[0m17:41:03.145010 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:03.145211 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m17:41:03.145583 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity_history, now model.elastic_dbt_interview.dim_pricebook)
[0m17:41:03.145811 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m17:41:03.147715 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m17:41:03.148146 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 17:41:03.145967 => 17:41:03.148046
[0m17:41:03.148353 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m17:41:03.148593 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 17:41:03.148503 => 17:41:03.148507
[0m17:41:03.149015 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m17:41:03.149221 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m17:41:03.149547 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m17:41:03.149755 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m17:41:03.151581 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m17:41:03.151969 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 17:41:03.149890 => 17:41:03.151881
[0m17:41:03.152160 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m17:41:03.152377 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 17:41:03.152298 => 17:41:03.152301
[0m17:41:03.152783 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m17:41:03.152993 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m17:41:03.153311 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_solution)
[0m17:41:03.153500 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m17:41:03.155292 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m17:41:03.155679 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 17:41:03.153627 => 17:41:03.155584
[0m17:41:03.155868 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m17:41:03.156078 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 17:41:03.155999 => 17:41:03.156002
[0m17:41:03.156480 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m17:41:03.156682 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m17:41:03.156994 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m17:41:03.157176 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m17:41:03.158879 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m17:41:03.159588 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 17:41:03.157303 => 17:41:03.159497
[0m17:41:03.159775 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m17:41:03.159983 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 17:41:03.159905 => 17:41:03.159908
[0m17:41:03.160385 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m17:41:03.160583 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user_role
[0m17:41:03.160924 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.dim_user_role)
[0m17:41:03.161120 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user_role
[0m17:41:03.163704 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user_role"
[0m17:41:03.164098 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (compile): 17:41:03.161256 => 17:41:03.164007
[0m17:41:03.164287 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user_role
[0m17:41:03.164500 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (execute): 17:41:03.164419 => 17:41:03.164423
[0m17:41:03.164906 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user_role
[0m17:41:03.165412 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:41:03.165595 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user_role' was properly closed.
[0m17:41:03.167711 [debug] [MainThread]: Command end result
[0m17:41:03.175288 [debug] [MainThread]: Command `dbt compile` succeeded at 17:41:03.175196 after 1.23 seconds
[0m17:41:03.175516 [debug] [MainThread]: Flushing usage events


============================== 17:41:40.371182 | 1bba16fb-169c-4153-9d6e-bcd945922b41 ==============================
[0m17:41:40.371182 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:41:40.374670 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m17:41:40.374934 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:41:40.467451 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:41:40.487406 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:41:40.539415 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:41:40.539702 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:41:40.540666 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m17:41:40.555371 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m17:41:40.557188 [info ] [MainThread]: 
[0m17:41:40.557655 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:41:40.559125 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m17:41:40.566317 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:41:40.566575 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:41:40.566747 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:41:40.578813 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.579892 [debug] [ThreadPool]: On list_dbt: Close
[0m17:41:40.583061 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:41:40.583383 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:41:40.583623 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:40.589686 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.590490 [debug] [ThreadPool]: On list_dbt: Close
[0m17:41:40.592458 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m17:41:40.592901 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m17:41:40.595759 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:41:40.595963 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m17:41:40.596124 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:40.601688 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.601946 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:41:40.602115 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m17:41:40.602644 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.603189 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m17:41:40.603358 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:41:40.603510 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m17:41:40.603720 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.603874 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m17:41:40.605514 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now create_dbt_main)
[0m17:41:40.605924 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m17:41:40.607446 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:41:40.607623 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m17:41:40.607773 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:40.612590 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.612823 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:41:40.612984 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m17:41:40.613219 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.613696 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:41:40.613861 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:41:40.614016 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:41:40.614234 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.614394 [debug] [ThreadPool]: On create_dbt_main: Close
[0m17:41:40.617114 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m17:41:40.620380 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:41:40.620569 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m17:41:40.620720 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:40.626083 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.626347 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:41:40.626527 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m17:41:40.644484 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.645453 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m17:41:40.646124 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m17:41:40.646294 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m17:41:40.648439 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m17:41:40.651724 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:41:40.651935 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m17:41:40.652093 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:41:40.657795 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.658083 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:41:40.658268 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m17:41:40.677729 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:41:40.682444 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m17:41:40.682800 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m17:41:40.682988 [debug] [ThreadPool]: On list_dbt_main: Close
[0m17:41:40.687111 [debug] [MainThread]: Using duckdb connection "master"
[0m17:41:40.687388 [debug] [MainThread]: On master: BEGIN
[0m17:41:40.687557 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:41:40.693598 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:41:40.693911 [debug] [MainThread]: On master: COMMIT
[0m17:41:40.694094 [debug] [MainThread]: Using duckdb connection "master"
[0m17:41:40.694261 [debug] [MainThread]: On master: COMMIT
[0m17:41:40.694477 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:41:40.694645 [debug] [MainThread]: On master: Close
[0m17:41:40.696121 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:41:40.696431 [info ] [MainThread]: 
[0m17:41:40.698571 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:40.698957 [info ] [Thread-1  ]: 1 of 32 START sql table model main.fact_campaign_performance ................... [RUN]
[0m17:41:40.699520 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_campaign_performance)
[0m17:41:40.699792 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:40.706394 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:41:40.707659 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 17:41:40.699955 => 17:41:40.707354
[0m17:41:40.707944 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:40.727740 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:41:40.728696 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:41:40.728939 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: BEGIN
[0m17:41:40.729127 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.734613 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.734898 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:41:40.735113 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */

  
    
    

    create  table
      "dbt"."main"."fact_campaign_performance__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."campaign"  -- Hypothetical source for campaign performance metrics
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS campaign_performance_id,  -- Surrogate Key
    campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
    leads_generated,
    opportunities_created,
    revenue_generated,
    expenses,
    createddate AS performance_created_at
FROM source
    );
  
  
[0m17:41:40.739632 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 17:41:40.708109 => 17:41:40.739498
[0m17:41:40.739886 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: ROLLBACK
[0m17:41:40.743628 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_campaign_performance'
[0m17:41:40.743834 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: Close
[0m17:41:40.745549 [debug] [Thread-1  ]: Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m17:41:40.745970 [error] [Thread-1  ]: 1 of 32 ERROR creating sql table model main.fact_campaign_performance .......... [[31mERROR[0m in 0.05s]
[0m17:41:40.746327 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:41:40.746569 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m17:41:40.746865 [info ] [Thread-1  ]: 2 of 32 START sql table model main.fact_case_history ........................... [RUN]
[0m17:41:40.747226 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_case_history)
[0m17:41:40.747423 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m17:41:40.749603 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.750188 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 17:41:40.747556 => 17:41:40.750091
[0m17:41:40.750384 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m17:41:40.752907 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.753421 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.753619 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: BEGIN
[0m17:41:40.753796 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.759233 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.759496 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.759700 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */

  
    
    

    create  table
      "dbt"."main"."fact_case_history__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY systemmodstamp) AS case_history_id,  -- Surrogate Key
    caseid AS case_fk,                                               -- Foreign Key to fact_case
    status,
    systemmodstamp AS history_date
FROM source
    );
  
  
[0m17:41:40.764770 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.769723 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.770028 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
alter table "dbt"."main"."fact_case_history" rename to "fact_case_history__dbt_backup"
[0m17:41:40.770790 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.772608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.772815 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
alter table "dbt"."main"."fact_case_history__dbt_tmp" rename to "fact_case_history"
[0m17:41:40.773122 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.783692 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: COMMIT
[0m17:41:40.783920 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.784108 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: COMMIT
[0m17:41:40.785029 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.788006 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:41:40.788232 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
drop table if exists "dbt"."main"."fact_case_history__dbt_backup" cascade
[0m17:41:40.788617 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.789363 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 17:41:40.750518 => 17:41:40.789270
[0m17:41:40.789569 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: Close
[0m17:41:40.807525 [info ] [Thread-1  ]: 2 of 32 OK created sql table model main.fact_case_history ...................... [[32mOK[0m in 0.06s]
[0m17:41:40.807930 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m17:41:40.808172 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m17:41:40.808444 [info ] [Thread-1  ]: 3 of 32 START sql table model main.fact_product_sales .......................... [RUN]
[0m17:41:40.808803 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_product_sales)
[0m17:41:40.808996 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m17:41:40.811101 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m17:41:40.811545 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 17:41:40.809127 => 17:41:40.811445
[0m17:41:40.811744 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m17:41:40.814201 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_product_sales"
[0m17:41:40.815169 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m17:41:40.815418 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: BEGIN
[0m17:41:40.815594 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.820840 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.821110 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m17:41:40.821325 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_product_sales"} */

  
    
    

    create  table
      "dbt"."main"."fact_product_sales__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."pricebook_entry"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS product_sales_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
    product2id AS product_fk,                                      -- Foreign Key to dim_product
    pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
    unitprice AS unit_price,
    isactive AS is_active,
    createddate AS product_sales_created_at,
    lastmodifieddate AS product_sales_last_modified_date
FROM source
    );
  
  
[0m17:41:40.821978 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 17:41:40.811875 => 17:41:40.821873
[0m17:41:40.822193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: ROLLBACK
[0m17:41:40.822768 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_product_sales'
[0m17:41:40.822948 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: Close
[0m17:41:40.824919 [debug] [Thread-1  ]: Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:40.825350 [error] [Thread-1  ]: 3 of 32 ERROR creating sql table model main.fact_product_sales ................. [[31mERROR[0m in 0.02s]
[0m17:41:40.825680 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m17:41:40.825901 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:40.826289 [info ] [Thread-1  ]: 4 of 32 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m17:41:40.826840 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m17:41:40.827067 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:40.855451 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.856138 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 17:41:40.827234 => 17:41:40.856032
[0m17:41:40.856483 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:40.867581 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.868325 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.868557 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m17:41:40.868762 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.874019 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.874295 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.874533 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m17:41:40.875354 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.877649 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.877893 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m17:41:40.878531 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.880636 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.880887 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m17:41:40.881220 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.882142 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m17:41:40.882337 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.882527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m17:41:40.883114 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.884801 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:41:40.885024 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m17:41:40.885380 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.886113 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 17:41:40.856702 => 17:41:40.886021
[0m17:41:40.886326 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m17:41:40.894013 [info ] [Thread-1  ]: 4 of 32 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.07s]
[0m17:41:40.894405 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:41:40.894679 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:40.895012 [info ] [Thread-1  ]: 5 of 32 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m17:41:40.895386 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m17:41:40.895588 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:40.898339 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.898870 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 17:41:40.895718 => 17:41:40.898764
[0m17:41:40.899063 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:40.901751 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.902206 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.902410 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m17:41:40.902592 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.907978 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.908242 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.908474 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m17:41:40.909264 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.911434 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.911662 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m17:41:40.911968 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.913765 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.913994 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m17:41:40.914327 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.915365 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m17:41:40.915665 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.915854 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m17:41:40.916604 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.918367 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:41:40.918576 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m17:41:40.919033 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.919804 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 17:41:40.899196 => 17:41:40.919706
[0m17:41:40.920037 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m17:41:40.927886 [info ] [Thread-1  ]: 5 of 32 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.03s]
[0m17:41:40.928316 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:41:40.928560 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:40.928897 [info ] [Thread-1  ]: 6 of 32 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m17:41:40.929301 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m17:41:40.929504 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:40.931551 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.932075 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 17:41:40.929637 => 17:41:40.931978
[0m17:41:40.932269 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:40.935822 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.936351 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.936548 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m17:41:40.936730 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.942012 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.942257 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.942478 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m17:41:40.943167 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.945270 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.945488 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m17:41:40.945783 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.947460 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.947653 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m17:41:40.947930 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.948760 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m17:41:40.948950 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.949125 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m17:41:40.949969 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.951478 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:41:40.951675 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m17:41:40.952113 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.953173 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 17:41:40.932397 => 17:41:40.953071
[0m17:41:40.953430 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m17:41:40.961562 [info ] [Thread-1  ]: 6 of 32 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.03s]
[0m17:41:40.961983 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:41:40.962239 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:40.962570 [info ] [Thread-1  ]: 7 of 32 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m17:41:40.962939 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m17:41:40.963137 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:40.965304 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.965783 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 17:41:40.963266 => 17:41:40.965683
[0m17:41:40.965983 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:40.969359 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.969833 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.970036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m17:41:40.970230 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:40.975637 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.975873 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.976070 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m17:41:40.976507 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.978608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.978831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m17:41:40.979128 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.980809 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.981004 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m17:41:40.981311 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.982148 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m17:41:40.982343 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.982528 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m17:41:40.983425 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.985391 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:41:40.985664 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m17:41:40.986136 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:40.987026 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 17:41:40.966113 => 17:41:40.986930
[0m17:41:40.987246 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m17:41:40.994980 [info ] [Thread-1  ]: 7 of 32 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.03s]
[0m17:41:40.995369 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:41:40.995632 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:40.995944 [info ] [Thread-1  ]: 8 of 32 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m17:41:40.996378 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m17:41:40.996574 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:40.998780 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:40.999311 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 17:41:40.996705 => 17:41:40.999214
[0m17:41:40.999516 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:41.002262 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.002832 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.003014 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m17:41:41.003183 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.008520 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.008796 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.009036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m17:41:41.009823 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.012682 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.012911 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m17:41:41.013222 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.014910 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.015110 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m17:41:41.015442 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.016428 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m17:41:41.016730 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.016927 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m17:41:41.017591 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.019495 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:41:41.019766 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m17:41:41.020216 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.021029 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 17:41:40.999644 => 17:41:41.020933
[0m17:41:41.021242 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m17:41:41.029017 [info ] [Thread-1  ]: 8 of 32 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.03s]
[0m17:41:41.029401 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:41:41.029663 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:41.030019 [info ] [Thread-1  ]: 9 of 32 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m17:41:41.030415 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m17:41:41.030712 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:41.032878 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.033391 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 17:41:41.030860 => 17:41:41.033289
[0m17:41:41.033594 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:41.036261 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.036672 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.036871 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m17:41:41.037049 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.042587 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.042864 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.043100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m17:41:41.043902 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.046832 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.047059 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m17:41:41.047395 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.049136 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.049343 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m17:41:41.049647 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.050496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m17:41:41.050814 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.051058 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m17:41:41.051706 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.053497 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:41:41.053704 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m17:41:41.054112 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.054951 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 17:41:41.033733 => 17:41:41.054851
[0m17:41:41.055172 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m17:41:41.062939 [info ] [Thread-1  ]: 9 of 32 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.03s]
[0m17:41:41.063308 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:41:41.063548 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:41.063872 [info ] [Thread-1  ]: 10 of 32 START sql view model staging.stg_salesforce__opportunity .............. [RUN]
[0m17:41:41.064334 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m17:41:41.064571 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:41.066612 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.067102 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 17:41:41.064715 => 17:41:41.066998
[0m17:41:41.067297 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:41.069765 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.070152 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.070339 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m17:41:41.070510 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.075972 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.076275 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.076514 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m17:41:41.077259 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.079671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.080033 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m17:41:41.080441 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.083204 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.083433 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m17:41:41.083750 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.084763 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m17:41:41.085046 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.085245 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m17:41:41.085865 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.087656 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:41:41.087879 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m17:41:41.088265 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.089093 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 17:41:41.067426 => 17:41:41.088992
[0m17:41:41.089311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m17:41:41.096993 [info ] [Thread-1  ]: 10 of 32 OK created sql view model staging.stg_salesforce__opportunity ......... [[32mOK[0m in 0.03s]
[0m17:41:41.097409 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:41:41.097682 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:41.098149 [info ] [Thread-1  ]: 11 of 32 START sql view model staging.stg_salesforce__opportunity_history ...... [RUN]
[0m17:41:41.098570 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m17:41:41.098781 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:41.100823 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.101315 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 17:41:41.098919 => 17:41:41.101213
[0m17:41:41.101518 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:41.104188 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.104692 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.104890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m17:41:41.105072 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.110408 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.110666 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.110878 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m17:41:41.111457 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.113619 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.113838 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m17:41:41.114141 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.115826 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.116024 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m17:41:41.116317 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.117155 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m17:41:41.117353 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.117530 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m17:41:41.118125 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.120864 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:41:41.121092 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m17:41:41.121532 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.122393 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 17:41:41.101649 => 17:41:41.122297
[0m17:41:41.122637 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m17:41:41.130446 [info ] [Thread-1  ]: 11 of 32 OK created sql view model staging.stg_salesforce__opportunity_history . [[32mOK[0m in 0.03s]
[0m17:41:41.130903 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:41:41.131138 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:41.131510 [info ] [Thread-1  ]: 12 of 32 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m17:41:41.131958 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m17:41:41.132172 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:41.134176 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.134654 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 17:41:41.132318 => 17:41:41.134554
[0m17:41:41.134881 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:41.137492 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.137934 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.138119 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m17:41:41.138295 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.143747 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.144068 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.144295 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m17:41:41.144777 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.147190 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.147462 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m17:41:41.147823 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.149671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.149890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m17:41:41.150182 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.151028 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m17:41:41.151217 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.151394 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m17:41:41.152123 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.154767 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:41:41.154988 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m17:41:41.155393 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.156197 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 17:41:41.135014 => 17:41:41.156094
[0m17:41:41.156411 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m17:41:41.164454 [info ] [Thread-1  ]: 12 of 32 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.03s]
[0m17:41:41.164879 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:41:41.165124 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:41.165441 [info ] [Thread-1  ]: 13 of 32 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m17:41:41.165813 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m17:41:41.166007 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:41.168078 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.168597 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 17:41:41.166142 => 17:41:41.168490
[0m17:41:41.168803 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:41.171483 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.171970 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.172167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m17:41:41.172354 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.177755 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.178063 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.178288 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m17:41:41.178828 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.181301 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.181581 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m17:41:41.181940 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.183714 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.183913 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m17:41:41.184205 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.185055 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m17:41:41.185243 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.185415 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m17:41:41.186187 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.188013 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:41:41.188228 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m17:41:41.188598 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.189454 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 17:41:41.168934 => 17:41:41.189358
[0m17:41:41.189677 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m17:41:41.196938 [info ] [Thread-1  ]: 13 of 32 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.03s]
[0m17:41:41.197297 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:41:41.197524 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:41.197947 [info ] [Thread-1  ]: 14 of 32 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m17:41:41.198416 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m17:41:41.198624 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:41.201411 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.201913 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 17:41:41.198764 => 17:41:41.201804
[0m17:41:41.202119 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:41.204803 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.205868 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.206110 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m17:41:41.206315 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.211565 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.211831 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.212036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m17:41:41.212660 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.214831 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.215041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m17:41:41.215356 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.217481 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.217751 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m17:41:41.218152 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.219086 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m17:41:41.219285 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.219459 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m17:41:41.219989 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.221528 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:41:41.221732 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m17:41:41.222174 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.222981 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 17:41:41.202251 => 17:41:41.222876
[0m17:41:41.223203 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m17:41:41.230916 [info ] [Thread-1  ]: 14 of 32 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.03s]
[0m17:41:41.231263 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:41:41.231485 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:41.231837 [info ] [Thread-1  ]: 15 of 32 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m17:41:41.232310 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m17:41:41.232518 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:41.235371 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.235873 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 17:41:41.232654 => 17:41:41.235771
[0m17:41:41.236076 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:41.238713 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.239217 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.239413 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m17:41:41.239588 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.245054 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.245353 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.245571 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m17:41:41.246196 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.248349 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.248592 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m17:41:41.248908 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.250636 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.250835 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m17:41:41.251138 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.252116 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m17:41:41.252402 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.252592 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m17:41:41.253197 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.254958 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:41:41.255160 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m17:41:41.255525 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.256392 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 17:41:41.236204 => 17:41:41.256300
[0m17:41:41.256606 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m17:41:41.264301 [info ] [Thread-1  ]: 15 of 32 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.03s]
[0m17:41:41.264689 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:41:41.264944 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:41.265254 [info ] [Thread-1  ]: 16 of 32 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m17:41:41.265627 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m17:41:41.265827 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:41.267925 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.268390 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 17:41:41.265960 => 17:41:41.268294
[0m17:41:41.268589 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:41.272103 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.272565 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.272760 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m17:41:41.272938 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.278398 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.278726 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.278998 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m17:41:41.280031 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.282323 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.282540 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m17:41:41.282853 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.284684 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.284916 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m17:41:41.285236 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.286156 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m17:41:41.286346 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.286521 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m17:41:41.287083 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.288624 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:41:41.288820 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m17:41:41.289159 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.289853 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 17:41:41.268717 => 17:41:41.289762
[0m17:41:41.290055 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m17:41:41.297211 [info ] [Thread-1  ]: 16 of 32 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.03s]
[0m17:41:41.297597 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:41:41.297844 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:41.298153 [info ] [Thread-1  ]: 17 of 32 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m17:41:41.298505 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m17:41:41.298697 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:41.300610 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.301069 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 17:41:41.298828 => 17:41:41.300973
[0m17:41:41.301259 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:41.304467 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.304960 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.305260 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m17:41:41.305482 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.310894 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.311176 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.311392 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m17:41:41.311913 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.314120 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.314332 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m17:41:41.314630 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.316777 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.317055 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m17:41:41.317454 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.318371 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m17:41:41.318585 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.318764 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m17:41:41.319295 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.320837 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:41:41.321054 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m17:41:41.321408 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.322265 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 17:41:41.301388 => 17:41:41.322169
[0m17:41:41.322494 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m17:41:41.329745 [info ] [Thread-1  ]: 17 of 32 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.03s]
[0m17:41:41.330077 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:41:41.330309 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m17:41:41.330731 [info ] [Thread-1  ]: 18 of 32 START sql table model main.dim_account ................................ [RUN]
[0m17:41:41.331256 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m17:41:41.331463 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m17:41:41.333559 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m17:41:41.334023 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 17:41:41.331605 => 17:41:41.333919
[0m17:41:41.334217 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m17:41:41.336849 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m17:41:41.337892 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m17:41:41.338167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m17:41:41.338356 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.343790 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.344073 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m17:41:41.344273 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."main"."dim_account__dbt_tmp"
  
    as (
      

WITH base AS (
    SELECT
        id AS account_id,
        name AS account_name,
        industry,
        type AS account_type,
        billing_city,
        billing_state,
        billing_country,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__account"
)
SELECT * FROM base;
    );
  
  
[0m17:41:41.345130 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 17:41:41.334354 => 17:41:41.344962
[0m17:41:41.345404 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: ROLLBACK
[0m17:41:41.346036 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_account'
[0m17:41:41.346219 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m17:41:41.348055 [debug] [Thread-1  ]: Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.348484 [error] [Thread-1  ]: 18 of 32 ERROR creating sql table model main.dim_account ....................... [[31mERROR[0m in 0.02s]
[0m17:41:41.348816 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m17:41:41.349058 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m17:41:41.349339 [info ] [Thread-1  ]: 19 of 32 START sql table model main.dim_campaign ............................... [RUN]
[0m17:41:41.349700 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m17:41:41.349889 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m17:41:41.352835 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m17:41:41.353404 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 17:41:41.350027 => 17:41:41.353268
[0m17:41:41.353623 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m17:41:41.356153 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m17:41:41.356584 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:41:41.356773 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m17:41:41.356947 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.362148 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.362581 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:41:41.362811 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."main"."dim_campaign__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__campaign"
)

SELECT
    id AS campaign_id,                       -- Surrogate Key
    name AS campaign_name,
    type AS campaign_type,
    status AS campaign_status,
    startdate AS campaign_start_date,
    enddate AS campaign_end_date,
    createddate AS campaign_created_date,
    lastmodifieddate AS campaign_last_modified_date
FROM source
    );
  
  
[0m17:41:41.363977 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 17:41:41.353750 => 17:41:41.363873
[0m17:41:41.364193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: ROLLBACK
[0m17:41:41.364754 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_campaign'
[0m17:41:41.364934 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m17:41:41.366900 [debug] [Thread-1  ]: Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.ownerid"
  LINE 19:     id AS campaign_id,                       -- Surrogate Key
      name AS campaign_name,
      type AS campaign_type,
      status AS campaign_status,
      startdate AS campaign_start_date,
      enddate AS campaign_end_date,
      createddate AS campaign_created_date,
      lastmodifieddate AS campaign_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.367382 [error] [Thread-1  ]: 19 of 32 ERROR creating sql table model main.dim_campaign ...................... [[31mERROR[0m in 0.02s]
[0m17:41:41.367734 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m17:41:41.367978 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m17:41:41.368254 [info ] [Thread-1  ]: 20 of 32 START sql table model main.fact_case .................................. [RUN]
[0m17:41:41.368620 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.fact_case)
[0m17:41:41.368811 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m17:41:41.371111 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m17:41:41.371636 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 17:41:41.368947 => 17:41:41.371532
[0m17:41:41.371838 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m17:41:41.374141 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m17:41:41.374536 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m17:41:41.374731 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m17:41:41.375036 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.380302 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.380567 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m17:41:41.380783 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."main"."fact_case__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__case"
)

SELECT
    id AS case_id,                           -- Surrogate Key
    accountid AS account_fk,                 -- Foreign Key to dim_account
    contactid AS contact_fk,                 -- Foreign Key to dim_contact
    ownerid AS user_fk,                      -- Foreign Key to dim_user
    status AS status_name,
    priority,
    origin,
    subject,
    createddate AS case_created_at,
    lastmodifieddate AS case_last_modified_date
FROM source
    );
  
  
[0m17:41:41.381805 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 17:41:41.371974 => 17:41:41.381707
[0m17:41:41.382016 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m17:41:41.382564 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m17:41:41.382755 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m17:41:41.384723 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.case_id", "source.assetid", "source.ownerid"
  LINE 19:     id AS case_id,                           -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      contactid AS contact_fk,                 -- Foreign Key to dim_contact
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      status AS status_name,
      priority,
      origin,
      subject,
      createddate AS case_created_at,
      lastmodifieddate AS case_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.385199 [error] [Thread-1  ]: 20 of 32 ERROR creating sql table model main.fact_case ......................... [[31mERROR[0m in 0.02s]
[0m17:41:41.385551 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m17:41:41.385842 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m17:41:41.386248 [info ] [Thread-1  ]: 21 of 32 START sql table model main.dim_case_status ............................ [RUN]
[0m17:41:41.386669 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.dim_case_status)
[0m17:41:41.386868 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m17:41:41.389028 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.389525 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 17:41:41.387002 => 17:41:41.389428
[0m17:41:41.389720 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m17:41:41.392885 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.393357 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.393546 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m17:41:41.393724 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.399077 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.399398 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.399613 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."main"."dim_case_status__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        status AS status_name,
        status AS status_description  -- Adjust as needed, typically a description field should be separate
    FROM "dbt"."staging"."stg_salesforce__case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY status_name) AS status_id,  -- Surrogate Key
    status_name,
    status_description
FROM source
    );
  
  
[0m17:41:41.402087 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.404357 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.404630 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m17:41:41.404975 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.405885 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m17:41:41.406077 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.406252 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m17:41:41.406812 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.408649 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:41:41.408884 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."main"."dim_case_status__dbt_backup" cascade
[0m17:41:41.409214 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.410000 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 17:41:41.389847 => 17:41:41.409902
[0m17:41:41.410212 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m17:41:41.418449 [info ] [Thread-1  ]: 21 of 32 OK created sql table model main.dim_case_status ....................... [[32mOK[0m in 0.03s]
[0m17:41:41.418852 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m17:41:41.419145 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m17:41:41.419443 [info ] [Thread-1  ]: 22 of 32 START sql table model main.dim_contact ................................ [RUN]
[0m17:41:41.419835 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m17:41:41.420038 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m17:41:41.422160 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m17:41:41.422654 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 17:41:41.420179 => 17:41:41.422554
[0m17:41:41.422869 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m17:41:41.425421 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m17:41:41.425891 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m17:41:41.426077 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m17:41:41.426250 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.431731 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.432010 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m17:41:41.432212 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

WITH base AS (
    SELECT
        id AS contact_id,
        account_id,
        first_name,
        last_name,
        email,
        phone,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__contact"
)
SELECT * FROM base;
    );
  
  
[0m17:41:41.432642 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 17:41:41.423005 => 17:41:41.432547
[0m17:41:41.432839 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m17:41:41.433364 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m17:41:41.433547 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m17:41:41.435467 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.435940 [error] [Thread-1  ]: 22 of 32 ERROR creating sql table model main.dim_contact ....................... [[31mERROR[0m in 0.02s]
[0m17:41:41.436290 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m17:41:41.436525 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m17:41:41.436910 [info ] [Thread-1  ]: 23 of 32 START sql view model main.dim_lead .................................... [RUN]
[0m17:41:41.437368 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m17:41:41.437583 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m17:41:41.439717 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m17:41:41.440263 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 17:41:41.437730 => 17:41:41.440157
[0m17:41:41.440459 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m17:41:41.443945 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m17:41:41.444568 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m17:41:41.444921 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m17:41:41.445139 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.450232 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.450493 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m17:41:41.450699 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
  create view "dbt"."main"."dim_lead__dbt_tmp" as (
    

WITH base AS (
    SELECT
        id AS lead_id,
        first_name,
        last_name,
        company,
        email,
        phone,
        status,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__lead"
)

SELECT * FROM base;
  );

[0m17:41:41.451116 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 17:41:41.440589 => 17:41:41.451022
[0m17:41:41.451317 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: ROLLBACK
[0m17:41:41.451841 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_lead'
[0m17:41:41.452022 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m17:41:41.453704 [debug] [Thread-1  ]: Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.454074 [error] [Thread-1  ]: 23 of 32 ERROR creating sql view model main.dim_lead ........................... [[31mERROR[0m in 0.02s]
[0m17:41:41.454369 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m17:41:41.454590 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m17:41:41.454969 [info ] [Thread-1  ]: 24 of 32 START sql table model main.dim_opportunity ............................ [RUN]
[0m17:41:41.455437 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m17:41:41.455645 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m17:41:41.458020 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:41:41.458545 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 17:41:41.455785 => 17:41:41.458433
[0m17:41:41.458752 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m17:41:41.461307 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:41:41.461867 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:41:41.462196 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m17:41:41.462380 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.467718 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.467997 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:41:41.468276 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity__dbt_tmp"
  
    as (
      



renamed AS (

    SELECT
        /* IDs */
        id AS opportunity_id,
        accountid AS account_fk,
        ownerid AS user_fk,
        contactid AS primary_contact_fk,
        campaignid AS campaign_fk,
        pricebook2id AS pricebook_fk,
        contractid AS contract_fk,
        primarypartneraccountid AS primary_partner_account_fk,
        createdbyid AS created_by_fk,
        lastmodifiedbyid AS last_modified_by_fk,
        lastamountchangedhistoryid AS last_amount_change_history_fk,
        lastclosedatechangedhistoryid AS last_close_date_change_history_fk,

        /* Metrics */
        amount,
        probability,
        expectedrevenue AS expected_revenue,
        totalopportunityquantity AS total_opportunity_quantity,

        /* Dimensions */
        name AS opportunity_name,
        description,
        stagename AS stage_name,
        stagesortorder AS stage_sort_order,
        type AS opportunity_type,
        nextstep AS next_step,
        leadsource AS lead_source,
        isclosed AS is_closed,
        iswon AS is_won,
        forecastcategory AS forecast_category,
        forecastcategoryname AS forecast_category_name,
        hasopportunitylineitem AS has_opportunity_line_item,
        deliveryinstallationstatus__c AS delivery_installation_status,
        trackingnumber__c AS tracking_number,
        ordernumber__c AS order_number,
        currentgenerators__c AS current_generators,
        maincompetitors__c AS main_competitors,

        /* Dates */
        closedate AS close_date,
        createddate AS created_at,
        lastmodifieddate AS last_modified_date,
        systemmodstamp AS system_mod_stamp,
        lastactivitydate AS last_activity_date,
        laststagechangedate AS last_stage_change_date,
        fiscalyear AS fiscal_year,
        fiscalquarter AS fiscal_quarter

    FROM "dbt"."staging"."stg_salesforce__opportunity"

)

SELECT
    /* IDs */
    opportunity_id,
    account_fk,
    user_fk,
    primary_contact_fk,
    campaign_fk,
    pricebook_fk,
    contract_fk,
    primary_partner_account_fk,
    created_by_fk,
    last_modified_by_fk,
    last_amount_change_history_fk,
    last_close_date_change_history_fk,

    /* Metrics */
    amount,
    probability,
    expected_revenue,
    total_opportunity_quantity,

    /* Dimensions */
    opportunity_name,
    description,
    stage_name,
    stage_sort_order,
    opportunity_type,
    next_step,
    lead_source,
    is_closed,
    is_won,
    forecast_category,
    forecast_category_name,
    has_opportunity_line_item,
    delivery_installation_status,
    tracking_number,
    order_number,
    current_generators,
    main_competitors,

    /* Dates */
    close_date,
    created_at,
    last_modified_date,
    system_mod_stamp,
    last_activity_date,
    last_stage_change_date,
    fiscal_year,
    fiscal_quarter

FROM renamed
    );
  
  
[0m17:41:41.468769 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 17:41:41.458885 => 17:41:41.468677
[0m17:41:41.468967 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: ROLLBACK
[0m17:41:41.469473 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_opportunity'
[0m17:41:41.469650 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m17:41:41.471661 [debug] [Thread-1  ]: Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Parser Error: syntax error at or near "renamed"
[0m17:41:41.472124 [error] [Thread-1  ]: 24 of 32 ERROR creating sql table model main.dim_opportunity ................... [[31mERROR[0m in 0.02s]
[0m17:41:41.472463 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m17:41:41.472714 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:41.473032 [info ] [Thread-1  ]: 25 of 32 START sql table model main.dim_opportunity_stage ...................... [RUN]
[0m17:41:41.473469 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m17:41:41.473710 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:41.475958 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.476458 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 17:41:41.473855 => 17:41:41.476356
[0m17:41:41.476653 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:41.479260 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.479773 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.479980 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m17:41:41.480169 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.485485 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.485805 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.486007 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        stagename AS stage_name,
        stagesortorder AS stage_sort_order
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY stage_sort_order) AS stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
FROM source
    );
  
  
[0m17:41:41.489490 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.492461 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.492698 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m17:41:41.493055 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.521544 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.521895 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m17:41:41.522340 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.523454 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m17:41:41.523657 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.523831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m17:41:41.524474 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.526104 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:41:41.526339 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."main"."dim_opportunity_stage__dbt_backup" cascade
[0m17:41:41.526739 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.527460 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 17:41:41.476783 => 17:41:41.527367
[0m17:41:41.527663 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m17:41:41.536726 [info ] [Thread-1  ]: 25 of 32 OK created sql table model main.dim_opportunity_stage ................. [[32mOK[0m in 0.06s]
[0m17:41:41.537101 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:41:41.537320 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m17:41:41.537668 [info ] [Thread-1  ]: 26 of 32 START sql table model main.fact_opportunity ........................... [RUN]
[0m17:41:41.538156 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.fact_opportunity)
[0m17:41:41.538371 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m17:41:41.540579 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m17:41:41.541051 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 17:41:41.538511 => 17:41:41.540952
[0m17:41:41.541256 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m17:41:41.543867 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m17:41:41.544319 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m17:41:41.544514 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m17:41:41.544697 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.550035 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.550301 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m17:41:41.550538 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."fact_opportunity__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    id AS opportunity_id,                    -- Surrogate Key
    accountid AS account_fk,                 -- Foreign Key to dim_account
    ownerid AS user_fk,                      -- Foreign Key to dim_user
    campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
    pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
    stagename AS stage_name,                 -- Captures the current stage
    stagesortorder AS stage_sort_order,
    amount,
    probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,
    closedate AS close_date,
    createddate AS opportunity_created_at,
    lastmodifieddate AS opportunity_last_modified_date,
    isclosed AS is_closed,
    iswon AS is_won
FROM source
    );
  
  
[0m17:41:41.551656 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 17:41:41.541390 => 17:41:41.551556
[0m17:41:41.551861 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m17:41:41.552449 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m17:41:41.552632 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m17:41:41.554654 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.iswon", "source.ownerid"
  LINE 19:     id AS opportunity_id,                    -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
      pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
      stagename AS stage_name,                 -- Captures the current stage
      stagesortorder AS stage_sort_order,
      amount,
      probability,
      expectedrevenue AS expected_revenue,
      totalopportunityquantity AS total_opportunity_quantity,
      closedate AS close_date,
      createddate AS opportunity_created_at,
      lastmodifieddate AS opportunity_last_modified_date,
      isclosed AS is_closed,
      iswon AS is_won
  FROM source
      );
    
    ...
               ^
[0m17:41:41.555141 [error] [Thread-1  ]: 26 of 32 ERROR creating sql table model main.fact_opportunity .................. [[31mERROR[0m in 0.02s]
[0m17:41:41.555506 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m17:41:41.555754 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:41.556071 [info ] [Thread-1  ]: 27 of 32 START sql table model main.fact_opportunity_history ................... [RUN]
[0m17:41:41.556497 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_opportunity_history)
[0m17:41:41.556713 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:41.558848 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:41:41.559360 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 17:41:41.556847 => 17:41:41.559255
[0m17:41:41.559557 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:41.562868 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:41:41.563703 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:41:41.563984 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m17:41:41.564174 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.569382 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.569659 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:41:41.569866 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."main"."fact_opportunity_history__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__opportunity_history"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY systemmodstamp) AS history_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                            -- Foreign Key to fact_opportunity
    stage_fk AS stage_fk,                                       -- Foreign Key to dim_opportunity_stage
    amount,
    probability,
    closedate AS close_date,
    systemmodstamp AS history_date
FROM source
    );
  
  
[0m17:41:41.571273 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 17:41:41.559693 => 17:41:41.571112
[0m17:41:41.571543 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m17:41:41.572101 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m17:41:41.572291 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m17:41:41.574305 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Column "stage_fk" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m17:41:41.574718 [error] [Thread-1  ]: 27 of 32 ERROR creating sql table model main.fact_opportunity_history .......... [[31mERROR[0m in 0.02s]
[0m17:41:41.575055 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:41:41.575294 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m17:41:41.575570 [info ] [Thread-1  ]: 28 of 32 START sql table model main.dim_pricebook .............................. [RUN]
[0m17:41:41.575937 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity_history, now model.elastic_dbt_interview.dim_pricebook)
[0m17:41:41.576130 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m17:41:41.578266 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m17:41:41.578757 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 17:41:41.576262 => 17:41:41.578656
[0m17:41:41.578952 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m17:41:41.581443 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m17:41:41.581922 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m17:41:41.582119 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m17:41:41.582304 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.587640 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.587958 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m17:41:41.588157 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."main"."dim_pricebook__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        pricebook2id AS pricebook_id,        -- Surrogate Key
        name AS pricebook_name,
        isactive AS is_active,
        description,
        createddate AS pricebook_created_date,
        lastmodifieddate AS pricebook_last_modified_date
    FROM "dbt"."staging"."stg_salesforce__pricebook_entry"
)

SELECT * FROM source
    );
  
  
[0m17:41:41.588912 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 17:41:41.579077 => 17:41:41.588813
[0m17:41:41.589117 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: ROLLBACK
[0m17:41:41.589649 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_pricebook'
[0m17:41:41.589881 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m17:41:41.591748 [debug] [Thread-1  ]: Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m17:41:41.592178 [error] [Thread-1  ]: 28 of 32 ERROR creating sql table model main.dim_pricebook ..................... [[31mERROR[0m in 0.02s]
[0m17:41:41.592505 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m17:41:41.592732 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m17:41:41.593067 [info ] [Thread-1  ]: 29 of 32 START sql table model main.dim_product ................................ [RUN]
[0m17:41:41.593512 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m17:41:41.593722 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m17:41:41.595958 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m17:41:41.596443 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 17:41:41.593861 => 17:41:41.596340
[0m17:41:41.596641 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m17:41:41.599300 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m17:41:41.600174 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m17:41:41.600486 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m17:41:41.600700 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.606217 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.606448 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m17:41:41.606643 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."main"."dim_product__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__product_2"
)

SELECT
    id AS product_id,                        -- Surrogate Key
    name AS product_name,
    productcode AS product_code,
    description,
    isactive AS is_active,
    createddate AS product_created_date,
    lastmodifieddate AS product_last_modified_date
FROM source
    );
  
  
[0m17:41:41.607511 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 17:41:41.596773 => 17:41:41.607406
[0m17:41:41.607721 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: ROLLBACK
[0m17:41:41.608248 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_product'
[0m17:41:41.608432 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m17:41:41.610173 [debug] [Thread-1  ]: Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS product_id,                        -- Surrogate Key
      name AS product_name,
      productcode AS product_code,
      description,
      isactive AS is_active,
      createddate AS product_created_date,
      lastmodifieddate AS product_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.610524 [error] [Thread-1  ]: 29 of 32 ERROR creating sql table model main.dim_product ....................... [[31mERROR[0m in 0.02s]
[0m17:41:41.610832 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m17:41:41.611046 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m17:41:41.611454 [info ] [Thread-1  ]: 30 of 32 START sql table model main.dim_solution ............................... [RUN]
[0m17:41:41.611956 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_solution)
[0m17:41:41.612169 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m17:41:41.614359 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m17:41:41.614911 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 17:41:41.612306 => 17:41:41.614770
[0m17:41:41.615111 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m17:41:41.618500 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m17:41:41.618989 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m17:41:41.619175 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m17:41:41.619352 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.624512 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.624773 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m17:41:41.625115 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."main"."dim_solution__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__solution"
)

SELECT
    id AS solution_id,                       -- Surrogate Key
    solutionname AS solution_name,
    status,
    description,
    createddate AS solution_created_date,
    lastmodifieddate AS solution_last_modified_date
FROM source
    );
  
  
[0m17:41:41.626037 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 17:41:41.615248 => 17:41:41.625924
[0m17:41:41.626256 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: ROLLBACK
[0m17:41:41.626819 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_solution'
[0m17:41:41.627029 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m17:41:41.628889 [debug] [Thread-1  ]: Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.caseid", "source.ownerid"
  LINE 19:     id AS solution_id,                       -- Surrogate Key
      solutionname AS solution_name,
      status,
      description,
      createddate AS solution_created_date,
      lastmodifieddate AS solution_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.629284 [error] [Thread-1  ]: 30 of 32 ERROR creating sql table model main.dim_solution ...................... [[31mERROR[0m in 0.02s]
[0m17:41:41.629620 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m17:41:41.629872 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m17:41:41.630139 [info ] [Thread-1  ]: 31 of 32 START sql table model main.dim_user ................................... [RUN]
[0m17:41:41.630500 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m17:41:41.630690 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m17:41:41.632819 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m17:41:41.633288 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 17:41:41.630824 => 17:41:41.633194
[0m17:41:41.633479 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m17:41:41.635795 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m17:41:41.636358 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m17:41:41.636574 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m17:41:41.636888 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.642197 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.642473 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m17:41:41.642669 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

WITH base AS (
    SELECT
        id AS user_id,
        username,
        first_name,
        last_name,
        email,
        role_id,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__user"
)
SELECT * FROM base;
    );
  
  
[0m17:41:41.643087 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 17:41:41.633607 => 17:41:41.642992
[0m17:41:41.643287 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m17:41:41.643812 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m17:41:41.643989 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m17:41:41.645994 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.646432 [error] [Thread-1  ]: 31 of 32 ERROR creating sql table model main.dim_user .......................... [[31mERROR[0m in 0.02s]
[0m17:41:41.646764 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m17:41:41.647006 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user_role
[0m17:41:41.647288 [info ] [Thread-1  ]: 32 of 32 START sql table model main.dim_user_role .............................. [RUN]
[0m17:41:41.647654 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.dim_user_role)
[0m17:41:41.647856 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user_role
[0m17:41:41.650123 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user_role"
[0m17:41:41.650684 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (compile): 17:41:41.647986 => 17:41:41.650578
[0m17:41:41.650880 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user_role
[0m17:41:41.653247 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user_role"
[0m17:41:41.653633 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m17:41:41.653940 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: BEGIN
[0m17:41:41.654190 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:41:41.659467 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:41:41.659778 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m17:41:41.659981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user_role"} */

  
    
    

    create  table
      "dbt"."main"."dim_user_role__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__user_role"
)

SELECT
    id AS role_id,                           -- Surrogate Key
    name AS role_name,
    roledescription AS role_description
FROM source
    );
  
  
[0m17:41:41.660868 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (execute): 17:41:41.651013 => 17:41:41.660763
[0m17:41:41.661085 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: ROLLBACK
[0m17:41:41.661633 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user_role'
[0m17:41:41.661815 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: Close
[0m17:41:41.663680 [debug] [Thread-1  ]: Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS role_id,                           -- Surrogate Key
      name AS role_name,
      roledescription AS role_description
  FROM source
      );
    
    ...
               ^
[0m17:41:41.664057 [error] [Thread-1  ]: 32 of 32 ERROR creating sql table model main.dim_user_role ..................... [[31mERROR[0m in 0.02s]
[0m17:41:41.664382 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user_role
[0m17:41:41.665195 [debug] [MainThread]: Using duckdb connection "master"
[0m17:41:41.665369 [debug] [MainThread]: On master: BEGIN
[0m17:41:41.665517 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:41:41.670770 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:41:41.671061 [debug] [MainThread]: On master: COMMIT
[0m17:41:41.671245 [debug] [MainThread]: Using duckdb connection "master"
[0m17:41:41.671398 [debug] [MainThread]: On master: COMMIT
[0m17:41:41.671604 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:41:41.671762 [debug] [MainThread]: On master: Close
[0m17:41:41.673405 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:41:41.673646 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user_role' was properly closed.
[0m17:41:41.673895 [info ] [MainThread]: 
[0m17:41:41.674078 [info ] [MainThread]: Finished running 17 table models, 15 view models in 0 hours 0 minutes and 1.12 seconds (1.12s).
[0m17:41:41.676322 [debug] [MainThread]: Command end result
[0m17:41:41.685459 [info ] [MainThread]: 
[0m17:41:41.685765 [info ] [MainThread]: [31mCompleted with 15 errors and 0 warnings:[0m
[0m17:41:41.686017 [info ] [MainThread]: 
[0m17:41:41.686283 [error] [MainThread]:   Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m17:41:41.686532 [info ] [MainThread]: 
[0m17:41:41.686732 [error] [MainThread]:   Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.686918 [info ] [MainThread]: 
[0m17:41:41.687067 [error] [MainThread]:   Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.687217 [info ] [MainThread]: 
[0m17:41:41.687377 [error] [MainThread]:   Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.ownerid"
  LINE 19:     id AS campaign_id,                       -- Surrogate Key
      name AS campaign_name,
      type AS campaign_type,
      status AS campaign_status,
      startdate AS campaign_start_date,
      enddate AS campaign_end_date,
      createddate AS campaign_created_date,
      lastmodifieddate AS campaign_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.687537 [info ] [MainThread]: 
[0m17:41:41.687701 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.case_id", "source.assetid", "source.ownerid"
  LINE 19:     id AS case_id,                           -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      contactid AS contact_fk,                 -- Foreign Key to dim_contact
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      status AS status_name,
      priority,
      origin,
      subject,
      createddate AS case_created_at,
      lastmodifieddate AS case_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.687867 [info ] [MainThread]: 
[0m17:41:41.688012 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.688154 [info ] [MainThread]: 
[0m17:41:41.688297 [error] [MainThread]:   Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.688444 [info ] [MainThread]: 
[0m17:41:41.688586 [error] [MainThread]:   Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Parser Error: syntax error at or near "renamed"
[0m17:41:41.688729 [info ] [MainThread]: 
[0m17:41:41.688900 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.iswon", "source.ownerid"
  LINE 19:     id AS opportunity_id,                    -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
      pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
      stagename AS stage_name,                 -- Captures the current stage
      stagesortorder AS stage_sort_order,
      amount,
      probability,
      expectedrevenue AS expected_revenue,
      totalopportunityquantity AS total_opportunity_quantity,
      closedate AS close_date,
      createddate AS opportunity_created_at,
      lastmodifieddate AS opportunity_last_modified_date,
      isclosed AS is_closed,
      iswon AS is_won
  FROM source
      );
    
    ...
               ^
[0m17:41:41.689204 [info ] [MainThread]: 
[0m17:41:41.689415 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Column "stage_fk" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m17:41:41.689586 [info ] [MainThread]: 
[0m17:41:41.689774 [error] [MainThread]:   Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m17:41:41.689942 [info ] [MainThread]: 
[0m17:41:41.690110 [error] [MainThread]:   Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS product_id,                        -- Surrogate Key
      name AS product_name,
      productcode AS product_code,
      description,
      isactive AS is_active,
      createddate AS product_created_date,
      lastmodifieddate AS product_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.690283 [info ] [MainThread]: 
[0m17:41:41.690449 [error] [MainThread]:   Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.caseid", "source.ownerid"
  LINE 19:     id AS solution_id,                       -- Surrogate Key
      solutionname AS solution_name,
      status,
      description,
      createddate AS solution_created_date,
      lastmodifieddate AS solution_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:41:41.690615 [info ] [MainThread]: 
[0m17:41:41.690806 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m17:41:41.690986 [info ] [MainThread]: 
[0m17:41:41.691151 [error] [MainThread]:   Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS role_id,                           -- Surrogate Key
      name AS role_name,
      roledescription AS role_description
  FROM source
      );
    
    ...
               ^
[0m17:41:41.691335 [info ] [MainThread]: 
[0m17:41:41.691523 [info ] [MainThread]: Done. PASS=17 WARN=0 ERROR=15 SKIP=0 TOTAL=32
[0m17:41:41.691912 [debug] [MainThread]: Command `dbt run` failed at 17:41:41.691850 after 1.34 seconds
[0m17:41:41.692132 [debug] [MainThread]: Flushing usage events


============================== 17:47:47.440212 | 1c681bd4-4916-4c64-8bef-2e58d3a55d8e ==============================
[0m17:47:47.440212 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:47:47.443229 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m17:47:47.443500 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:47:47.527915 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:47:47.548562 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:47:47.597658 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 5 files changed.
[0m17:47:47.598108 [debug] [MainThread]: Partial parsing: deleted file: elastic_dbt_interview://models/mart/mart_sales_performance.sql
[0m17:47:47.598396 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_campaign.sql
[0m17:47:47.598640 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_solution.sql
[0m17:47:47.598848 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m17:47:47.599055 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user_role.sql
[0m17:47:47.599263 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_account.sql
[0m17:47:47.635862 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m17:47:47.651586 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m17:47:47.653456 [info ] [MainThread]: 
[0m17:47:47.653938 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:47:47.655242 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m17:47:47.662357 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:47:47.662655 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:47:47.662835 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:47:47.674036 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.675019 [debug] [ThreadPool]: On list_dbt: Close
[0m17:47:47.678216 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:47:47.678494 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:47:47.678711 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:47:47.685475 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.686238 [debug] [ThreadPool]: On list_dbt: Close
[0m17:47:47.688136 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m17:47:47.688696 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m17:47:47.692395 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:47:47.692633 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m17:47:47.692806 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:47:47.699221 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.699464 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:47:47.699632 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m17:47:47.699869 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.700361 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:47:47.700535 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:47:47.700690 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:47:47.700964 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.701223 [debug] [ThreadPool]: On create_dbt_main: Close
[0m17:47:47.703207 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now create_dbt_staging)
[0m17:47:47.703849 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m17:47:47.706537 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:47:47.706842 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m17:47:47.707044 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:47:47.713260 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.713501 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:47:47.713867 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m17:47:47.714254 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.714947 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m17:47:47.715175 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m17:47:47.715370 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m17:47:47.715717 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.715962 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m17:47:47.719464 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_main)
[0m17:47:47.723896 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:47:47.724172 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m17:47:47.724398 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:47:47.731085 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.731262 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:47:47.731426 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m17:47:47.747501 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.751610 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m17:47:47.752324 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m17:47:47.752583 [debug] [ThreadPool]: On list_dbt_main: Close
[0m17:47:47.754968 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m17:47:47.757534 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:47:47.757767 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m17:47:47.757936 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:47:47.763698 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.763948 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:47:47.764157 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m17:47:47.782986 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:47:47.783985 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m17:47:47.784222 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m17:47:47.784381 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m17:47:47.787530 [debug] [MainThread]: Using duckdb connection "master"
[0m17:47:47.787795 [debug] [MainThread]: On master: BEGIN
[0m17:47:47.787972 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:47:47.793558 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:47:47.793832 [debug] [MainThread]: On master: COMMIT
[0m17:47:47.794008 [debug] [MainThread]: Using duckdb connection "master"
[0m17:47:47.794170 [debug] [MainThread]: On master: COMMIT
[0m17:47:47.794385 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:47:47.794546 [debug] [MainThread]: On master: Close
[0m17:47:47.796076 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:47:47.796327 [info ] [MainThread]: 
[0m17:47:47.798099 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:47:47.798463 [info ] [Thread-1  ]: 1 of 32 START sql table model main.fact_campaign_performance ................... [RUN]
[0m17:47:47.798991 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_campaign_performance)
[0m17:47:47.799212 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m17:47:47.804799 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:47:47.805936 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 17:47:47.799357 => 17:47:47.805804
[0m17:47:47.806173 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m17:47:47.825810 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:47:47.826556 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:47:47.826770 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: BEGIN
[0m17:47:47.826962 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:47.832358 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.832617 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m17:47:47.832831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */

  
    
    

    create  table
      "dbt"."main"."fact_campaign_performance__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."campaign"  -- Hypothetical source for campaign performance metrics
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS campaign_performance_id,  -- Surrogate Key
    campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
    leads_generated,
    opportunities_created,
    revenue_generated,
    expenses,
    createddate AS performance_created_at
FROM source
    );
  
  
[0m17:47:47.833888 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 17:47:47.806328 => 17:47:47.833788
[0m17:47:47.834128 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: ROLLBACK
[0m17:47:47.838017 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_campaign_performance'
[0m17:47:47.838314 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: Close
[0m17:47:47.840262 [debug] [Thread-1  ]: Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m17:47:47.840692 [error] [Thread-1  ]: 1 of 32 ERROR creating sql table model main.fact_campaign_performance .......... [[31mERROR[0m in 0.04s]
[0m17:47:47.841073 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m17:47:47.841330 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m17:47:47.841623 [info ] [Thread-1  ]: 2 of 32 START sql table model main.fact_case_history ........................... [RUN]
[0m17:47:47.842023 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_case_history)
[0m17:47:47.842234 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m17:47:47.870193 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.871229 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 17:47:47.842392 => 17:47:47.871053
[0m17:47:47.871504 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m17:47:47.874212 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.874705 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.874921 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: BEGIN
[0m17:47:47.875111 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:47.880698 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.880983 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.881185 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */

  
    
    

    create  table
      "dbt"."main"."fact_case_history__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY systemmodstamp) AS case_history_id,  -- Surrogate Key
    caseid AS case_fk,                                               -- Foreign Key to fact_case
    status,
    systemmodstamp AS history_date
FROM source
    );
  
  
[0m17:47:47.882431 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.886388 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.886631 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
alter table "dbt"."main"."fact_case_history" rename to "fact_case_history__dbt_backup"
[0m17:47:47.887004 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.888812 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.889039 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
alter table "dbt"."main"."fact_case_history__dbt_tmp" rename to "fact_case_history"
[0m17:47:47.889342 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.899885 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: COMMIT
[0m17:47:47.900127 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.900320 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: COMMIT
[0m17:47:47.901000 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.903886 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m17:47:47.904105 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */
drop table if exists "dbt"."main"."fact_case_history__dbt_backup" cascade
[0m17:47:47.904499 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.905307 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 17:47:47.871652 => 17:47:47.905211
[0m17:47:47.905536 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: Close
[0m17:47:47.922397 [info ] [Thread-1  ]: 2 of 32 OK created sql table model main.fact_case_history ...................... [[32mOK[0m in 0.08s]
[0m17:47:47.922813 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m17:47:47.923054 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m17:47:47.923351 [info ] [Thread-1  ]: 3 of 32 START sql table model main.fact_product_sales .......................... [RUN]
[0m17:47:47.923772 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_product_sales)
[0m17:47:47.923989 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m17:47:47.926917 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m17:47:47.927417 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 17:47:47.924124 => 17:47:47.927313
[0m17:47:47.927623 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m17:47:47.930388 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_product_sales"
[0m17:47:47.939978 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m17:47:47.940338 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: BEGIN
[0m17:47:47.940542 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:47.946266 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.946529 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m17:47:47.946747 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_product_sales"} */

  
    
    

    create  table
      "dbt"."main"."fact_product_sales__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."pricebook_entry"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS product_sales_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
    product2id AS product_fk,                                      -- Foreign Key to dim_product
    pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
    unitprice AS unit_price,
    isactive AS is_active,
    createddate AS product_sales_created_at,
    lastmodifieddate AS product_sales_last_modified_date
FROM source
    );
  
  
[0m17:47:47.947431 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 17:47:47.927758 => 17:47:47.947331
[0m17:47:47.947633 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: ROLLBACK
[0m17:47:47.948190 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_product_sales'
[0m17:47:47.948377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: Close
[0m17:47:47.950459 [debug] [Thread-1  ]: Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:47.950962 [error] [Thread-1  ]: 3 of 32 ERROR creating sql table model main.fact_product_sales ................. [[31mERROR[0m in 0.03s]
[0m17:47:47.951307 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m17:47:47.951544 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:47:47.951769 [info ] [Thread-1  ]: 4 of 32 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m17:47:47.952208 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m17:47:47.952439 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m17:47:47.954606 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.955412 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 17:47:47.952588 => 17:47:47.955301
[0m17:47:47.955631 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m17:47:47.966168 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.967008 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.967301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m17:47:47.967515 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:47.973501 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.973831 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.974084 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m17:47:47.974944 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.977315 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.977558 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m17:47:47.977906 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.980702 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.980973 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m17:47:47.981346 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.982212 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m17:47:47.982417 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.982592 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m17:47:47.983352 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.985190 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m17:47:47.985420 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m17:47:47.985815 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:47.986586 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 17:47:47.955791 => 17:47:47.986491
[0m17:47:47.986801 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m17:47:47.994739 [info ] [Thread-1  ]: 4 of 32 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.04s]
[0m17:47:47.995119 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m17:47:47.995370 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:47:47.995734 [info ] [Thread-1  ]: 5 of 32 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m17:47:47.996138 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m17:47:47.996355 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:47:47.998523 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:47.999093 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 17:47:47.996500 => 17:47:47.998988
[0m17:47:47.999299 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:47:48.002074 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.002541 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.002730 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m17:47:48.002907 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.008387 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.008714 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.009001 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m17:47:48.009927 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.012146 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.012398 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m17:47:48.012747 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.015193 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.015409 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m17:47:48.015730 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.016936 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m17:47:48.017228 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.017431 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m17:47:48.018154 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.019742 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m17:47:48.019972 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m17:47:48.020389 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.021308 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 17:47:47.999427 => 17:47:48.021209
[0m17:47:48.021567 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m17:47:48.029333 [info ] [Thread-1  ]: 5 of 32 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.03s]
[0m17:47:48.029774 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m17:47:48.030033 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:47:48.030382 [info ] [Thread-1  ]: 6 of 32 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m17:47:48.030788 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m17:47:48.030981 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m17:47:48.033097 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.033621 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 17:47:48.031119 => 17:47:48.033522
[0m17:47:48.033831 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m17:47:48.036545 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.036981 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.037173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m17:47:48.037347 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.042884 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.043099 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.043324 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m17:47:48.044002 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.046044 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.046266 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m17:47:48.046596 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.048288 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.048490 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m17:47:48.048777 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.049680 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m17:47:48.049871 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.050041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m17:47:48.050708 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.052944 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m17:47:48.053218 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m17:47:48.053665 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.054674 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 17:47:48.034072 => 17:47:48.054571
[0m17:47:48.054932 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m17:47:48.062988 [info ] [Thread-1  ]: 6 of 32 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.03s]
[0m17:47:48.063380 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m17:47:48.063614 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:47:48.064016 [info ] [Thread-1  ]: 7 of 32 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m17:47:48.064505 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m17:47:48.064724 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:47:48.066720 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.067573 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 17:47:48.064866 => 17:47:48.067458
[0m17:47:48.067791 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:47:48.070441 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.070917 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.071119 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m17:47:48.071293 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.076555 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.076833 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.077035 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m17:47:48.077461 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.079747 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.080089 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m17:47:48.080498 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.082455 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.082696 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m17:47:48.083013 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.083887 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m17:47:48.084079 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.084252 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m17:47:48.084861 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.086733 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m17:47:48.086958 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m17:47:48.087368 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.088955 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 17:47:48.067922 => 17:47:48.088864
[0m17:47:48.089169 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m17:47:48.097056 [info ] [Thread-1  ]: 7 of 32 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.03s]
[0m17:47:48.097466 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m17:47:48.097707 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:47:48.098023 [info ] [Thread-1  ]: 8 of 32 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m17:47:48.098379 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m17:47:48.098573 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:47:48.100561 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.101091 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 17:47:48.098700 => 17:47:48.100976
[0m17:47:48.101285 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:47:48.103948 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.104392 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.104582 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m17:47:48.104757 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.110423 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.110696 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.110930 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m17:47:48.111706 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.113871 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.114079 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m17:47:48.114378 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.116081 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.116282 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m17:47:48.116565 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.117390 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m17:47:48.117579 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.117748 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m17:47:48.118476 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.120464 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m17:47:48.120715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m17:47:48.121140 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.122077 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 17:47:48.101411 => 17:47:48.121976
[0m17:47:48.122301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m17:47:48.130227 [info ] [Thread-1  ]: 8 of 32 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.03s]
[0m17:47:48.130645 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m17:47:48.130891 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:47:48.131226 [info ] [Thread-1  ]: 9 of 32 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m17:47:48.131583 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m17:47:48.131781 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:47:48.134567 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.135037 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 17:47:48.131918 => 17:47:48.134942
[0m17:47:48.135233 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:47:48.137921 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.138383 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.138577 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m17:47:48.138759 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.143996 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.144259 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.144498 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m17:47:48.145296 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.147468 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.147676 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m17:47:48.147975 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.149685 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.149879 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m17:47:48.150162 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.151053 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m17:47:48.151236 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.151404 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m17:47:48.152040 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.153971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m17:47:48.154189 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m17:47:48.154624 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.155448 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 17:47:48.135360 => 17:47:48.155345
[0m17:47:48.155660 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m17:47:48.163248 [info ] [Thread-1  ]: 9 of 32 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.03s]
[0m17:47:48.163630 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m17:47:48.163882 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:47:48.164210 [info ] [Thread-1  ]: 10 of 32 START sql view model staging.stg_salesforce__opportunity .............. [RUN]
[0m17:47:48.164561 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m17:47:48.164757 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:47:48.166806 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.167309 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 17:47:48.164886 => 17:47:48.167210
[0m17:47:48.167509 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:47:48.171041 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.171486 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.171680 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m17:47:48.171864 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.177126 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.177373 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.177599 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m17:47:48.178305 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.180388 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.180598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m17:47:48.180893 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.182594 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.182803 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m17:47:48.183089 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.183918 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m17:47:48.184121 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.184292 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m17:47:48.184946 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.186881 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m17:47:48.187102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m17:47:48.187542 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.188383 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 17:47:48.167642 => 17:47:48.188273
[0m17:47:48.188620 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m17:47:48.196384 [info ] [Thread-1  ]: 10 of 32 OK created sql view model staging.stg_salesforce__opportunity ......... [[32mOK[0m in 0.03s]
[0m17:47:48.196767 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m17:47:48.196992 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:47:48.197425 [info ] [Thread-1  ]: 11 of 32 START sql view model staging.stg_salesforce__opportunity_history ...... [RUN]
[0m17:47:48.197893 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m17:47:48.198122 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:47:48.200123 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.200589 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 17:47:48.198258 => 17:47:48.200491
[0m17:47:48.200795 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:47:48.204057 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.204486 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.204677 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m17:47:48.204851 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.210328 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.210587 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.210802 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m17:47:48.211437 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.213903 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.214137 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m17:47:48.214517 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.216632 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.216875 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m17:47:48.217261 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.218219 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m17:47:48.218442 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.218635 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m17:47:48.219259 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.220951 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m17:47:48.221188 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m17:47:48.221555 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.222291 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 17:47:48.200932 => 17:47:48.222196
[0m17:47:48.222504 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m17:47:48.230236 [info ] [Thread-1  ]: 11 of 32 OK created sql view model staging.stg_salesforce__opportunity_history . [[32mOK[0m in 0.03s]
[0m17:47:48.230624 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m17:47:48.230878 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:47:48.231213 [info ] [Thread-1  ]: 12 of 32 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m17:47:48.231573 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m17:47:48.231763 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:47:48.233777 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.234297 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 17:47:48.231898 => 17:47:48.234172
[0m17:47:48.234520 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:47:48.237426 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.237953 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.238174 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m17:47:48.238362 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.244059 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.244322 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.244529 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m17:47:48.244977 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.248108 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.248346 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m17:47:48.248664 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.250378 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.250580 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m17:47:48.250880 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.252069 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m17:47:48.252281 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.252454 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m17:47:48.253138 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.254855 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m17:47:48.255062 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m17:47:48.255487 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.256639 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 17:47:48.234658 => 17:47:48.256520
[0m17:47:48.256906 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m17:47:48.264510 [info ] [Thread-1  ]: 12 of 32 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.03s]
[0m17:47:48.264935 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m17:47:48.265190 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:47:48.265526 [info ] [Thread-1  ]: 13 of 32 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m17:47:48.265888 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m17:47:48.266082 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:47:48.268278 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.268816 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 17:47:48.266214 => 17:47:48.268717
[0m17:47:48.269011 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:47:48.271627 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.272009 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.272193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m17:47:48.272367 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.277782 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.278069 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.278280 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m17:47:48.278812 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.281654 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.281869 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m17:47:48.282205 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.283906 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.284101 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m17:47:48.284488 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.285460 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m17:47:48.285812 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.286034 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m17:47:48.286659 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.288585 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m17:47:48.288854 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m17:47:48.289321 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.290212 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 17:47:48.269141 => 17:47:48.290111
[0m17:47:48.290443 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m17:47:48.298229 [info ] [Thread-1  ]: 13 of 32 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.03s]
[0m17:47:48.298639 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m17:47:48.298887 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:47:48.299243 [info ] [Thread-1  ]: 14 of 32 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m17:47:48.299639 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m17:47:48.299846 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:47:48.301956 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.302418 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 17:47:48.299998 => 17:47:48.302320
[0m17:47:48.302616 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:47:48.305419 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.305903 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.306095 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m17:47:48.306271 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.311508 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.311804 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.312015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m17:47:48.312481 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.314651 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.314865 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m17:47:48.315170 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.317577 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.317777 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m17:47:48.318077 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.318942 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m17:47:48.319140 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.319471 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m17:47:48.320126 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.322017 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m17:47:48.322346 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m17:47:48.322784 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.323667 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 17:47:48.302745 => 17:47:48.323564
[0m17:47:48.323890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m17:47:48.331951 [info ] [Thread-1  ]: 14 of 32 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.03s]
[0m17:47:48.332358 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m17:47:48.332603 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:47:48.332875 [info ] [Thread-1  ]: 15 of 32 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m17:47:48.333239 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m17:47:48.333429 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:47:48.335504 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.335988 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 17:47:48.333560 => 17:47:48.335888
[0m17:47:48.336192 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:47:48.338833 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.339282 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.339467 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m17:47:48.339645 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.344949 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.345227 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.345436 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m17:47:48.345933 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.348153 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.348378 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m17:47:48.348719 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.351078 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.351296 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m17:47:48.351620 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.352467 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m17:47:48.352663 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.352835 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m17:47:48.353599 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.355551 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m17:47:48.355860 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m17:47:48.356319 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.357153 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 17:47:48.336320 => 17:47:48.357052
[0m17:47:48.357368 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m17:47:48.365238 [info ] [Thread-1  ]: 15 of 32 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.03s]
[0m17:47:48.365603 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m17:47:48.365823 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:47:48.366235 [info ] [Thread-1  ]: 16 of 32 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m17:47:48.366716 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m17:47:48.366925 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m17:47:48.369144 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.369638 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 17:47:48.367066 => 17:47:48.369534
[0m17:47:48.369838 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m17:47:48.372492 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.372978 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.373179 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m17:47:48.373356 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.378594 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.378864 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.379115 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m17:47:48.380097 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.382390 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.382614 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m17:47:48.382927 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.384653 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.384857 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m17:47:48.385185 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.386089 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m17:47:48.386463 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.386696 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m17:47:48.387384 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.389854 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m17:47:48.417313 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m17:47:48.417857 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.418706 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 17:47:48.369971 => 17:47:48.418612
[0m17:47:48.418924 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m17:47:48.426422 [info ] [Thread-1  ]: 16 of 32 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.06s]
[0m17:47:48.426798 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m17:47:48.427044 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:47:48.427360 [info ] [Thread-1  ]: 17 of 32 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m17:47:48.427715 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m17:47:48.427919 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:47:48.429960 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.430473 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 17:47:48.428050 => 17:47:48.430379
[0m17:47:48.430665 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:47:48.433281 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.433798 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.434029 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m17:47:48.434245 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.439610 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.439858 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.440061 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m17:47:48.440575 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.442880 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.443136 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m17:47:48.443509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.445277 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.445478 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m17:47:48.445781 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.446667 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m17:47:48.446853 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.447021 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m17:47:48.447635 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.449671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m17:47:48.449913 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m17:47:48.450395 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.451219 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 17:47:48.430796 => 17:47:48.451122
[0m17:47:48.451427 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m17:47:48.459650 [info ] [Thread-1  ]: 17 of 32 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.03s]
[0m17:47:48.461344 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m17:47:48.461684 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m17:47:48.462335 [info ] [Thread-1  ]: 18 of 32 START sql table model main.dim_account ................................ [RUN]
[0m17:47:48.462862 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m17:47:48.463122 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m17:47:48.466616 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m17:47:48.467669 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 17:47:48.463274 => 17:47:48.467538
[0m17:47:48.467943 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m17:47:48.471188 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m17:47:48.471944 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m17:47:48.472238 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m17:47:48.472448 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.480931 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.481375 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m17:47:48.481610 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."main"."dim_account__dbt_tmp"
  
    as (
      


select
    id as account_id,
    name as account_name,
    industry,
    type as account_type,
    billing_city,
    billing_state,
    billing_country,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__account"
    );
  
  
[0m17:47:48.483143 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 17:47:48.468554 => 17:47:48.483021
[0m17:47:48.483379 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: ROLLBACK
[0m17:47:48.483969 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_account'
[0m17:47:48.484188 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m17:47:48.486317 [debug] [Thread-1  ]: Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__account.sic", "stg_salesforce__account.site", "stg_salesforce__account.fax", "stg_salesforce__account.sicdesc", "stg_salesforce__account.ownerid"
  LINE 15:     id as account_id,
               ^
[0m17:47:48.486756 [error] [Thread-1  ]: 18 of 32 ERROR creating sql table model main.dim_account ....................... [[31mERROR[0m in 0.02s]
[0m17:47:48.487094 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m17:47:48.487330 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m17:47:48.487750 [info ] [Thread-1  ]: 19 of 32 START sql table model main.dim_campaign ............................... [RUN]
[0m17:47:48.488292 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m17:47:48.488529 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m17:47:48.490900 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.491469 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 17:47:48.488685 => 17:47:48.491355
[0m17:47:48.491692 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m17:47:48.494663 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.495286 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.495513 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m17:47:48.495707 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.501706 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.502048 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.502279 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."main"."dim_campaign__dbt_tmp"
  
    as (
      


select
    campaign_id,
    name as campaign_name,
    type as campaign_type,
    status as campaign_status,
    startdate as campaign_start_date,
    enddate as campaign_end_date,
    createddate as campaign_created_date,
    lastmodifieddate as campaign_last_modified_date
from "dbt"."staging"."stg_salesforce__campaign"
    );
  
  
[0m17:47:48.504161 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.506485 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.506733 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."main"."dim_campaign" rename to "dim_campaign__dbt_backup"
[0m17:47:48.507130 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.509971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.510224 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."main"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m17:47:48.510550 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.511533 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m17:47:48.511724 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.511894 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m17:47:48.512601 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.514527 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m17:47:48.514754 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."main"."dim_campaign__dbt_backup" cascade
[0m17:47:48.515233 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.516104 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 17:47:48.491836 => 17:47:48.516006
[0m17:47:48.516319 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m17:47:48.530297 [info ] [Thread-1  ]: 19 of 32 OK created sql table model main.dim_campaign .......................... [[32mOK[0m in 0.04s]
[0m17:47:48.530668 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m17:47:48.530888 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m17:47:48.531255 [info ] [Thread-1  ]: 20 of 32 START sql table model main.fact_case .................................. [RUN]
[0m17:47:48.531735 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.fact_case)
[0m17:47:48.531950 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m17:47:48.534253 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m17:47:48.534754 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 17:47:48.532083 => 17:47:48.534640
[0m17:47:48.534975 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m17:47:48.537618 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m17:47:48.538122 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m17:47:48.538311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m17:47:48.538477 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.543956 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.544219 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m17:47:48.544425 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."main"."fact_case__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__case"
)

SELECT
    id AS case_id,                           -- Surrogate Key
    accountid AS account_fk,                 -- Foreign Key to dim_account
    contactid AS contact_fk,                 -- Foreign Key to dim_contact
    ownerid AS user_fk,                      -- Foreign Key to dim_user
    status AS status_name,
    priority,
    origin,
    subject,
    createddate AS case_created_at,
    lastmodifieddate AS case_last_modified_date
FROM source
    );
  
  
[0m17:47:48.545475 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 17:47:48.535122 => 17:47:48.545376
[0m17:47:48.545694 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m17:47:48.546249 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m17:47:48.546429 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m17:47:48.548478 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.case_id", "source.assetid", "source.ownerid"
  LINE 19:     id AS case_id,                           -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      contactid AS contact_fk,                 -- Foreign Key to dim_contact
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      status AS status_name,
      priority,
      origin,
      subject,
      createddate AS case_created_at,
      lastmodifieddate AS case_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:48.548951 [error] [Thread-1  ]: 20 of 32 ERROR creating sql table model main.fact_case ......................... [[31mERROR[0m in 0.02s]
[0m17:47:48.549311 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m17:47:48.549556 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m17:47:48.549832 [info ] [Thread-1  ]: 21 of 32 START sql table model main.dim_case_status ............................ [RUN]
[0m17:47:48.550193 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.dim_case_status)
[0m17:47:48.550388 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m17:47:48.552682 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.553570 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 17:47:48.550519 => 17:47:48.553421
[0m17:47:48.553827 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m17:47:48.556501 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.557364 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.557628 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m17:47:48.557823 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.563213 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.563492 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.563699 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."main"."dim_case_status__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        status AS status_name,
        status AS status_description  -- Adjust as needed, typically a description field should be separate
    FROM "dbt"."staging"."stg_salesforce__case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY status_name) AS status_id,  -- Surrogate Key
    status_name,
    status_description
FROM source
    );
  
  
[0m17:47:48.565029 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.568184 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.568449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m17:47:48.568845 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.570707 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.570927 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m17:47:48.571218 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.572167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m17:47:48.572360 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.572535 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m17:47:48.573143 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.575021 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m17:47:48.575270 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."main"."dim_case_status__dbt_backup" cascade
[0m17:47:48.575695 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.576509 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 17:47:48.553970 => 17:47:48.576414
[0m17:47:48.576737 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m17:47:48.585174 [info ] [Thread-1  ]: 21 of 32 OK created sql table model main.dim_case_status ....................... [[32mOK[0m in 0.03s]
[0m17:47:48.585597 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m17:47:48.585845 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m17:47:48.586129 [info ] [Thread-1  ]: 22 of 32 START sql table model main.dim_contact ................................ [RUN]
[0m17:47:48.586507 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m17:47:48.586708 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m17:47:48.588805 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m17:47:48.589268 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 17:47:48.586844 => 17:47:48.589172
[0m17:47:48.589463 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m17:47:48.591973 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m17:47:48.592379 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m17:47:48.592601 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m17:47:48.592846 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.598337 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.598641 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m17:47:48.598849 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

WITH base AS (
    SELECT
        id AS contact_id,
        account_id,
        first_name,
        last_name,
        email,
        phone,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__contact"
)
SELECT * FROM base;
    );
  
  
[0m17:47:48.599309 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 17:47:48.589591 => 17:47:48.599211
[0m17:47:48.599512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m17:47:48.600049 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m17:47:48.600229 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m17:47:48.602171 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.602578 [error] [Thread-1  ]: 22 of 32 ERROR creating sql table model main.dim_contact ....................... [[31mERROR[0m in 0.02s]
[0m17:47:48.602897 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m17:47:48.603135 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m17:47:48.603410 [info ] [Thread-1  ]: 23 of 32 START sql view model main.dim_lead .................................... [RUN]
[0m17:47:48.603777 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m17:47:48.603962 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m17:47:48.606012 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m17:47:48.606500 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 17:47:48.604093 => 17:47:48.606408
[0m17:47:48.606683 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m17:47:48.610281 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m17:47:48.610779 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m17:47:48.610967 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m17:47:48.611141 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.616591 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.616891 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m17:47:48.617100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
  create view "dbt"."main"."dim_lead__dbt_tmp" as (
    

WITH base AS (
    SELECT
        id AS lead_id,
        first_name,
        last_name,
        company,
        email,
        phone,
        status,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__lead"
)

SELECT * FROM base;
  );

[0m17:47:48.617547 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 17:47:48.606808 => 17:47:48.617423
[0m17:47:48.617775 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: ROLLBACK
[0m17:47:48.618379 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_lead'
[0m17:47:48.618615 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m17:47:48.620490 [debug] [Thread-1  ]: Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.620926 [error] [Thread-1  ]: 23 of 32 ERROR creating sql view model main.dim_lead ........................... [[31mERROR[0m in 0.02s]
[0m17:47:48.621251 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m17:47:48.621490 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m17:47:48.621773 [info ] [Thread-1  ]: 24 of 32 START sql table model main.dim_opportunity ............................ [RUN]
[0m17:47:48.622134 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m17:47:48.622328 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m17:47:48.624657 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:47:48.625193 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 17:47:48.622457 => 17:47:48.625089
[0m17:47:48.625398 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m17:47:48.627978 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:47:48.628614 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:47:48.628892 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m17:47:48.629163 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.634924 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.635153 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:47:48.635424 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity__dbt_tmp"
  
    as (
      



renamed AS (

    SELECT
        /* IDs */
        id AS opportunity_id,
        accountid AS account_fk,
        ownerid AS user_fk,
        contactid AS primary_contact_fk,
        campaignid AS campaign_fk,
        pricebook2id AS pricebook_fk,
        contractid AS contract_fk,
        primarypartneraccountid AS primary_partner_account_fk,
        createdbyid AS created_by_fk,
        lastmodifiedbyid AS last_modified_by_fk,
        lastamountchangedhistoryid AS last_amount_change_history_fk,
        lastclosedatechangedhistoryid AS last_close_date_change_history_fk,

        /* Metrics */
        amount,
        probability,
        expectedrevenue AS expected_revenue,
        totalopportunityquantity AS total_opportunity_quantity,

        /* Dimensions */
        name AS opportunity_name,
        description,
        stagename AS stage_name,
        stagesortorder AS stage_sort_order,
        type AS opportunity_type,
        nextstep AS next_step,
        leadsource AS lead_source,
        isclosed AS is_closed,
        iswon AS is_won,
        forecastcategory AS forecast_category,
        forecastcategoryname AS forecast_category_name,
        hasopportunitylineitem AS has_opportunity_line_item,
        deliveryinstallationstatus__c AS delivery_installation_status,
        trackingnumber__c AS tracking_number,
        ordernumber__c AS order_number,
        currentgenerators__c AS current_generators,
        maincompetitors__c AS main_competitors,

        /* Dates */
        closedate AS close_date,
        createddate AS created_at,
        lastmodifieddate AS last_modified_date,
        systemmodstamp AS system_mod_stamp,
        lastactivitydate AS last_activity_date,
        laststagechangedate AS last_stage_change_date,
        fiscalyear AS fiscal_year,
        fiscalquarter AS fiscal_quarter

    FROM "dbt"."staging"."stg_salesforce__opportunity"

)

SELECT
    /* IDs */
    opportunity_id,
    account_fk,
    user_fk,
    primary_contact_fk,
    campaign_fk,
    pricebook_fk,
    contract_fk,
    primary_partner_account_fk,
    created_by_fk,
    last_modified_by_fk,
    last_amount_change_history_fk,
    last_close_date_change_history_fk,

    /* Metrics */
    amount,
    probability,
    expected_revenue,
    total_opportunity_quantity,

    /* Dimensions */
    opportunity_name,
    description,
    stage_name,
    stage_sort_order,
    opportunity_type,
    next_step,
    lead_source,
    is_closed,
    is_won,
    forecast_category,
    forecast_category_name,
    has_opportunity_line_item,
    delivery_installation_status,
    tracking_number,
    order_number,
    current_generators,
    main_competitors,

    /* Dates */
    close_date,
    created_at,
    last_modified_date,
    system_mod_stamp,
    last_activity_date,
    last_stage_change_date,
    fiscal_year,
    fiscal_quarter

FROM renamed
    );
  
  
[0m17:47:48.635906 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 17:47:48.625537 => 17:47:48.635810
[0m17:47:48.636101 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: ROLLBACK
[0m17:47:48.636602 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_opportunity'
[0m17:47:48.636798 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m17:47:48.638693 [debug] [Thread-1  ]: Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Parser Error: syntax error at or near "renamed"
[0m17:47:48.639172 [error] [Thread-1  ]: 24 of 32 ERROR creating sql table model main.dim_opportunity ................... [[31mERROR[0m in 0.02s]
[0m17:47:48.639511 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m17:47:48.639755 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:47:48.640036 [info ] [Thread-1  ]: 25 of 32 START sql table model main.dim_opportunity_stage ...................... [RUN]
[0m17:47:48.640401 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m17:47:48.640601 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:47:48.642848 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.643372 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 17:47:48.640728 => 17:47:48.643267
[0m17:47:48.643577 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:47:48.646186 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.646614 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.646799 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m17:47:48.646971 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.652173 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.652441 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.652641 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        stagename AS stage_name,
        stagesortorder AS stage_sort_order
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY stage_sort_order) AS stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
FROM source
    );
  
  
[0m17:47:48.654942 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.657989 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.658241 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m17:47:48.658635 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.660585 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.660821 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m17:47:48.661152 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.662153 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m17:47:48.662346 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.662517 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m17:47:48.663142 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.664733 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m17:47:48.664935 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."main"."dim_opportunity_stage__dbt_backup" cascade
[0m17:47:48.665309 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.666006 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 17:47:48.643719 => 17:47:48.665908
[0m17:47:48.666223 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m17:47:48.674653 [info ] [Thread-1  ]: 25 of 32 OK created sql table model main.dim_opportunity_stage ................. [[32mOK[0m in 0.03s]
[0m17:47:48.675045 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m17:47:48.675292 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m17:47:48.675560 [info ] [Thread-1  ]: 26 of 32 START sql table model main.fact_opportunity ........................... [RUN]
[0m17:47:48.675913 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.fact_opportunity)
[0m17:47:48.676112 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m17:47:48.678241 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m17:47:48.678708 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 17:47:48.676249 => 17:47:48.678612
[0m17:47:48.678909 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m17:47:48.681272 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m17:47:48.681639 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m17:47:48.681824 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m17:47:48.682001 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.687525 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.687812 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m17:47:48.688025 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."fact_opportunity__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    id AS opportunity_id,                    -- Surrogate Key
    accountid AS account_fk,                 -- Foreign Key to dim_account
    ownerid AS user_fk,                      -- Foreign Key to dim_user
    campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
    pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
    stagename AS stage_name,                 -- Captures the current stage
    stagesortorder AS stage_sort_order,
    amount,
    probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,
    closedate AS close_date,
    createddate AS opportunity_created_at,
    lastmodifieddate AS opportunity_last_modified_date,
    isclosed AS is_closed,
    iswon AS is_won
FROM source
    );
  
  
[0m17:47:48.689218 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 17:47:48.679043 => 17:47:48.689109
[0m17:47:48.689435 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m17:47:48.690014 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m17:47:48.690195 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m17:47:48.692293 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.iswon", "source.ownerid"
  LINE 19:     id AS opportunity_id,                    -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
      pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
      stagename AS stage_name,                 -- Captures the current stage
      stagesortorder AS stage_sort_order,
      amount,
      probability,
      expectedrevenue AS expected_revenue,
      totalopportunityquantity AS total_opportunity_quantity,
      closedate AS close_date,
      createddate AS opportunity_created_at,
      lastmodifieddate AS opportunity_last_modified_date,
      isclosed AS is_closed,
      iswon AS is_won
  FROM source
      );
    
    ...
               ^
[0m17:47:48.692712 [error] [Thread-1  ]: 26 of 32 ERROR creating sql table model main.fact_opportunity .................. [[31mERROR[0m in 0.02s]
[0m17:47:48.693080 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m17:47:48.693323 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:47:48.693621 [info ] [Thread-1  ]: 27 of 32 START sql table model main.fact_opportunity_history ................... [RUN]
[0m17:47:48.693991 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_opportunity_history)
[0m17:47:48.694190 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m17:47:48.696290 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:47:48.696757 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 17:47:48.694319 => 17:47:48.696659
[0m17:47:48.696947 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m17:47:48.700583 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:47:48.701243 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:47:48.701453 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m17:47:48.701631 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.707097 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.707401 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m17:47:48.707644 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."main"."fact_opportunity_history__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__opportunity_history"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY systemmodstamp) AS history_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                            -- Foreign Key to fact_opportunity
    stage_fk AS stage_fk,                                       -- Foreign Key to dim_opportunity_stage
    amount,
    probability,
    closedate AS close_date,
    systemmodstamp AS history_date
FROM source
    );
  
  
[0m17:47:48.708533 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 17:47:48.697074 => 17:47:48.708421
[0m17:47:48.708768 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m17:47:48.709390 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m17:47:48.709623 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m17:47:48.711432 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Column "stage_fk" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m17:47:48.711844 [error] [Thread-1  ]: 27 of 32 ERROR creating sql table model main.fact_opportunity_history .......... [[31mERROR[0m in 0.02s]
[0m17:47:48.712184 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m17:47:48.712415 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m17:47:48.712719 [info ] [Thread-1  ]: 28 of 32 START sql table model main.dim_pricebook .............................. [RUN]
[0m17:47:48.713115 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity_history, now model.elastic_dbt_interview.dim_pricebook)
[0m17:47:48.713340 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m17:47:48.715509 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m17:47:48.715984 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 17:47:48.713477 => 17:47:48.715884
[0m17:47:48.716173 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m17:47:48.718814 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m17:47:48.719265 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m17:47:48.719491 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m17:47:48.719676 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.725067 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.725346 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m17:47:48.725546 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."main"."dim_pricebook__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        pricebook2id AS pricebook_id,        -- Surrogate Key
        name AS pricebook_name,
        isactive AS is_active,
        description,
        createddate AS pricebook_created_date,
        lastmodifieddate AS pricebook_last_modified_date
    FROM "dbt"."staging"."stg_salesforce__pricebook_entry"
)

SELECT * FROM source
    );
  
  
[0m17:47:48.726283 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 17:47:48.716303 => 17:47:48.726187
[0m17:47:48.726481 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: ROLLBACK
[0m17:47:48.726999 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_pricebook'
[0m17:47:48.727178 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m17:47:48.728905 [debug] [Thread-1  ]: Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m17:47:48.729275 [error] [Thread-1  ]: 28 of 32 ERROR creating sql table model main.dim_pricebook ..................... [[31mERROR[0m in 0.02s]
[0m17:47:48.729585 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m17:47:48.729807 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m17:47:48.730185 [info ] [Thread-1  ]: 29 of 32 START sql table model main.dim_product ................................ [RUN]
[0m17:47:48.730668 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m17:47:48.730896 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m17:47:48.733233 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m17:47:48.733780 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 17:47:48.731042 => 17:47:48.733671
[0m17:47:48.733979 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m17:47:48.736634 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m17:47:48.737123 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m17:47:48.737307 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m17:47:48.737477 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.742875 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.743189 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m17:47:48.743404 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."main"."dim_product__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__product_2"
)

SELECT
    id AS product_id,                        -- Surrogate Key
    name AS product_name,
    productcode AS product_code,
    description,
    isactive AS is_active,
    createddate AS product_created_date,
    lastmodifieddate AS product_last_modified_date
FROM source
    );
  
  
[0m17:47:48.744379 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 17:47:48.734116 => 17:47:48.744152
[0m17:47:48.744597 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: ROLLBACK
[0m17:47:48.745181 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_product'
[0m17:47:48.745368 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m17:47:48.747307 [debug] [Thread-1  ]: Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS product_id,                        -- Surrogate Key
      name AS product_name,
      productcode AS product_code,
      description,
      isactive AS is_active,
      createddate AS product_created_date,
      lastmodifieddate AS product_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:48.747757 [error] [Thread-1  ]: 29 of 32 ERROR creating sql table model main.dim_product ....................... [[31mERROR[0m in 0.02s]
[0m17:47:48.748253 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m17:47:48.748522 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m17:47:48.748868 [info ] [Thread-1  ]: 30 of 32 START sql table model main.dim_solution ............................... [RUN]
[0m17:47:48.749365 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_solution)
[0m17:47:48.749599 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m17:47:48.752447 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m17:47:48.752924 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 17:47:48.749735 => 17:47:48.752830
[0m17:47:48.753114 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m17:47:48.755657 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m17:47:48.756077 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m17:47:48.756262 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m17:47:48.756431 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.761798 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.762126 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m17:47:48.762338 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."main"."dim_solution__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__solution"
)

SELECT
    solution_id,                       -- Surrogate Key
    solutionname AS solution_name,
    status,
    description,
    createddate AS solution_created_date,
    lastmodifieddate AS solution_last_modified_date
FROM source
    );
  
  
[0m17:47:48.763154 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 17:47:48.753240 => 17:47:48.763051
[0m17:47:48.763374 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: ROLLBACK
[0m17:47:48.763975 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_solution'
[0m17:47:48.764214 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m17:47:48.766045 [debug] [Thread-1  ]: Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "description" not found in FROM clause!
  Candidate bindings: "source.ownerid"
  LINE 22:     description,
               ^
[0m17:47:48.766412 [error] [Thread-1  ]: 30 of 32 ERROR creating sql table model main.dim_solution ...................... [[31mERROR[0m in 0.02s]
[0m17:47:48.766719 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m17:47:48.766933 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m17:47:48.767305 [info ] [Thread-1  ]: 31 of 32 START sql table model main.dim_user ................................... [RUN]
[0m17:47:48.767810 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m17:47:48.768019 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m17:47:48.770169 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m17:47:48.770685 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 17:47:48.768154 => 17:47:48.770587
[0m17:47:48.770887 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m17:47:48.773525 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m17:47:48.774050 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m17:47:48.774260 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m17:47:48.774436 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.779664 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.779919 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m17:47:48.780111 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

WITH base AS (
    SELECT
        user_id,
        username,
        first_name,
        last_name,
        email,
        role_id,
        created_date,
        last_modified_date
    FROM "dbt"."staging"."stg_salesforce__user"
)
SELECT * FROM base;
    );
  
  
[0m17:47:48.780523 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 17:47:48.771018 => 17:47:48.780429
[0m17:47:48.780727 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m17:47:48.781251 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m17:47:48.781428 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m17:47:48.783361 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.783800 [error] [Thread-1  ]: 31 of 32 ERROR creating sql table model main.dim_user .......................... [[31mERROR[0m in 0.02s]
[0m17:47:48.784129 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m17:47:48.784361 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user_role
[0m17:47:48.784588 [info ] [Thread-1  ]: 32 of 32 START sql table model main.dim_user_role .............................. [RUN]
[0m17:47:48.784945 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.dim_user_role)
[0m17:47:48.785135 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user_role
[0m17:47:48.787322 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user_role"
[0m17:47:48.787809 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (compile): 17:47:48.785267 => 17:47:48.787711
[0m17:47:48.788002 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user_role
[0m17:47:48.791659 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user_role"
[0m17:47:48.792308 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m17:47:48.792521 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: BEGIN
[0m17:47:48.792703 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:47:48.798095 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:47:48.798340 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m17:47:48.798528 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user_role"} */

  
    
    

    create  table
      "dbt"."main"."dim_user_role__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__user_role"
)

SELECT
    user_role_id,                           -- Surrogate Key
    name AS role_name,
    roledescription AS role_description
FROM source
    );
  
  
[0m17:47:48.799321 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (execute): 17:47:48.788129 => 17:47:48.799227
[0m17:47:48.799529 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: ROLLBACK
[0m17:47:48.800045 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user_role'
[0m17:47:48.800221 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: Close
[0m17:47:48.802145 [debug] [Thread-1  ]: Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "roledescription" not found in FROM clause!
  Candidate bindings: "source.rollupdescription"
  LINE 21:     roledescription AS role_description
               ^
[0m17:47:48.802600 [error] [Thread-1  ]: 32 of 32 ERROR creating sql table model main.dim_user_role ..................... [[31mERROR[0m in 0.02s]
[0m17:47:48.802928 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user_role
[0m17:47:48.803743 [debug] [MainThread]: Using duckdb connection "master"
[0m17:47:48.803916 [debug] [MainThread]: On master: BEGIN
[0m17:47:48.804062 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:47:48.809335 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:47:48.809564 [debug] [MainThread]: On master: COMMIT
[0m17:47:48.809718 [debug] [MainThread]: Using duckdb connection "master"
[0m17:47:48.809863 [debug] [MainThread]: On master: COMMIT
[0m17:47:48.810054 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:47:48.810206 [debug] [MainThread]: On master: Close
[0m17:47:48.811827 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:47:48.812119 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user_role' was properly closed.
[0m17:47:48.812375 [info ] [MainThread]: 
[0m17:47:48.812578 [info ] [MainThread]: Finished running 17 table models, 15 view models in 0 hours 0 minutes and 1.16 seconds (1.16s).
[0m17:47:48.815083 [debug] [MainThread]: Command end result
[0m17:47:48.823408 [info ] [MainThread]: 
[0m17:47:48.823666 [info ] [MainThread]: [31mCompleted with 14 errors and 0 warnings:[0m
[0m17:47:48.823825 [info ] [MainThread]: 
[0m17:47:48.824014 [error] [MainThread]:   Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m17:47:48.824190 [info ] [MainThread]: 
[0m17:47:48.824366 [error] [MainThread]:   Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:48.824537 [info ] [MainThread]: 
[0m17:47:48.824699 [error] [MainThread]:   Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__account.sic", "stg_salesforce__account.site", "stg_salesforce__account.fax", "stg_salesforce__account.sicdesc", "stg_salesforce__account.ownerid"
  LINE 15:     id as account_id,
               ^
[0m17:47:48.824858 [info ] [MainThread]: 
[0m17:47:48.825025 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.case_id", "source.assetid", "source.ownerid"
  LINE 19:     id AS case_id,                           -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      contactid AS contact_fk,                 -- Foreign Key to dim_contact
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      status AS status_name,
      priority,
      origin,
      subject,
      createddate AS case_created_at,
      lastmodifieddate AS case_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:48.825193 [info ] [MainThread]: 
[0m17:47:48.825342 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.825488 [info ] [MainThread]: 
[0m17:47:48.825630 [error] [MainThread]:   Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.825777 [info ] [MainThread]: 
[0m17:47:48.825921 [error] [MainThread]:   Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Parser Error: syntax error at or near "renamed"
[0m17:47:48.826064 [info ] [MainThread]: 
[0m17:47:48.826246 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.iswon", "source.ownerid"
  LINE 19:     id AS opportunity_id,                    -- Surrogate Key
      accountid AS account_fk,                 -- Foreign Key to dim_account
      ownerid AS user_fk,                      -- Foreign Key to dim_user
      campaignid AS campaign_fk,               -- Foreign Key to dim_campaign
      pricebook2id AS pricebook_fk,            -- Foreign Key to dim_pricebook (if applicable)
      stagename AS stage_name,                 -- Captures the current stage
      stagesortorder AS stage_sort_order,
      amount,
      probability,
      expectedrevenue AS expected_revenue,
      totalopportunityquantity AS total_opportunity_quantity,
      closedate AS close_date,
      createddate AS opportunity_created_at,
      lastmodifieddate AS opportunity_last_modified_date,
      isclosed AS is_closed,
      iswon AS is_won
  FROM source
      );
    
    ...
               ^
[0m17:47:48.826596 [info ] [MainThread]: 
[0m17:47:48.826805 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Column "stage_fk" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m17:47:48.826978 [info ] [MainThread]: 
[0m17:47:48.827144 [error] [MainThread]:   Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m17:47:48.827314 [info ] [MainThread]: 
[0m17:47:48.827495 [error] [MainThread]:   Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "source.name"
  LINE 19:     id AS product_id,                        -- Surrogate Key
      name AS product_name,
      productcode AS product_code,
      description,
      isactive AS is_active,
      createddate AS product_created_date,
      lastmodifieddate AS product_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m17:47:48.827673 [info ] [MainThread]: 
[0m17:47:48.827833 [error] [MainThread]:   Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "description" not found in FROM clause!
  Candidate bindings: "source.ownerid"
  LINE 22:     description,
               ^
[0m17:47:48.827993 [info ] [MainThread]: 
[0m17:47:48.828217 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m17:47:48.828387 [info ] [MainThread]: 
[0m17:47:48.828545 [error] [MainThread]:   Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "roledescription" not found in FROM clause!
  Candidate bindings: "source.rollupdescription"
  LINE 21:     roledescription AS role_description
               ^
[0m17:47:48.828722 [info ] [MainThread]: 
[0m17:47:48.828888 [info ] [MainThread]: Done. PASS=18 WARN=0 ERROR=14 SKIP=0 TOTAL=32
[0m17:47:48.829249 [debug] [MainThread]: Command `dbt run` failed at 17:47:48.829192 after 1.41 seconds
[0m17:47:48.829449 [debug] [MainThread]: Flushing usage events


============================== 17:53:51.126907 | 1a8587ac-c80a-4d03-bad6-ab49176e9ccb ==============================
[0m17:53:51.126907 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:53:51.130259 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity.sql', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m17:53:51.130538 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:53:51.215267 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:53:51.235770 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:53:51.285298 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 12 files changed.
[0m17:53:51.285757 [debug] [MainThread]: Partial parsing: added file: elastic_dbt_interview://models/dimensions/dim_date.sql
[0m17:53:51.286014 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_account.sql
[0m17:53:51.286232 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m17:53:51.286445 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_lead.sql
[0m17:53:51.286656 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_case_history.sql
[0m17:53:51.286866 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m17:53:51.287079 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m17:53:51.287290 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m17:53:51.287508 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_case.sql
[0m17:53:51.287719 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_product.sql
[0m17:53:51.287921 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_solution.sql
[0m17:53:51.288124 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_campaign.sql
[0m17:53:51.288346 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_opportunity.sql
[0m17:53:51.338805 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m17:53:51.354967 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m17:53:51.356455 [info ] [MainThread]: 
[0m17:53:51.356920 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:53:51.357499 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m17:53:51.364913 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:53:51.365209 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:53:51.365391 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:53:51.376655 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.377565 [debug] [ThreadPool]: On list_dbt: Close
[0m17:53:51.379343 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m17:53:51.379830 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m17:53:51.382696 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:53:51.382907 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m17:53:51.383067 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:53:51.388624 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.388925 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:53:51.389087 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m17:53:51.389499 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.390006 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:53:51.390187 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:53:51.390336 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:53:51.390563 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.390718 [debug] [ThreadPool]: On create_dbt_main: Close
[0m17:53:51.393606 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m17:53:51.397229 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:53:51.397453 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m17:53:51.397616 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:53:51.402991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.403269 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:53:51.403443 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m17:53:51.421194 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.422256 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m17:53:51.422926 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m17:53:51.423112 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m17:53:51.425034 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m17:53:51.427417 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:53:51.427589 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m17:53:51.427733 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:53:51.432680 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.432920 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:53:51.433088 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m17:53:51.448345 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:53:51.451992 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m17:53:51.452323 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m17:53:51.452492 [debug] [ThreadPool]: On list_dbt_main: Close
[0m17:53:51.455167 [debug] [MainThread]: Using duckdb connection "master"
[0m17:53:51.455373 [debug] [MainThread]: On master: BEGIN
[0m17:53:51.455537 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:53:51.460686 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:53:51.460939 [debug] [MainThread]: On master: COMMIT
[0m17:53:51.461099 [debug] [MainThread]: Using duckdb connection "master"
[0m17:53:51.461248 [debug] [MainThread]: On master: COMMIT
[0m17:53:51.461449 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:53:51.461608 [debug] [MainThread]: On master: Close
[0m17:53:51.462866 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:53:51.463078 [info ] [MainThread]: 
[0m17:53:51.464624 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m17:53:51.464918 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_opportunity .............................. [RUN]
[0m17:53:51.465427 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_opportunity)
[0m17:53:51.465629 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m17:53:51.471317 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:53:51.472202 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 17:53:51.465769 => 17:53:51.472088
[0m17:53:51.472403 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m17:53:51.491264 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:53:51.491990 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:53:51.492246 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m17:53:51.492444 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:53:51.497710 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:53:51.497979 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:53:51.498222 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    id AS opportunity_id,
    accountid AS account_id,
    ownerid AS user_id,
    contactid AS primary_contact_id,
    campaignid AS campaign_id,
    pricebook2id AS pricebook_id,
    contractid AS contract_id,
    primarypartneraccountid AS primary_partner_account_id,
    createdbyid AS created_by_id,
    lastmodifiedbyid AS last_modified_by_id,
    lastamountchangedhistoryid AS last_amount_change_history_id,
    lastclosedatechangedhistoryid AS last_close_date_change_history_id,

    /* Metrics */
    amount,
    probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,

    /* Dimensions */
    name AS opportunity_name,
    description,
    stagename AS stage_name,
    stagesortorder AS stage_sort_order,
    type AS opportunity_type,
    nextstep AS next_step,
    leadsource AS lead_source,
    isclosed AS is_closed,
    iswon AS is_won,
    forecastcategory AS forecast_category,
    forecastcategoryname AS forecast_category_name,
    hasopportunitylineitem AS has_opportunity_line_item,
    deliveryinstallationstatus__c AS delivery_installation_status,
    trackingnumber__c AS tracking_number,
    ordernumber__c AS order_number,
    currentgenerators__c AS current_generators,
    maincompetitors__c AS main_competitors,

    /* Dates */
    closedate AS close_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    laststagechangedate AS last_stage_change_date,
    fiscalyear AS fiscal_year,
    fiscalquarter AS fiscal_quarter

FROM "dbt"."staging"."stg_salesforce__opportunity"
    );
  
  
[0m17:53:51.499414 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 17:53:51.472535 => 17:53:51.499302
[0m17:53:51.499656 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: ROLLBACK
[0m17:53:51.503356 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_opportunity'
[0m17:53:51.503556 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m17:53:51.505259 [debug] [Thread-1  ]: Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__opportunity.iswon", "stg_salesforce__opportunity.ownerid"
  LINE 15:     id AS opportunity_id,
               ^
[0m17:53:51.505653 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_opportunity ..................... [[31mERROR[0m in 0.04s]
[0m17:53:51.505997 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m17:53:51.506762 [debug] [MainThread]: Using duckdb connection "master"
[0m17:53:51.506938 [debug] [MainThread]: On master: BEGIN
[0m17:53:51.507089 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:53:51.512558 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:53:51.512891 [debug] [MainThread]: On master: COMMIT
[0m17:53:51.513103 [debug] [MainThread]: Using duckdb connection "master"
[0m17:53:51.513264 [debug] [MainThread]: On master: COMMIT
[0m17:53:51.513531 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:53:51.513695 [debug] [MainThread]: On master: Close
[0m17:53:51.515117 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:53:51.515318 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_opportunity' was properly closed.
[0m17:53:51.515516 [info ] [MainThread]: 
[0m17:53:51.515720 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m17:53:51.516069 [debug] [MainThread]: Command end result
[0m17:53:51.550330 [info ] [MainThread]: 
[0m17:53:51.550649 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m17:53:51.550827 [info ] [MainThread]: 
[0m17:53:51.551002 [error] [MainThread]:   Runtime Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__opportunity.iswon", "stg_salesforce__opportunity.ownerid"
  LINE 15:     id AS opportunity_id,
               ^
[0m17:53:51.551183 [info ] [MainThread]: 
[0m17:53:51.551362 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m17:53:51.551696 [debug] [MainThread]: Command `dbt run` failed at 17:53:51.551645 after 0.46 seconds
[0m17:53:51.551890 [debug] [MainThread]: Flushing usage events


============================== 17:54:01.639483 | a1a7ed9a-ab70-4fd5-823a-97d8d28bc9f8 ==============================
[0m17:54:01.639483 [info ] [MainThread]: Running with dbt=1.6.18
[0m17:54:01.642114 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m17:54:01.642368 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m17:54:01.717977 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m17:54:01.736921 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m17:54:01.780387 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:54:01.780863 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_opportunity.sql
[0m17:54:01.808164 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m17:54:01.821779 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m17:54:01.823142 [info ] [MainThread]: 
[0m17:54:01.823730 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:54:01.824278 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m17:54:01.831143 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m17:54:01.831403 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m17:54:01.831590 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:54:01.838765 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.839682 [debug] [ThreadPool]: On list_dbt: Close
[0m17:54:01.841610 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m17:54:01.842178 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m17:54:01.845342 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:54:01.845581 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m17:54:01.845739 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:54:01.851069 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.851337 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:54:01.851649 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m17:54:01.851992 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.852507 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:54:01.852686 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m17:54:01.852848 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m17:54:01.853081 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.853250 [debug] [ThreadPool]: On create_dbt_main: Close
[0m17:54:01.856028 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m17:54:01.859415 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:54:01.859628 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m17:54:01.859780 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:54:01.865555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.865822 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m17:54:01.865999 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m17:54:01.881776 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.885344 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m17:54:01.887193 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m17:54:01.887384 [debug] [ThreadPool]: On list_dbt_main: Close
[0m17:54:01.889263 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m17:54:01.891411 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:54:01.891592 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m17:54:01.891743 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:54:01.896834 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.897073 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m17:54:01.897241 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m17:54:01.915415 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:54:01.916540 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m17:54:01.916880 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m17:54:01.917052 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m17:54:01.919970 [debug] [MainThread]: Using duckdb connection "master"
[0m17:54:01.920187 [debug] [MainThread]: On master: BEGIN
[0m17:54:01.920346 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:54:01.926196 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:54:01.926441 [debug] [MainThread]: On master: COMMIT
[0m17:54:01.926622 [debug] [MainThread]: Using duckdb connection "master"
[0m17:54:01.926774 [debug] [MainThread]: On master: COMMIT
[0m17:54:01.926989 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:54:01.927152 [debug] [MainThread]: On master: Close
[0m17:54:01.928563 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:54:01.928767 [info ] [MainThread]: 
[0m17:54:01.930165 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m17:54:01.931261 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_opportunity .............................. [RUN]
[0m17:54:01.931637 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_opportunity)
[0m17:54:01.931833 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m17:54:01.937380 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.938111 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 17:54:01.931969 => 17:54:01.937894
[0m17:54:01.938340 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m17:54:01.957542 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.958151 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.958472 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m17:54:01.958665 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:54:01.964383 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:01.964656 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.964908 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    opportunity_id,
    accountid AS account_id,
    ownerid AS user_id,
    contactid AS primary_contact_id,
    campaignid AS campaign_id,
    pricebook2id AS pricebook_id,
    contractid AS contract_id,
    primarypartneraccountid AS primary_partner_account_id,
    createdbyid AS created_by_id,
    lastmodifiedbyid AS last_modified_by_id,
    lastamountchangedhistoryid AS last_amount_change_history_id,
    lastclosedatechangedhistoryid AS last_close_date_change_history_id,

    /* Metrics */
    amount,
    probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,

    /* Dimensions */
    name AS opportunity_name,
    description,
    stagename AS stage_name,
    stagesortorder AS stage_sort_order,
    type AS opportunity_type,
    nextstep AS next_step,
    leadsource AS lead_source,
    isclosed AS is_closed,
    iswon AS is_won,
    forecastcategory AS forecast_category,
    forecastcategoryname AS forecast_category_name,
    hasopportunitylineitem AS has_opportunity_line_item,
    deliveryinstallationstatus__c AS delivery_installation_status,
    trackingnumber__c AS tracking_number,
    ordernumber__c AS order_number,
    currentgenerators__c AS current_generators,
    maincompetitors__c AS main_competitors,

    /* Dates */
    closedate AS close_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    laststagechangedate AS last_stage_change_date,
    fiscalyear AS fiscal_year,
    fiscalquarter AS fiscal_quarter

FROM "dbt"."staging"."stg_salesforce__opportunity"
    );
  
  
[0m17:54:01.969368 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:01.973613 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.973889 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."main"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m17:54:01.974303 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:01.976122 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.976327 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."main"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m17:54:01.976645 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:01.987562 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m17:54:01.987936 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:01.988145 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m17:54:01.989716 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:02.017122 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m17:54:02.017493 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."main"."dim_opportunity__dbt_backup" cascade
[0m17:54:02.018215 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m17:54:02.018995 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 17:54:01.938490 => 17:54:02.018903
[0m17:54:02.019196 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m17:54:02.040662 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_opportunity ......................... [[32mOK[0m in 0.11s]
[0m17:54:02.041085 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m17:54:02.041842 [debug] [MainThread]: Using duckdb connection "master"
[0m17:54:02.042017 [debug] [MainThread]: On master: BEGIN
[0m17:54:02.042166 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:54:02.047823 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:54:02.048050 [debug] [MainThread]: On master: COMMIT
[0m17:54:02.048216 [debug] [MainThread]: Using duckdb connection "master"
[0m17:54:02.048364 [debug] [MainThread]: On master: COMMIT
[0m17:54:02.048559 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m17:54:02.048715 [debug] [MainThread]: On master: Close
[0m17:54:02.050006 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:54:02.050208 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_opportunity' was properly closed.
[0m17:54:02.050398 [info ] [MainThread]: 
[0m17:54:02.050588 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m17:54:02.050922 [debug] [MainThread]: Command end result
[0m17:54:02.058495 [info ] [MainThread]: 
[0m17:54:02.058822 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:54:02.059010 [info ] [MainThread]: 
[0m17:54:02.059200 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:54:02.059582 [debug] [MainThread]: Command `dbt run` succeeded at 17:54:02.059524 after 0.44 seconds
[0m17:54:02.059783 [debug] [MainThread]: Flushing usage events


============================== 18:00:09.168780 | fc8ec8e3-d567-429a-aa0d-dbc303f20176 ==============================
[0m18:00:09.168780 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:00:09.171971 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_case_status.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:00:09.172251 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:00:09.256930 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:00:09.277537 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:00:09.328401 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:00:09.328698 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:00:09.329693 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m18:00:09.351960 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:00:09.353422 [info ] [MainThread]: 
[0m18:00:09.353882 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:00:09.354609 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:00:09.361868 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:00:09.362129 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:00:09.362320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:00:09.374219 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.375174 [debug] [ThreadPool]: On list_dbt: Close
[0m18:00:09.377096 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:00:09.377611 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:00:09.380479 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:09.380687 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:00:09.380843 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:09.386305 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.386590 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:09.386757 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:00:09.387218 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.387823 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:09.388017 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:09.388176 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:09.388398 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.388561 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:00:09.391285 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m18:00:09.394816 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:09.395080 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:00:09.395237 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:09.400230 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.400477 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:09.400649 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:00:09.419112 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.420136 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:00:09.420775 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:00:09.420976 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:00:09.423091 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m18:00:09.425566 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:09.425754 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:00:09.425905 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:09.431466 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.431711 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:09.431885 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:00:09.447210 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:09.450738 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:00:09.450973 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:00:09.451127 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:00:09.454053 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:09.454316 [debug] [MainThread]: On master: BEGIN
[0m18:00:09.454484 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:00:09.459910 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:09.460173 [debug] [MainThread]: On master: COMMIT
[0m18:00:09.460337 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:09.460491 [debug] [MainThread]: On master: COMMIT
[0m18:00:09.460693 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:09.460854 [debug] [MainThread]: On master: Close
[0m18:00:09.462375 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:00:09.462620 [info ] [MainThread]: 
[0m18:00:09.464361 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m18:00:09.464697 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_case_status .............................. [RUN]
[0m18:00:09.465108 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_case_status)
[0m18:00:09.465324 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m18:00:09.470693 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.471774 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 18:00:09.465467 => 18:00:09.471578
[0m18:00:09.472039 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m18:00:09.491480 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.492678 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.492993 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m18:00:09.493191 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:00:09.498776 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.499042 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.499249 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."main"."dim_case_status__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        status AS status_name,
        status AS status_description  -- Adjust as needed, typically a description field should be separate
    FROM "dbt"."staging"."stg_salesforce__case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY status_name) AS status_id,  -- Surrogate Key
    status_name,
    status_description
FROM source
    );
  
  
[0m18:00:09.500978 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.504962 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.505225 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m18:00:09.505614 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.507408 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.507613 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m18:00:09.508022 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.519194 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m18:00:09.519485 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.519670 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m18:00:09.520362 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.523394 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m18:00:09.523618 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."main"."dim_case_status__dbt_backup" cascade
[0m18:00:09.524024 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:09.524879 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 18:00:09.472187 => 18:00:09.524773
[0m18:00:09.525096 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m18:00:09.544059 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_case_status ......................... [[32mOK[0m in 0.08s]
[0m18:00:09.544450 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m18:00:09.545188 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:09.545376 [debug] [MainThread]: On master: BEGIN
[0m18:00:09.545532 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:00:09.550990 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:09.551206 [debug] [MainThread]: On master: COMMIT
[0m18:00:09.551365 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:09.551511 [debug] [MainThread]: On master: COMMIT
[0m18:00:09.551705 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:09.551860 [debug] [MainThread]: On master: Close
[0m18:00:09.553438 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:00:09.553636 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_case_status' was properly closed.
[0m18:00:09.553827 [info ] [MainThread]: 
[0m18:00:09.554043 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m18:00:09.554392 [debug] [MainThread]: Command end result
[0m18:00:09.562296 [info ] [MainThread]: 
[0m18:00:09.562573 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:00:09.562744 [info ] [MainThread]: 
[0m18:00:09.562925 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m18:00:09.563292 [debug] [MainThread]: Command `dbt run` succeeded at 18:00:09.563243 after 0.41 seconds
[0m18:00:09.563484 [debug] [MainThread]: Flushing usage events


============================== 18:00:31.372193 | 81787442-1f31-4f75-ab64-0f178f442873 ==============================
[0m18:00:31.372193 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:00:31.374994 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:00:31.375272 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:00:31.452009 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:00:31.472496 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:00:31.518217 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:00:31.518525 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:00:31.519502 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m18:00:31.533321 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:00:31.534703 [info ] [MainThread]: 
[0m18:00:31.535124 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:00:31.535670 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:00:31.542473 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:00:31.542817 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:00:31.543008 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:00:31.554907 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.555861 [debug] [ThreadPool]: On list_dbt: Close
[0m18:00:31.557808 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:00:31.558370 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:00:31.561233 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:31.561440 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:00:31.561599 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:31.567395 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.567832 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:31.568087 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:00:31.568450 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.569020 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:31.569205 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:31.569356 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:31.569575 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.569734 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:00:31.572656 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m18:00:31.576549 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:31.576907 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:00:31.577139 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:31.582962 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.583227 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:31.583405 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:00:31.598599 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.602141 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:00:31.603006 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:00:31.603247 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:00:31.605300 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m18:00:31.607610 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:31.607788 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:00:31.607936 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:31.613398 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.613647 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:31.613816 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:00:31.631329 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:31.632265 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:00:31.632508 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:00:31.632669 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:00:31.635562 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:31.635785 [debug] [MainThread]: On master: BEGIN
[0m18:00:31.635940 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:00:31.641000 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:31.641257 [debug] [MainThread]: On master: COMMIT
[0m18:00:31.641431 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:31.641592 [debug] [MainThread]: On master: COMMIT
[0m18:00:31.641799 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:31.641966 [debug] [MainThread]: On master: Close
[0m18:00:31.643447 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:00:31.643714 [info ] [MainThread]: 
[0m18:00:31.645251 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:00:31.645554 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:00:31.645934 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_contact)
[0m18:00:31.646149 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:00:31.651317 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:00:31.651849 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:00:31.646288 => 18:00:31.651738
[0m18:00:31.652050 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:00:31.672716 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:00:31.673348 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:00:31.673556 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:00:31.673738 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:00:31.679347 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:31.679608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:00:31.679812 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

select
    id as contact_id,
    account_id,
    first_name,
    last_name,
    email,
    phone,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__contact"
where is_deleted = false;
    );
  
  
[0m18:00:31.680276 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:00:31.652186 => 18:00:31.680182
[0m18:00:31.680476 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m18:00:31.684368 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m18:00:31.684653 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:00:31.686386 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m18:00:31.686825 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_contact ......................... [[31mERROR[0m in 0.04s]
[0m18:00:31.687193 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:00:31.687922 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:31.688114 [debug] [MainThread]: On master: BEGIN
[0m18:00:31.688266 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:00:31.693530 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:31.693891 [debug] [MainThread]: On master: COMMIT
[0m18:00:31.694055 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:31.694205 [debug] [MainThread]: On master: COMMIT
[0m18:00:31.694411 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:31.694571 [debug] [MainThread]: On master: Close
[0m18:00:31.695900 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:00:31.696063 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:00:31.696326 [info ] [MainThread]: 
[0m18:00:31.696698 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m18:00:31.697195 [debug] [MainThread]: Command end result
[0m18:00:31.704172 [info ] [MainThread]: 
[0m18:00:31.704473 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:00:31.704646 [info ] [MainThread]: 
[0m18:00:31.704814 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Parser Error: syntax error at or near ";"
[0m18:00:31.705126 [info ] [MainThread]: 
[0m18:00:31.705383 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:00:31.705820 [debug] [MainThread]: Command `dbt run` failed at 18:00:31.705761 after 0.36 seconds
[0m18:00:31.706032 [debug] [MainThread]: Flushing usage events


============================== 18:00:41.861836 | 408deb3a-0be0-4678-8899-96411c2855e4 ==============================
[0m18:00:41.861836 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:00:41.864506 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:00:41.864765 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:00:41.941501 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:00:41.960168 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:00:42.002645 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:00:42.003142 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:00:42.030750 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m18:00:42.044847 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:00:42.046651 [info ] [MainThread]: 
[0m18:00:42.047195 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:00:42.047920 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:00:42.055188 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:00:42.055473 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:00:42.055661 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:00:42.067454 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.068489 [debug] [ThreadPool]: On list_dbt: Close
[0m18:00:42.070280 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:00:42.070728 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:00:42.073977 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:42.074210 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:00:42.074383 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:42.080283 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.080579 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:42.080765 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:00:42.081015 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.081521 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:42.081696 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:00:42.081842 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:00:42.082053 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.082219 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:00:42.084841 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m18:00:42.088132 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:42.088335 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:00:42.088506 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:42.093650 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.093898 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:00:42.094079 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:00:42.112471 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.113457 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:00:42.114206 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:00:42.114400 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:00:42.116373 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m18:00:42.118749 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:42.118932 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:00:42.119085 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:00:42.124534 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.124795 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:00:42.124976 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:00:42.139857 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:00:42.143229 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:00:42.143468 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:00:42.143628 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:00:42.146110 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:42.146305 [debug] [MainThread]: On master: BEGIN
[0m18:00:42.146460 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:00:42.151347 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:42.151591 [debug] [MainThread]: On master: COMMIT
[0m18:00:42.151756 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:42.151907 [debug] [MainThread]: On master: COMMIT
[0m18:00:42.152115 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:42.152274 [debug] [MainThread]: On master: Close
[0m18:00:42.153991 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:00:42.154286 [info ] [MainThread]: 
[0m18:00:42.155727 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:00:42.156065 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:00:42.157407 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_contact)
[0m18:00:42.157712 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:00:42.163004 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:00:42.163665 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:00:42.157859 => 18:00:42.163553
[0m18:00:42.163866 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:00:42.183073 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:00:42.183790 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:00:42.184064 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:00:42.184249 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:00:42.189724 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:00:42.190001 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:00:42.190233 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

select
    id as contact_id,
    account_id,
    first_name,
    last_name,
    email,
    phone,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__contact"
where is_deleted = false
    );
  
  
[0m18:00:42.191886 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:00:42.164001 => 18:00:42.191748
[0m18:00:42.192151 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m18:00:42.195844 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m18:00:42.196117 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:00:42.197884 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "is_deleted" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.isdeleted"
  LINE 23: where is_deleted = false
                 ^
[0m18:00:42.198297 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_contact ......................... [[31mERROR[0m in 0.04s]
[0m18:00:42.198628 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:00:42.199452 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:42.199682 [debug] [MainThread]: On master: BEGIN
[0m18:00:42.199842 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:00:42.205255 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:42.205447 [debug] [MainThread]: On master: COMMIT
[0m18:00:42.205599 [debug] [MainThread]: Using duckdb connection "master"
[0m18:00:42.205743 [debug] [MainThread]: On master: COMMIT
[0m18:00:42.205951 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:00:42.206100 [debug] [MainThread]: On master: Close
[0m18:00:42.207501 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:00:42.207743 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:00:42.207927 [info ] [MainThread]: 
[0m18:00:42.208127 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m18:00:42.208502 [debug] [MainThread]: Command end result
[0m18:00:42.215962 [info ] [MainThread]: 
[0m18:00:42.216253 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:00:42.216587 [info ] [MainThread]: 
[0m18:00:42.216780 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "is_deleted" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.isdeleted"
  LINE 23: where is_deleted = false
                 ^
[0m18:00:42.216948 [info ] [MainThread]: 
[0m18:00:42.217133 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:00:42.217508 [debug] [MainThread]: Command `dbt run` failed at 18:00:42.217457 after 0.38 seconds
[0m18:00:42.217719 [debug] [MainThread]: Flushing usage events


============================== 18:01:31.805138 | 9f4dc736-c017-47a6-b2f3-40a0f61750a3 ==============================
[0m18:01:31.805138 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:01:31.808225 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:01:31.808482 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:01:31.892691 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:01:31.911309 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:01:31.954952 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:01:31.955477 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:01:31.984147 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m18:01:31.999760 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:01:32.001299 [info ] [MainThread]: 
[0m18:01:32.001808 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:01:32.002534 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:01:32.009541 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:01:32.009797 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:01:32.010138 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:01:32.021555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.022468 [debug] [ThreadPool]: On list_dbt: Close
[0m18:01:32.024441 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:01:32.024945 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:01:32.027896 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:32.028109 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:01:32.028280 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:32.034164 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.034497 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:32.034679 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:01:32.034991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.035534 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:01:32.035713 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:32.035856 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:01:32.036078 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.036241 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:01:32.039179 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m18:01:32.042833 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:01:32.043090 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:01:32.043257 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:32.114393 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.114708 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:01:32.114894 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:01:32.137078 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.138387 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:01:32.139229 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:01:32.139426 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:01:32.141888 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m18:01:32.144542 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:01:32.144732 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:01:32.144912 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:32.150774 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.151014 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:01:32.151193 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:01:32.166908 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:32.170675 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:01:32.170989 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:01:32.171163 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:01:32.175587 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:32.175830 [debug] [MainThread]: On master: BEGIN
[0m18:01:32.176002 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:01:32.183346 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:32.183661 [debug] [MainThread]: On master: COMMIT
[0m18:01:32.183839 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:32.184005 [debug] [MainThread]: On master: COMMIT
[0m18:01:32.184215 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:32.184387 [debug] [MainThread]: On master: Close
[0m18:01:32.187631 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:01:32.187956 [info ] [MainThread]: 
[0m18:01:32.189691 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:01:32.190021 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:01:32.191263 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_contact)
[0m18:01:32.191466 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:01:32.197139 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:01:32.198034 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:01:32.191607 => 18:01:32.197795
[0m18:01:32.198350 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:01:32.218059 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:01:32.218769 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:01:32.218988 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:01:32.219172 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:01:32.225093 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:01:32.225350 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:01:32.225553 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

select
    contact_id,
    account_id,
    first_name,
    last_name,
    email,
    phone,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__contact"
where is_deleted = false
    );
  
  
[0m18:01:32.226923 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:01:32.198506 => 18:01:32.226826
[0m18:01:32.227131 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m18:01:32.231620 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m18:01:32.231925 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:01:32.233755 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "is_deleted" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.isdeleted"
  LINE 23: where is_deleted = false
                 ^
[0m18:01:32.234193 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_contact ......................... [[31mERROR[0m in 0.04s]
[0m18:01:32.234534 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:01:32.235333 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:32.235576 [debug] [MainThread]: On master: BEGIN
[0m18:01:32.235745 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:01:32.240936 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:32.241213 [debug] [MainThread]: On master: COMMIT
[0m18:01:32.241373 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:32.241524 [debug] [MainThread]: On master: COMMIT
[0m18:01:32.241748 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:32.241908 [debug] [MainThread]: On master: Close
[0m18:01:32.243399 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:01:32.243664 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:01:32.243862 [info ] [MainThread]: 
[0m18:01:32.244066 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m18:01:32.244388 [debug] [MainThread]: Command end result
[0m18:01:32.251884 [info ] [MainThread]: 
[0m18:01:32.252165 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:01:32.252564 [info ] [MainThread]: 
[0m18:01:32.252782 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "is_deleted" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.isdeleted"
  LINE 23: where is_deleted = false
                 ^
[0m18:01:32.252975 [info ] [MainThread]: 
[0m18:01:32.253176 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:01:32.253598 [debug] [MainThread]: Command `dbt run` failed at 18:01:32.253542 after 0.47 seconds
[0m18:01:32.253812 [debug] [MainThread]: Flushing usage events


============================== 18:01:39.621627 | 126b03bd-1b4b-42ca-a0a6-edcdd983223c ==============================
[0m18:01:39.621627 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:01:39.625232 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'send_anonymous_usage_stats': 'False'}
[0m18:01:39.625624 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:01:39.703004 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:01:39.721435 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:01:39.765566 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:01:39.766111 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:01:39.798224 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m18:01:39.813456 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:01:39.815122 [info ] [MainThread]: 
[0m18:01:39.815612 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:01:39.816220 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:01:39.823325 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:01:39.823622 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:01:39.823799 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:01:39.831064 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.831988 [debug] [ThreadPool]: On list_dbt: Close
[0m18:01:39.833968 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:01:39.834464 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:01:39.837569 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:39.837793 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:01:39.837954 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:39.843248 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.843487 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:39.843648 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:01:39.844016 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.844605 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:01:39.844795 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:01:39.844955 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:01:39.845191 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.845384 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:01:39.848299 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m18:01:39.851604 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:01:39.851811 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:01:39.851968 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:39.858000 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.858256 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:01:39.858429 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:01:39.873931 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.877601 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:01:39.879062 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:01:39.879317 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:01:39.881543 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m18:01:39.883801 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:01:39.884007 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:01:39.884154 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:01:39.889333 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.889601 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:01:39.889776 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:01:39.907709 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:01:39.908707 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:01:39.908953 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:01:39.909107 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:01:39.912085 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:39.912268 [debug] [MainThread]: On master: BEGIN
[0m18:01:39.912426 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:01:39.917922 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:39.918170 [debug] [MainThread]: On master: COMMIT
[0m18:01:39.918334 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:39.918489 [debug] [MainThread]: On master: COMMIT
[0m18:01:39.918686 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:39.918847 [debug] [MainThread]: On master: Close
[0m18:01:39.920178 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:01:39.920389 [info ] [MainThread]: 
[0m18:01:39.921867 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:01:39.922917 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:01:39.923305 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_contact)
[0m18:01:39.923514 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:01:39.929063 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:01:39.929823 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:01:39.923661 => 18:01:39.929667
[0m18:01:39.930097 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:01:39.977128 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:01:39.977795 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:01:39.978001 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:01:39.978182 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:01:39.983530 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:01:39.983819 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:01:39.984035 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

select
    contact_id,
    account_id,
    first_name,
    last_name,
    email,
    phone,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__contact"
    );
  
  
[0m18:01:39.985482 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:01:39.930233 => 18:01:39.985378
[0m18:01:39.985692 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m18:01:39.988761 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m18:01:39.988969 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:01:39.990785 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "account_id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.accountid"
  LINE 15:     account_id,
               ^
[0m18:01:39.991189 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_contact ......................... [[31mERROR[0m in 0.07s]
[0m18:01:39.991549 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:01:39.992291 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:39.992468 [debug] [MainThread]: On master: BEGIN
[0m18:01:39.992614 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:01:39.997850 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:39.998321 [debug] [MainThread]: On master: COMMIT
[0m18:01:39.998508 [debug] [MainThread]: Using duckdb connection "master"
[0m18:01:39.998663 [debug] [MainThread]: On master: COMMIT
[0m18:01:39.998920 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:01:39.999081 [debug] [MainThread]: On master: Close
[0m18:01:40.000456 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:01:40.000653 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:01:40.001044 [info ] [MainThread]: 
[0m18:01:40.001267 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m18:01:40.001622 [debug] [MainThread]: Command end result
[0m18:01:40.009032 [info ] [MainThread]: 
[0m18:01:40.009296 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:01:40.009618 [info ] [MainThread]: 
[0m18:01:40.009829 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "account_id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.accountid"
  LINE 15:     account_id,
               ^
[0m18:01:40.010014 [info ] [MainThread]: 
[0m18:01:40.010243 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:01:40.010676 [debug] [MainThread]: Command `dbt run` failed at 18:01:40.010617 after 0.41 seconds
[0m18:01:40.010933 [debug] [MainThread]: Flushing usage events


============================== 18:04:54.525179 | 61b8da07-6663-4f95-a2e2-6fc202f52b5e ==============================
[0m18:04:54.525179 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:04:54.528288 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m18:04:54.528534 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:04:54.611547 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:04:54.630380 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:04:54.676061 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:04:54.676547 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:04:54.695149 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_contact (models/dimensions/dim_contact.sql)
  unexpected '}', expected ')'
    line 70
      FROM {{ ref(stg_salesforce__contact }}
[0m18:04:54.695682 [debug] [MainThread]: Command `dbt run` failed at 18:04:54.695609 after 0.20 seconds
[0m18:04:54.695932 [debug] [MainThread]: Flushing usage events


============================== 18:05:05.345055 | f865b42f-8994-4cdf-8e36-35cc0186bbfe ==============================
[0m18:05:05.345055 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:05:05.347664 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:05:05.347934 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:05:05.426775 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:05:05.445468 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:05:05.489681 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:05:05.490232 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:05:05.508988 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_contact (models/dimensions/dim_contact.sql)
  unexpected '}', expected ')'
    line 70
      FROM {{ ref(stg_salesforce__contact }}
[0m18:05:05.509524 [debug] [MainThread]: Command `dbt run` failed at 18:05:05.509453 after 0.19 seconds
[0m18:05:05.509753 [debug] [MainThread]: Flushing usage events


============================== 18:05:22.861219 | 2e447f76-37fc-41e3-8b4c-1bd54e4dc46c ==============================
[0m18:05:22.861219 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:05:22.864055 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m18:05:22.864312 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:05:22.940961 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:05:22.959238 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:05:23.002238 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:05:23.002831 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:05:23.032801 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m18:05:23.047383 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:05:23.048981 [info ] [MainThread]: 
[0m18:05:23.049459 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:05:23.050092 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:05:23.057509 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:05:23.057795 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:05:23.057976 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:05:23.069335 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.070209 [debug] [ThreadPool]: On list_dbt: Close
[0m18:05:23.072080 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:05:23.072582 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:05:23.075515 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:23.075714 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:05:23.075874 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:23.081453 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.081712 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:23.081947 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:05:23.082338 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.082877 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:05:23.083054 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:23.083208 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:05:23.083440 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.083596 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:05:23.086405 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m18:05:23.089774 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:05:23.090016 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:05:23.090174 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:23.095761 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.096025 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:05:23.096193 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:05:23.114343 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.115398 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:05:23.116075 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:05:23.116269 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:05:23.118246 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m18:05:23.120635 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:05:23.120838 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:05:23.121054 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:23.126594 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.126850 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:05:23.127035 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:05:23.142287 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:23.145650 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:05:23.145882 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:05:23.146040 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:05:23.148539 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:23.148712 [debug] [MainThread]: On master: BEGIN
[0m18:05:23.148859 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:05:23.153907 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:23.154145 [debug] [MainThread]: On master: COMMIT
[0m18:05:23.154303 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:23.154457 [debug] [MainThread]: On master: COMMIT
[0m18:05:23.154661 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:23.154825 [debug] [MainThread]: On master: Close
[0m18:05:23.156180 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:05:23.156385 [info ] [MainThread]: 
[0m18:05:23.157838 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:05:23.158127 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:05:23.159281 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_contact)
[0m18:05:23.159471 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:05:23.165176 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:05:23.166258 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:05:23.159608 => 18:05:23.166141
[0m18:05:23.166471 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:05:23.185782 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:05:23.186826 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:23.187133 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:05:23.187347 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:05:23.199316 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:23.199676 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:23.199936 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    id AS contact_id,
    masterrecordid AS master_record_id,
    accountid AS account_id,
    reportstoid AS reports_to_id,
    ownerid AS owner_id,
    jigsawcontactid AS jigsaw_contact_id,
    individualid AS individual_id,

    /* Dates */
    birthdate AS birth_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    lastcurequestdate AS last_cu_request_date,
    lastcuupdatedate AS last_cu_update_date,
    emailbounceddate AS email_bounced_date,

    /* Dimensions */
    salutation,
    firstname AS first_name,
    lastname AS last_name,
    otherstreet AS other_street,
    othercity AS other_city,
    otherstate AS other_state,
    otherpostalcode AS other_postal_code,
    othercountry AS other_country,
    otherlatitude AS other_latitude,
    otherlongitude AS other_longitude,
    othergeocodeaccuracy AS other_geocode_accuracy,
    mailingstreet AS mailing_street,
    mailingcity AS mailing_city,
    mailingstate AS mailing_state,
    mailingpostalcode AS mailing_postal_code,
    mailingcountry AS mailing_country,
    mailinglatitude AS mailing_latitude,
    mailinglongitude AS mailing_longitude,
    mailinggeocodeaccuracy AS mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone AS mobile_phone,
    homephone AS home_phone,
    otherphone AS other_phone,
    assistantphone AS assistant_phone,
    email,
    title,
    department,
    assistantname AS assistant_name,
    leadsource AS lead_source,
    description,
    pronouns,
    genderidentity AS gender_identity,
    cleanstatus AS clean_status,
    emailbouncedreason AS email_bounced_reason,
    level__c AS level,
    languages__c AS languages,

    /* Metrics */
    hasoptedoutofemail AS has_opted_out_of_email,
    hasoptedoutoffax AS has_opted_out_of_fax,
    donotcall AS do_not_call

FROM "dbt"."staging"."stg_salesforce__contact"
WHERE isdeleted = FALSE
    );
  
  
[0m18:05:23.201264 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:05:23.166607 => 18:05:23.201155
[0m18:05:23.201491 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: ROLLBACK
[0m18:05:23.205868 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_contact'
[0m18:05:23.206176 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:05:23.207950 [debug] [Thread-1  ]: Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.fax", "stg_salesforce__contact.email", "stg_salesforce__contact.title", "stg_salesforce__contact.ownerid"
  LINE 15:     id AS contact_id,
               ^
[0m18:05:23.208441 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_contact ......................... [[31mERROR[0m in 0.05s]
[0m18:05:23.208823 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:05:23.209611 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:23.209792 [debug] [MainThread]: On master: BEGIN
[0m18:05:23.209945 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:05:23.215902 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:23.216157 [debug] [MainThread]: On master: COMMIT
[0m18:05:23.216328 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:23.216485 [debug] [MainThread]: On master: COMMIT
[0m18:05:23.216686 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:23.216845 [debug] [MainThread]: On master: Close
[0m18:05:23.219652 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:05:23.220282 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:05:23.221622 [info ] [MainThread]: 
[0m18:05:23.223226 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m18:05:23.226759 [debug] [MainThread]: Command end result
[0m18:05:23.263819 [info ] [MainThread]: 
[0m18:05:23.264169 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:05:23.264433 [info ] [MainThread]: 
[0m18:05:23.264747 [error] [MainThread]:   Runtime Error in model dim_contact (models/dimensions/dim_contact.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__contact.fax", "stg_salesforce__contact.email", "stg_salesforce__contact.title", "stg_salesforce__contact.ownerid"
  LINE 15:     id AS contact_id,
               ^
[0m18:05:23.264992 [info ] [MainThread]: 
[0m18:05:23.265254 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:05:23.265793 [debug] [MainThread]: Command `dbt run` failed at 18:05:23.265724 after 0.43 seconds
[0m18:05:23.266599 [debug] [MainThread]: Flushing usage events


============================== 18:05:34.379688 | 956f9ec9-0795-4f23-a2af-22f6ae7604bc ==============================
[0m18:05:34.379688 [info ] [MainThread]: Running with dbt=1.6.18
[0m18:05:34.382739 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m18:05:34.382986 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m18:05:34.472159 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m18:05:34.491283 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m18:05:34.537695 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:05:34.538191 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_contact.sql
[0m18:05:34.566065 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m18:05:34.580590 [info ] [MainThread]: Found 34 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m18:05:34.582211 [info ] [MainThread]: 
[0m18:05:34.582704 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:05:34.583470 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m18:05:34.590360 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m18:05:34.590639 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m18:05:34.590825 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:05:34.602301 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.603185 [debug] [ThreadPool]: On list_dbt: Close
[0m18:05:34.605048 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m18:05:34.605577 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m18:05:34.608642 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:34.608854 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m18:05:34.609020 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:34.614697 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.614968 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:34.615139 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m18:05:34.615401 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.615924 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:05:34.616090 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m18:05:34.616258 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m18:05:34.616472 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.616635 [debug] [ThreadPool]: On create_dbt_main: Close
[0m18:05:34.619484 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m18:05:34.622876 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:05:34.623084 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m18:05:34.623243 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:34.628609 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.628847 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m18:05:34.629021 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m18:05:34.646843 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.647827 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m18:05:34.648430 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m18:05:34.648601 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m18:05:34.650586 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m18:05:34.652966 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:05:34.653141 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m18:05:34.653291 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:05:34.658496 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.658737 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m18:05:34.658911 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m18:05:34.674593 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m18:05:34.678012 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m18:05:34.678246 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m18:05:34.678410 [debug] [ThreadPool]: On list_dbt_main: Close
[0m18:05:34.683387 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:34.683601 [debug] [MainThread]: On master: BEGIN
[0m18:05:34.683764 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:05:34.691260 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:34.691550 [debug] [MainThread]: On master: COMMIT
[0m18:05:34.691727 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:34.691888 [debug] [MainThread]: On master: COMMIT
[0m18:05:34.692103 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:34.692266 [debug] [MainThread]: On master: Close
[0m18:05:34.693830 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:05:34.694091 [info ] [MainThread]: 
[0m18:05:34.695436 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m18:05:34.695736 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_contact .................................. [RUN]
[0m18:05:34.696913 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_contact)
[0m18:05:34.697123 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m18:05:34.702925 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.703502 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 18:05:34.697267 => 18:05:34.703383
[0m18:05:34.703719 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m18:05:34.724761 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.726832 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.727096 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m18:05:34.727295 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:05:34.733399 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.733740 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.734020 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    contact_id,
    masterrecordid AS master_record_id,
    accountid AS account_id,
    reportstoid AS reports_to_id,
    ownerid AS owner_id,
    jigsawcontactid AS jigsaw_contact_id,
    individualid AS individual_id,

    /* Dates */
    birthdate AS birth_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    lastcurequestdate AS last_cu_request_date,
    lastcuupdatedate AS last_cu_update_date,
    emailbounceddate AS email_bounced_date,

    /* Dimensions */
    salutation,
    firstname AS first_name,
    lastname AS last_name,
    otherstreet AS other_street,
    othercity AS other_city,
    otherstate AS other_state,
    otherpostalcode AS other_postal_code,
    othercountry AS other_country,
    otherlatitude AS other_latitude,
    otherlongitude AS other_longitude,
    othergeocodeaccuracy AS other_geocode_accuracy,
    mailingstreet AS mailing_street,
    mailingcity AS mailing_city,
    mailingstate AS mailing_state,
    mailingpostalcode AS mailing_postal_code,
    mailingcountry AS mailing_country,
    mailinglatitude AS mailing_latitude,
    mailinglongitude AS mailing_longitude,
    mailinggeocodeaccuracy AS mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone AS mobile_phone,
    homephone AS home_phone,
    otherphone AS other_phone,
    assistantphone AS assistant_phone,
    email,
    title,
    department,
    assistantname AS assistant_name,
    leadsource AS lead_source,
    description,
    pronouns,
    genderidentity AS gender_identity,
    cleanstatus AS clean_status,
    emailbouncedreason AS email_bounced_reason,
    level__c AS level,
    languages__c AS languages,

    /* Metrics */
    hasoptedoutofemail AS has_opted_out_of_email,
    hasoptedoutoffax AS has_opted_out_of_fax,
    donotcall AS do_not_call

FROM "dbt"."staging"."stg_salesforce__contact"
WHERE isdeleted = FALSE
    );
  
  
[0m18:05:34.739220 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.743532 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.743845 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."main"."dim_contact" rename to "dim_contact__dbt_backup"
[0m18:05:34.744391 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.746573 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.746835 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."main"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m18:05:34.747301 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.759525 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m18:05:34.759893 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.760100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m18:05:34.761698 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.789963 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m18:05:34.790423 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."main"."dim_contact__dbt_backup" cascade
[0m18:05:34.791052 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m18:05:34.791936 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 18:05:34.703863 => 18:05:34.791836
[0m18:05:34.792149 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m18:05:34.817210 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_contact ............................. [[32mOK[0m in 0.12s]
[0m18:05:34.817665 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m18:05:34.818408 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:34.818585 [debug] [MainThread]: On master: BEGIN
[0m18:05:34.818735 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:05:34.824340 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:34.824563 [debug] [MainThread]: On master: COMMIT
[0m18:05:34.824723 [debug] [MainThread]: Using duckdb connection "master"
[0m18:05:34.824874 [debug] [MainThread]: On master: COMMIT
[0m18:05:34.825088 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m18:05:34.825246 [debug] [MainThread]: On master: Close
[0m18:05:34.826624 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:05:34.826835 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m18:05:34.827048 [info ] [MainThread]: 
[0m18:05:34.827246 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m18:05:34.827619 [debug] [MainThread]: Command end result
[0m18:05:34.834316 [info ] [MainThread]: 
[0m18:05:34.834580 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:05:34.834745 [info ] [MainThread]: 
[0m18:05:34.834922 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m18:05:34.835244 [debug] [MainThread]: Command `dbt run` succeeded at 18:05:34.835196 after 0.48 seconds
[0m18:05:34.835441 [debug] [MainThread]: Flushing usage events


============================== 19:21:28.476143 | 4146744c-3e5a-4ca9-b438-9f4d8a139559 ==============================
[0m19:21:28.476143 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:21:28.479536 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m19:21:28.479791 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:21:28.648782 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:21:28.669676 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:21:28.723702 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 5 files changed.
[0m19:21:28.724108 [debug] [MainThread]: Partial parsing: deleted file: elastic_dbt_interview://models/dimensions/dim_record_type.sql
[0m19:21:28.724375 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_campaign.sql
[0m19:21:28.724599 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:21:28.724812 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_solution.sql
[0m19:21:28.725014 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_pricebook.sql
[0m19:21:28.725215 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_product.sql
[0m19:21:28.762632 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m19:21:28.778296 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:21:28.780070 [info ] [MainThread]: 
[0m19:21:28.780495 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:21:28.781868 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:21:28.789015 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:21:28.789296 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:21:28.789478 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:21:28.807103 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.808451 [debug] [ThreadPool]: On list_dbt: Close
[0m19:21:28.812094 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:21:28.812340 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:21:28.812545 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:21:28.818831 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.819530 [debug] [ThreadPool]: On list_dbt: Close
[0m19:21:28.821375 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:21:28.821838 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:21:28.824682 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:21:28.824913 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:21:28.825066 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:21:28.830662 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.830922 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:21:28.831080 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:21:28.832062 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.832632 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:21:28.832895 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:21:28.833057 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:21:28.833287 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.833445 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:21:28.835303 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now create_dbt_staging)
[0m19:21:28.835828 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m19:21:28.837940 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m19:21:28.838135 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m19:21:28.838290 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:21:28.843758 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.844029 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m19:21:28.844208 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m19:21:28.844441 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.844926 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m19:21:28.845089 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m19:21:28.845242 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m19:21:28.845450 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.845700 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m19:21:28.848687 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_main)
[0m19:21:28.852163 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:21:28.852384 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:21:28.852551 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:21:28.858174 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.858446 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:21:28.858628 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:21:28.877208 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.881213 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:21:28.881860 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:21:28.882051 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:21:28.884288 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m19:21:28.886498 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:21:28.886674 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:21:28.886822 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:21:28.892161 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.892412 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:21:28.892587 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:21:28.910766 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:28.911683 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:21:28.911912 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:21:28.912068 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:21:28.915428 [debug] [MainThread]: Using duckdb connection "master"
[0m19:21:28.915702 [debug] [MainThread]: On master: BEGIN
[0m19:21:28.915875 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:21:28.921392 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:21:28.921658 [debug] [MainThread]: On master: COMMIT
[0m19:21:28.921828 [debug] [MainThread]: Using duckdb connection "master"
[0m19:21:28.921995 [debug] [MainThread]: On master: COMMIT
[0m19:21:28.922198 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:21:28.922359 [debug] [MainThread]: On master: Close
[0m19:21:28.923796 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:21:28.924033 [info ] [MainThread]: 
[0m19:21:28.925794 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m19:21:28.926181 [info ] [Thread-1  ]: 1 of 33 START sql table model main.dim_date .................................... [RUN]
[0m19:21:28.926731 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_date)
[0m19:21:28.926940 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m19:21:28.933127 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 19:21:28.927078 => 19:21:28.932867
[0m19:21:28.936765 [debug] [Thread-1  ]: Compilation Error in model dim_date (models/dimensions/dim_date.sql)
  'dict object' has no attribute 'generate_date_spine'. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m19:21:28.937191 [error] [Thread-1  ]: 1 of 33 ERROR creating sql table model main.dim_date ........................... [[31mERROR[0m in 0.01s]
[0m19:21:28.937490 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m19:21:28.937706 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m19:21:28.938099 [info ] [Thread-1  ]: 2 of 33 START sql table model main.fact_campaign_performance ................... [RUN]
[0m19:21:28.938583 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.fact_campaign_performance)
[0m19:21:28.938804 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m19:21:28.941163 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m19:21:28.942331 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 19:21:28.938998 => 19:21:28.942160
[0m19:21:28.942603 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m19:21:28.990743 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m19:21:28.991672 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m19:21:28.991967 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: BEGIN
[0m19:21:28.992163 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:28.997818 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:28.998108 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m19:21:28.998327 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */

  
    
    

    create  table
      "dbt"."main"."fact_campaign_performance__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."campaign"  -- Hypothetical source for campaign performance metrics
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS campaign_performance_id,  -- Surrogate Key
    campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
    leads_generated,
    opportunities_created,
    revenue_generated,
    expenses,
    createddate AS performance_created_at
FROM source
    );
  
  
[0m19:21:29.002888 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 19:21:28.942754 => 19:21:29.002715
[0m19:21:29.003200 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: ROLLBACK
[0m19:21:29.006046 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_campaign_performance'
[0m19:21:29.006273 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: Close
[0m19:21:29.008359 [debug] [Thread-1  ]: Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m19:21:29.008800 [error] [Thread-1  ]: 2 of 33 ERROR creating sql table model main.fact_campaign_performance .......... [[31mERROR[0m in 0.07s]
[0m19:21:29.009161 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m19:21:29.009412 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m19:21:29.009726 [info ] [Thread-1  ]: 3 of 33 START sql table model main.fact_product_sales .......................... [RUN]
[0m19:21:29.010142 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_product_sales)
[0m19:21:29.010357 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m19:21:29.012841 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m19:21:29.013721 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 19:21:29.010497 => 19:21:29.013576
[0m19:21:29.013987 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m19:21:29.016734 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_product_sales"
[0m19:21:29.017182 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m19:21:29.017371 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: BEGIN
[0m19:21:29.017549 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.023195 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.023496 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m19:21:29.023715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_product_sales"} */

  
    
    

    create  table
      "dbt"."main"."fact_product_sales__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."pricebook_entry"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS product_sales_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
    product2id AS product_fk,                                      -- Foreign Key to dim_product
    pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
    unitprice AS unit_price,
    isactive AS is_active,
    createddate AS product_sales_created_at,
    lastmodifieddate AS product_sales_last_modified_date
FROM source
    );
  
  
[0m19:21:29.024367 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 19:21:29.014125 => 19:21:29.024265
[0m19:21:29.024572 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: ROLLBACK
[0m19:21:29.025207 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_product_sales'
[0m19:21:29.025498 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: Close
[0m19:21:29.027692 [debug] [Thread-1  ]: Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m19:21:29.028101 [error] [Thread-1  ]: 3 of 33 ERROR creating sql table model main.fact_product_sales ................. [[31mERROR[0m in 0.02s]
[0m19:21:29.028424 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m19:21:29.028646 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m19:21:29.029016 [info ] [Thread-1  ]: 4 of 33 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m19:21:29.029465 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m19:21:29.029679 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m19:21:29.031755 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.032434 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 19:21:29.029819 => 19:21:29.032330
[0m19:21:29.032639 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m19:21:29.043376 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.044236 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.044444 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m19:21:29.044649 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.049950 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.050214 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.050449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m19:21:29.051275 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.056154 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.056465 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m19:21:29.057174 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.059231 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.059445 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m19:21:29.059833 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.069256 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m19:21:29.069498 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.069689 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m19:21:29.070581 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.073656 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m19:21:29.073910 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m19:21:29.074299 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.075060 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 19:21:29.032768 => 19:21:29.074964
[0m19:21:29.075265 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m19:21:29.094516 [info ] [Thread-1  ]: 4 of 33 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.07s]
[0m19:21:29.094935 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m19:21:29.095190 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m19:21:29.095507 [info ] [Thread-1  ]: 5 of 33 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m19:21:29.095904 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m19:21:29.096116 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m19:21:29.098222 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.098706 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 19:21:29.096252 => 19:21:29.098607
[0m19:21:29.098908 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m19:21:29.101689 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.102169 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.102400 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m19:21:29.102590 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.108382 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.108697 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.108936 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m19:21:29.109835 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.112759 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.113019 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m19:21:29.113359 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.115123 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.115337 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m19:21:29.115628 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.116738 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m19:21:29.116978 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.117173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m19:21:29.117894 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.119734 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m19:21:29.119968 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m19:21:29.120378 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.121173 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 19:21:29.099036 => 19:21:29.121072
[0m19:21:29.121383 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m19:21:29.130614 [info ] [Thread-1  ]: 5 of 33 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.03s]
[0m19:21:29.131004 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m19:21:29.131230 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m19:21:29.131652 [info ] [Thread-1  ]: 6 of 33 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m19:21:29.132129 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m19:21:29.132381 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m19:21:29.134528 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.135076 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 19:21:29.132517 => 19:21:29.134968
[0m19:21:29.135273 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m19:21:29.137980 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.138454 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.138665 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m19:21:29.138853 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.144724 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.145052 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.145287 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m19:21:29.146029 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.148500 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.148779 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m19:21:29.149115 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.151959 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.152277 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m19:21:29.152680 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.153681 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m19:21:29.153893 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.154070 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m19:21:29.154783 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.156750 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m19:21:29.157041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m19:21:29.157431 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.158324 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 19:21:29.135401 => 19:21:29.158214
[0m19:21:29.158559 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m19:21:29.166716 [info ] [Thread-1  ]: 6 of 33 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.03s]
[0m19:21:29.167180 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m19:21:29.167435 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m19:21:29.167727 [info ] [Thread-1  ]: 7 of 33 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m19:21:29.168098 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m19:21:29.168297 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m19:21:29.170299 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.170993 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 19:21:29.168429 => 19:21:29.170895
[0m19:21:29.171191 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m19:21:29.173868 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.174313 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.174512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m19:21:29.174690 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.180152 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.180441 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.180650 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m19:21:29.181085 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.183528 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.183855 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m19:21:29.184299 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.186945 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.187208 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m19:21:29.187564 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.188423 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m19:21:29.188622 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.188801 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m19:21:29.189449 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.191100 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m19:21:29.191304 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m19:21:29.191715 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.192680 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 19:21:29.171321 => 19:21:29.192573
[0m19:21:29.192950 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m19:21:29.202060 [info ] [Thread-1  ]: 7 of 33 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.03s]
[0m19:21:29.202467 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m19:21:29.202716 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m19:21:29.203041 [info ] [Thread-1  ]: 8 of 33 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m19:21:29.203408 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m19:21:29.203603 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m19:21:29.205735 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.206295 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 19:21:29.203740 => 19:21:29.206189
[0m19:21:29.206521 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m19:21:29.209427 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.209960 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.210302 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m19:21:29.210495 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.216063 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.216310 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.216545 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m19:21:29.217324 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.219467 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.219688 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m19:21:29.220031 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.221749 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.221946 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m19:21:29.222230 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.223068 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m19:21:29.223258 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.223429 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m19:21:29.224131 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.226862 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m19:21:29.227094 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m19:21:29.227542 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.228379 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 19:21:29.206656 => 19:21:29.228278
[0m19:21:29.228594 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m19:21:29.236552 [info ] [Thread-1  ]: 8 of 33 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.03s]
[0m19:21:29.236938 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m19:21:29.237164 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m19:21:29.237561 [info ] [Thread-1  ]: 9 of 33 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m19:21:29.238038 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m19:21:29.238255 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m19:21:29.240400 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.240896 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 19:21:29.238395 => 19:21:29.240789
[0m19:21:29.241098 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m19:21:29.244471 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.244975 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.245173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m19:21:29.245351 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.250909 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.251172 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.251406 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m19:21:29.252190 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.254346 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.254559 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m19:21:29.254858 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.256578 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.256789 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m19:21:29.257068 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.258090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m19:21:29.258382 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.258570 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m19:21:29.259248 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.261719 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m19:21:29.261934 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m19:21:29.262336 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.263149 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 19:21:29.241228 => 19:21:29.263054
[0m19:21:29.263356 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m19:21:29.272043 [info ] [Thread-1  ]: 9 of 33 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.03s]
[0m19:21:29.272462 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m19:21:29.272717 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m19:21:29.273057 [info ] [Thread-1  ]: 10 of 33 START sql view model staging.stg_salesforce__opportunity .............. [RUN]
[0m19:21:29.273432 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m19:21:29.273628 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m19:21:29.275672 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.276164 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 19:21:29.273762 => 19:21:29.276063
[0m19:21:29.276363 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m19:21:29.278951 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.279406 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.279598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m19:21:29.279771 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.285254 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.285565 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.285806 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m19:21:29.286554 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.288872 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.289211 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m19:21:29.289592 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.291581 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.291795 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m19:21:29.292106 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.292980 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m19:21:29.293244 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.293426 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m19:21:29.294274 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.296113 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m19:21:29.296353 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m19:21:29.296751 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.297665 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 19:21:29.276502 => 19:21:29.297563
[0m19:21:29.297883 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m19:21:29.307064 [info ] [Thread-1  ]: 10 of 33 OK created sql view model staging.stg_salesforce__opportunity ......... [[32mOK[0m in 0.03s]
[0m19:21:29.307490 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m19:21:29.307734 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m19:21:29.308074 [info ] [Thread-1  ]: 11 of 33 START sql view model staging.stg_salesforce__opportunity_history ...... [RUN]
[0m19:21:29.308468 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m19:21:29.308702 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m19:21:29.311450 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.312194 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 19:21:29.308838 => 19:21:29.312084
[0m19:21:29.312390 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m19:21:29.315521 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.316075 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.316342 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m19:21:29.316589 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.322273 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.322559 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.322772 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m19:21:29.323323 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.325706 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.325984 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m19:21:29.326360 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.328316 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.328527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m19:21:29.328827 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.329705 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m19:21:29.329899 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.330096 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m19:21:29.330671 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.332519 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m19:21:29.332726 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m19:21:29.333122 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.333958 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 19:21:29.312522 => 19:21:29.333856
[0m19:21:29.334176 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m19:21:29.342892 [info ] [Thread-1  ]: 11 of 33 OK created sql view model staging.stg_salesforce__opportunity_history . [[32mOK[0m in 0.03s]
[0m19:21:29.343319 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m19:21:29.343567 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m19:21:29.343863 [info ] [Thread-1  ]: 12 of 33 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m19:21:29.344258 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m19:21:29.344466 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m19:21:29.346476 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.346964 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 19:21:29.344602 => 19:21:29.346868
[0m19:21:29.347156 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m19:21:29.350563 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.350971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.351175 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m19:21:29.351399 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.356793 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.357111 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.357337 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m19:21:29.357840 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.360234 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.360472 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m19:21:29.360810 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.362582 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.362789 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m19:21:29.363106 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.364022 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m19:21:29.364222 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.364400 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m19:21:29.364980 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.366594 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m19:21:29.366804 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m19:21:29.367170 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.367984 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 19:21:29.347281 => 19:21:29.367882
[0m19:21:29.368231 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m19:21:29.376214 [info ] [Thread-1  ]: 12 of 33 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.03s]
[0m19:21:29.376626 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m19:21:29.376877 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m19:21:29.377198 [info ] [Thread-1  ]: 13 of 33 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m19:21:29.377567 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m19:21:29.377845 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m19:21:29.379907 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.380377 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 19:21:29.377997 => 19:21:29.380281
[0m19:21:29.380569 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m19:21:29.383734 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.384153 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.384344 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m19:21:29.384523 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.389902 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.390193 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.390425 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m19:21:29.391112 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.393241 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.393467 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m19:21:29.393769 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.395451 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.395658 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m19:21:29.395940 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.396794 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m19:21:29.396983 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.397157 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m19:21:29.397883 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.399723 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m19:21:29.399943 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m19:21:29.400340 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.401329 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 19:21:29.380696 => 19:21:29.401224
[0m19:21:29.401598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m19:21:29.412218 [info ] [Thread-1  ]: 13 of 33 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.03s]
[0m19:21:29.412608 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m19:21:29.412836 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m19:21:29.413295 [info ] [Thread-1  ]: 14 of 33 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m19:21:29.413801 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m19:21:29.414043 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m19:21:29.416158 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.416657 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 19:21:29.414184 => 19:21:29.416557
[0m19:21:29.416869 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m19:21:29.419955 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.420428 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.420626 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m19:21:29.420801 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.426184 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.426461 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.426667 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m19:21:29.427127 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.430010 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.430219 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m19:21:29.430522 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.432230 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.432439 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m19:21:29.432723 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.433757 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m19:21:29.434106 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.434337 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m19:21:29.434990 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.436819 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m19:21:29.437051 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m19:21:29.437424 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.438196 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 19:21:29.417000 => 19:21:29.438099
[0m19:21:29.438405 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m19:21:29.447256 [info ] [Thread-1  ]: 14 of 33 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.03s]
[0m19:21:29.447644 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m19:21:29.447880 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m19:21:29.448226 [info ] [Thread-1  ]: 15 of 33 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m19:21:29.448697 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m19:21:29.448908 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m19:21:29.450944 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.451588 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 19:21:29.449044 => 19:21:29.451484
[0m19:21:29.451796 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m19:21:29.454507 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.455016 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.455211 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m19:21:29.455387 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.460809 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.461075 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.461281 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m19:21:29.461785 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.464986 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.465311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m19:21:29.465716 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.467608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.467815 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m19:21:29.468127 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.469000 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m19:21:29.469188 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.469362 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m19:21:29.469915 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.471442 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m19:21:29.471642 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m19:21:29.471976 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.472657 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 19:21:29.451927 => 19:21:29.472565
[0m19:21:29.472860 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m19:21:29.480720 [info ] [Thread-1  ]: 15 of 33 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.03s]
[0m19:21:29.481130 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m19:21:29.481385 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m19:21:29.481738 [info ] [Thread-1  ]: 16 of 33 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m19:21:29.482142 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m19:21:29.482357 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m19:21:29.484464 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.484916 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 19:21:29.482495 => 19:21:29.484818
[0m19:21:29.485112 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m19:21:29.487918 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.488403 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.488604 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m19:21:29.488780 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.494163 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.494407 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.494654 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m19:21:29.495639 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.497771 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.497986 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m19:21:29.498312 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.500700 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.500901 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m19:21:29.501203 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.502257 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m19:21:29.502544 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.502736 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m19:21:29.503413 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.505190 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m19:21:29.505393 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m19:21:29.505767 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.506619 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 19:21:29.485247 => 19:21:29.506522
[0m19:21:29.506860 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m19:21:29.515639 [info ] [Thread-1  ]: 16 of 33 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.03s]
[0m19:21:29.516066 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m19:21:29.516316 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m19:21:29.516592 [info ] [Thread-1  ]: 17 of 33 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m19:21:29.516946 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m19:21:29.517142 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m19:21:29.519129 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.519606 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 19:21:29.517275 => 19:21:29.519498
[0m19:21:29.519812 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m19:21:29.522570 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.523036 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.523229 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m19:21:29.523405 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.528822 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.529142 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.529373 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m19:21:29.530010 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.532584 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.532858 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m19:21:29.533216 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.535787 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.536015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m19:21:29.536314 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.537351 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m19:21:29.537623 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.537817 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m19:21:29.538447 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.566914 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m19:21:29.567252 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m19:21:29.567766 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.568545 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 19:21:29.519941 => 19:21:29.568451
[0m19:21:29.568748 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m19:21:29.576778 [info ] [Thread-1  ]: 17 of 33 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.06s]
[0m19:21:29.577171 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m19:21:29.577418 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m19:21:29.577697 [info ] [Thread-1  ]: 18 of 33 START sql table model main.dim_account ................................ [RUN]
[0m19:21:29.578131 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m19:21:29.578331 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m19:21:29.580349 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m19:21:29.580839 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 19:21:29.578466 => 19:21:29.580741
[0m19:21:29.581032 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m19:21:29.583675 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m19:21:29.584422 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m19:21:29.584623 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m19:21:29.584794 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.590207 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.590463 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m19:21:29.590655 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."main"."dim_account__dbt_tmp"
  
    as (
      

select
    id as account_id,
    name as account_name,
    industry,
    type as account_type,
    billing_city,
    billing_state,
    billing_country,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__account"
where is_deleted = false;
    );
  
  
[0m19:21:29.592017 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 19:21:29.581162 => 19:21:29.591859
[0m19:21:29.592290 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: ROLLBACK
[0m19:21:29.592830 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_account'
[0m19:21:29.593016 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m19:21:29.594964 [debug] [Thread-1  ]: Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.595334 [error] [Thread-1  ]: 18 of 33 ERROR creating sql table model main.dim_account ....................... [[31mERROR[0m in 0.02s]
[0m19:21:29.595652 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m19:21:29.595897 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m19:21:29.596183 [info ] [Thread-1  ]: 19 of 33 START sql table model main.dim_campaign ............................... [RUN]
[0m19:21:29.596539 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m19:21:29.596744 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m19:21:29.599068 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m19:21:29.599597 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 19:21:29.596874 => 19:21:29.599497
[0m19:21:29.599796 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m19:21:29.602492 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m19:21:29.603102 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m19:21:29.603295 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m19:21:29.603467 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.608581 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.608834 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m19:21:29.609082 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."main"."dim_campaign__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    id AS campaign_id,
    parentid AS parent_campaign_id,
    ownerid AS owner_id,
    createdbyid AS created_by_id,
    lastmodifiedbyid AS last_modified_by_id,
    campaignmemberrecordtypeid AS campaign_member_record_type_id,

    /* Dates */
    startdate AS start_date,
    enddate AS end_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,

    /* Dimensions */
    name AS campaign_name,
    type AS campaign_type,
    status,
    description,
    isactive AS is_active,

    /* Metrics */
    expectedrevenue AS expected_revenue,
    budgetedcost AS budgeted_cost,
    actualcost AS actual_cost,
    expectedresponse AS expected_response,
    numbersent AS number_sent,
    numberofleads AS number_of_leads,
    numberofconvertedleads AS number_of_converted_leads,
    numberofcontacts AS number_of_contacts,
    numberofresponses AS number_of_responses,
    numberofopportunities AS number_of_opportunities,
    numberofwonopportunities AS number_of_won_opportunities,
    amountallopportunities AS amount_all_opportunities,
    amountwonopportunities AS amount_won_opportunities,
    hierarchynumberofleads AS hierarchy_number_of_leads,
    hierarchynumberofconvertedleads AS hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts AS hierarchy_number_of_contacts,
    hierarchynumberofresponses AS hierarchy_number_of_responses,
    hierarchynumberofopportunities AS hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities AS hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities AS hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities AS hierarchy_amount_won_opportunities,
    hierarchynumbersent AS hierarchy_number_sent,
    hierarchyexpectedrevenue AS hierarchy_expected_revenue,
    hierarchybudgetedcost AS hierarchy_budgeted_cost,
    hierarchyactualcost AS hierarchy_actual_cost

FROM "dbt"."staging"."stg_salesforce__campaign"
WHERE isdeleted = FALSE;
    );
  
  
[0m19:21:29.609581 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 19:21:29.599931 => 19:21:29.609487
[0m19:21:29.609778 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: ROLLBACK
[0m19:21:29.610291 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_campaign'
[0m19:21:29.610482 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m19:21:29.612457 [debug] [Thread-1  ]: Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.612917 [error] [Thread-1  ]: 19 of 33 ERROR creating sql table model main.dim_campaign ...................... [[31mERROR[0m in 0.02s]
[0m19:21:29.613248 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m19:21:29.613486 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m19:21:29.613782 [info ] [Thread-1  ]: 20 of 33 START sql table model main.dim_case_status ............................ [RUN]
[0m19:21:29.614200 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m19:21:29.614448 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m19:21:29.617337 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.618424 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 19:21:29.614587 => 19:21:29.618287
[0m19:21:29.618668 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m19:21:29.621244 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.622095 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.622351 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m19:21:29.622524 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.627884 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.628151 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.628353 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."main"."dim_case_status__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        status AS status_name,
        status AS status_description  -- Adjust as needed, typically a description field should be separate
    FROM "dbt"."staging"."stg_salesforce__case_history_2"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY status_name) AS status_id,  -- Surrogate Key
    status_name,
    status_description
FROM source
    );
  
  
[0m19:21:29.631412 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.633504 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.633726 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m19:21:29.634041 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.635751 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.635950 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."main"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m19:21:29.636234 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.639079 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m19:21:29.639288 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.639465 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m19:21:29.640051 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.641834 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m19:21:29.642133 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."main"."dim_case_status__dbt_backup" cascade
[0m19:21:29.642565 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.643415 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 19:21:29.618802 => 19:21:29.643319
[0m19:21:29.643639 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m19:21:29.653326 [info ] [Thread-1  ]: 20 of 33 OK created sql table model main.dim_case_status ....................... [[32mOK[0m in 0.04s]
[0m19:21:29.653728 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m19:21:29.653956 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m19:21:29.654351 [info ] [Thread-1  ]: 21 of 33 START sql table model main.dim_contact ................................ [RUN]
[0m19:21:29.654899 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m19:21:29.655164 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m19:21:29.657649 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.658641 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 19:21:29.655321 => 19:21:29.658514
[0m19:21:29.659240 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m19:21:29.663942 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.664740 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.665076 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m19:21:29.665297 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.672350 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.672668 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.672940 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."main"."dim_contact__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    contact_id,
    masterrecordid AS master_record_id,
    accountid AS account_id,
    reportstoid AS reports_to_id,
    ownerid AS owner_id,
    jigsawcontactid AS jigsaw_contact_id,
    individualid AS individual_id,

    /* Dates */
    birthdate AS birth_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    lastcurequestdate AS last_cu_request_date,
    lastcuupdatedate AS last_cu_update_date,
    emailbounceddate AS email_bounced_date,

    /* Dimensions */
    salutation,
    firstname AS first_name,
    lastname AS last_name,
    otherstreet AS other_street,
    othercity AS other_city,
    otherstate AS other_state,
    otherpostalcode AS other_postal_code,
    othercountry AS other_country,
    otherlatitude AS other_latitude,
    otherlongitude AS other_longitude,
    othergeocodeaccuracy AS other_geocode_accuracy,
    mailingstreet AS mailing_street,
    mailingcity AS mailing_city,
    mailingstate AS mailing_state,
    mailingpostalcode AS mailing_postal_code,
    mailingcountry AS mailing_country,
    mailinglatitude AS mailing_latitude,
    mailinglongitude AS mailing_longitude,
    mailinggeocodeaccuracy AS mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone AS mobile_phone,
    homephone AS home_phone,
    otherphone AS other_phone,
    assistantphone AS assistant_phone,
    email,
    title,
    department,
    assistantname AS assistant_name,
    leadsource AS lead_source,
    description,
    pronouns,
    genderidentity AS gender_identity,
    cleanstatus AS clean_status,
    emailbouncedreason AS email_bounced_reason,
    level__c AS level,
    languages__c AS languages,

    /* Metrics */
    hasoptedoutofemail AS has_opted_out_of_email,
    hasoptedoutoffax AS has_opted_out_of_fax,
    donotcall AS do_not_call

FROM "dbt"."staging"."stg_salesforce__contact"
WHERE isdeleted = FALSE
    );
  
  
[0m19:21:29.677660 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.680067 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.680388 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."main"."dim_contact" rename to "dim_contact__dbt_backup"
[0m19:21:29.680866 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.683160 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.683446 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."main"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m19:21:29.683890 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.685313 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m19:21:29.685607 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.685827 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m19:21:29.687385 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.689394 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m19:21:29.689647 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."main"."dim_contact__dbt_backup" cascade
[0m19:21:29.690321 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.691253 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 19:21:29.659682 => 19:21:29.691146
[0m19:21:29.691658 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m19:21:29.711888 [info ] [Thread-1  ]: 21 of 33 OK created sql table model main.dim_contact ........................... [[32mOK[0m in 0.06s]
[0m19:21:29.712324 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m19:21:29.712558 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m19:21:29.712959 [info ] [Thread-1  ]: 22 of 33 START sql table model main.dim_lead ................................... [RUN]
[0m19:21:29.713467 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m19:21:29.713687 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m19:21:29.715814 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m19:21:29.716343 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 19:21:29.713824 => 19:21:29.716236
[0m19:21:29.716544 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m19:21:29.720208 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m19:21:29.720710 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m19:21:29.720901 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m19:21:29.721072 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.726536 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.726811 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m19:21:29.727007 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."main"."dim_lead__dbt_tmp"
  
    as (
      

select
    id as lead_id,
    first_name,
    last_name,
    company,
    status,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__lead";
    );
  
  
[0m19:21:29.727416 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 19:21:29.716677 => 19:21:29.727321
[0m19:21:29.727610 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: ROLLBACK
[0m19:21:29.728135 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_lead'
[0m19:21:29.728313 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m19:21:29.730296 [debug] [Thread-1  ]: Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.730632 [error] [Thread-1  ]: 22 of 33 ERROR creating sql table model main.dim_lead .......................... [[31mERROR[0m in 0.02s]
[0m19:21:29.730940 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m19:21:29.731163 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m19:21:29.731472 [info ] [Thread-1  ]: 23 of 33 START sql table model main.dim_opportunity ............................ [RUN]
[0m19:21:29.731810 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m19:21:29.731995 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m19:21:29.734185 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.735221 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 19:21:29.732124 => 19:21:29.735042
[0m19:21:29.735517 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m19:21:29.738225 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.738974 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.739209 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m19:21:29.739392 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.744852 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.745131 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.745367 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    opportunity_id,
    accountid AS account_id,
    ownerid AS user_id,
    contactid AS primary_contact_id,
    campaignid AS campaign_id,
    pricebook2id AS pricebook_id,
    contractid AS contract_id,
    primarypartneraccountid AS primary_partner_account_id,
    createdbyid AS created_by_id,
    lastmodifiedbyid AS last_modified_by_id,
    lastamountchangedhistoryid AS last_amount_change_history_id,
    lastclosedatechangedhistoryid AS last_close_date_change_history_id,

    /* Metrics */
    amount,
    probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,

    /* Dimensions */
    name AS opportunity_name,
    description,
    stagename AS stage_name,
    stagesortorder AS stage_sort_order,
    type AS opportunity_type,
    nextstep AS next_step,
    leadsource AS lead_source,
    isclosed AS is_closed,
    iswon AS is_won,
    forecastcategory AS forecast_category,
    forecastcategoryname AS forecast_category_name,
    hasopportunitylineitem AS has_opportunity_line_item,
    deliveryinstallationstatus__c AS delivery_installation_status,
    trackingnumber__c AS tracking_number,
    ordernumber__c AS order_number,
    currentgenerators__c AS current_generators,
    maincompetitors__c AS main_competitors,

    /* Dates */
    closedate AS close_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    laststagechangedate AS last_stage_change_date,
    fiscalyear AS fiscal_year,
    fiscalquarter AS fiscal_quarter

FROM "dbt"."staging"."stg_salesforce__opportunity"
    );
  
  
[0m19:21:29.748543 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.750820 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.751077 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."main"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m19:21:29.751491 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.753290 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.753494 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."main"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m19:21:29.753821 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.754863 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m19:21:29.755055 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.755230 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m19:21:29.756682 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.758311 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m19:21:29.758520 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."main"."dim_opportunity__dbt_backup" cascade
[0m19:21:29.759025 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.759744 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 19:21:29.735675 => 19:21:29.759649
[0m19:21:29.759960 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m19:21:29.773903 [info ] [Thread-1  ]: 23 of 33 OK created sql table model main.dim_opportunity ....................... [[32mOK[0m in 0.04s]
[0m19:21:29.774282 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m19:21:29.774514 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m19:21:29.774895 [info ] [Thread-1  ]: 24 of 33 START sql table model main.dim_opportunity_stage ...................... [RUN]
[0m19:21:29.775376 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m19:21:29.775584 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m19:21:29.778720 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.779210 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 19:21:29.775725 => 19:21:29.779102
[0m19:21:29.779414 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m19:21:29.782179 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.782643 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.782834 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m19:21:29.783006 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.788842 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.789181 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.789400 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."main"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        stagename AS stage_name,
        stagesortorder AS stage_sort_order
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY stage_sort_order) AS stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
FROM source
    );
  
  
[0m19:21:29.793078 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.795305 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.795539 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m19:21:29.795912 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.797679 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.797880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."main"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m19:21:29.798258 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.799207 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m19:21:29.799398 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.799573 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m19:21:29.800195 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.801836 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m19:21:29.802045 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."main"."dim_opportunity_stage__dbt_backup" cascade
[0m19:21:29.802416 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.803175 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 19:21:29.779546 => 19:21:29.803078
[0m19:21:29.803384 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m19:21:29.818996 [info ] [Thread-1  ]: 24 of 33 OK created sql table model main.dim_opportunity_stage ................. [[32mOK[0m in 0.04s]
[0m19:21:29.819426 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m19:21:29.819686 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m19:21:29.819963 [info ] [Thread-1  ]: 25 of 33 START sql table model main.dim_pricebook .............................. [RUN]
[0m19:21:29.820329 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m19:21:29.820519 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m19:21:29.822632 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m19:21:29.823133 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 19:21:29.820651 => 19:21:29.823032
[0m19:21:29.823323 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m19:21:29.826904 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m19:21:29.827720 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m19:21:29.827988 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m19:21:29.828168 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.833850 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.834125 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m19:21:29.834329 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."main"."dim_pricebook__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        pricebook_entry_id AS pricebook_id,        -- Surrogate Key
        name AS pricebook_name,
        isactive AS is_active,
        description,
        createddate AS pricebook_created_date,
        lastmodifieddate AS pricebook_last_modified_date
    FROM "dbt"."staging"."stg_salesforce__pricebook_entry"
)

SELECT * FROM source
    );
  
  
[0m19:21:29.835073 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 19:21:29.823450 => 19:21:29.834975
[0m19:21:29.835273 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: ROLLBACK
[0m19:21:29.835785 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_pricebook'
[0m19:21:29.835966 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m19:21:29.837992 [debug] [Thread-1  ]: Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m19:21:29.838445 [error] [Thread-1  ]: 25 of 33 ERROR creating sql table model main.dim_pricebook ..................... [[31mERROR[0m in 0.02s]
[0m19:21:29.838796 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m19:21:29.839047 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m19:21:29.839365 [info ] [Thread-1  ]: 26 of 33 START sql table model main.dim_product ................................ [RUN]
[0m19:21:29.839809 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m19:21:29.840042 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m19:21:29.842105 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m19:21:29.842615 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 19:21:29.840175 => 19:21:29.842522
[0m19:21:29.842813 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m19:21:29.845160 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m19:21:29.845526 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m19:21:29.845711 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m19:21:29.845879 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.851408 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.851689 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m19:21:29.851884 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."main"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    product_code,
    is_active,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__product_2"
where is_active = true;
    );
  
  
[0m19:21:29.852299 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 19:21:29.842942 => 19:21:29.852204
[0m19:21:29.852496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: ROLLBACK
[0m19:21:29.853015 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_product'
[0m19:21:29.853194 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m19:21:29.855297 [debug] [Thread-1  ]: Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.855740 [error] [Thread-1  ]: 26 of 33 ERROR creating sql table model main.dim_product ....................... [[31mERROR[0m in 0.02s]
[0m19:21:29.856102 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m19:21:29.856408 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m19:21:29.856679 [info ] [Thread-1  ]: 27 of 33 START sql table model main.dim_solution ............................... [RUN]
[0m19:21:29.857055 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_solution)
[0m19:21:29.857358 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m19:21:29.859616 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m19:21:29.860134 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 19:21:29.857549 => 19:21:29.860043
[0m19:21:29.860335 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m19:21:29.862794 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m19:21:29.863208 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m19:21:29.863399 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m19:21:29.863577 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.869145 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.869435 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m19:21:29.869631 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."main"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solution_name,
    status,
    is_active,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__solution";
    );
  
  
[0m19:21:29.870058 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 19:21:29.860464 => 19:21:29.869956
[0m19:21:29.870375 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: ROLLBACK
[0m19:21:29.870894 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_solution'
[0m19:21:29.871087 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m19:21:29.873238 [debug] [Thread-1  ]: Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.873678 [error] [Thread-1  ]: 27 of 33 ERROR creating sql table model main.dim_solution ...................... [[31mERROR[0m in 0.02s]
[0m19:21:29.874000 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m19:21:29.874256 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:21:29.874641 [info ] [Thread-1  ]: 28 of 33 START sql table model main.dim_user ................................... [RUN]
[0m19:21:29.875113 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m19:21:29.875324 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:21:29.878347 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:21:29.878863 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:21:29.875460 => 19:21:29.878765
[0m19:21:29.879058 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:21:29.881709 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:21:29.882171 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:21:29.882362 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:21:29.882532 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.888193 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.888499 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:21:29.888712 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.first_name,
    u.last_name,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.role_id = r.id
where u.is_active = true;
    );
  
  
[0m19:21:29.889141 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:21:29.879189 => 19:21:29.889040
[0m19:21:29.889336 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:21:29.889998 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:21:29.890318 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:21:29.892415 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.892858 [error] [Thread-1  ]: 28 of 33 ERROR creating sql table model main.dim_user .......................... [[31mERROR[0m in 0.02s]
[0m19:21:29.893192 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:21:29.893410 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user_role
[0m19:21:29.893886 [info ] [Thread-1  ]: 29 of 33 START sql table model main.dim_user_role .............................. [RUN]
[0m19:21:29.894313 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.dim_user_role)
[0m19:21:29.894519 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user_role
[0m19:21:29.896751 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user_role"
[0m19:21:29.897266 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (compile): 19:21:29.894657 => 19:21:29.897160
[0m19:21:29.897466 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user_role
[0m19:21:29.900147 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user_role"
[0m19:21:29.900686 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m19:21:29.900954 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: BEGIN
[0m19:21:29.901150 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:21:29.906603 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:21:29.906921 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user_role"
[0m19:21:29.907124 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user_role"} */

  
    
    

    create  table
      "dbt"."main"."dim_user_role__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."staging"."stg_salesforce__user_role"
)

SELECT
    user_role_id,                           -- Surrogate Key
    name AS role_name,
    roledescription AS role_description
FROM source
    );
  
  
[0m19:21:29.907948 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user_role (execute): 19:21:29.897597 => 19:21:29.907847
[0m19:21:29.908148 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: ROLLBACK
[0m19:21:29.908667 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user_role'
[0m19:21:29.908858 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user_role: Close
[0m19:21:29.911023 [debug] [Thread-1  ]: Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "roledescription" not found in FROM clause!
  Candidate bindings: "source.rollupdescription"
  LINE 21:     roledescription AS role_description
               ^
[0m19:21:29.911481 [error] [Thread-1  ]: 29 of 33 ERROR creating sql table model main.dim_user_role ..................... [[31mERROR[0m in 0.02s]
[0m19:21:29.911813 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user_role
[0m19:21:29.912045 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m19:21:29.912255 [info ] [Thread-1  ]: 30 of 33 SKIP relation main.fact_case .......................................... [[33mSKIP[0m]
[0m19:21:29.912542 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m19:21:29.912744 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m19:21:29.912937 [info ] [Thread-1  ]: 31 of 33 SKIP relation main.fact_opportunity ................................... [[33mSKIP[0m]
[0m19:21:29.913271 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m19:21:29.913523 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m19:21:29.913733 [info ] [Thread-1  ]: 32 of 33 SKIP relation main.fact_case_history .................................. [[33mSKIP[0m]
[0m19:21:29.914060 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m19:21:29.914327 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m19:21:29.914623 [info ] [Thread-1  ]: 33 of 33 SKIP relation main.fact_opportunity_history ........................... [[33mSKIP[0m]
[0m19:21:29.914965 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m19:21:29.915812 [debug] [MainThread]: Using duckdb connection "master"
[0m19:21:29.916043 [debug] [MainThread]: On master: BEGIN
[0m19:21:29.916204 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:21:29.921869 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:21:29.922134 [debug] [MainThread]: On master: COMMIT
[0m19:21:29.922294 [debug] [MainThread]: Using duckdb connection "master"
[0m19:21:29.922452 [debug] [MainThread]: On master: COMMIT
[0m19:21:29.922656 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:21:29.922810 [debug] [MainThread]: On master: Close
[0m19:21:29.924289 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:21:29.924475 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user_role' was properly closed.
[0m19:21:29.924717 [info ] [MainThread]: 
[0m19:21:29.924899 [info ] [MainThread]: Finished running 15 table models, 14 view models, 4 incremental models in 0 hours 0 minutes and 1.14 seconds (1.14s).
[0m19:21:29.926877 [debug] [MainThread]: Command end result
[0m19:21:29.935910 [info ] [MainThread]: 
[0m19:21:29.936212 [info ] [MainThread]: [31mCompleted with 11 errors and 0 warnings:[0m
[0m19:21:29.936387 [info ] [MainThread]: 
[0m19:21:29.936559 [error] [MainThread]:   Compilation Error in model dim_date (models/dimensions/dim_date.sql)
  'dict object' has no attribute 'generate_date_spine'. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m19:21:29.936724 [info ] [MainThread]: 
[0m19:21:29.936893 [error] [MainThread]:   Runtime Error in model fact_campaign_performance (models/facts/fact_campaign_performance.sql)
  Binder Error: Referenced column "campaignid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     campaignid AS campaign_fk,                                            -- Foreign Key to dim_campaign
      leads_generated,
      opportunities_created,
      revenue_generated,
      expenses,
      createddate AS performance_created_at
  FROM source
      );
    
    ...
               ^
[0m19:21:29.937072 [info ] [MainThread]: 
[0m19:21:29.937243 [error] [MainThread]:   Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m19:21:29.937430 [info ] [MainThread]: 
[0m19:21:29.937580 [error] [MainThread]:   Runtime Error in model dim_account (models/dimensions/dim_account.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.937737 [info ] [MainThread]: 
[0m19:21:29.937890 [error] [MainThread]:   Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.938038 [info ] [MainThread]: 
[0m19:21:29.938188 [error] [MainThread]:   Runtime Error in model dim_lead (models/dimensions/dim_lead.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.938348 [info ] [MainThread]: 
[0m19:21:29.938612 [error] [MainThread]:   Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "name" not found in FROM clause!
  Candidate bindings: "stg_salesforce__pricebook_entry.isactive"
  LINE 16:         name AS pricebook_name,
                   ^
[0m19:21:29.938818 [info ] [MainThread]: 
[0m19:21:29.938973 [error] [MainThread]:   Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.939126 [info ] [MainThread]: 
[0m19:21:29.939274 [error] [MainThread]:   Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.939418 [info ] [MainThread]: 
[0m19:21:29.939656 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m19:21:29.939874 [info ] [MainThread]: 
[0m19:21:29.940039 [error] [MainThread]:   Runtime Error in model dim_user_role (models/dimensions/dim_user_role.sql)
  Binder Error: Referenced column "roledescription" not found in FROM clause!
  Candidate bindings: "source.rollupdescription"
  LINE 21:     roledescription AS role_description
               ^
[0m19:21:29.940217 [info ] [MainThread]: 
[0m19:21:29.940384 [info ] [MainThread]: Done. PASS=18 WARN=0 ERROR=11 SKIP=4 TOTAL=33
[0m19:21:29.940741 [debug] [MainThread]: Command `dbt run` failed at 19:21:29.940690 after 1.49 seconds
[0m19:21:29.940943 [debug] [MainThread]: Flushing usage events


============================== 19:22:27.482415 | 395eca3c-b778-45ad-ab1a-dad8e2026b5e ==============================
[0m19:22:27.482415 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:22:27.485517 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'send_anonymous_usage_stats': 'False'}
[0m19:22:27.485781 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:22:27.571315 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:22:27.590783 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:22:27.634297 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m19:22:27.634769 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user_role.sql
[0m19:22:27.635007 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:22:27.665676 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m19:22:27.680083 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:22:27.681631 [info ] [MainThread]: 
[0m19:22:27.682118 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:22:27.682681 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:22:27.689852 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:22:27.690103 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:22:27.690280 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:22:27.702991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.704017 [debug] [ThreadPool]: On list_dbt: Close
[0m19:22:27.706028 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:22:27.706495 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:22:27.709484 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:27.709785 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:22:27.709959 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:27.716190 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.716484 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:27.716655 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:22:27.716903 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.717437 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:22:27.717614 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:27.717769 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:22:27.718025 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.718198 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:22:27.721683 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m19:22:27.725603 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:22:27.725898 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:22:27.726060 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:27.731874 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.732267 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:22:27.732500 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:22:27.752413 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.753648 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:22:27.754270 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:22:27.754481 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:22:27.756891 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m19:22:27.759524 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:22:27.759747 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:22:27.759917 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:27.765579 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.765833 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:22:27.766011 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:22:27.781646 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:27.785305 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:22:27.785580 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:22:27.785742 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:22:27.788503 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:27.788686 [debug] [MainThread]: On master: BEGIN
[0m19:22:27.788837 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:22:27.794316 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:27.794557 [debug] [MainThread]: On master: COMMIT
[0m19:22:27.794716 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:27.794868 [debug] [MainThread]: On master: COMMIT
[0m19:22:27.795070 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:27.795229 [debug] [MainThread]: On master: Close
[0m19:22:27.796841 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:22:27.797116 [info ] [MainThread]: 
[0m19:22:27.798619 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:22:27.798951 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:22:27.800225 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m19:22:27.800468 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:22:27.805931 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:22:27.806437 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:22:27.800614 => 19:22:27.806328
[0m19:22:27.806635 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:22:27.826172 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:22:27.826774 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:22:27.826981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:22:27.827159 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:22:27.832841 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:22:27.833125 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:22:27.833329 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.first_name,
    u.last_name,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.role_id = r.id
where u.is_active = true
    );
  
  
[0m19:22:27.834855 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:22:27.806766 => 19:22:27.834751
[0m19:22:27.835068 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:22:27.838387 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:22:27.838575 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:22:27.840348 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "role_id"
  LINE 24:     on u.role_id = r.id
                  ^
[0m19:22:27.840730 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:22:27.841060 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:22:27.841804 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:27.841974 [debug] [MainThread]: On master: BEGIN
[0m19:22:27.842122 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:22:27.847725 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:27.848031 [debug] [MainThread]: On master: COMMIT
[0m19:22:27.848205 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:27.848357 [debug] [MainThread]: On master: COMMIT
[0m19:22:27.848569 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:27.848734 [debug] [MainThread]: On master: Close
[0m19:22:27.850258 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:22:27.850501 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:22:27.850685 [info ] [MainThread]: 
[0m19:22:27.850878 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m19:22:27.851218 [debug] [MainThread]: Command end result
[0m19:22:27.859828 [info ] [MainThread]: 
[0m19:22:27.860112 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:22:27.860285 [info ] [MainThread]: 
[0m19:22:27.860455 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "role_id"
  LINE 24:     on u.role_id = r.id
                  ^
[0m19:22:27.860621 [info ] [MainThread]: 
[0m19:22:27.860805 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:22:27.861179 [debug] [MainThread]: Command `dbt run` failed at 19:22:27.861131 after 0.40 seconds
[0m19:22:27.861382 [debug] [MainThread]: Flushing usage events


============================== 19:22:50.914008 | 7281f893-b6ce-4563-b77f-90ad3724ee8f ==============================
[0m19:22:50.914008 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:22:50.917176 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m19:22:50.917443 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:22:50.993361 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:22:51.011860 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:22:51.055102 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:22:51.055579 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:22:51.083686 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m19:22:51.098577 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:22:51.100167 [info ] [MainThread]: 
[0m19:22:51.100662 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:22:51.101242 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:22:51.108654 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:22:51.108956 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:22:51.109132 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:22:51.121134 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.122071 [debug] [ThreadPool]: On list_dbt: Close
[0m19:22:51.123944 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:22:51.124413 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:22:51.127598 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:51.127833 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:22:51.127999 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:51.133834 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.134112 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:51.134290 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:22:51.134536 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.135099 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:22:51.135333 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:22:51.135494 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:22:51.135739 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.135893 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:22:51.138704 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m19:22:51.142175 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:22:51.142444 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:22:51.142604 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:51.147650 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.147893 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:22:51.148065 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:22:51.165726 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.166836 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:22:51.167460 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:22:51.167635 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:22:51.169650 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m19:22:51.172082 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:22:51.172260 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:22:51.172407 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:22:51.178128 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.178413 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:22:51.178598 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:22:51.193997 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:51.197474 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:22:51.197716 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:22:51.197881 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:22:51.200729 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:51.200912 [debug] [MainThread]: On master: BEGIN
[0m19:22:51.201066 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:22:51.206300 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:51.206565 [debug] [MainThread]: On master: COMMIT
[0m19:22:51.206725 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:51.206904 [debug] [MainThread]: On master: COMMIT
[0m19:22:51.207113 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:51.207291 [debug] [MainThread]: On master: Close
[0m19:22:51.208609 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:22:51.208807 [info ] [MainThread]: 
[0m19:22:51.210215 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:22:51.210499 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:22:51.211621 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m19:22:51.211814 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:22:51.217272 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:22:51.218162 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:22:51.211953 => 19:22:51.217978
[0m19:22:51.218545 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:22:51.237784 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:22:51.238452 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:22:51.238655 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:22:51.238832 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:22:51.244408 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:22:51.244689 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:22:51.244895 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.first_name,
    u.last_name,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.user_role_id = r.id
where u.is_active = true
    );
  
  
[0m19:22:51.246382 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:22:51.218731 => 19:22:51.246271
[0m19:22:51.246596 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:22:51.250176 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:22:51.250392 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:22:51.252187 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "user_role_id"
  LINE 24:     on u.user_role_id = r.id
                  ^
[0m19:22:51.252584 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:22:51.252930 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:22:51.253682 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:51.253859 [debug] [MainThread]: On master: BEGIN
[0m19:22:51.254007 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:22:51.259132 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:51.259436 [debug] [MainThread]: On master: COMMIT
[0m19:22:51.259642 [debug] [MainThread]: Using duckdb connection "master"
[0m19:22:51.259799 [debug] [MainThread]: On master: COMMIT
[0m19:22:51.260060 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:22:51.260215 [debug] [MainThread]: On master: Close
[0m19:22:51.261763 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:22:51.262096 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:22:51.262286 [info ] [MainThread]: 
[0m19:22:51.262500 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m19:22:51.262882 [debug] [MainThread]: Command end result
[0m19:22:51.271162 [info ] [MainThread]: 
[0m19:22:51.271455 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:22:51.271650 [info ] [MainThread]: 
[0m19:22:51.271845 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "user_role_id"
  LINE 24:     on u.user_role_id = r.id
                  ^
[0m19:22:51.272077 [info ] [MainThread]: 
[0m19:22:51.272291 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:22:51.272785 [debug] [MainThread]: Command `dbt run` failed at 19:22:51.272715 after 0.38 seconds
[0m19:22:51.272995 [debug] [MainThread]: Flushing usage events


============================== 19:23:59.129958 | 3d62a8ac-c8ca-41e6-902a-b3d7a0788c2b ==============================
[0m19:23:59.129958 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:23:59.132809 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'send_anonymous_usage_stats': 'False'}
[0m19:23:59.133047 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:23:59.209455 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:23:59.228157 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:23:59.270434 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:23:59.270919 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:23:59.298823 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m19:23:59.313157 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:23:59.314696 [info ] [MainThread]: 
[0m19:23:59.315158 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:23:59.315869 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:23:59.323289 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:23:59.323517 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:23:59.323685 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:23:59.335808 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.336885 [debug] [ThreadPool]: On list_dbt: Close
[0m19:23:59.338831 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:23:59.339331 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:23:59.342217 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:23:59.342430 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:23:59.342582 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:23:59.348326 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.348635 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:23:59.348910 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:23:59.349232 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.349763 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:23:59.349945 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:23:59.350096 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:23:59.350317 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.350477 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:23:59.353312 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m19:23:59.356678 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:23:59.356920 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:23:59.357076 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:23:59.362860 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.363127 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:23:59.363296 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:23:59.378839 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.382436 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:23:59.383158 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:23:59.383354 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:23:59.385553 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m19:23:59.387884 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:23:59.388093 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:23:59.388246 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:23:59.393560 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.393810 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:23:59.393982 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:23:59.411810 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:59.412815 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:23:59.413052 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:23:59.413223 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:23:59.416157 [debug] [MainThread]: Using duckdb connection "master"
[0m19:23:59.416360 [debug] [MainThread]: On master: BEGIN
[0m19:23:59.416516 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:23:59.421979 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:23:59.422245 [debug] [MainThread]: On master: COMMIT
[0m19:23:59.422411 [debug] [MainThread]: Using duckdb connection "master"
[0m19:23:59.422558 [debug] [MainThread]: On master: COMMIT
[0m19:23:59.422758 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:23:59.422914 [debug] [MainThread]: On master: Close
[0m19:23:59.424305 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:23:59.424546 [info ] [MainThread]: 
[0m19:23:59.426023 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:23:59.427127 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:23:59.427533 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_user)
[0m19:23:59.427728 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:23:59.433051 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:23:59.433606 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:23:59.427866 => 19:23:59.433501
[0m19:23:59.433808 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:23:59.453154 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:23:59.453834 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:23:59.454040 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:23:59.454218 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:23:59.459651 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:23:59.459933 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:23:59.460220 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.first_name,
    u.last_name,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.userroleid = r.user_role_id
where u.is_active = true
    );
  
  
[0m19:23:59.461824 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:23:59.433947 => 19:23:59.461722
[0m19:23:59.462044 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:23:59.465470 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:23:59.465675 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:23:59.467560 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "is_active"
  LINE 25: where u.is_active = true
                 ^
[0m19:23:59.467983 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:23:59.468325 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:23:59.469102 [debug] [MainThread]: Using duckdb connection "master"
[0m19:23:59.469342 [debug] [MainThread]: On master: BEGIN
[0m19:23:59.469501 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:23:59.475119 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:23:59.475313 [debug] [MainThread]: On master: COMMIT
[0m19:23:59.475466 [debug] [MainThread]: Using duckdb connection "master"
[0m19:23:59.475609 [debug] [MainThread]: On master: COMMIT
[0m19:23:59.475800 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:23:59.475949 [debug] [MainThread]: On master: Close
[0m19:23:59.477433 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:23:59.477599 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:23:59.477764 [info ] [MainThread]: 
[0m19:23:59.477941 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m19:23:59.478258 [debug] [MainThread]: Command end result
[0m19:23:59.486421 [info ] [MainThread]: 
[0m19:23:59.486774 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:23:59.486982 [info ] [MainThread]: 
[0m19:23:59.487196 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "is_active"
  LINE 25: where u.is_active = true
                 ^
[0m19:23:59.487396 [info ] [MainThread]: 
[0m19:23:59.487736 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:23:59.488250 [debug] [MainThread]: Command `dbt run` failed at 19:23:59.488177 after 0.38 seconds
[0m19:23:59.488472 [debug] [MainThread]: Flushing usage events


============================== 19:24:35.236642 | 5d8f4b67-3272-4cd4-bd65-b4b223658678 ==============================
[0m19:24:35.236642 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:24:35.239725 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m19:24:35.240022 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:24:35.316527 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:24:35.335010 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:24:35.377077 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:24:35.377603 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:24:35.405032 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m19:24:35.419133 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:24:35.420547 [info ] [MainThread]: 
[0m19:24:35.420983 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:24:35.421648 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:24:35.428890 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:24:35.429157 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:24:35.429339 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:24:35.441308 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.442415 [debug] [ThreadPool]: On list_dbt: Close
[0m19:24:35.444300 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:24:35.444737 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:24:35.447666 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:35.447868 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:24:35.448027 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:35.453563 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.453811 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:35.453978 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:24:35.454240 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.454752 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:24:35.454931 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:35.455080 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:24:35.455298 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.455455 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:24:35.458263 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m19:24:35.461603 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:24:35.461828 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:24:35.462000 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:35.467612 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.467859 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:24:35.468034 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:24:35.483964 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.487487 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:24:35.488199 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:24:35.488448 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:24:35.490547 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m19:24:35.492966 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:24:35.493192 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:24:35.493356 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:35.498861 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.499120 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:24:35.499290 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:24:35.516932 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:35.517856 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:24:35.518085 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:24:35.518244 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:24:35.521083 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:35.521281 [debug] [MainThread]: On master: BEGIN
[0m19:24:35.521436 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:24:35.526545 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:35.526786 [debug] [MainThread]: On master: COMMIT
[0m19:24:35.526949 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:35.527100 [debug] [MainThread]: On master: COMMIT
[0m19:24:35.527300 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:35.527456 [debug] [MainThread]: On master: Close
[0m19:24:35.529019 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:24:35.529281 [info ] [MainThread]: 
[0m19:24:35.530663 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:24:35.531800 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:24:35.532209 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_user)
[0m19:24:35.532412 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:24:35.537857 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:24:35.538432 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:24:35.532553 => 19:24:35.538313
[0m19:24:35.538633 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:24:35.557069 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:24:35.557575 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:24:35.557772 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:24:35.557952 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:24:35.563538 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:24:35.563839 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:24:35.564050 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.first_name,
    u.last_name,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.userroleid = r.user_role_id
where u.isactive = true
    );
  
  
[0m19:24:35.565822 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:24:35.538766 => 19:24:35.565696
[0m19:24:35.566053 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:24:35.569626 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:24:35.569855 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:24:35.571629 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "first_name"
  LINE 16:     u.first_name,
               ^
[0m19:24:35.572090 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:24:35.572428 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:24:35.573200 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:35.573435 [debug] [MainThread]: On master: BEGIN
[0m19:24:35.573594 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:24:35.579351 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:35.579568 [debug] [MainThread]: On master: COMMIT
[0m19:24:35.579729 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:35.579878 [debug] [MainThread]: On master: COMMIT
[0m19:24:35.580105 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:35.580273 [debug] [MainThread]: On master: Close
[0m19:24:35.581667 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:24:35.581832 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:24:35.582012 [info ] [MainThread]: 
[0m19:24:35.582196 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m19:24:35.582528 [debug] [MainThread]: Command end result
[0m19:24:35.589874 [info ] [MainThread]: 
[0m19:24:35.590151 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:24:35.590324 [info ] [MainThread]: 
[0m19:24:35.590505 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "first_name"
  LINE 16:     u.first_name,
               ^
[0m19:24:35.590670 [info ] [MainThread]: 
[0m19:24:35.590852 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:24:35.591262 [debug] [MainThread]: Command `dbt run` failed at 19:24:35.591212 after 0.37 seconds
[0m19:24:35.591469 [debug] [MainThread]: Flushing usage events


============================== 19:24:47.121376 | a2ce601d-6d0d-497b-9afc-589f9ddc6631 ==============================
[0m19:24:47.121376 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:24:47.124526 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m19:24:47.124785 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:24:47.209731 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:24:47.227752 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:24:47.274045 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:24:47.274567 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:24:47.303578 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m19:24:47.318146 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:24:47.319555 [info ] [MainThread]: 
[0m19:24:47.319987 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:24:47.320580 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:24:47.327912 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:24:47.328177 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:24:47.328357 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:24:47.340546 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.341459 [debug] [ThreadPool]: On list_dbt: Close
[0m19:24:47.343498 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:24:47.344034 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:24:47.347015 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:47.347226 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:24:47.347380 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:47.352955 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.353224 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:47.353579 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:24:47.353924 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.354486 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:24:47.354673 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:24:47.354830 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:24:47.355077 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.355257 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:24:47.358340 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m19:24:47.361906 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:24:47.362221 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:24:47.362424 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:47.368065 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.368287 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:24:47.368455 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:24:47.383664 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.387437 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:24:47.388094 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:24:47.388282 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:24:47.390612 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m19:24:47.393021 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:24:47.393278 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:24:47.393443 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:24:47.398834 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.399078 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:24:47.399260 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:24:47.417027 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:47.418026 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:24:47.418270 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:24:47.418432 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:24:47.421417 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:47.421635 [debug] [MainThread]: On master: BEGIN
[0m19:24:47.421800 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:24:47.427164 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:47.427415 [debug] [MainThread]: On master: COMMIT
[0m19:24:47.427584 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:47.427734 [debug] [MainThread]: On master: COMMIT
[0m19:24:47.427938 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:47.428104 [debug] [MainThread]: On master: Close
[0m19:24:47.429537 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:24:47.429749 [info ] [MainThread]: 
[0m19:24:47.431221 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:24:47.432341 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:24:47.432739 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_user)
[0m19:24:47.432951 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:24:47.438393 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:24:47.439261 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:24:47.433094 => 19:24:47.439090
[0m19:24:47.439521 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:24:47.458824 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:24:47.459634 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:24:47.459935 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:24:47.460147 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:24:47.465794 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:24:47.466076 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:24:47.466285 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    user_id,
    u.username,
    u.firstname,
    u.lastname,
    u.email,
    r.name as role_name,
    u.created_date,
    u.last_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.userroleid = r.user_role_id
where u.isactive = true
    );
  
  
[0m19:24:47.467860 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:24:47.439662 => 19:24:47.467753
[0m19:24:47.468072 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:24:47.471925 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:24:47.472131 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:24:47.473969 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "created_date"
  LINE 20:     u.created_date,
               ^
[0m19:24:47.474364 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:24:47.474702 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:24:47.475492 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:47.475698 [debug] [MainThread]: On master: BEGIN
[0m19:24:47.475860 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:24:47.481436 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:47.481671 [debug] [MainThread]: On master: COMMIT
[0m19:24:47.482009 [debug] [MainThread]: Using duckdb connection "master"
[0m19:24:47.482286 [debug] [MainThread]: On master: COMMIT
[0m19:24:47.482523 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:24:47.482941 [debug] [MainThread]: On master: Close
[0m19:24:47.484645 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:24:47.487293 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:24:47.487565 [info ] [MainThread]: 
[0m19:24:47.487767 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m19:24:47.488116 [debug] [MainThread]: Command end result
[0m19:24:47.495260 [info ] [MainThread]: 
[0m19:24:47.495530 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:24:47.495697 [info ] [MainThread]: 
[0m19:24:47.495865 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "created_date"
  LINE 20:     u.created_date,
               ^
[0m19:24:47.496028 [info ] [MainThread]: 
[0m19:24:47.496209 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:24:47.496582 [debug] [MainThread]: Command `dbt run` failed at 19:24:47.496532 after 0.40 seconds
[0m19:24:47.496782 [debug] [MainThread]: Flushing usage events


============================== 19:31:20.537559 | 94e1354e-b587-4707-8b61-23cbc85b335f ==============================
[0m19:31:20.537559 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:31:20.540735 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'send_anonymous_usage_stats': 'False'}
[0m19:31:20.541004 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:31:20.625475 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:31:20.643553 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:31:20.688896 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:31:20.689413 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:31:20.718633 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m19:31:20.734326 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:31:20.735832 [info ] [MainThread]: 
[0m19:31:20.736320 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:31:20.736891 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:31:20.744284 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:31:20.744633 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:31:20.744821 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:31:20.765431 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.766648 [debug] [ThreadPool]: On list_dbt: Close
[0m19:31:20.769599 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:31:20.770175 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:31:20.773456 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:20.773725 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:31:20.773899 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:20.779940 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.780228 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:20.780405 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:31:20.780657 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.781202 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:31:20.781374 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:20.781525 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:31:20.781790 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.781958 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:31:20.785274 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m19:31:20.788983 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:31:20.789357 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:31:20.789577 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:20.795642 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.795951 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:31:20.796139 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:31:20.812517 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.816138 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:31:20.816788 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:31:20.816958 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:31:20.819223 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m19:31:20.821576 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:31:20.821774 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:31:20.821922 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:20.827646 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.827893 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:31:20.828062 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:31:20.845623 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:20.846531 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:31:20.846756 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:31:20.846911 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:31:20.849837 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:20.850033 [debug] [MainThread]: On master: BEGIN
[0m19:31:20.850185 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:31:20.855276 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:20.855513 [debug] [MainThread]: On master: COMMIT
[0m19:31:20.855672 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:20.855819 [debug] [MainThread]: On master: COMMIT
[0m19:31:20.856012 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:20.856172 [debug] [MainThread]: On master: Close
[0m19:31:20.857498 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:31:20.857701 [info ] [MainThread]: 
[0m19:31:20.859220 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:31:20.860272 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:31:20.860658 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_user)
[0m19:31:20.860859 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:31:20.866609 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:31:20.867769 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:31:20.861004 => 19:31:20.867658
[0m19:31:20.867975 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:31:20.886072 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:31:20.886606 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:31:20.886805 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:31:20.886987 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:31:20.892616 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:31:20.892919 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:31:20.893248 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    -- IDs
    u.id as user_id,
    u.userroleid as user_role_id,
    u.profileid as user_profile_id,
    u.contactid as contact_id,
    u.accountid as account_id,
    u.managerid as manager_id,
    u.delegatedapproverid as delegated_approver_id,
    u.createdbyid as created_by_id,
    u.lastmodifiedbyid as last_modified_by_id,
    u.workspaceid as workspace_id,
    u.individualid as individual_id,
    u.globalidentity as global_identity,
    r.name as user_role_name,
    
    -- Metrics
    u.numberoffailedlogins as number_of_failed_logins,
    u.loginlimit as login_limit,
    u.forecastenabled as forecast_enabled,
    
    -- Dimensions
    u.username as user_username,
    u.firstname as user_first_name,
    u.lastname as user_last_name,
    u.companyname as company_name,
    u.division as user_division,
    u.department as user_department,
    u.title as user_title,
    u.street as user_street,
    u.city as user_city,
    u.state as user_state,
    u.postalcode as user_postal_code,
    u.country as user_country,
    u.latitude as user_latitude,
    u.longitude as user_longitude,
    u.geocodeaccuracy as user_geocode_accuracy,
    u.email as user_email,
    u.senderemail as user_sender_email,
    u.sendername as user_sender_name,
    u.signature as user_signature,
    u.stayintouchsubject as stay_in_touch_subject,
    u.stayintouchsignature as stay_in_touch_signature,
    u.stayintouchnote as stay_in_touch_note,
    u.phone as user_phone,
    u.fax as user_fax,
    u.mobilephone as user_mobile_phone,
    u.alias as user_alias,
    u.communitynickname as user_community_nickname,
    u.isactive as user_is_active,
    u.issystemcontrolled as user_is_system_controlled,
    u.timezonesidkey as user_timezone_sid_key,
    u.localesidkey as user_locale_sid_key,
    u.receivesinfoemails as user_receives_info_emails,
    u.receivesadmininfoemails as user_receives_admin_info_emails,
    u.emailencodingkey as user_email_encoding_key,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.startday as user_start_day,
    u.endday as user_end_day,
    u.languagelocalekey as user_language_locale_key,
    u.employee_number,
    u.callcenterid as call_center_id,
    u.extension as user_extension,
    u.federationidentifier as federation_identifier,
    u.aboutme as about_me,
    u.profilephotoid as profile_photo_id,
    u.digestfrequency as digest_frequency,
    u.defaultgroupnotificationfrequency as default_group_notification_frequency,
    u.jigsawimportlimitoverride as jigsaw_import_limit_override,
    u.sharingtype as sharing_type,
    u.chatteradoptionstage as chatter_adoption_stage,
    u.bannerphotoid as banner_photo_id,
    u.isprofilephotoactive as is_profile_photo_active,
    
    -- Dates
    u.lastlogindate as last_login_date,
    u.lastpasswordchangedate as last_password_change_date,
    u.createddate as created_date,
    u.lastmodifieddate as last_modified_date,
    u.systemmodstamp as system_mod_stamp,
    u.suaccessexpirationdate as su_access_expiration_date,
    u.suorgadminexpirationdate as su_org_admin_expiration_date,
    u.offlinetrialexpirationdate as offline_trial_expiration_date,
    u.wirelesstrialexpirationdate as wireless_trial_expiration_date,
    u.offlinepdatrialexpirationdate as offline_pda_trial_expiration_date,
    u.chatteradoptionstagemodifieddate as chatter_adoption_stage_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.userroleid = r.user_role_id
where u.isactive = true
    );
  
  
[0m19:31:20.895064 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:31:20.868110 => 19:31:20.894951
[0m19:31:20.895278 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:31:20.899178 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:31:20.899377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:31:20.901211 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "id"
  LINE 15:     u.id as user_id,
               ^
[0m19:31:20.901659 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:31:20.902021 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:31:20.902824 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:20.903022 [debug] [MainThread]: On master: BEGIN
[0m19:31:20.903184 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:31:20.908685 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:20.908902 [debug] [MainThread]: On master: COMMIT
[0m19:31:20.909061 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:20.909212 [debug] [MainThread]: On master: COMMIT
[0m19:31:20.909405 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:20.909561 [debug] [MainThread]: On master: Close
[0m19:31:20.911023 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:31:20.911204 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:31:20.911377 [info ] [MainThread]: 
[0m19:31:20.911566 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m19:31:20.911933 [debug] [MainThread]: Command end result
[0m19:31:20.920381 [info ] [MainThread]: 
[0m19:31:20.920695 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:31:20.920868 [info ] [MainThread]: 
[0m19:31:20.921036 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "id"
  LINE 15:     u.id as user_id,
               ^
[0m19:31:20.921204 [info ] [MainThread]: 
[0m19:31:20.921382 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:31:20.921765 [debug] [MainThread]: Command `dbt run` failed at 19:31:20.921712 after 0.41 seconds
[0m19:31:20.921980 [debug] [MainThread]: Flushing usage events


============================== 19:31:35.615753 | 14034102-de5a-4c96-9f62-eced90b61d32 ==============================
[0m19:31:35.615753 [info ] [MainThread]: Running with dbt=1.6.18
[0m19:31:35.618365 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m19:31:35.618625 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m19:31:35.694139 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m19:31:35.712258 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m19:31:35.755279 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:31:35.755794 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m19:31:35.783281 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m19:31:35.797420 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m19:31:35.798925 [info ] [MainThread]: 
[0m19:31:35.799420 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:31:35.800277 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m19:31:35.807222 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m19:31:35.807485 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m19:31:35.807672 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:31:35.815264 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.816220 [debug] [ThreadPool]: On list_dbt: Close
[0m19:31:35.818105 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m19:31:35.818611 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m19:31:35.821465 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:35.821657 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m19:31:35.821813 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:35.827555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.827895 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:35.828184 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m19:31:35.828532 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.829091 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:31:35.829278 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m19:31:35.829430 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m19:31:35.829648 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.829808 [debug] [ThreadPool]: On create_dbt_main: Close
[0m19:31:35.832641 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m19:31:35.836188 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:31:35.836391 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m19:31:35.836548 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:35.842564 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.842848 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m19:31:35.843038 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m19:31:35.860890 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.861978 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m19:31:35.863346 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m19:31:35.863532 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m19:31:35.865691 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m19:31:35.868244 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:31:35.868434 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m19:31:35.868580 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:35.873979 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.874252 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m19:31:35.874430 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m19:31:35.890214 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:31:35.893925 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m19:31:35.894174 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m19:31:35.894336 [debug] [ThreadPool]: On list_dbt_main: Close
[0m19:31:35.897097 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:35.897304 [debug] [MainThread]: On master: BEGIN
[0m19:31:35.897468 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:31:35.903161 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:35.903436 [debug] [MainThread]: On master: COMMIT
[0m19:31:35.903600 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:35.903755 [debug] [MainThread]: On master: COMMIT
[0m19:31:35.903961 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:35.904156 [debug] [MainThread]: On master: Close
[0m19:31:35.905558 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:31:35.905760 [info ] [MainThread]: 
[0m19:31:35.907299 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m19:31:35.907594 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m19:31:35.908782 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m19:31:35.908978 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m19:31:35.914571 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m19:31:35.915543 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 19:31:35.909113 => 19:31:35.915349
[0m19:31:35.915893 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m19:31:35.934294 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m19:31:35.934831 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:31:35.935031 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m19:31:35.935210 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:31:35.940844 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m19:31:35.941120 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m19:31:35.941416 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    -- IDs
    user_id,
    u.userroleid as user_role_id,
    u.profileid as user_profile_id,
    u.contactid as contact_id,
    u.accountid as account_id,
    u.managerid as manager_id,
    u.delegatedapproverid as delegated_approver_id,
    u.createdbyid as created_by_id,
    u.lastmodifiedbyid as last_modified_by_id,
    u.workspaceid as workspace_id,
    u.individualid as individual_id,
    u.globalidentity as global_identity,
    r.name as user_role_name,
    
    -- Metrics
    u.numberoffailedlogins as number_of_failed_logins,
    u.loginlimit as login_limit,
    u.forecastenabled as forecast_enabled,
    
    -- Dimensions
    u.username as user_username,
    u.firstname as user_first_name,
    u.lastname as user_last_name,
    u.companyname as company_name,
    u.division as user_division,
    u.department as user_department,
    u.title as user_title,
    u.street as user_street,
    u.city as user_city,
    u.state as user_state,
    u.postalcode as user_postal_code,
    u.country as user_country,
    u.latitude as user_latitude,
    u.longitude as user_longitude,
    u.geocodeaccuracy as user_geocode_accuracy,
    u.email as user_email,
    u.senderemail as user_sender_email,
    u.sendername as user_sender_name,
    u.signature as user_signature,
    u.stayintouchsubject as stay_in_touch_subject,
    u.stayintouchsignature as stay_in_touch_signature,
    u.stayintouchnote as stay_in_touch_note,
    u.phone as user_phone,
    u.fax as user_fax,
    u.mobilephone as user_mobile_phone,
    u.alias as user_alias,
    u.communitynickname as user_community_nickname,
    u.isactive as user_is_active,
    u.issystemcontrolled as user_is_system_controlled,
    u.timezonesidkey as user_timezone_sid_key,
    u.localesidkey as user_locale_sid_key,
    u.receivesinfoemails as user_receives_info_emails,
    u.receivesadmininfoemails as user_receives_admin_info_emails,
    u.emailencodingkey as user_email_encoding_key,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.startday as user_start_day,
    u.endday as user_end_day,
    u.languagelocalekey as user_language_locale_key,
    u.employee_number,
    u.callcenterid as call_center_id,
    u.extension as user_extension,
    u.federationidentifier as federation_identifier,
    u.aboutme as about_me,
    u.profilephotoid as profile_photo_id,
    u.digestfrequency as digest_frequency,
    u.defaultgroupnotificationfrequency as default_group_notification_frequency,
    u.jigsawimportlimitoverride as jigsaw_import_limit_override,
    u.sharingtype as sharing_type,
    u.chatteradoptionstage as chatter_adoption_stage,
    u.bannerphotoid as banner_photo_id,
    u.isprofilephotoactive as is_profile_photo_active,
    
    -- Dates
    u.lastlogindate as last_login_date,
    u.lastpasswordchangedate as last_password_change_date,
    u.createddate as created_date,
    u.lastmodifieddate as last_modified_date,
    u.systemmodstamp as system_mod_stamp,
    u.suaccessexpirationdate as su_access_expiration_date,
    u.suorgadminexpirationdate as su_org_admin_expiration_date,
    u.offlinetrialexpirationdate as offline_trial_expiration_date,
    u.wirelesstrialexpirationdate as wireless_trial_expiration_date,
    u.offlinepdatrialexpirationdate as offline_pda_trial_expiration_date,
    u.chatteradoptionstagemodifieddate as chatter_adoption_stage_modified_date
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" r
    on u.userroleid = r.user_role_id
where u.isactive = true
    );
  
  
[0m19:31:35.943235 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 19:31:35.916082 => 19:31:35.943127
[0m19:31:35.943448 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m19:31:35.946809 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m19:31:35.947122 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m19:31:35.948975 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "employee_number"
  LINE 74:     u.employee_number,
               ^
[0m19:31:35.949360 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m19:31:35.949705 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m19:31:35.950461 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:35.950644 [debug] [MainThread]: On master: BEGIN
[0m19:31:35.950792 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:31:35.955911 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:35.956198 [debug] [MainThread]: On master: COMMIT
[0m19:31:35.956375 [debug] [MainThread]: Using duckdb connection "master"
[0m19:31:35.956535 [debug] [MainThread]: On master: COMMIT
[0m19:31:35.956756 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:31:35.956912 [debug] [MainThread]: On master: Close
[0m19:31:35.958406 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:31:35.958645 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m19:31:35.958833 [info ] [MainThread]: 
[0m19:31:35.959028 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m19:31:35.959395 [debug] [MainThread]: Command end result
[0m19:31:35.970212 [info ] [MainThread]: 
[0m19:31:35.970618 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:31:35.970827 [info ] [MainThread]: 
[0m19:31:35.971007 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "u" does not have a column named "employee_number"
  LINE 74:     u.employee_number,
               ^
[0m19:31:35.971177 [info ] [MainThread]: 
[0m19:31:35.971438 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:31:35.971845 [debug] [MainThread]: Command `dbt run` failed at 19:31:35.971789 after 0.37 seconds
[0m19:31:35.972061 [debug] [MainThread]: Flushing usage events


============================== 21:03:39.118315 | 1ca0cf8b-793a-4ca1-8fe2-d85d1334a802 ==============================
[0m21:03:39.118315 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:03:39.121457 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:03:39.121721 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:03:39.206536 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:03:39.226503 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:03:39.275914 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:03:39.276421 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:03:39.304866 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m21:03:39.319929 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:03:39.321520 [info ] [MainThread]: 
[0m21:03:39.322024 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:03:39.322684 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:03:39.329688 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:03:39.329964 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:03:39.330263 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:03:39.340086 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:03:39.340450 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m21:03:39.340638 [info ] [MainThread]: 
[0m21:03:39.340856 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m21:03:39.341163 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 7815) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m21:03:39.341614 [debug] [MainThread]: Command `dbt run` failed at 21:03:39.341551 after 0.24 seconds
[0m21:03:39.341816 [debug] [MainThread]: Flushing usage events


============================== 21:04:02.660952 | 244df88e-7119-4283-95f9-20c1e31e3328 ==============================
[0m21:04:02.660952 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:04:02.663640 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:04:02.663908 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:04:02.741145 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:04:02.761471 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:04:02.808797 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:04:02.809139 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:04:02.810105 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:04:02.824435 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:04:02.825844 [info ] [MainThread]: 
[0m21:04:02.826285 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:04:02.826970 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:04:02.834662 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:04:02.834932 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:04:02.835114 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:04:02.847322 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.848257 [debug] [ThreadPool]: On list_dbt: Close
[0m21:04:02.850144 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:04:02.850649 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:04:02.853864 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:02.854199 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:04:02.854376 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:02.860346 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.860550 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:02.860713 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:04:02.860965 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.861465 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:04:02.861635 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:02.861782 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:04:02.861995 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.862174 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:04:02.865245 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:04:02.868831 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:04:02.869095 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:04:02.869262 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:02.874976 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.875230 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:04:02.875414 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:04:02.893683 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.894739 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:04:02.895459 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:04:02.895638 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:04:02.897831 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:04:02.900414 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:04:02.900618 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:04:02.900772 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:02.906278 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.906522 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:04:02.906693 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:04:02.921868 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:02.925618 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:04:02.925878 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:04:02.926044 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:04:02.928799 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:02.928978 [debug] [MainThread]: On master: BEGIN
[0m21:04:02.929129 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:04:02.934432 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:02.934672 [debug] [MainThread]: On master: COMMIT
[0m21:04:02.934833 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:02.934982 [debug] [MainThread]: On master: COMMIT
[0m21:04:02.935185 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:02.935342 [debug] [MainThread]: On master: Close
[0m21:04:02.936734 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:04:02.936979 [info ] [MainThread]: 
[0m21:04:02.938558 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:04:02.938847 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:04:02.939238 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:04:02.939451 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:04:02.945059 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:04:02.946435 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:04:02.939592 => 21:04:02.946258
[0m21:04:02.946751 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:04:02.966665 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:04:02.967325 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:02.967530 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:04:02.967711 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:04:02.973261 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:02.973529 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:02.973758 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

SELECT
    u.id AS user_id,
    u.username AS user_name,
    u.firstname AS first_name,
    u.lastname AS last_name,
    u.companyname AS company_name,
    u.division AS division,
    u.department AS department,
    u.title AS title,
    u.street AS street,
    u.city AS city,
    u.state AS state,
    u.postalcode AS postal_code,
    u.country AS country,
    u.latitude AS latitude,
    u.longitude AS longitude,
    u.email AS email,
    u.phone AS phone,
    u.mobilephone AS mobile_phone,
    u.alias AS alias,
    u.isactive AS is_active,
    u.timezonesidkey AS timezone_sid_key,
    u.localesidkey AS locale_sid_key,
    u.emailencodingkey AS email_encoding_key,
    u.profileid AS profile_id,
    u.usertype AS user_type,
    u.usersubtype AS user_subtype,
    u.lastlogindate AS last_login_date,
    u.createddate AS created_date,
    r.name AS role_name,
    r.parentroleid AS parent_role_id,
    r.opportunityaccessforaccountowner AS opportunity_access_for_account_owner,
    r.caseaccessforaccountowner AS case_access_for_account_owner,
    r.contactaccessforaccountowner AS contact_access_for_account_owner
FROM
    "dbt"."raw"."user" u
LEFT JOIN
    "dbt"."raw"."user_role" r
ON
    u.userroleid = r.id;
    );
  
  
[0m21:04:02.974292 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:04:02.946929 => 21:04:02.974190
[0m21:04:02.974496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m21:04:02.978370 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m21:04:02.978632 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:04:02.980441 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m21:04:02.980872 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m21:04:02.981273 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:04:02.982026 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:02.982203 [debug] [MainThread]: On master: BEGIN
[0m21:04:02.982350 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:04:02.988005 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:02.988243 [debug] [MainThread]: On master: COMMIT
[0m21:04:02.988413 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:02.988561 [debug] [MainThread]: On master: COMMIT
[0m21:04:02.988763 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:02.988918 [debug] [MainThread]: On master: Close
[0m21:04:02.990441 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:04:02.990719 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:04:02.990943 [info ] [MainThread]: 
[0m21:04:02.991170 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m21:04:02.991596 [debug] [MainThread]: Command end result
[0m21:04:02.999086 [info ] [MainThread]: 
[0m21:04:02.999485 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:04:02.999697 [info ] [MainThread]: 
[0m21:04:02.999865 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Parser Error: syntax error at or near ";"
[0m21:04:03.000028 [info ] [MainThread]: 
[0m21:04:03.000250 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:04:03.000648 [debug] [MainThread]: Command `dbt run` failed at 21:04:03.000568 after 0.36 seconds
[0m21:04:03.000861 [debug] [MainThread]: Flushing usage events


============================== 21:04:10.440746 | 5c86d2f5-f6de-4575-9cb2-905a9d05e8af ==============================
[0m21:04:10.440746 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:04:10.443095 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:04:10.443333 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:04:10.517116 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:04:10.535450 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:04:10.571394 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:04:10.571865 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:04:10.599539 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:04:10.612424 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:04:10.613819 [info ] [MainThread]: 
[0m21:04:10.614256 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:04:10.614898 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:04:10.621914 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:04:10.622195 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:04:10.622389 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:04:10.629946 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.630886 [debug] [ThreadPool]: On list_dbt: Close
[0m21:04:10.632728 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:04:10.633275 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:04:10.636378 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:10.636585 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:04:10.636745 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:10.642785 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.643056 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:10.643229 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:04:10.643491 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.644028 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:04:10.644202 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:04:10.644350 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:04:10.644562 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.644728 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:04:10.647547 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:04:10.651069 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:04:10.651302 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:04:10.651466 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:10.656919 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.657172 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:04:10.657350 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:04:10.675382 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.676353 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:04:10.676745 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:04:10.676911 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:04:10.679114 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:04:10.681744 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:04:10.681942 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:04:10.682101 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:04:10.687738 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.687992 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:04:10.688168 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:04:10.703843 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:04:10.707500 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:04:10.707733 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:04:10.707898 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:04:10.710674 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:10.710859 [debug] [MainThread]: On master: BEGIN
[0m21:04:10.711011 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:04:10.716390 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:10.716653 [debug] [MainThread]: On master: COMMIT
[0m21:04:10.716814 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:10.716968 [debug] [MainThread]: On master: COMMIT
[0m21:04:10.717169 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:10.717327 [debug] [MainThread]: On master: Close
[0m21:04:10.718886 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:04:10.719135 [info ] [MainThread]: 
[0m21:04:10.720191 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:04:10.720529 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:04:10.721778 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:04:10.721984 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:04:10.727539 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:04:10.728072 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:04:10.722125 => 21:04:10.727967
[0m21:04:10.728273 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:04:10.747323 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:04:10.747880 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:10.748086 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:04:10.748263 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:04:10.753895 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:10.754207 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:10.754444 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

SELECT
    u.id AS user_id,
    u.username AS user_name,
    u.firstname AS first_name,
    u.lastname AS last_name,
    u.companyname AS company_name,
    u.division AS division,
    u.department AS department,
    u.title AS title,
    u.street AS street,
    u.city AS city,
    u.state AS state,
    u.postalcode AS postal_code,
    u.country AS country,
    u.latitude AS latitude,
    u.longitude AS longitude,
    u.email AS email,
    u.phone AS phone,
    u.mobilephone AS mobile_phone,
    u.alias AS alias,
    u.isactive AS is_active,
    u.timezonesidkey AS timezone_sid_key,
    u.localesidkey AS locale_sid_key,
    u.emailencodingkey AS email_encoding_key,
    u.profileid AS profile_id,
    u.usertype AS user_type,
    u.usersubtype AS user_subtype,
    u.lastlogindate AS last_login_date,
    u.createddate AS created_date,
    r.name AS role_name,
    r.parentroleid AS parent_role_id,
    r.opportunityaccessforaccountowner AS opportunity_access_for_account_owner,
    r.caseaccessforaccountowner AS case_access_for_account_owner,
    r.contactaccessforaccountowner AS contact_access_for_account_owner
FROM
    "dbt"."raw"."user" u
LEFT JOIN
    "dbt"."raw"."user_role" r
ON
    u.userroleid = r.id
    );
  
  
[0m21:04:10.758409 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:10.762406 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:10.762676 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."main"."dim_user__dbt_tmp" rename to "dim_user"
[0m21:04:10.763097 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:10.772966 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:04:10.773278 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:10.773469 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:04:10.774591 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:10.777846 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:04:10.778102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."main"."dim_user__dbt_backup" cascade
[0m21:04:10.778445 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:04:10.779221 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:04:10.728409 => 21:04:10.779128
[0m21:04:10.779437 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:04:10.801299 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_user ................................ [[32mOK[0m in 0.08s]
[0m21:04:10.801728 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:04:10.802497 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:10.802690 [debug] [MainThread]: On master: BEGIN
[0m21:04:10.802848 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:04:10.808734 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:10.808956 [debug] [MainThread]: On master: COMMIT
[0m21:04:10.809113 [debug] [MainThread]: Using duckdb connection "master"
[0m21:04:10.809262 [debug] [MainThread]: On master: COMMIT
[0m21:04:10.809459 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:04:10.809615 [debug] [MainThread]: On master: Close
[0m21:04:10.811034 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:04:10.811275 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:04:10.811463 [info ] [MainThread]: 
[0m21:04:10.811657 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m21:04:10.811997 [debug] [MainThread]: Command end result
[0m21:04:10.845029 [info ] [MainThread]: 
[0m21:04:10.845317 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:04:10.845486 [info ] [MainThread]: 
[0m21:04:10.845666 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:04:10.845990 [debug] [MainThread]: Command `dbt run` succeeded at 21:04:10.845934 after 0.42 seconds
[0m21:04:10.846175 [debug] [MainThread]: Flushing usage events


============================== 21:08:20.980737 | 6d771c70-971c-49e9-8441-fd3b16b272ec ==============================
[0m21:08:20.980737 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:08:20.983907 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:08:20.984174 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:08:21.066354 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:08:21.084968 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:08:21.129268 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 1 files changed.
[0m21:08:21.129676 [debug] [MainThread]: Partial parsing: deleted file: elastic_dbt_interview://models/dimensions/dim_user_role.sql
[0m21:08:21.129960 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:08:21.158855 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:08:21.175590 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:08:21.177076 [info ] [MainThread]: 
[0m21:08:21.177516 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:08:21.178187 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:08:21.185067 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:08:21.185320 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:08:21.185498 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:08:21.191791 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:08:21.191989 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m21:08:21.192154 [info ] [MainThread]: 
[0m21:08:21.192451 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m21:08:21.192782 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 10867) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m21:08:21.193174 [debug] [MainThread]: Command `dbt run` failed at 21:08:21.193119 after 0.24 seconds
[0m21:08:21.193373 [debug] [MainThread]: Flushing usage events


============================== 21:08:29.071797 | 1e7ea831-adc5-437c-b580-b54ae928cd31 ==============================
[0m21:08:29.071797 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:08:29.074537 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:08:29.074809 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:08:29.151855 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:08:29.171189 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:08:29.213756 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:08:29.214048 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:08:29.215000 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:08:29.229176 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:08:29.230573 [info ] [MainThread]: 
[0m21:08:29.231004 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:08:29.231677 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:08:29.239146 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:08:29.239430 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:08:29.239600 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:08:29.246040 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:08:29.246363 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m21:08:29.246549 [info ] [MainThread]: 
[0m21:08:29.246809 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m21:08:29.247149 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 10867) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m21:08:29.247805 [debug] [MainThread]: Command `dbt run` failed at 21:08:29.247715 after 0.20 seconds
[0m21:08:29.248063 [debug] [MainThread]: Flushing usage events


============================== 21:08:36.519430 | a4d343e9-3ef9-496d-add2-1a3ad9df97ed ==============================
[0m21:08:36.519430 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:08:36.521889 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:08:36.522167 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:08:36.598618 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:08:36.617630 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:08:36.654087 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:08:36.654383 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:08:36.655336 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m21:08:36.669118 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:08:36.670501 [info ] [MainThread]: 
[0m21:08:36.670933 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:08:36.671594 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:08:36.678733 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:08:36.679014 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:08:36.679204 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:08:36.690896 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.691984 [debug] [ThreadPool]: On list_dbt: Close
[0m21:08:36.693908 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:08:36.694407 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:08:36.697336 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:36.697540 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:08:36.697691 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:36.703342 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.703586 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:36.703749 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:08:36.704284 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.704876 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:08:36.705098 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:36.705249 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:08:36.705482 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.705648 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:08:36.708536 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:08:36.712140 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:08:36.712355 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:08:36.712518 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:36.718264 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.718563 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:08:36.718773 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:08:36.736994 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.738021 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:08:36.738741 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:08:36.738941 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:08:36.741091 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:08:36.743708 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:08:36.743906 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:08:36.744057 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:36.749681 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.749924 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:08:36.750099 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:08:36.765705 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:36.769322 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:08:36.769565 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:08:36.769724 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:08:36.772368 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:36.772542 [debug] [MainThread]: On master: BEGIN
[0m21:08:36.772694 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:08:36.778200 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:36.778447 [debug] [MainThread]: On master: COMMIT
[0m21:08:36.778609 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:36.778753 [debug] [MainThread]: On master: COMMIT
[0m21:08:36.778957 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:36.779109 [debug] [MainThread]: On master: Close
[0m21:08:36.780632 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:08:36.780898 [info ] [MainThread]: 
[0m21:08:36.782874 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:08:36.783207 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:08:36.783642 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:08:36.783848 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:08:36.789346 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:08:36.790716 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:08:36.783984 => 21:08:36.790525
[0m21:08:36.791008 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:08:36.811271 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:08:36.811968 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:36.812179 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:08:36.812357 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:08:36.817938 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:36.818216 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:36.818445 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    u.id as user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."raw"."user" u
left join "dbt"."raw"."user_role" ur on u.userroleid = r.id
where is_active = 1
    );
  
  
[0m21:08:36.819197 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:08:36.791162 => 21:08:36.819094
[0m21:08:36.819402 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m21:08:36.823266 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m21:08:36.823489 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:08:36.825468 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Referenced table "r" not found!
  Candidate tables: "u", "ur"
  LINE 48: ..., "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.id as user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."raw"."user" u
  left join "dbt"."raw"."user_role" ur on u.userroleid = r.id
                                                     ^
[0m21:08:36.825918 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m21:08:36.826323 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:08:36.827073 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:36.827245 [debug] [MainThread]: On master: BEGIN
[0m21:08:36.827393 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:08:36.832396 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:36.832817 [debug] [MainThread]: On master: COMMIT
[0m21:08:36.833033 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:36.833202 [debug] [MainThread]: On master: COMMIT
[0m21:08:36.833529 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:36.833729 [debug] [MainThread]: On master: Close
[0m21:08:36.835154 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:08:36.835395 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:08:36.835574 [info ] [MainThread]: 
[0m21:08:36.835896 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m21:08:36.836324 [debug] [MainThread]: Command end result
[0m21:08:36.843700 [info ] [MainThread]: 
[0m21:08:36.844042 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:08:36.844248 [info ] [MainThread]: 
[0m21:08:36.844461 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Referenced table "r" not found!
  Candidate tables: "u", "ur"
  LINE 48: ..., "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.id as user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."raw"."user" u
  left join "dbt"."raw"."user_role" ur on u.userroleid = r.id
                                                     ^
[0m21:08:36.844868 [info ] [MainThread]: 
[0m21:08:36.845120 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:08:36.845537 [debug] [MainThread]: Command `dbt run` failed at 21:08:36.845474 after 0.34 seconds
[0m21:08:36.845755 [debug] [MainThread]: Flushing usage events


============================== 21:08:56.848351 | c527ff2a-1bc4-41bc-b649-c738ed51ba86 ==============================
[0m21:08:56.848351 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:08:56.850726 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:08:56.850974 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:08:56.925611 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:08:56.945092 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:08:56.980694 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:08:56.981238 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:08:57.008659 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:08:57.022443 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:08:57.023910 [info ] [MainThread]: 
[0m21:08:57.024361 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:08:57.025040 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:08:57.032383 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:08:57.032635 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:08:57.032804 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:08:57.040232 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.041224 [debug] [ThreadPool]: On list_dbt: Close
[0m21:08:57.043214 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:08:57.043732 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:08:57.046811 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:57.047028 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:08:57.047180 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:57.052749 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.053025 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:57.053398 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:08:57.053768 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.054343 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:08:57.054518 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:08:57.054668 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:08:57.054893 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.055054 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:08:57.058002 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:08:57.061633 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:08:57.061945 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:08:57.062165 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:57.068117 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.068385 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:08:57.068563 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:08:57.086428 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.087641 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:08:57.088104 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:08:57.088274 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:08:57.090354 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:08:57.092790 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:08:57.092975 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:08:57.093124 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:08:57.098557 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.098795 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:08:57.098972 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:08:57.114785 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:08:57.118440 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:08:57.118687 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:08:57.118852 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:08:57.121673 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:57.121883 [debug] [MainThread]: On master: BEGIN
[0m21:08:57.122046 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:08:57.127543 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:57.127796 [debug] [MainThread]: On master: COMMIT
[0m21:08:57.127957 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:57.128110 [debug] [MainThread]: On master: COMMIT
[0m21:08:57.128309 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:57.128468 [debug] [MainThread]: On master: Close
[0m21:08:57.129916 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:08:57.130148 [info ] [MainThread]: 
[0m21:08:57.131267 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:08:57.132345 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:08:57.132719 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:08:57.132919 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:08:57.138550 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:08:57.139190 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:08:57.133061 => 21:08:57.139073
[0m21:08:57.139418 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:08:57.158730 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:08:57.159912 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.160226 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:08:57.160418 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:08:57.166528 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.166771 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.167001 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    u.id as user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."raw"."user" u
left join "dbt"."raw"."user_role" ur on u.userroleid = ur.id
where is_active = 1
    );
  
  
[0m21:08:57.170153 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.174323 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.174608 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."main"."dim_user" rename to "dim_user__dbt_backup"
[0m21:08:57.175026 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.176850 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.177055 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."main"."dim_user__dbt_tmp" rename to "dim_user"
[0m21:08:57.177363 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.188077 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:08:57.188327 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.188520 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:08:57.189635 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.216522 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:08:57.216865 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."main"."dim_user__dbt_backup" cascade
[0m21:08:57.217526 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:08:57.218342 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:08:57.139552 => 21:08:57.218247
[0m21:08:57.218556 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:08:57.236609 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_user ................................ [[32mOK[0m in 0.10s]
[0m21:08:57.237001 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:08:57.237758 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:57.237947 [debug] [MainThread]: On master: BEGIN
[0m21:08:57.238102 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:08:57.243770 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:57.244006 [debug] [MainThread]: On master: COMMIT
[0m21:08:57.244172 [debug] [MainThread]: Using duckdb connection "master"
[0m21:08:57.244319 [debug] [MainThread]: On master: COMMIT
[0m21:08:57.244522 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:08:57.244678 [debug] [MainThread]: On master: Close
[0m21:08:57.246079 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:08:57.246263 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:08:57.246440 [info ] [MainThread]: 
[0m21:08:57.246687 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m21:08:57.247030 [debug] [MainThread]: Command end result
[0m21:08:57.254478 [info ] [MainThread]: 
[0m21:08:57.254777 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:08:57.254953 [info ] [MainThread]: 
[0m21:08:57.255140 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:08:57.255487 [debug] [MainThread]: Command `dbt run` succeeded at 21:08:57.255434 after 0.42 seconds
[0m21:08:57.255693 [debug] [MainThread]: Flushing usage events


============================== 21:11:44.233266 | 035feb77-c5fd-445e-a12e-d55000c1844f ==============================
[0m21:11:44.233266 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:11:44.235595 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_date.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:11:44.235846 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:11:44.311854 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:11:44.330064 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:11:44.368340 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:11:44.368817 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_date.sql
[0m21:11:44.456051 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:11:44.468648 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:11:44.469987 [info ] [MainThread]: 
[0m21:11:44.470404 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:11:44.471098 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:11:44.475674 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:11:44.475940 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:11:44.476118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:11:44.483329 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.484296 [debug] [ThreadPool]: On list_dbt: Close
[0m21:11:44.485984 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:11:44.486358 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:11:44.489263 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:44.489474 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:11:44.489628 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:44.494924 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.495207 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:44.495368 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:11:44.495607 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.496250 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:11:44.496551 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:44.496721 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:11:44.497034 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.497238 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:11:44.500307 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:11:44.503977 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:11:44.504201 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:11:44.504361 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:44.509622 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.509878 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:11:44.510045 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:11:44.528897 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.529916 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:11:44.530321 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:11:44.530485 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:11:44.532802 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:11:44.534500 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:11:44.535684 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:11:44.535862 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:44.541643 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.541899 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:11:44.542080 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:11:44.557841 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:44.561480 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:11:44.561731 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:11:44.561892 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:11:44.564622 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:44.564824 [debug] [MainThread]: On master: BEGIN
[0m21:11:44.564984 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:11:44.570430 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:44.570678 [debug] [MainThread]: On master: COMMIT
[0m21:11:44.570843 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:44.570990 [debug] [MainThread]: On master: COMMIT
[0m21:11:44.571193 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:44.571349 [debug] [MainThread]: On master: Close
[0m21:11:44.572715 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:11:44.572914 [info ] [MainThread]: 
[0m21:11:44.573954 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m21:11:44.574230 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_date ..................................... [RUN]
[0m21:11:44.574582 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_date)
[0m21:11:44.574771 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m21:11:44.610973 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:44.611292 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m21:11:44.611484 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:11:44.616964 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:44.617196 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:44.617389 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m21:11:44.619844 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:44.629699 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m21:11:44.630337 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 21:11:44.574906 => 21:11:44.630175
[0m21:11:44.630604 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m21:11:44.649042 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m21:11:44.649516 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:44.649937 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."main"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    *,
    CASE 
        WHEN date_day = CURRENT_DATE THEN TRUE 
        ELSE FALSE 
    END AS is_current_date,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
            AND EXTRACT(month FROM date_day) = EXTRACT(month FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_month,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
            AND EXTRACT(quarter FROM date_day) = EXTRACT(quarter FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_quarter,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_year,
    
    CONCAT(month_name, ' ', year_number) AS year_qualified_month_name
FROM 
    date_range;
    );
  
  
[0m21:11:44.650820 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 21:11:44.630754 => 21:11:44.650723
[0m21:11:44.651018 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: ROLLBACK
[0m21:11:44.654351 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_date'
[0m21:11:44.654577 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m21:11:44.656470 [debug] [Thread-1  ]: Runtime Error in model dim_date (models/dimensions/dim_date.sql)
  Parser Error: syntax error at or near ";"
[0m21:11:44.656822 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_date ............................ [[31mERROR[0m in 0.08s]
[0m21:11:44.657167 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m21:11:44.657984 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:44.658168 [debug] [MainThread]: On master: BEGIN
[0m21:11:44.658313 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:11:44.663793 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:44.664011 [debug] [MainThread]: On master: COMMIT
[0m21:11:44.664169 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:44.664321 [debug] [MainThread]: On master: COMMIT
[0m21:11:44.664515 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:44.664674 [debug] [MainThread]: On master: Close
[0m21:11:44.666179 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:11:44.666481 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_date' was properly closed.
[0m21:11:44.666687 [info ] [MainThread]: 
[0m21:11:44.666985 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m21:11:44.667340 [debug] [MainThread]: Command end result
[0m21:11:44.674772 [info ] [MainThread]: 
[0m21:11:44.675104 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:11:44.675399 [info ] [MainThread]: 
[0m21:11:44.675591 [error] [MainThread]:   Runtime Error in model dim_date (models/dimensions/dim_date.sql)
  Parser Error: syntax error at or near ";"
[0m21:11:44.675772 [info ] [MainThread]: 
[0m21:11:44.676002 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:11:44.676383 [debug] [MainThread]: Command `dbt run` failed at 21:11:44.676322 after 0.46 seconds
[0m21:11:44.676603 [debug] [MainThread]: Flushing usage events


============================== 21:11:50.848568 | f426a088-d423-40f4-8487-fad3f1f76f41 ==============================
[0m21:11:50.848568 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:11:50.851011 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_date.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:11:50.851339 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:11:50.925910 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:11:50.944284 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:11:50.983953 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:11:50.984451 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_date.sql
[0m21:11:51.076471 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:11:51.089668 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:11:51.091059 [info ] [MainThread]: 
[0m21:11:51.091508 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:11:51.092232 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:11:51.096932 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:11:51.097187 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:11:51.097359 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:11:51.104483 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.105481 [debug] [ThreadPool]: On list_dbt: Close
[0m21:11:51.107258 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:11:51.107658 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:11:51.110549 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:51.110795 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:11:51.111010 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:51.116718 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.116982 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:51.117358 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:11:51.117790 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.118391 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:11:51.118597 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:11:51.118766 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:11:51.119014 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.119185 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:11:51.122251 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m21:11:51.125780 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:11:51.125977 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:11:51.126134 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:51.132104 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.132410 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:11:51.132715 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:11:51.148527 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.152210 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:11:51.152649 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:11:51.152824 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:11:51.155083 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m21:11:51.157519 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:11:51.157761 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:11:51.157920 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:51.163585 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.163784 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:11:51.163952 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:11:51.182659 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:11:51.183750 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:11:51.183991 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:11:51.184155 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:11:51.187379 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:51.187576 [debug] [MainThread]: On master: BEGIN
[0m21:11:51.187734 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:11:51.193434 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:51.193687 [debug] [MainThread]: On master: COMMIT
[0m21:11:51.193850 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:51.193998 [debug] [MainThread]: On master: COMMIT
[0m21:11:51.194200 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:51.194364 [debug] [MainThread]: On master: Close
[0m21:11:51.195754 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:11:51.195960 [info ] [MainThread]: 
[0m21:11:51.197046 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m21:11:51.197333 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_date ..................................... [RUN]
[0m21:11:51.197684 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_date)
[0m21:11:51.197873 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m21:11:51.236608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.236958 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m21:11:51.237153 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:11:51.243082 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.243391 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.243592 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m21:11:51.243963 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.254683 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m21:11:51.255402 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 21:11:51.198007 => 21:11:51.255222
[0m21:11:51.255698 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m21:11:51.275203 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m21:11:51.275802 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.276232 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."main"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    *,
    CASE 
        WHEN date_day = CURRENT_DATE THEN TRUE 
        ELSE FALSE 
    END AS is_current_date,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
            AND EXTRACT(month FROM date_day) = EXTRACT(month FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_month,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
            AND EXTRACT(quarter FROM date_day) = EXTRACT(quarter FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_quarter,
    
    CASE 
        WHEN EXTRACT(year FROM date_day) = EXTRACT(year FROM CURRENT_DATE) 
        THEN TRUE
        ELSE FALSE 
    END AS is_current_year,
    
    CONCAT(month_name, ' ', year_number) AS year_qualified_month_name
FROM 
    date_range
    );
  
  
[0m21:11:51.363242 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.367492 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.367817 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."main"."dim_date__dbt_tmp" rename to "dim_date"
[0m21:11:51.368282 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.378062 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m21:11:51.378301 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.378496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m21:11:51.382137 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.385233 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m21:11:51.385447 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."main"."dim_date__dbt_backup" cascade
[0m21:11:51.385739 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:11:51.386510 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 21:11:51.255848 => 21:11:51.386414
[0m21:11:51.386718 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m21:11:51.411395 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_date ................................ [[32mOK[0m in 0.21s]
[0m21:11:51.411807 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m21:11:51.412550 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:51.412744 [debug] [MainThread]: On master: BEGIN
[0m21:11:51.412899 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:11:51.418638 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:51.418890 [debug] [MainThread]: On master: COMMIT
[0m21:11:51.419061 [debug] [MainThread]: Using duckdb connection "master"
[0m21:11:51.419212 [debug] [MainThread]: On master: COMMIT
[0m21:11:51.419426 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:11:51.419583 [debug] [MainThread]: On master: Close
[0m21:11:51.420995 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:11:51.421205 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_date' was properly closed.
[0m21:11:51.421391 [info ] [MainThread]: 
[0m21:11:51.421581 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.33 seconds (0.33s).
[0m21:11:51.421933 [debug] [MainThread]: Command end result
[0m21:11:51.428912 [info ] [MainThread]: 
[0m21:11:51.429248 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:11:51.429449 [info ] [MainThread]: 
[0m21:11:51.429789 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:11:51.430376 [debug] [MainThread]: Command `dbt run` succeeded at 21:11:51.430259 after 0.60 seconds
[0m21:11:51.430601 [debug] [MainThread]: Flushing usage events


============================== 21:17:35.542956 | b296a61c-2526-43f1-8ab5-140cb2724669 ==============================
[0m21:17:35.542956 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:17:35.545779 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:17:35.546053 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:17:35.622342 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:17:35.639861 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:17:35.682275 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:17:35.682731 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:17:35.701552 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_user (models/dimensions/dim_user.sql)
  expected token ':', got '}'
    line 40
      left join {{{ ref('stg_salesforce__user_role') }}} ur on u.userroleid = ur.id
[0m21:17:35.702098 [debug] [MainThread]: Command `dbt run` failed at 21:17:35.702024 after 0.18 seconds
[0m21:17:35.702335 [debug] [MainThread]: Flushing usage events


============================== 21:17:44.700111 | e03c4b5f-3998-4ca9-943d-aae116cfde33 ==============================
[0m21:17:44.700111 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:17:44.703059 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:17:44.703313 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:17:44.785935 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:17:44.803165 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:17:44.847244 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:17:44.847779 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:17:44.866388 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_user (models/dimensions/dim_user.sql)
  expected token ':', got '}'
    line 40
      left join {{{ ref('stg_salesforce__user_role') }} ur on u.userroleid = ur.id
[0m21:17:44.866915 [debug] [MainThread]: Command `dbt run` failed at 21:17:44.866840 after 0.19 seconds
[0m21:17:44.867175 [debug] [MainThread]: Flushing usage events


============================== 21:18:10.292856 | 1a29f0d7-feef-487a-b439-5b86ada52224 ==============================
[0m21:18:10.292856 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:18:10.295321 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:18:10.295569 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:18:10.369596 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:18:10.388348 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:18:10.423468 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:18:10.423956 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:18:10.452741 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m21:18:10.466203 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:18:10.467566 [info ] [MainThread]: 
[0m21:18:10.467995 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:18:10.468530 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:18:10.476169 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:18:10.476444 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:18:10.476618 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:10.489280 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.490330 [debug] [ThreadPool]: On list_dbt: Close
[0m21:18:10.492138 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:18:10.492559 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:18:10.495590 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:10.495851 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:18:10.496013 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:10.502045 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.502320 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:10.502484 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:18:10.502807 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.503317 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:18:10.503482 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:10.503633 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:18:10.503859 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.504023 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:18:10.506894 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:18:10.510422 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:18:10.510641 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:18:10.510806 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:10.516668 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.516945 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:18:10.517151 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:18:10.535267 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.536309 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:18:10.537008 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:18:10.537204 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:18:10.539239 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:18:10.541607 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:18:10.541920 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:18:10.542134 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:10.547311 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.547567 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:18:10.547743 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:18:10.563392 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:10.567080 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:18:10.567339 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:18:10.567499 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:18:10.570291 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:10.570503 [debug] [MainThread]: On master: BEGIN
[0m21:18:10.570661 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:18:10.576173 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:10.576423 [debug] [MainThread]: On master: COMMIT
[0m21:18:10.576589 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:10.576742 [debug] [MainThread]: On master: COMMIT
[0m21:18:10.576951 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:10.577113 [debug] [MainThread]: On master: Close
[0m21:18:10.578530 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:10.578744 [info ] [MainThread]: 
[0m21:18:10.580903 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:18:10.581205 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:18:10.581583 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:18:10.581776 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:18:10.587519 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:18:10.588106 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:18:10.581913 => 21:18:10.587997
[0m21:18:10.588339 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:18:10.607793 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:18:10.608449 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:18:10.608650 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:18:10.608823 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:18:10.614594 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:18:10.614877 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:18:10.615207 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    u.id as user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
where is_active = 1
    );
  
  
[0m21:18:10.616818 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:18:10.588468 => 21:18:10.616710
[0m21:18:10.617031 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m21:18:10.620861 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m21:18:10.621146 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:18:10.623164 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "ur" does not have a column named "id"
  LINE 48: ...6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.id as user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."staging"."stg_salesforce__user" u
  left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
                                                     ^
[0m21:18:10.623663 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m21:18:10.624137 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:18:10.624893 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:10.625073 [debug] [MainThread]: On master: BEGIN
[0m21:18:10.625223 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:18:10.631361 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:10.631613 [debug] [MainThread]: On master: COMMIT
[0m21:18:10.631778 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:10.631940 [debug] [MainThread]: On master: COMMIT
[0m21:18:10.632156 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:10.632312 [debug] [MainThread]: On master: Close
[0m21:18:10.633937 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:10.634234 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:18:10.634430 [info ] [MainThread]: 
[0m21:18:10.634630 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m21:18:10.635048 [debug] [MainThread]: Command end result
[0m21:18:10.642995 [info ] [MainThread]: 
[0m21:18:10.643339 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:18:10.643526 [info ] [MainThread]: 
[0m21:18:10.643754 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "ur" does not have a column named "id"
  LINE 48: ...6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.id as user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."staging"."stg_salesforce__user" u
  left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
                                                     ^
[0m21:18:10.643998 [info ] [MainThread]: 
[0m21:18:10.644189 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:18:10.644644 [debug] [MainThread]: Command `dbt run` failed at 21:18:10.644568 after 0.37 seconds
[0m21:18:10.644902 [debug] [MainThread]: Flushing usage events


============================== 21:18:59.437144 | 1640fdfd-bba2-4c8a-ac2f-1a2258167220 ==============================
[0m21:18:59.437144 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:18:59.439651 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m21:18:59.439941 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:18:59.517060 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:18:59.535812 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:18:59.571747 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:18:59.572239 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:18:59.600958 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m21:18:59.614643 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:18:59.615993 [info ] [MainThread]: 
[0m21:18:59.616419 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:18:59.616951 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:18:59.624135 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:18:59.624389 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:18:59.624563 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:59.632423 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.633392 [debug] [ThreadPool]: On list_dbt: Close
[0m21:18:59.635376 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:18:59.635880 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:18:59.638990 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:59.639240 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:18:59.639395 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:59.645228 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.645570 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:59.645761 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:18:59.646124 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.646725 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:18:59.646931 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:18:59.647089 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:18:59.647308 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.647473 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:18:59.650477 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_staging)
[0m21:18:59.654366 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:18:59.654585 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:18:59.654735 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:59.660517 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.660793 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:18:59.661163 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:18:59.680171 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.681362 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:18:59.681903 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:18:59.682087 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:18:59.684279 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:18:59.686946 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:18:59.687175 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:18:59.687335 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:18:59.693663 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.693919 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:18:59.694098 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:18:59.709508 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:18:59.713103 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:18:59.713412 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:18:59.713585 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:18:59.716430 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:59.716670 [debug] [MainThread]: On master: BEGIN
[0m21:18:59.716840 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:18:59.722331 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:59.722587 [debug] [MainThread]: On master: COMMIT
[0m21:18:59.722762 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:59.722909 [debug] [MainThread]: On master: COMMIT
[0m21:18:59.723112 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:59.723283 [debug] [MainThread]: On master: Close
[0m21:18:59.724700 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:59.724902 [info ] [MainThread]: 
[0m21:18:59.726897 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:18:59.727218 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:18:59.727610 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_user)
[0m21:18:59.727812 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:18:59.733289 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:18:59.733889 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:18:59.727951 => 21:18:59.733773
[0m21:18:59.734146 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:18:59.753994 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:18:59.754579 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:18:59.754782 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:18:59.754972 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:18:59.761001 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:18:59.761359 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:18:59.761605 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
where is_active = 1
    );
  
  
[0m21:18:59.763178 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:18:59.734281 => 21:18:59.763066
[0m21:18:59.763403 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: ROLLBACK
[0m21:18:59.764876 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_user'
[0m21:18:59.765077 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:18:59.767019 [debug] [Thread-1  ]: Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "ur" does not have a column named "id"
  LINE 48: ...6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."staging"."stg_salesforce__user" u
  left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
                                                     ^
[0m21:18:59.767479 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model main.dim_user ............................ [[31mERROR[0m in 0.04s]
[0m21:18:59.767868 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:18:59.768605 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:59.768773 [debug] [MainThread]: On master: BEGIN
[0m21:18:59.768922 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:18:59.774429 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:59.774726 [debug] [MainThread]: On master: COMMIT
[0m21:18:59.774911 [debug] [MainThread]: Using duckdb connection "master"
[0m21:18:59.775199 [debug] [MainThread]: On master: COMMIT
[0m21:18:59.775444 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:18:59.775618 [debug] [MainThread]: On master: Close
[0m21:18:59.777104 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:59.777324 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:18:59.777536 [info ] [MainThread]: 
[0m21:18:59.777740 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m21:18:59.778132 [debug] [MainThread]: Command end result
[0m21:18:59.785498 [info ] [MainThread]: 
[0m21:18:59.785778 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:18:59.785952 [info ] [MainThread]: 
[0m21:18:59.786345 [error] [MainThread]:   Runtime Error in model dim_user (models/dimensions/dim_user.sql)
  Binder Error: Values list "ur" does not have a column named "id"
  LINE 48: ...6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
  
    
      
      
  
      create  table
        "dbt"."main"."dim_user__dbt_tmp"
    
      as (
        
  
  select
      u.user_id,
      u.username as user_name,
      u.firstname as first_name,
      u.lastname as last_name,
      u.companyname as company_name,
      u.division as division,
      u.department as department,
      u.title as title,
      u.street as street,
      u.city as city,
      u.state as state,
      u.postalcode as postal_code,
      u.country as country,
      u.latitude as latitude,
      u.longitude as longitude,
      u.email as email,
      u.phone as phone,
      u.mobilephone as mobile_phone,
      u.alias as alias,
      u.isactive as is_active,
      u.timezonesidkey as timezone_sid_key,
      u.localesidkey as locale_sid_key,
      u.emailencodingkey as email_encoding_key,
      u.profileid as profile_id,
      u.usertype as user_type,
      u.usersubtype as user_subtype,
      u.lastlogindate as last_login_date,
      u.createddate as created_date,
      ur.name as role_name,
      ur.parentroleid as parent_role_id,
      ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
      ur.caseaccessforaccountowner as case_access_for_account_owner,
      ur.contactaccessforaccountowner as contact_access_for_account_owner
  from "dbt"."staging"."stg_salesforce__user" u
  left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.id
                                                     ^
[0m21:18:59.786633 [info ] [MainThread]: 
[0m21:18:59.786852 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:18:59.787273 [debug] [MainThread]: Command `dbt run` failed at 21:18:59.787215 after 0.37 seconds
[0m21:18:59.787512 [debug] [MainThread]: Flushing usage events


============================== 21:19:45.523718 | 9201c432-444c-4e01-a757-61d8e8fbd4a4 ==============================
[0m21:19:45.523718 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:19:45.526146 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:19:45.526404 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:19:45.601770 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:19:45.620550 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:19:45.656920 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:19:45.657450 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_user.sql
[0m21:19:45.684128 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:19:45.696783 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:19:45.698711 [info ] [MainThread]: 
[0m21:19:45.699391 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:19:45.700016 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:19:45.707421 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:19:45.707745 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:19:45.707936 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:19:45.715600 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.716712 [debug] [ThreadPool]: On list_dbt: Close
[0m21:19:45.718772 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_main)
[0m21:19:45.719274 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "main"
"
[0m21:19:45.722204 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:19:45.722452 [debug] [ThreadPool]: On create_dbt_main: BEGIN
[0m21:19:45.722614 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:19:45.728412 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.728832 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:19:45.729040 [debug] [ThreadPool]: On create_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_main"} */
create schema if not exists "dbt"."main"
[0m21:19:45.729399 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.730030 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:19:45.730299 [debug] [ThreadPool]: Using duckdb connection "create_dbt_main"
[0m21:19:45.730475 [debug] [ThreadPool]: On create_dbt_main: COMMIT
[0m21:19:45.730719 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.730912 [debug] [ThreadPool]: On create_dbt_main: Close
[0m21:19:45.733966 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_main, now list_dbt_main)
[0m21:19:45.737593 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:19:45.737863 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:19:45.738048 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:19:45.744276 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.744569 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:19:45.744754 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:19:45.760114 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.763825 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:19:45.764270 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:19:45.764437 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:19:45.766521 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m21:19:45.768793 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:19:45.768979 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:19:45.769129 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:19:45.774697 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.774898 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:19:45.775070 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:19:45.793279 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:19:45.794330 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:19:45.794573 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:19:45.794735 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:19:45.797817 [debug] [MainThread]: Using duckdb connection "master"
[0m21:19:45.798021 [debug] [MainThread]: On master: BEGIN
[0m21:19:45.798182 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:19:45.803650 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:19:45.803897 [debug] [MainThread]: On master: COMMIT
[0m21:19:45.804060 [debug] [MainThread]: Using duckdb connection "master"
[0m21:19:45.804209 [debug] [MainThread]: On master: COMMIT
[0m21:19:45.804407 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:19:45.804562 [debug] [MainThread]: On master: Close
[0m21:19:45.806142 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:19:45.806407 [info ] [MainThread]: 
[0m21:19:45.808445 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:19:45.808788 [info ] [Thread-1  ]: 1 of 1 START sql table model main.dim_user ..................................... [RUN]
[0m21:19:45.809182 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_user)
[0m21:19:45.809372 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:19:45.814852 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:19:45.815397 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:19:45.809516 => 21:19:45.815291
[0m21:19:45.815604 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:19:45.834899 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:19:45.835583 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.835786 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:19:45.835963 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:19:45.841945 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.842239 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.842475 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."main"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m21:19:45.846941 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.850781 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.851113 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."main"."dim_user" rename to "dim_user__dbt_backup"
[0m21:19:45.851536 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.853290 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.853496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."main"."dim_user__dbt_tmp" rename to "dim_user"
[0m21:19:45.853810 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.865092 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:19:45.865391 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.865588 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:19:45.866653 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.893955 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:19:45.894301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."main"."dim_user__dbt_backup" cascade
[0m21:19:45.894945 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:19:45.895747 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:19:45.815745 => 21:19:45.895652
[0m21:19:45.895988 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:19:45.910969 [info ] [Thread-1  ]: 1 of 1 OK created sql table model main.dim_user ................................ [[32mOK[0m in 0.10s]
[0m21:19:45.911376 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:19:45.912116 [debug] [MainThread]: Using duckdb connection "master"
[0m21:19:45.912325 [debug] [MainThread]: On master: BEGIN
[0m21:19:45.912475 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:19:45.918233 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:19:45.918488 [debug] [MainThread]: On master: COMMIT
[0m21:19:45.918655 [debug] [MainThread]: Using duckdb connection "master"
[0m21:19:45.918805 [debug] [MainThread]: On master: COMMIT
[0m21:19:45.919005 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:19:45.919164 [debug] [MainThread]: On master: Close
[0m21:19:45.920747 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:19:45.920954 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:19:45.921148 [info ] [MainThread]: 
[0m21:19:45.921328 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m21:19:45.921748 [debug] [MainThread]: Command end result
[0m21:19:45.928732 [info ] [MainThread]: 
[0m21:19:45.929005 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:19:45.929173 [info ] [MainThread]: 
[0m21:19:45.929347 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:19:45.929867 [debug] [MainThread]: Command `dbt run` succeeded at 21:19:45.929781 after 0.42 seconds
[0m21:19:45.930088 [debug] [MainThread]: Flushing usage events


============================== 21:27:43.281992 | 726474dd-a0e2-42bf-8dba-032bb69e6e28 ==============================
[0m21:27:43.281992 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:27:43.285091 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_user.sql', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m21:27:43.285342 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:27:43.369373 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:27:43.388204 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:27:43.429259 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:27:43.429664 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:27:43.430739 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:27:43.446246 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:27:43.447663 [info ] [MainThread]: 
[0m21:27:43.448126 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:27:43.448832 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:27:43.455794 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:27:43.456045 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:27:43.456218 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:27:43.469116 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.470083 [debug] [ThreadPool]: On list_dbt: Close
[0m21:27:43.471953 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:27:43.472511 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:27:43.475696 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:27:43.475968 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:27:43.476159 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:27:43.482106 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.482383 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:27:43.482559 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:27:43.482821 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.483450 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:27:43.483743 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:27:43.483930 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:27:43.484577 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.484808 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:27:43.504820 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m21:27:43.508691 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:27:43.508961 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:27:43.509123 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:27:43.514734 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.515022 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:27:43.515329 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:27:43.531241 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.535381 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:27:43.536277 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:27:43.536525 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:27:43.538833 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:27:43.541348 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:27:43.541570 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:27:43.541804 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:27:43.547177 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.547551 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:27:43.547739 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:27:43.566524 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.567420 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:27:43.567662 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:27:43.567824 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:27:43.570021 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m21:27:43.571555 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:27:43.571772 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:27:43.571932 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:27:43.577774 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.578028 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:27:43.578199 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:27:43.595985 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.596875 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:27:43.597106 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:27:43.597262 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:27:43.599372 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m21:27:43.601734 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:27:43.601909 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:27:43.602054 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:27:43.607591 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.607845 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:27:43.608018 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:27:43.625828 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:27:43.626517 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:27:43.626742 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:27:43.626896 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:27:43.628825 [debug] [MainThread]: Using duckdb connection "master"
[0m21:27:43.629035 [debug] [MainThread]: On master: BEGIN
[0m21:27:43.629198 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:27:43.634847 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:27:43.635116 [debug] [MainThread]: On master: COMMIT
[0m21:27:43.635279 [debug] [MainThread]: Using duckdb connection "master"
[0m21:27:43.635433 [debug] [MainThread]: On master: COMMIT
[0m21:27:43.635640 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:27:43.635800 [debug] [MainThread]: On master: Close
[0m21:27:43.637191 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:27:43.637396 [info ] [MainThread]: 
[0m21:27:43.640657 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m21:27:43.640948 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_user ...................................... [RUN]
[0m21:27:43.641309 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_user)
[0m21:27:43.641507 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m21:27:43.647269 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m21:27:43.648507 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 21:27:43.641646 => 21:27:43.648227
[0m21:27:43.648822 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m21:27:43.668092 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m21:27:43.668729 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:27:43.668935 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m21:27:43.669111 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:27:43.674764 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:27:43.675059 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:27:43.675312 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m21:27:43.679614 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:27:43.683516 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:27:43.683775 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m21:27:43.684132 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:27:43.693599 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:27:43.693833 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:27:43.694015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m21:27:43.695041 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:27:43.698355 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m21:27:43.698583 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m21:27:43.698913 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:27:43.699681 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 21:27:43.648975 => 21:27:43.699584
[0m21:27:43.699897 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m21:27:43.714493 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_user ................................. [[32mOK[0m in 0.07s]
[0m21:27:43.714945 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m21:27:43.715752 [debug] [MainThread]: Using duckdb connection "master"
[0m21:27:43.715948 [debug] [MainThread]: On master: BEGIN
[0m21:27:43.716106 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:27:43.721795 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:27:43.722048 [debug] [MainThread]: On master: COMMIT
[0m21:27:43.722213 [debug] [MainThread]: Using duckdb connection "master"
[0m21:27:43.722365 [debug] [MainThread]: On master: COMMIT
[0m21:27:43.722567 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:27:43.722733 [debug] [MainThread]: On master: Close
[0m21:27:43.724249 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:27:43.724480 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_user' was properly closed.
[0m21:27:43.724668 [info ] [MainThread]: 
[0m21:27:43.724855 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m21:27:43.725201 [debug] [MainThread]: Command end result
[0m21:27:43.733945 [info ] [MainThread]: 
[0m21:27:43.734225 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:27:43.734388 [info ] [MainThread]: 
[0m21:27:43.734586 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:27:43.734995 [debug] [MainThread]: Command `dbt run` succeeded at 21:27:43.734945 after 0.47 seconds
[0m21:27:43.735196 [debug] [MainThread]: Flushing usage events


============================== 21:28:04.708645 | b143d771-27ab-4b67-a941-58ebe35d5318 ==============================
[0m21:28:04.708645 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:28:04.711408 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:28:04.711668 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:28:04.794503 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:28:04.814366 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:28:04.862259 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:28:04.862564 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:28:04.863534 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m21:28:04.877782 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:28:04.878666 [warn ] [MainThread]: The selection criterion 'models/dimensions/dim_record_type.sql' does not match any nodes
[0m21:28:04.879302 [info ] [MainThread]: 
[0m21:28:04.879482 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:28:04.879740 [debug] [MainThread]: Command end result
[0m21:28:04.886745 [debug] [MainThread]: Command `dbt run` succeeded at 21:28:04.886667 after 0.20 seconds
[0m21:28:04.886997 [debug] [MainThread]: Flushing usage events


============================== 21:28:21.524853 | 59de5ae6-a8f3-411e-8e10-adf5bfa648bd ==============================
[0m21:28:21.524853 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:28:21.527748 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select dim_record_type.sql', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:28:21.528009 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:28:21.605522 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:28:21.625406 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:28:21.671930 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:28:21.672252 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:28:21.673255 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:28:21.687381 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:28:21.688240 [warn ] [MainThread]: The selection criterion 'dim_record_type.sql' does not match any nodes
[0m21:28:21.688928 [info ] [MainThread]: 
[0m21:28:21.689122 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:28:21.689380 [debug] [MainThread]: Command end result
[0m21:28:21.696484 [debug] [MainThread]: Command `dbt run` succeeded at 21:28:21.696371 after 0.20 seconds
[0m21:28:21.696803 [debug] [MainThread]: Flushing usage events


============================== 21:28:50.879388 | 1b87cc3f-f2f1-4604-a2bf-3327eabcebc9 ==============================
[0m21:28:50.879388 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:28:50.881791 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:28:50.882040 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:28:50.957290 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:28:50.975865 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:28:51.011124 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:28:51.011438 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:28:51.012441 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:28:51.026319 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:28:51.027222 [warn ] [MainThread]: The selection criterion 'models/dimensions/dim_record_type.sql' does not match any nodes
[0m21:28:51.027862 [info ] [MainThread]: 
[0m21:28:51.028047 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:28:51.028305 [debug] [MainThread]: Command end result
[0m21:28:51.035704 [debug] [MainThread]: Command `dbt run` succeeded at 21:28:51.035590 after 0.17 seconds
[0m21:28:51.036145 [debug] [MainThread]: Flushing usage events


============================== 21:29:50.676266 | 3d0c7d58-e844-4d8e-b9fd-25eda35c8cd6 ==============================
[0m21:29:50.676266 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:29:50.679467 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:29:50.679767 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:29:50.756843 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:29:50.775160 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:29:50.811318 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:29:50.811796 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_record_type.sql
[0m21:29:50.830610 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_record_type (models/dimensions/dim_record_type.sql)
  expected token ',', got 'the'
    line 3
      schema='dim'  -- Specifies the schema for dimension tables
[0m21:29:50.831198 [debug] [MainThread]: Command `dbt run` failed at 21:29:50.831119 after 0.18 seconds
[0m21:29:50.831463 [debug] [MainThread]: Flushing usage events


============================== 21:30:00.324549 | c675d5e1-7a06-423b-8ce9-5ff68f40c061 ==============================
[0m21:30:00.324549 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:30:00.327533 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:30:00.327812 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:30:00.410112 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:30:00.428149 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:30:00.466629 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:30:00.467070 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_record_type.sql
[0m21:30:00.494784 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m21:30:00.508559 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:30:00.510149 [info ] [MainThread]: 
[0m21:30:00.510680 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:30:00.511267 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:30:00.518907 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:30:00.519409 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:30:00.519804 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:30:00.534780 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.535741 [debug] [ThreadPool]: On list_dbt: Close
[0m21:30:00.537607 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:30:00.538047 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:30:00.540966 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:00.541202 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:30:00.541378 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:00.547309 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.547675 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:00.547884 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:30:00.548206 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.548724 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:00.548902 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:00.549056 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:00.549284 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.549446 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:30:00.552231 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:30:00.555696 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:00.556047 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:30:00.556409 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:00.562292 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.562538 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:00.562730 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:30:00.582081 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.583137 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:30:00.584922 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:30:00.585154 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:30:00.589473 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m21:30:00.592514 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:00.592744 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:30:00.592924 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:00.600637 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.600933 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:00.601163 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:30:00.620042 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.620818 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:30:00.621057 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:30:00.621221 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:30:00.623576 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m21:30:00.625649 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:00.625833 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:30:00.625988 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:00.631577 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.631853 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:00.632040 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:30:00.647810 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.651341 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:30:00.651671 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:30:00.651927 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:30:00.654122 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:30:00.655594 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:00.656000 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:30:00.656173 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:00.661436 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.661674 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:00.661850 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:30:00.678027 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:00.681583 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:30:00.681829 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:30:00.682001 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:30:00.684808 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:00.685014 [debug] [MainThread]: On master: BEGIN
[0m21:30:00.685176 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:30:00.690738 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:00.691002 [debug] [MainThread]: On master: COMMIT
[0m21:30:00.691172 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:00.691332 [debug] [MainThread]: On master: COMMIT
[0m21:30:00.691538 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:00.691706 [debug] [MainThread]: On master: Close
[0m21:30:00.693239 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:30:00.693469 [info ] [MainThread]: 
[0m21:30:00.696486 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m21:30:00.696927 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_record_type ............................... [RUN]
[0m21:30:00.697320 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_record_type)
[0m21:30:00.697528 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m21:30:00.703204 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:00.703847 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 21:30:00.697677 => 21:30:00.703730
[0m21:30:00.704061 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m21:30:00.722835 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:00.723423 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:00.723626 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m21:30:00.723806 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:30:00.729670 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:00.730061 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:00.730302 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

SELECT
    id AS record_type_id,
    name AS record_type_name,
    modulenamespace AS module_namespace,
    description AS record_type_description,
    businessprocessid AS business_process_id,
    sobjecttype AS sobject_type,
    isactive AS is_active,
    createdbyid AS created_by_id,
    createddate AS created_date,
    lastmodifiedbyid AS last_modified_by_id,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."raw"."record_type"
WHERE
    isdeleted = FALSE
ORDER BY
    record_type_name;
    );
  
  
[0m21:30:00.730789 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 21:30:00.704336 => 21:30:00.730690
[0m21:30:00.730993 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: ROLLBACK
[0m21:30:00.738443 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_record_type'
[0m21:30:00.738746 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m21:30:00.740759 [debug] [Thread-1  ]: Runtime Error in model dim_record_type (models/dimensions/dim_record_type.sql)
  Parser Error: syntax error at or near ";"
[0m21:30:00.741357 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_record_type ...................... [[31mERROR[0m in 0.04s]
[0m21:30:00.741849 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m21:30:00.742692 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:00.742877 [debug] [MainThread]: On master: BEGIN
[0m21:30:00.743030 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:30:00.749213 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:00.749510 [debug] [MainThread]: On master: COMMIT
[0m21:30:00.749690 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:00.749865 [debug] [MainThread]: On master: COMMIT
[0m21:30:00.750083 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:00.750287 [debug] [MainThread]: On master: Close
[0m21:30:00.752052 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:30:00.752345 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_record_type' was properly closed.
[0m21:30:00.754793 [info ] [MainThread]: 
[0m21:30:00.755308 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m21:30:00.755892 [debug] [MainThread]: Command end result
[0m21:30:00.792347 [info ] [MainThread]: 
[0m21:30:00.792637 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:30:00.792809 [info ] [MainThread]: 
[0m21:30:00.792972 [error] [MainThread]:   Runtime Error in model dim_record_type (models/dimensions/dim_record_type.sql)
  Parser Error: syntax error at or near ";"
[0m21:30:00.793129 [info ] [MainThread]: 
[0m21:30:00.793300 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:30:00.793612 [debug] [MainThread]: Command `dbt run` failed at 21:30:00.793565 after 0.49 seconds
[0m21:30:00.793806 [debug] [MainThread]: Flushing usage events


============================== 21:30:06.839850 | 051bf2af-3efe-43cd-a170-56507136583f ==============================
[0m21:30:06.839850 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:30:06.842245 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:30:06.842505 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:30:06.916030 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:30:06.934679 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:30:06.969528 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:30:06.969974 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_record_type.sql
[0m21:30:06.998465 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:30:07.012666 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:30:07.014343 [info ] [MainThread]: 
[0m21:30:07.014822 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:30:07.015527 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:30:07.023888 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:30:07.024289 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:30:07.024482 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:30:07.032498 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.033578 [debug] [ThreadPool]: On list_dbt: Close
[0m21:30:07.035556 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:30:07.036058 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:30:07.039036 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:07.039269 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:30:07.039438 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:07.045498 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.045674 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:07.045824 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:30:07.046065 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.046542 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:07.046706 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:07.046851 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:07.047051 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.047210 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:30:07.050183 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:30:07.053382 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:07.053628 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:30:07.053780 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:07.059742 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.059993 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:07.060165 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:30:07.079026 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.079971 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:30:07.080350 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:30:07.080516 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:30:07.082637 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m21:30:07.084956 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:07.085125 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:30:07.085268 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:07.090879 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.091161 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:07.091366 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:30:07.110555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.111316 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:30:07.111550 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:30:07.111709 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:30:07.113829 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m21:30:07.116179 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:07.116468 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:30:07.116638 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:07.122780 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.123015 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:07.123190 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:30:07.138340 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.141793 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:30:07.142031 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:30:07.142191 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:30:07.144263 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:30:07.145815 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:07.145992 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:30:07.146144 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:07.151428 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.151653 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:07.151822 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:30:07.167471 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:07.171150 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:30:07.171388 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:30:07.171546 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:30:07.174408 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:07.174609 [debug] [MainThread]: On master: BEGIN
[0m21:30:07.174770 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:30:07.180223 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:07.180465 [debug] [MainThread]: On master: COMMIT
[0m21:30:07.180626 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:07.180839 [debug] [MainThread]: On master: COMMIT
[0m21:30:07.181116 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:07.181298 [debug] [MainThread]: On master: Close
[0m21:30:07.182705 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:30:07.182910 [info ] [MainThread]: 
[0m21:30:07.183961 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m21:30:07.184391 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_record_type ............................... [RUN]
[0m21:30:07.184755 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_record_type)
[0m21:30:07.184947 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m21:30:07.190253 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.190898 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 21:30:07.185086 => 21:30:07.190767
[0m21:30:07.191192 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m21:30:07.210462 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.211078 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.211285 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m21:30:07.211464 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:30:07.217162 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:07.217442 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.217660 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

SELECT
    id AS record_type_id,
    name AS record_type_name,
    modulenamespace AS module_namespace,
    description AS record_type_description,
    businessprocessid AS business_process_id,
    sobjecttype AS sobject_type,
    isactive AS is_active,
    createdbyid AS created_by_id,
    createddate AS created_date,
    lastmodifiedbyid AS last_modified_by_id,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."raw"."record_type"
WHERE
    isdeleted = FALSE
ORDER BY
    record_type_name
    );
  
  
[0m21:30:07.219368 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:07.223861 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.224111 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m21:30:07.224493 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:07.256930 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m21:30:07.257267 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.257459 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m21:30:07.258271 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:07.261236 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:07.261460 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m21:30:07.261763 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:07.262560 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 21:30:07.191423 => 21:30:07.262463
[0m21:30:07.262764 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m21:30:07.274473 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_record_type .......................... [[32mOK[0m in 0.09s]
[0m21:30:07.274860 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m21:30:07.275585 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:07.275763 [debug] [MainThread]: On master: BEGIN
[0m21:30:07.275911 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:30:07.281447 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:07.281737 [debug] [MainThread]: On master: COMMIT
[0m21:30:07.281903 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:07.282056 [debug] [MainThread]: On master: COMMIT
[0m21:30:07.282351 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:07.282656 [debug] [MainThread]: On master: Close
[0m21:30:07.284275 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:30:07.284521 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_record_type' was properly closed.
[0m21:30:07.284722 [info ] [MainThread]: 
[0m21:30:07.284911 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m21:30:07.285252 [debug] [MainThread]: Command end result
[0m21:30:07.292037 [info ] [MainThread]: 
[0m21:30:07.292314 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:30:07.292490 [info ] [MainThread]: 
[0m21:30:07.292670 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:30:07.293098 [debug] [MainThread]: Command `dbt run` succeeded at 21:30:07.292989 after 0.47 seconds
[0m21:30:07.293402 [debug] [MainThread]: Flushing usage events


============================== 21:30:27.137626 | 3f8a4e2f-c184-46ae-b3ed-7af5d0969fc5 ==============================
[0m21:30:27.137626 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:30:27.140032 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:30:27.140282 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:30:27.214054 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:30:27.232067 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:30:27.268395 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:30:27.268708 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:30:27.269683 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m21:30:27.283566 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:30:27.285087 [info ] [MainThread]: 
[0m21:30:27.285522 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:30:27.286194 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:30:27.293348 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:30:27.293617 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:30:27.293802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:30:27.300894 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.301805 [debug] [ThreadPool]: On list_dbt: Close
[0m21:30:27.303804 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:30:27.304366 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:30:27.307422 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:27.307648 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:30:27.307813 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:27.314039 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.314292 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:27.314463 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:30:27.314712 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.315225 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:27.315464 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:27.315657 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:27.315902 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.316067 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:30:27.318842 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m21:30:27.322301 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:27.322548 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:30:27.322722 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:27.328415 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.328664 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:27.328844 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:30:27.344854 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.348303 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:30:27.348749 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:30:27.348922 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:30:27.351162 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:30:27.352860 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:27.353062 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:30:27.353221 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:27.358873 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.359127 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:27.359298 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:30:27.374683 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.378359 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:30:27.378615 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:30:27.378784 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:30:27.380960 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:30:27.383188 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:27.383373 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:30:27.383523 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:27.389305 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.389552 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:27.389725 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:30:27.407295 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.407977 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:30:27.408196 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:30:27.408349 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:30:27.410335 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m21:30:27.411800 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:27.411990 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:30:27.412143 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:27.417514 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.417748 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:27.417918 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:30:27.436404 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:27.437430 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:30:27.437665 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:30:27.437824 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:30:27.440985 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:27.441234 [debug] [MainThread]: On master: BEGIN
[0m21:30:27.441397 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:30:27.447223 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:27.447487 [debug] [MainThread]: On master: COMMIT
[0m21:30:27.447666 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:27.447834 [debug] [MainThread]: On master: COMMIT
[0m21:30:27.448049 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:27.448218 [debug] [MainThread]: On master: Close
[0m21:30:27.449770 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:30:27.450011 [info ] [MainThread]: 
[0m21:30:27.451947 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m21:30:27.452234 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_record_type ............................... [RUN]
[0m21:30:27.452594 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_record_type)
[0m21:30:27.452786 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m21:30:27.458038 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:27.458619 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 21:30:27.452921 => 21:30:27.458498
[0m21:30:27.458824 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m21:30:27.477649 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:27.478112 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:27.478311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m21:30:27.478490 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:30:27.484370 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:27.484664 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:27.484873 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

SELECT
    id AS record_type_id,
    name AS record_type_name,
    modulenamespace AS module_namespace,
    description AS record_type_description,
    businessprocessid AS business_process_id,
    sobjecttype AS sobject_type,
    isactive AS is_active,
    createdbyid AS created_by_id,
    createddate AS created_date,
    lastmodifiedbyid AS last_modified_by_id,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."staging"."stg_salesforce__record_type"
WHERE
    isdeleted = FALSE
ORDER BY
    record_type_name
    );
  
  
[0m21:30:27.485659 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 21:30:27.458958 => 21:30:27.485559
[0m21:30:27.485859 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: ROLLBACK
[0m21:30:27.487172 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_record_type'
[0m21:30:27.487366 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m21:30:27.489309 [debug] [Thread-1  ]: Runtime Error in model dim_record_type (models/dimensions/dim_record_type.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__record_type.name"
  LINE 14:     id AS record_type_id,
               ^
[0m21:30:27.489721 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_record_type ...................... [[31mERROR[0m in 0.04s]
[0m21:30:27.490064 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m21:30:27.490979 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:27.491162 [debug] [MainThread]: On master: BEGIN
[0m21:30:27.491310 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:30:27.496856 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:27.497115 [debug] [MainThread]: On master: COMMIT
[0m21:30:27.497271 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:27.497420 [debug] [MainThread]: On master: COMMIT
[0m21:30:27.497620 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:27.497777 [debug] [MainThread]: On master: Close
[0m21:30:27.499222 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:30:27.499400 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_record_type' was properly closed.
[0m21:30:27.499587 [info ] [MainThread]: 
[0m21:30:27.499785 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m21:30:27.500129 [debug] [MainThread]: Command end result
[0m21:30:27.507289 [info ] [MainThread]: 
[0m21:30:27.507537 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:30:27.507698 [info ] [MainThread]: 
[0m21:30:27.507861 [error] [MainThread]:   Runtime Error in model dim_record_type (models/dimensions/dim_record_type.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__record_type.name"
  LINE 14:     id AS record_type_id,
               ^
[0m21:30:27.508027 [info ] [MainThread]: 
[0m21:30:27.508213 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:30:27.508831 [debug] [MainThread]: Command `dbt run` failed at 21:30:27.508718 after 0.39 seconds
[0m21:30:27.509169 [debug] [MainThread]: Flushing usage events


============================== 21:30:47.471326 | d68433ed-97e8-4159-8bc7-a2ae2865186c ==============================
[0m21:30:47.471326 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:30:47.473788 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_record_type.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:30:47.474053 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:30:47.549484 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:30:47.567191 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:30:47.604266 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:30:47.604608 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:30:47.605601 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:30:47.619208 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:30:47.620580 [info ] [MainThread]: 
[0m21:30:47.621011 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:30:47.621580 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:30:47.628630 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:30:47.628911 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:30:47.629092 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:30:47.636937 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.637867 [debug] [ThreadPool]: On list_dbt: Close
[0m21:30:47.639769 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:30:47.640136 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:30:47.642948 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:47.643143 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:30:47.643296 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:47.649220 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.649490 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:47.649803 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:30:47.650199 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.650738 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:47.650927 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:30:47.651084 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:30:47.651309 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.651474 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:30:47.654549 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m21:30:47.658030 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:47.658233 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:30:47.658396 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:47.664422 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.664678 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:30:47.664859 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:30:47.680323 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.683831 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:30:47.684255 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:30:47.684424 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:30:47.686728 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:30:47.688464 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:47.688652 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:30:47.688806 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:47.694394 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.694650 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:30:47.694824 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:30:47.710596 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.714154 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:30:47.714529 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:30:47.714786 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:30:47.716928 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:30:47.718592 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:47.718850 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:30:47.719105 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:47.725586 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.725840 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:30:47.726021 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:30:47.744236 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.745050 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:30:47.745300 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:30:47.745463 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:30:47.747776 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m21:30:47.749450 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:47.749681 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:30:47.749830 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:47.755576 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.755848 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:30:47.756025 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:30:47.773768 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:30:47.774704 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:30:47.774936 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:30:47.775110 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:30:47.778205 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:47.778417 [debug] [MainThread]: On master: BEGIN
[0m21:30:47.778579 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:30:47.784478 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:47.784755 [debug] [MainThread]: On master: COMMIT
[0m21:30:47.784919 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:47.785067 [debug] [MainThread]: On master: COMMIT
[0m21:30:47.785271 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:47.785429 [debug] [MainThread]: On master: Close
[0m21:30:47.786947 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:30:47.787175 [info ] [MainThread]: 
[0m21:30:47.789207 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m21:30:47.789507 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_record_type ............................... [RUN]
[0m21:30:47.789883 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_record_type)
[0m21:30:47.790081 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m21:30:47.795493 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.796116 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 21:30:47.790220 => 21:30:47.796001
[0m21:30:47.796331 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m21:30:47.815715 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.816279 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.816481 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m21:30:47.816660 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:30:47.822392 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.822688 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.822908 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

SELECT
    record_type_id,
    name AS record_type_name,
    modulenamespace AS module_namespace,
    description AS record_type_description,
    businessprocessid AS business_process_id,
    sobjecttype AS sobject_type,
    isactive AS is_active,
    createdbyid AS created_by_id,
    createddate AS created_date,
    lastmodifiedbyid AS last_modified_by_id,
    lastmodifieddate AS last_modified_date,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."staging"."stg_salesforce__record_type"
WHERE
    isdeleted = FALSE
ORDER BY
    record_type_name
    );
  
  
[0m21:30:47.824409 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.828082 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.828338 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m21:30:47.828701 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.830458 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.830663 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m21:30:47.830967 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.841224 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m21:30:47.841452 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.841638 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m21:30:47.842421 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.845983 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m21:30:47.846230 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m21:30:47.846681 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:30:47.847436 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 21:30:47.796473 => 21:30:47.847344
[0m21:30:47.847639 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m21:30:47.859485 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_record_type .......................... [[32mOK[0m in 0.07s]
[0m21:30:47.859881 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m21:30:47.860627 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:47.860827 [debug] [MainThread]: On master: BEGIN
[0m21:30:47.860985 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:30:47.867172 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:47.867427 [debug] [MainThread]: On master: COMMIT
[0m21:30:47.867599 [debug] [MainThread]: Using duckdb connection "master"
[0m21:30:47.867763 [debug] [MainThread]: On master: COMMIT
[0m21:30:47.867983 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:30:47.868157 [debug] [MainThread]: On master: Close
[0m21:30:47.869595 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:30:47.869802 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_record_type' was properly closed.
[0m21:30:47.869993 [info ] [MainThread]: 
[0m21:30:47.870193 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m21:30:47.870531 [debug] [MainThread]: Command end result
[0m21:30:47.877554 [info ] [MainThread]: 
[0m21:30:47.877800 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:30:47.877969 [info ] [MainThread]: 
[0m21:30:47.878154 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:30:47.878618 [debug] [MainThread]: Command `dbt run` succeeded at 21:30:47.878555 after 0.42 seconds
[0m21:30:47.878835 [debug] [MainThread]: Flushing usage events


============================== 21:31:59.894984 | 3e0fedb3-a980-42bf-bfc3-542304061ef6 ==============================
[0m21:31:59.894984 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:31:59.897350 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:31:59.897594 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:31:59.972085 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:31:59.990128 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:32:00.026202 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:32:00.026734 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/staging/stg_salesforce__solution.sql
[0m21:32:00.045404 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_salesforce__solution (models/staging/stg_salesforce__solution.sql)
  expected token ',', got 'the'
    line 3
      schema='dim'  -- Ensures the model is written to the 'dim' schema
[0m21:32:00.045952 [debug] [MainThread]: Command `dbt run` failed at 21:32:00.045885 after 0.17 seconds
[0m21:32:00.046179 [debug] [MainThread]: Flushing usage events


============================== 21:32:05.575479 | b9e35f89-a836-4488-9d58-05ffb5034810 ==============================
[0m21:32:05.575479 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:32:05.577932 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:32:05.578174 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:32:05.652287 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:32:05.670975 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:32:05.713297 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:32:05.713811 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/staging/stg_salesforce__solution.sql
[0m21:32:05.732665 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_salesforce__solution (models/staging/stg_salesforce__solution.sql)
  expected token ',', got 'the'
    line 3
      schema='dim'  -- Ensures the model is written to the 'dim' schema
[0m21:32:05.733142 [debug] [MainThread]: Command `dbt run` failed at 21:32:05.733081 after 0.17 seconds
[0m21:32:05.733356 [debug] [MainThread]: Flushing usage events


============================== 21:32:17.396327 | 114e6dc1-4619-48e9-a16e-0e24fc5eb251 ==============================
[0m21:32:17.396327 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:32:17.398726 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:32:17.398976 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:32:17.476924 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:32:17.495165 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:32:17.532675 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:32:17.532989 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:32:17.533985 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m21:32:17.546341 [error] [MainThread]: Encountered an error:
Found a cycle: model.elastic_dbt_interview.stg_salesforce__solution
[0m21:32:17.548884 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 87, in wrapper
    result, success = func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 143, in wrapper
    return func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 172, in wrapper
    return func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 219, in wrapper
    return func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/requires.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/cli/main.py", line 596, in run
    results = task.run()
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/task/runnable.py", line 449, in run
    self._runtime_initialize()
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/task/compile.py", line 147, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/task/runnable.py", line 142, in _runtime_initialize
    self.compile_manifest()
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/task/base.py", line 173, in compile_manifest
    self.graph = compiler.compile(self.manifest)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/compilation.py", line 453, in compile
    linker.link_graph(manifest)
  File "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/dbt-env/lib/python3.9/site-packages/dbt/compilation.py", line 198, in link_graph
    raise RuntimeError("Found a cycle: {}".format(cycle))
RuntimeError: Found a cycle: model.elastic_dbt_interview.stg_salesforce__solution

[0m21:32:17.549716 [debug] [MainThread]: Command `dbt run` failed at 21:32:17.549622 after 0.17 seconds
[0m21:32:17.549973 [debug] [MainThread]: Flushing usage events


============================== 21:35:42.330365 | be7326a5-44c4-4143-8ead-a93eb2654bae ==============================
[0m21:35:42.330365 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:35:42.333075 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:35:42.333342 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:35:42.410802 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:35:42.429388 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:35:42.466034 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:35:42.466334 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:35:42.467318 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:35:42.481174 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:35:42.482564 [info ] [MainThread]: 
[0m21:35:42.482993 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:35:42.483663 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:35:42.490613 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:35:42.490864 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:35:42.491042 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:35:42.498657 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.499600 [debug] [ThreadPool]: On list_dbt: Close
[0m21:35:42.501572 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:35:42.502009 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:35:42.504827 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:35:42.505031 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:35:42.505190 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:35:42.511266 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.511653 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:35:42.511861 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:35:42.512238 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.512807 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:35:42.512980 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:35:42.513131 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:35:42.513353 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.513517 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:35:42.516652 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m21:35:42.520485 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:35:42.520717 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:35:42.520884 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:35:42.526655 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.526942 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:35:42.527215 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:35:42.542726 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.546521 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:35:42.547869 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:35:42.548065 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:35:42.550263 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m21:35:42.552079 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:35:42.552286 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:35:42.552440 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:35:42.558581 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.558827 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:35:42.558999 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:35:42.577223 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.578256 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:35:42.578501 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:35:42.578666 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:35:42.580972 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:35:42.583697 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:35:42.583904 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:35:42.584054 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:35:42.589534 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.589801 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:35:42.589982 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:35:42.606665 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.610357 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:35:42.610611 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:35:42.610782 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:35:42.613175 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:35:42.614830 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:35:42.615826 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:35:42.616002 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:35:42.622082 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.622321 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:35:42.622497 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:35:42.641369 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:35:42.643281 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:35:42.643624 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:35:42.643807 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:35:42.646308 [debug] [MainThread]: Using duckdb connection "master"
[0m21:35:42.646581 [debug] [MainThread]: On master: BEGIN
[0m21:35:42.646767 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:35:42.652693 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:35:42.652989 [debug] [MainThread]: On master: COMMIT
[0m21:35:42.653155 [debug] [MainThread]: Using duckdb connection "master"
[0m21:35:42.653311 [debug] [MainThread]: On master: COMMIT
[0m21:35:42.653514 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:35:42.653682 [debug] [MainThread]: On master: Close
[0m21:35:42.655381 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:35:42.655683 [info ] [MainThread]: 
[0m21:35:42.658007 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m21:35:42.658368 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_solution .................................. [RUN]
[0m21:35:42.658818 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_solution)
[0m21:35:42.659032 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m21:35:42.664684 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m21:35:42.665726 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 21:35:42.659179 => 21:35:42.665551
[0m21:35:42.665992 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m21:35:42.687509 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m21:35:42.688152 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:35:42.688368 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m21:35:42.688548 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:35:42.694348 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:35:42.694622 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:35:42.694825 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solution_name,
    status,
    is_active,
    created_date,
    last_modified_date
from "dbt"."staging"."stg_salesforce__solution";
    );
  
  
[0m21:35:42.695275 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 21:35:42.666135 => 21:35:42.695174
[0m21:35:42.695472 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: ROLLBACK
[0m21:35:42.699359 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_solution'
[0m21:35:42.699654 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m21:35:42.701656 [debug] [Thread-1  ]: Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Parser Error: syntax error at or near ";"
[0m21:35:42.702077 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_solution ......................... [[31mERROR[0m in 0.04s]
[0m21:35:42.702465 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m21:35:42.703229 [debug] [MainThread]: Using duckdb connection "master"
[0m21:35:42.703417 [debug] [MainThread]: On master: BEGIN
[0m21:35:42.703572 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:35:42.709217 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:35:42.709448 [debug] [MainThread]: On master: COMMIT
[0m21:35:42.709619 [debug] [MainThread]: Using duckdb connection "master"
[0m21:35:42.709772 [debug] [MainThread]: On master: COMMIT
[0m21:35:42.709968 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:35:42.710124 [debug] [MainThread]: On master: Close
[0m21:35:42.711757 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:35:42.711962 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_solution' was properly closed.
[0m21:35:42.712149 [info ] [MainThread]: 
[0m21:35:42.712339 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m21:35:42.712677 [debug] [MainThread]: Command end result
[0m21:35:42.719961 [info ] [MainThread]: 
[0m21:35:42.720220 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:35:42.720385 [info ] [MainThread]: 
[0m21:35:42.720680 [error] [MainThread]:   Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Parser Error: syntax error at or near ";"
[0m21:35:42.720933 [info ] [MainThread]: 
[0m21:35:42.721148 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:35:42.721560 [debug] [MainThread]: Command `dbt run` failed at 21:35:42.721504 after 0.41 seconds
[0m21:35:42.721776 [debug] [MainThread]: Flushing usage events


============================== 21:36:13.863390 | c1d618e7-94b9-48e6-a75a-f1f73eb00789 ==============================
[0m21:36:13.863390 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:36:13.866783 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:36:13.867035 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:36:13.950424 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:36:13.968133 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:36:14.004643 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:36:14.005137 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_solution.sql
[0m21:36:14.024253 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_solution (models/dimensions/dim_solution.sql)
  expected token ',', got 'the'
    line 3
      schema='dim'  -- Ensures the model is written to the 'dim' schema
[0m21:36:14.024770 [debug] [MainThread]: Command `dbt run` failed at 21:36:14.024704 after 0.18 seconds
[0m21:36:14.024990 [debug] [MainThread]: Flushing usage events


============================== 21:36:23.056459 | 05372af6-6191-4ac1-9a4e-7d0d86ad4ec7 ==============================
[0m21:36:23.056459 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:36:23.058950 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:36:23.059202 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:36:23.133272 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:36:23.151647 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:36:23.187213 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:36:23.187503 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:36:23.188502 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:36:23.203276 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:36:23.204662 [info ] [MainThread]: 
[0m21:36:23.205109 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:36:23.205834 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:36:23.213496 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:36:23.213786 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:36:23.213961 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:36:23.227175 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.228071 [debug] [ThreadPool]: On list_dbt: Close
[0m21:36:23.230072 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:36:23.230585 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:36:23.233613 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:36:23.233843 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:36:23.234007 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:23.240200 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.240473 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:36:23.240646 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:36:23.240918 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.241515 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:36:23.241697 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:36:23.241849 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:36:23.242079 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.242248 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:36:23.245308 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:36:23.248785 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:36:23.249007 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:36:23.249166 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:23.255257 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.255521 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:36:23.255697 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:36:23.274524 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.275583 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:36:23.278997 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:36:23.279189 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:36:23.281461 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m21:36:23.283836 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:36:23.284016 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:36:23.284166 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:23.290015 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.290267 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:36:23.290437 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:36:23.305654 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.309020 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:36:23.309239 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:36:23.309393 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:36:23.311486 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m21:36:23.313122 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:36:23.313303 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:36:23.313454 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:23.318853 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.319097 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:36:23.319265 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:36:23.336787 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.337468 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:36:23.337686 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:36:23.337841 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:36:23.339938 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m21:36:23.341365 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:36:23.341545 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:36:23.341703 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:23.347044 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.347285 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:36:23.347463 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:36:23.363463 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:36:23.366955 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:36:23.367178 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:36:23.367529 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:36:23.370701 [debug] [MainThread]: Using duckdb connection "master"
[0m21:36:23.370934 [debug] [MainThread]: On master: BEGIN
[0m21:36:23.371109 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:36:23.377304 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:36:23.377564 [debug] [MainThread]: On master: COMMIT
[0m21:36:23.377729 [debug] [MainThread]: Using duckdb connection "master"
[0m21:36:23.377883 [debug] [MainThread]: On master: COMMIT
[0m21:36:23.378089 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:36:23.378251 [debug] [MainThread]: On master: Close
[0m21:36:23.379877 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:36:23.380190 [info ] [MainThread]: 
[0m21:36:23.383820 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m21:36:23.384135 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_solution .................................. [RUN]
[0m21:36:23.384520 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_solution)
[0m21:36:23.384719 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m21:36:23.390262 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.390872 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 21:36:23.384861 => 21:36:23.390762
[0m21:36:23.391082 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m21:36:23.410390 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.410982 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.411188 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m21:36:23.411370 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:36:23.417436 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:36:23.417729 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.417973 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

SELECT
    id AS solution_id,
    solutionnumber AS solution_number,
    solutionname AS solution_name,
    ispublished AS is_published,
    ispublishedinpublickb AS is_published_in_public_kb,
    status AS solution_status,
    isreviewed AS is_reviewed,
    solutionnote AS solution_note,
    caseid AS case_id,
    ownerid AS owner_id,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    timesused AS times_used,
    ishtml AS is_html
FROM
    "dbt"."raw"."solution"
WHERE
    isdeleted = FALSE
ORDER BY
    solution_name
    );
  
  
[0m21:36:23.420008 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:36:23.423832 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.424090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m21:36:23.424453 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:36:23.433765 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m21:36:23.433999 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.434184 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m21:36:23.435034 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:36:23.437911 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m21:36:23.438149 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m21:36:23.438441 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:36:23.439201 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 21:36:23.391220 => 21:36:23.439108
[0m21:36:23.439405 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m21:36:23.462183 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_solution ............................. [[32mOK[0m in 0.08s]
[0m21:36:23.462603 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m21:36:23.463370 [debug] [MainThread]: Using duckdb connection "master"
[0m21:36:23.463570 [debug] [MainThread]: On master: BEGIN
[0m21:36:23.463731 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:36:23.474852 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:36:23.476859 [debug] [MainThread]: On master: COMMIT
[0m21:36:23.477069 [debug] [MainThread]: Using duckdb connection "master"
[0m21:36:23.477243 [debug] [MainThread]: On master: COMMIT
[0m21:36:23.483467 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:36:23.484220 [debug] [MainThread]: On master: Close
[0m21:36:23.492411 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:36:23.492691 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_solution' was properly closed.
[0m21:36:23.492882 [info ] [MainThread]: 
[0m21:36:23.493078 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m21:36:23.493420 [debug] [MainThread]: Command end result
[0m21:36:23.503036 [info ] [MainThread]: 
[0m21:36:23.503337 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:36:23.503528 [info ] [MainThread]: 
[0m21:36:23.503730 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:36:23.504103 [debug] [MainThread]: Command `dbt run` succeeded at 21:36:23.504054 after 0.46 seconds
[0m21:36:23.504301 [debug] [MainThread]: Flushing usage events


============================== 21:38:49.988415 | b7e638c6-95d4-43ea-ac03-8d193bb7829f ==============================
[0m21:38:49.988415 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:38:49.991502 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_product.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:38:49.991801 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:38:50.069853 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:38:50.087579 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:38:50.125532 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:38:50.125818 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:38:50.126772 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:38:50.141216 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:38:50.142670 [info ] [MainThread]: 
[0m21:38:50.143100 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:38:50.143765 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:38:50.151115 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:38:50.151461 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:38:50.151643 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:38:50.159987 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.160977 [debug] [ThreadPool]: On list_dbt: Close
[0m21:38:50.163081 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:38:50.163546 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:38:50.166556 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:50.166810 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:38:50.166969 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:50.173303 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.173567 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:50.173732 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:38:50.173989 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.174495 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:38:50.174665 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:50.174813 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:38:50.175023 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.175180 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:38:50.178351 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_fact)
[0m21:38:50.181930 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:38:50.182151 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:38:50.182305 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:50.188736 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.189032 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:38:50.189212 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:38:50.207657 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.208458 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:38:50.209897 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:38:50.210075 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:38:50.212173 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m21:38:50.213726 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:38:50.213909 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:38:50.214063 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:50.219528 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.219769 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:38:50.219939 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:38:50.235327 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.238741 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:38:50.239008 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:38:50.239184 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:38:50.245315 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m21:38:50.247197 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:38:50.248396 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:38:50.248620 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:50.255338 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.255626 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:38:50.255801 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:38:50.271557 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.275479 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:38:50.275737 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:38:50.275905 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:38:50.278297 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m21:38:50.280093 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:38:50.280278 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:38:50.280426 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:50.286252 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.286507 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:38:50.286678 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:38:50.304318 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:50.305201 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:38:50.305428 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:38:50.305586 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:38:50.308963 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:50.309153 [debug] [MainThread]: On master: BEGIN
[0m21:38:50.309303 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:38:50.315419 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:50.315698 [debug] [MainThread]: On master: COMMIT
[0m21:38:50.315865 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:50.316015 [debug] [MainThread]: On master: COMMIT
[0m21:38:50.316223 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:50.316414 [debug] [MainThread]: On master: Close
[0m21:38:50.318150 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:38:50.318416 [info ] [MainThread]: 
[0m21:38:50.321672 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m21:38:50.321974 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_product ................................... [RUN]
[0m21:38:50.322375 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_product)
[0m21:38:50.322594 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m21:38:50.328440 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m21:38:50.329467 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 21:38:50.322736 => 21:38:50.329266
[0m21:38:50.329781 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m21:38:50.350264 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m21:38:50.350967 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:50.351189 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m21:38:50.351374 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:38:50.357813 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:50.358117 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:50.358356 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

SELECT
    product_id,
    name AS product_name,
    productcode AS product_code,
    description AS product_description,
    isactive AS is_active,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    family AS product_family,
    externaldatasourceid AS external_datasource_id,
    externalid AS external_id,
    displayurl AS display_url,
    quantityunitofmeasure AS quantity_unit_of_measure,
    stockkeepingunit AS stock_keeping_unit,
    type AS product_type,
    productclass AS product_class,
    sourceproductid AS source_product_id,
    sellerid AS seller_id
FROM
    "dbt"."staging"."stg_salesforce__product_2"
WHERE
    isdeleted = FALSE
    --isactive =1 ?
ORDER BY
    product_name;
    );
  
  
[0m21:38:50.358919 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 21:38:50.329949 => 21:38:50.358813
[0m21:38:50.359128 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: ROLLBACK
[0m21:38:50.363022 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_product'
[0m21:38:50.363244 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m21:38:50.365109 [debug] [Thread-1  ]: Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Parser Error: syntax error at or near ";"
[0m21:38:50.365539 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_product .......................... [[31mERROR[0m in 0.04s]
[0m21:38:50.365886 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m21:38:50.366676 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:50.366849 [debug] [MainThread]: On master: BEGIN
[0m21:38:50.366995 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:38:50.372774 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:50.373031 [debug] [MainThread]: On master: COMMIT
[0m21:38:50.373195 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:50.373348 [debug] [MainThread]: On master: COMMIT
[0m21:38:50.373550 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:50.373750 [debug] [MainThread]: On master: Close
[0m21:38:50.375198 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:38:50.375370 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_product' was properly closed.
[0m21:38:50.375547 [info ] [MainThread]: 
[0m21:38:50.375728 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m21:38:50.376057 [debug] [MainThread]: Command end result
[0m21:38:50.383431 [info ] [MainThread]: 
[0m21:38:50.383738 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:38:50.383906 [info ] [MainThread]: 
[0m21:38:50.384242 [error] [MainThread]:   Runtime Error in model dim_product (models/dimensions/dim_product.sql)
  Parser Error: syntax error at or near ";"
[0m21:38:50.384470 [info ] [MainThread]: 
[0m21:38:50.384670 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:38:50.385169 [debug] [MainThread]: Command `dbt run` failed at 21:38:50.385092 after 0.42 seconds
[0m21:38:50.385423 [debug] [MainThread]: Flushing usage events


============================== 21:38:58.329733 | 11f06eb1-aafb-412e-a571-8498f66b0419 ==============================
[0m21:38:58.329733 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:38:58.332615 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_product.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:38:58.332949 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:38:58.414342 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:38:58.432247 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:38:58.467308 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:38:58.467600 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:38:58.468534 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m21:38:58.482916 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:38:58.484558 [info ] [MainThread]: 
[0m21:38:58.485065 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:38:58.485653 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:38:58.494809 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:38:58.496069 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:38:58.496352 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:38:58.530348 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.532678 [debug] [ThreadPool]: On list_dbt: Close
[0m21:38:58.535122 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:38:58.535656 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:38:58.538479 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:58.538673 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:38:58.538827 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:58.545218 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.545492 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:58.545656 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:38:58.545912 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.546427 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:38:58.546591 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:38:58.546737 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:38:58.546945 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.547104 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:38:58.549866 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m21:38:58.553320 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:38:58.553533 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:38:58.553692 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:58.559890 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.560226 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:38:58.560472 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:38:58.576490 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.579994 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:38:58.581393 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:38:58.581602 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:38:58.583845 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m21:38:58.586720 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:38:58.586926 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:38:58.587087 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:58.592768 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.593040 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:38:58.593238 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:38:58.612209 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.613190 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:38:58.613436 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:38:58.613596 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:38:58.616089 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m21:38:58.618478 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:38:58.618661 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:38:58.618808 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:58.624743 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.624994 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:38:58.625164 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:38:58.640324 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.643594 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:38:58.643820 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:38:58.643978 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:38:58.646314 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m21:38:58.648042 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:38:58.648223 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:38:58.648370 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:58.654090 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.654337 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:38:58.654509 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:38:58.672411 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:38:58.673125 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:38:58.673356 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:38:58.673515 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:38:58.675593 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:58.675793 [debug] [MainThread]: On master: BEGIN
[0m21:38:58.675953 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:38:58.681709 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:58.681963 [debug] [MainThread]: On master: COMMIT
[0m21:38:58.682126 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:58.682276 [debug] [MainThread]: On master: COMMIT
[0m21:38:58.682490 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:58.682651 [debug] [MainThread]: On master: Close
[0m21:38:58.684089 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:38:58.684294 [info ] [MainThread]: 
[0m21:38:58.687390 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m21:38:58.687666 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_product ................................... [RUN]
[0m21:38:58.688023 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_product)
[0m21:38:58.688211 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m21:38:58.693812 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m21:38:58.694459 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 21:38:58.688343 => 21:38:58.694349
[0m21:38:58.694680 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m21:38:58.713827 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m21:38:58.714305 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:58.714510 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m21:38:58.714693 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:38:58.720643 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:58.720945 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:58.721187 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

SELECT
    product_id,
    name AS product_name,
    productcode AS product_code,
    description AS product_description,
    isactive AS is_active,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    family AS product_family,
    externaldatasourceid AS external_datasource_id,
    externalid AS external_id,
    displayurl AS display_url,
    quantityunitofmeasure AS quantity_unit_of_measure,
    stockkeepingunit AS stock_keeping_unit,
    type AS product_type,
    productclass AS product_class,
    sourceproductid AS source_product_id,
    sellerid AS seller_id
FROM
    "dbt"."staging"."stg_salesforce__product_2"
WHERE
    isdeleted = FALSE
    --isactive =1 ?
ORDER BY
    product_name
    );
  
  
[0m21:38:58.723166 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:58.727088 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:58.727331 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m21:38:58.727693 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:58.737080 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m21:38:58.737308 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:58.737497 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m21:38:58.738360 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:58.741330 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m21:38:58.741567 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m21:38:58.741852 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:38:58.742598 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 21:38:58.694819 => 21:38:58.742506
[0m21:38:58.742800 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m21:38:58.759210 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_product .............................. [[32mOK[0m in 0.07s]
[0m21:38:58.759621 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m21:38:58.760467 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:58.760671 [debug] [MainThread]: On master: BEGIN
[0m21:38:58.760828 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:38:58.766599 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:58.766833 [debug] [MainThread]: On master: COMMIT
[0m21:38:58.766993 [debug] [MainThread]: Using duckdb connection "master"
[0m21:38:58.767141 [debug] [MainThread]: On master: COMMIT
[0m21:38:58.767336 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:38:58.767732 [debug] [MainThread]: On master: Close
[0m21:38:58.769643 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:38:58.769931 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_product' was properly closed.
[0m21:38:58.770127 [info ] [MainThread]: 
[0m21:38:58.770331 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m21:38:58.770696 [debug] [MainThread]: Command end result
[0m21:38:58.778596 [info ] [MainThread]: 
[0m21:38:58.778876 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:38:58.779074 [info ] [MainThread]: 
[0m21:38:58.779268 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:38:58.779656 [debug] [MainThread]: Command `dbt run` succeeded at 21:38:58.779605 after 0.47 seconds
[0m21:38:58.779866 [debug] [MainThread]: Flushing usage events


============================== 21:40:11.828890 | 528fe3d3-49e3-4538-a7ee-5dfb34aa9dfe ==============================
[0m21:40:11.828890 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:40:11.831932 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_pricebook.sql', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m21:40:11.832192 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:40:11.915443 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:40:11.933690 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:40:11.975229 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:40:11.975527 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:40:11.976549 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m21:40:11.990723 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:40:11.992199 [info ] [MainThread]: 
[0m21:40:11.992660 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:40:11.993394 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:40:12.001099 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:40:12.001401 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:40:12.001586 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:40:12.015381 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.016263 [debug] [ThreadPool]: On list_dbt: Close
[0m21:40:12.018252 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:40:12.018781 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:40:12.021770 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:12.021980 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:40:12.022143 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:12.028242 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.028519 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:12.028690 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:40:12.028948 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.029478 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:40:12.029650 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:12.029799 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:40:12.030075 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.030259 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:40:12.033365 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m21:40:12.036772 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:40:12.037016 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:40:12.037177 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:12.043437 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.043721 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:40:12.043901 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:40:12.059725 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.063292 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:40:12.063985 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:40:12.064186 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:40:12.066382 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:40:12.068895 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:40:12.069112 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:40:12.069275 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:12.075286 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.075544 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:40:12.075717 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:40:12.095313 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.096135 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:40:12.096373 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:40:12.096537 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:40:12.098959 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m21:40:12.100633 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:40:12.100850 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:40:12.101019 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:12.108842 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.109191 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:40:12.109391 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:40:12.128505 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.132769 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:40:12.133126 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:40:12.133305 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:40:12.135923 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m21:40:12.137716 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:40:12.138340 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:40:12.138585 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:12.144820 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.145120 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:40:12.145321 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:40:12.164902 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:12.166038 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:40:12.166348 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:40:12.166516 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:40:12.169742 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:12.169948 [debug] [MainThread]: On master: BEGIN
[0m21:40:12.170109 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:40:12.176284 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:12.176556 [debug] [MainThread]: On master: COMMIT
[0m21:40:12.176719 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:12.176884 [debug] [MainThread]: On master: COMMIT
[0m21:40:12.177094 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:12.177256 [debug] [MainThread]: On master: Close
[0m21:40:12.179006 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:40:12.179328 [info ] [MainThread]: 
[0m21:40:12.182215 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m21:40:12.182552 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_pricebook ................................. [RUN]
[0m21:40:12.182988 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_pricebook)
[0m21:40:12.183201 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m21:40:12.188803 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:12.189887 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 21:40:12.183343 => 21:40:12.189691
[0m21:40:12.190173 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m21:40:12.209324 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:12.210412 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:12.210686 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m21:40:12.210873 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:40:12.216869 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:40:12.217145 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:12.217368 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

SELECT
    pricebook_entry_id,
    pricebook2id AS pricebook_id,
    product2id AS product_id,
    unitprice AS unit_price,
    isactive AS is_active,
    usestandardprice AS use_standard_price,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."raw"."pricebook_entry"
WHERE
    isdeleted = FALSE  -- Exclude deleted entries
ORDER BY
    pricebook_entry_id
    );
  
  
[0m21:40:12.218070 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 21:40:12.190325 => 21:40:12.217965
[0m21:40:12.218294 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: ROLLBACK
[0m21:40:12.221990 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_pricebook'
[0m21:40:12.222237 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m21:40:12.224116 [debug] [Thread-1  ]: Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "pricebook_entry_id" not found in FROM clause!
  Candidate bindings: "pricebook_entry.pricebook2id"
  LINE 14:     pricebook_entry_id,
               ^
[0m21:40:12.224520 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_pricebook ........................ [[31mERROR[0m in 0.04s]
[0m21:40:12.224855 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m21:40:12.225621 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:12.225830 [debug] [MainThread]: On master: BEGIN
[0m21:40:12.225993 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:40:12.232103 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:12.232319 [debug] [MainThread]: On master: COMMIT
[0m21:40:12.232481 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:12.232642 [debug] [MainThread]: On master: COMMIT
[0m21:40:12.232889 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:12.233054 [debug] [MainThread]: On master: Close
[0m21:40:12.234650 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:40:12.234816 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_pricebook' was properly closed.
[0m21:40:12.234991 [info ] [MainThread]: 
[0m21:40:12.235176 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m21:40:12.235518 [debug] [MainThread]: Command end result
[0m21:40:12.243013 [info ] [MainThread]: 
[0m21:40:12.243287 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:40:12.243642 [info ] [MainThread]: 
[0m21:40:12.243829 [error] [MainThread]:   Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "pricebook_entry_id" not found in FROM clause!
  Candidate bindings: "pricebook_entry.pricebook2id"
  LINE 14:     pricebook_entry_id,
               ^
[0m21:40:12.244000 [info ] [MainThread]: 
[0m21:40:12.244182 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:40:12.244544 [debug] [MainThread]: Command `dbt run` failed at 21:40:12.244497 after 0.44 seconds
[0m21:40:12.244744 [debug] [MainThread]: Flushing usage events


============================== 21:40:41.873729 | bbb33c79-bb0b-46da-9890-82a54caa1eaa ==============================
[0m21:40:41.873729 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:40:41.876792 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_pricebook.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:40:41.877083 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:40:41.954443 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:40:41.972279 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:40:42.007500 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:40:42.007812 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:40:42.008755 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m21:40:42.024491 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:40:42.025931 [info ] [MainThread]: 
[0m21:40:42.026364 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:40:42.026909 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:40:42.033732 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:40:42.033963 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:40:42.034140 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:40:42.047530 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.048594 [debug] [ThreadPool]: On list_dbt: Close
[0m21:40:42.050587 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:40:42.051047 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:40:42.054071 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:42.054284 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:40:42.054442 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:42.060738 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.061007 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:42.061174 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:40:42.061427 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.061981 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:40:42.062152 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:40:42.062313 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:40:42.062519 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.062684 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:40:42.065443 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:40:42.068801 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:40:42.069017 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:40:42.069173 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:42.075042 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.075286 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:40:42.075467 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:40:42.093541 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.094507 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:40:42.095189 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:40:42.095372 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:40:42.097583 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m21:40:42.100158 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:40:42.100371 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:40:42.100526 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:42.106648 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.106918 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:40:42.107097 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:40:42.123059 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.126637 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:40:42.126931 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:40:42.127097 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:40:42.129521 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:40:42.131329 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:40:42.132534 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:40:42.132984 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:42.139186 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.139475 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:40:42.139656 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:40:42.155237 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.158762 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:40:42.159016 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:40:42.159193 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:40:42.161392 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:40:42.163819 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:40:42.164001 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:40:42.164152 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:40:42.169926 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.170180 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:40:42.170352 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:40:42.187976 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:40:42.189500 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:40:42.189749 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:40:42.189906 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:40:42.192356 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:42.192614 [debug] [MainThread]: On master: BEGIN
[0m21:40:42.192788 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:40:42.198384 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:42.198642 [debug] [MainThread]: On master: COMMIT
[0m21:40:42.198810 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:42.198960 [debug] [MainThread]: On master: COMMIT
[0m21:40:42.199169 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:42.199328 [debug] [MainThread]: On master: Close
[0m21:40:42.200907 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:40:42.201153 [info ] [MainThread]: 
[0m21:40:42.202590 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m21:40:42.202916 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_pricebook ................................. [RUN]
[0m21:40:42.203313 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_pricebook)
[0m21:40:42.203511 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m21:40:42.209105 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:42.209578 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 21:40:42.203650 => 21:40:42.209474
[0m21:40:42.209775 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m21:40:42.228883 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:42.229564 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:42.229770 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m21:40:42.229949 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:40:42.235892 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:40:42.238323 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:40:42.238553 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

SELECT
    pricebook_entry_id,
    pricebook2id AS pricebook_id,
    product2id AS product_id,
    unitprice AS unit_price,
    isactive AS is_active,
    usestandardprice AS use_standard_price,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."raw"."pricebook_entry"
WHERE
    isdeleted = FALSE  -- Exclude deleted entries
ORDER BY
    pricebook_entry_id
    );
  
  
[0m21:40:42.239291 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 21:40:42.209905 => 21:40:42.239180
[0m21:40:42.239509 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: ROLLBACK
[0m21:40:42.245904 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_pricebook'
[0m21:40:42.246130 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m21:40:42.248046 [debug] [Thread-1  ]: Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "pricebook_entry_id" not found in FROM clause!
  Candidate bindings: "pricebook_entry.pricebook2id"
  LINE 14:     pricebook_entry_id,
               ^
[0m21:40:42.248484 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_pricebook ........................ [[31mERROR[0m in 0.05s]
[0m21:40:42.248842 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m21:40:42.249598 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:42.250009 [debug] [MainThread]: On master: BEGIN
[0m21:40:42.250167 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:40:42.255958 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:42.256221 [debug] [MainThread]: On master: COMMIT
[0m21:40:42.256392 [debug] [MainThread]: Using duckdb connection "master"
[0m21:40:42.256549 [debug] [MainThread]: On master: COMMIT
[0m21:40:42.257086 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:40:42.257427 [debug] [MainThread]: On master: Close
[0m21:40:42.259169 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:40:42.259360 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_pricebook' was properly closed.
[0m21:40:42.259539 [info ] [MainThread]: 
[0m21:40:42.259734 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m21:40:42.260072 [debug] [MainThread]: Command end result
[0m21:40:42.267619 [info ] [MainThread]: 
[0m21:40:42.267911 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:40:42.268077 [info ] [MainThread]: 
[0m21:40:42.268244 [error] [MainThread]:   Runtime Error in model dim_pricebook (models/dimensions/dim_pricebook.sql)
  Binder Error: Referenced column "pricebook_entry_id" not found in FROM clause!
  Candidate bindings: "pricebook_entry.pricebook2id"
  LINE 14:     pricebook_entry_id,
               ^
[0m21:40:42.268415 [info ] [MainThread]: 
[0m21:40:42.268600 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:40:42.268988 [debug] [MainThread]: Command `dbt run` failed at 21:40:42.268931 after 0.42 seconds
[0m21:40:42.269199 [debug] [MainThread]: Flushing usage events


============================== 21:41:03.599502 | 87100356-2f83-4fdb-8b48-44e74aa40c15 ==============================
[0m21:41:03.599502 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:41:03.602366 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_pricebook.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:41:03.602653 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:41:03.679645 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:41:03.698513 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:41:03.733574 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:41:03.734037 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_pricebook.sql
[0m21:41:03.763528 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:41:03.777926 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:41:03.779417 [info ] [MainThread]: 
[0m21:41:03.779869 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:41:03.780462 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:41:03.787681 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:41:03.787970 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:41:03.788168 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:41:03.801548 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.802669 [debug] [ThreadPool]: On list_dbt: Close
[0m21:41:03.804895 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:41:03.805417 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:41:03.808461 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:41:03.808744 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:41:03.809006 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:03.815021 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.815313 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:41:03.815487 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:41:03.815744 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.816272 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:41:03.816447 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:41:03.816599 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:41:03.816820 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.816981 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:41:03.820452 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:41:03.824095 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:41:03.824388 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:41:03.824586 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:03.831105 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.831367 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:41:03.831554 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:41:03.850098 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.851260 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:41:03.851978 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:41:03.852278 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:41:03.854874 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m21:41:03.857814 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:41:03.858215 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:41:03.858456 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:03.864340 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.864630 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:41:03.864823 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:41:03.888168 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.891948 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:41:03.892320 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:41:03.892535 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:41:03.894955 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m21:41:03.897662 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:41:03.897912 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:41:03.898076 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:03.904984 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.905316 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:41:03.905517 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:41:03.924121 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.927886 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:41:03.928239 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:41:03.928420 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:41:03.930871 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:41:03.933285 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:41:03.933483 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:41:03.933641 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:03.939817 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.940047 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:41:03.940227 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:41:03.958161 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:41:03.958963 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:41:03.959325 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:41:03.959502 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:41:03.961746 [debug] [MainThread]: Using duckdb connection "master"
[0m21:41:03.962026 [debug] [MainThread]: On master: BEGIN
[0m21:41:03.962197 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:41:03.967939 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:41:03.968208 [debug] [MainThread]: On master: COMMIT
[0m21:41:03.968375 [debug] [MainThread]: Using duckdb connection "master"
[0m21:41:03.968528 [debug] [MainThread]: On master: COMMIT
[0m21:41:03.968724 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:41:03.968881 [debug] [MainThread]: On master: Close
[0m21:41:03.970356 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:41:03.970574 [info ] [MainThread]: 
[0m21:41:03.972993 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m21:41:03.973303 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_pricebook ................................. [RUN]
[0m21:41:03.973672 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_pricebook)
[0m21:41:03.973871 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m21:41:03.979134 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:03.979649 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 21:41:03.974013 => 21:41:03.979544
[0m21:41:03.979847 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m21:41:03.998441 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:03.998930 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:03.999134 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m21:41:03.999314 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:41:04.005208 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:41:04.005598 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:04.005830 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

SELECT
    pricebook_entry_id,
    pricebook2id AS pricebook_id,
    product2id AS product_id,
    unitprice AS unit_price,
    isactive AS is_active,
    usestandardprice AS use_standard_price,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp
FROM
    "dbt"."staging"."stg_salesforce__pricebook_entry"
WHERE
    isdeleted = FALSE  -- Exclude deleted entries
ORDER BY
    pricebook_entry_id
    );
  
  
[0m21:41:04.007727 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:41:04.012317 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:04.012590 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m21:41:04.013008 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:41:04.051715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m21:41:04.052096 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:04.052305 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m21:41:04.053285 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:41:04.057324 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m21:41:04.057645 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m21:41:04.058021 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:41:04.058878 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 21:41:03.979984 => 21:41:04.058780
[0m21:41:04.059102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m21:41:04.081501 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_pricebook ............................ [[32mOK[0m in 0.11s]
[0m21:41:04.081922 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m21:41:04.082738 [debug] [MainThread]: Using duckdb connection "master"
[0m21:41:04.083016 [debug] [MainThread]: On master: BEGIN
[0m21:41:04.083187 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:41:04.089925 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:41:04.090268 [debug] [MainThread]: On master: COMMIT
[0m21:41:04.090440 [debug] [MainThread]: Using duckdb connection "master"
[0m21:41:04.090593 [debug] [MainThread]: On master: COMMIT
[0m21:41:04.090809 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:41:04.090984 [debug] [MainThread]: On master: Close
[0m21:41:04.092878 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:41:04.093196 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_pricebook' was properly closed.
[0m21:41:04.093403 [info ] [MainThread]: 
[0m21:41:04.093653 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.31 seconds (0.31s).
[0m21:41:04.094006 [debug] [MainThread]: Command end result
[0m21:41:04.101547 [info ] [MainThread]: 
[0m21:41:04.101932 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:41:04.102131 [info ] [MainThread]: 
[0m21:41:04.102309 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:41:04.102628 [debug] [MainThread]: Command `dbt run` succeeded at 21:41:04.102580 after 0.53 seconds
[0m21:41:04.102819 [debug] [MainThread]: Flushing usage events


============================== 21:52:42.314340 | e33c42b5-3b06-4985-8cfd-e79c1f9e99f8 ==============================
[0m21:52:42.314340 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:52:42.317549 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity.sql', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:52:42.317814 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:52:42.402661 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:52:42.423050 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:52:42.469973 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:52:42.470491 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_opportunity.sql
[0m21:52:42.489272 [error] [MainThread]: Encountered an error:
Compilation Error in model dim_opportunity (models/dimensions/dim_opportunity.sql)
  expected token 'end of print statement', got 'WHERE'
    line 50
      WHERE
[0m21:52:42.489795 [debug] [MainThread]: Command `dbt run` failed at 21:52:42.489715 after 0.20 seconds
[0m21:52:42.490025 [debug] [MainThread]: Flushing usage events


============================== 21:52:55.993051 | ce54ddaf-9857-46c1-bd72-bdb1e3c2ba3e ==============================
[0m21:52:55.993051 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:52:55.995952 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:52:55.996244 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:52:56.073773 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:52:56.091859 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:52:56.128590 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:52:56.128903 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:52:56.129893 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m21:52:56.146952 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:52:56.148544 [info ] [MainThread]: 
[0m21:52:56.149013 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:52:56.149614 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:52:56.157296 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:52:56.157626 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:52:56.157804 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:52:56.173651 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.174852 [debug] [ThreadPool]: On list_dbt: Close
[0m21:52:56.176877 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:52:56.177265 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:52:56.180165 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:52:56.180369 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:52:56.180523 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:56.186914 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.187210 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:52:56.187381 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:52:56.187623 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.188153 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:52:56.188324 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:52:56.188473 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:52:56.188687 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.188847 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:52:56.192058 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m21:52:56.195792 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:52:56.196082 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:52:56.196261 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:56.202466 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.202739 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:52:56.202908 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:52:56.218619 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.222195 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:52:56.224618 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:52:56.224826 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:52:56.227125 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m21:52:56.229009 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:52:56.229191 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:52:56.229351 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:56.235007 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.235244 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:52:56.235421 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:52:56.252965 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.253683 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:52:56.253903 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:52:56.254059 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:52:56.256421 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m21:52:56.258055 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:52:56.258252 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:52:56.258411 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:56.264685 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.264876 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:52:56.265046 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:52:56.282637 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.283605 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:52:56.283961 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:52:56.284144 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:52:56.286722 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:52:56.288256 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:52:56.289537 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:52:56.289759 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:56.297639 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.297909 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:52:56.298093 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:52:56.314413 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:52:56.318072 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:52:56.318430 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:52:56.318594 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:52:56.322564 [debug] [MainThread]: Using duckdb connection "master"
[0m21:52:56.322766 [debug] [MainThread]: On master: BEGIN
[0m21:52:56.322925 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:52:56.329150 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:52:56.329411 [debug] [MainThread]: On master: COMMIT
[0m21:52:56.329575 [debug] [MainThread]: Using duckdb connection "master"
[0m21:52:56.329724 [debug] [MainThread]: On master: COMMIT
[0m21:52:56.329927 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:52:56.330085 [debug] [MainThread]: On master: Close
[0m21:52:56.331652 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:52:56.331888 [info ] [MainThread]: 
[0m21:52:56.336242 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m21:52:56.336591 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_opportunity ............................... [RUN]
[0m21:52:56.337076 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_opportunity)
[0m21:52:56.337343 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m21:52:56.342637 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.344260 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 21:52:56.337488 => 21:52:56.344086
[0m21:52:56.344522 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m21:52:56.363577 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.364256 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.364473 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m21:52:56.364657 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:52:56.370749 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:52:56.371020 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.371267 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

SELECT
    opportunity_id,
    accountid AS account_id,
    isprivate AS is_private,
    name AS opportunity_name,
    description AS opportunity_description,
    stagename AS stage_name,
    stagesortorder AS stage_sort_order,
    amount AS opportunity_amount,
    probability AS opportunity_probability,
    expectedrevenue AS expected_revenue,
    totalopportunityquantity AS total_opportunity_quantity,
    closedate AS close_date,
    type AS opportunity_type,
    nextstep AS next_step,
    leadsource AS lead_source,
    isclosed AS is_closed,
    iswon AS is_won,
    forecastcategory AS forecast_category,
    forecastcategoryname AS forecast_category_name,
    campaignid AS campaign_id,
    hasopportunitylineitem AS has_opportunity_line_item,
    pricebook2id AS pricebook_id,
    ownerid AS owner_id,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    lastactivitydate AS last_activity_date,
    laststagechangedate AS last_stage_change_date,
    fiscalyear AS fiscal_year,
    fiscalquarter AS fiscal_quarter,
    contactid AS contact_id,
    primarypartneraccountid AS primary_partner_account_id,
    contractid AS contract_id,
    lastamountchangedhistoryid AS last_amount_changed_history_id,
    lastclosedatechangedhistoryid AS last_close_date_changed_history_id,
    deliveryinstallationstatus__c AS delivery_installation_status,
    trackingnumber__c AS tracking_number,
    ordernumber__c AS order_number,
    currentgenerators__c AS current_generators,
    maincompetitors__c AS main_competitors
FROM
     "dbt"."staging"."stg_salesforce__opportunity"
WHERE
    isdeleted = FALSE
ORDER BY
    opportunity_name
    );
  
  
[0m21:52:56.375107 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:52:56.379406 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.379723 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m21:52:56.380192 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:52:56.390431 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m21:52:56.390826 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.391130 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m21:52:56.392525 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:52:56.396363 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m21:52:56.396665 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m21:52:56.396962 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:52:56.397747 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 21:52:56.344663 => 21:52:56.397647
[0m21:52:56.397946 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m21:52:56.431704 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_opportunity .......................... [[32mOK[0m in 0.09s]
[0m21:52:56.432301 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m21:52:56.433224 [debug] [MainThread]: Using duckdb connection "master"
[0m21:52:56.433411 [debug] [MainThread]: On master: BEGIN
[0m21:52:56.433565 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:52:56.439581 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:52:56.439868 [debug] [MainThread]: On master: COMMIT
[0m21:52:56.440039 [debug] [MainThread]: Using duckdb connection "master"
[0m21:52:56.440190 [debug] [MainThread]: On master: COMMIT
[0m21:52:56.440397 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:52:56.440555 [debug] [MainThread]: On master: Close
[0m21:52:56.442341 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:52:56.442542 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_opportunity' was properly closed.
[0m21:52:56.442729 [info ] [MainThread]: 
[0m21:52:56.442916 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m21:52:56.443257 [debug] [MainThread]: Command end result
[0m21:52:56.450321 [info ] [MainThread]: 
[0m21:52:56.450581 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:52:56.450752 [info ] [MainThread]: 
[0m21:52:56.450945 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:52:56.451318 [debug] [MainThread]: Command `dbt run` succeeded at 21:52:56.451272 after 0.48 seconds
[0m21:52:56.451529 [debug] [MainThread]: Flushing usage events


============================== 21:53:20.674395 | f3d21fdd-7fcb-4732-843b-11aed2d56e6d ==============================
[0m21:53:20.674395 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:53:20.677795 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity_sstage.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m21:53:20.678087 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:53:20.795534 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:53:20.821011 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:53:20.895734 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:53:20.896030 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:53:20.896973 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:53:20.937344 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:53:20.938373 [warn ] [MainThread]: The selection criterion 'models/dimensions/dim_opportunity_sstage.sql' does not match any nodes
[0m21:53:20.939098 [info ] [MainThread]: 
[0m21:53:20.939294 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:53:20.939557 [debug] [MainThread]: Command end result
[0m21:53:20.946603 [debug] [MainThread]: Command `dbt run` succeeded at 21:53:20.946509 after 0.30 seconds
[0m21:53:20.946861 [debug] [MainThread]: Flushing usage events


============================== 21:53:44.437558 | 058a3c58-ac54-4b2c-9b23-05ccc49124bc ==============================
[0m21:53:44.437558 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:53:44.440572 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity_stage.sql', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m21:53:44.440833 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:53:44.525440 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:53:44.545223 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:53:44.592840 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:53:44.593129 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:53:44.594101 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m21:53:44.608577 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:53:44.610223 [info ] [MainThread]: 
[0m21:53:44.610704 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:53:44.611306 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:53:44.618881 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:53:44.619174 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:53:44.619351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:53:44.633727 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.634831 [debug] [ThreadPool]: On list_dbt: Close
[0m21:53:44.636845 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:53:44.637299 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:53:44.640299 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:53:44.640504 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:53:44.640663 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:44.647018 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.647304 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:53:44.647489 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:53:44.647730 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.648249 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:53:44.648415 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:53:44.648562 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:53:44.648772 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.648935 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:53:44.652071 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m21:53:44.655823 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:53:44.656056 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:53:44.656274 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:44.662081 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.662332 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:53:44.662524 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:53:44.678509 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.682228 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:53:44.685430 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:53:44.685613 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:53:44.687833 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m21:53:44.690275 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:53:44.690476 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:53:44.690625 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:44.696823 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.697089 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:53:44.697258 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:53:44.712586 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.716541 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:53:44.716908 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:53:44.717096 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:53:44.719359 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m21:53:44.721328 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:53:44.721529 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:53:44.721679 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:44.728093 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.728372 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:53:44.728553 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:53:44.747843 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.748754 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:53:44.748993 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:53:44.749154 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:53:44.751932 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m21:53:44.753679 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:53:44.753905 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:53:44.754059 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:44.760223 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.760492 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:53:44.760664 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:53:44.778381 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:53:44.779283 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:53:44.779512 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:53:44.779672 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:53:44.782846 [debug] [MainThread]: Using duckdb connection "master"
[0m21:53:44.783038 [debug] [MainThread]: On master: BEGIN
[0m21:53:44.783188 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:53:44.788976 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:53:44.789227 [debug] [MainThread]: On master: COMMIT
[0m21:53:44.789387 [debug] [MainThread]: Using duckdb connection "master"
[0m21:53:44.789533 [debug] [MainThread]: On master: COMMIT
[0m21:53:44.789772 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:53:44.789956 [debug] [MainThread]: On master: Close
[0m21:53:44.791440 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:53:44.791690 [info ] [MainThread]: 
[0m21:53:44.794954 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m21:53:44.795237 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_opportunity_stage ......................... [RUN]
[0m21:53:44.795607 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m21:53:44.795801 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m21:53:44.801482 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.802149 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 21:53:44.795937 => 21:53:44.802025
[0m21:53:44.802355 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m21:53:44.821353 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.822022 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.822227 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m21:53:44.822419 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:53:44.828256 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:53:44.828549 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.828763 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT DISTINCT
        stagename AS stage_name,
        stagesortorder AS stage_sort_order
    FROM "dbt"."staging"."stg_salesforce__opportunity"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY stage_sort_order) AS stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
FROM source
    );
  
  
[0m21:53:44.831447 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:53:44.835392 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.835647 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m21:53:44.836042 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:53:44.845489 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m21:53:44.845722 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.845906 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m21:53:44.846551 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:53:44.849410 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m21:53:44.849632 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m21:53:44.849911 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:53:44.850654 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 21:53:44.802491 => 21:53:44.850559
[0m21:53:44.850864 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m21:53:44.864412 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_opportunity_stage .................... [[32mOK[0m in 0.07s]
[0m21:53:44.864836 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m21:53:44.865717 [debug] [MainThread]: Using duckdb connection "master"
[0m21:53:44.865979 [debug] [MainThread]: On master: BEGIN
[0m21:53:44.866149 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:53:44.872533 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:53:44.872844 [debug] [MainThread]: On master: COMMIT
[0m21:53:44.873023 [debug] [MainThread]: Using duckdb connection "master"
[0m21:53:44.873188 [debug] [MainThread]: On master: COMMIT
[0m21:53:44.873395 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:53:44.873557 [debug] [MainThread]: On master: Close
[0m21:53:44.875061 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:53:44.875260 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_opportunity_stage' was properly closed.
[0m21:53:44.875659 [info ] [MainThread]: 
[0m21:53:44.875948 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m21:53:44.876404 [debug] [MainThread]: Command end result
[0m21:53:44.885224 [info ] [MainThread]: 
[0m21:53:44.885567 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:53:44.885757 [info ] [MainThread]: 
[0m21:53:44.885962 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:53:44.886371 [debug] [MainThread]: Command `dbt run` succeeded at 21:53:44.886316 after 0.48 seconds
[0m21:53:44.886594 [debug] [MainThread]: Flushing usage events


============================== 21:56:57.313289 | 0c785e82-2234-4ef9-882e-2d927414686b ==============================
[0m21:56:57.313289 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:56:57.316386 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/dimensions/dim_lead.sql', 'send_anonymous_usage_stats': 'False'}
[0m21:56:57.316639 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:56:57.399851 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:56:57.418228 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:56:57.455659 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:56:57.455974 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:56:57.456977 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m21:56:57.472214 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:56:57.473998 [info ] [MainThread]: 
[0m21:56:57.474485 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:56:57.475165 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:56:57.483452 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:56:57.483817 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:56:57.484000 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:56:57.490297 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:56:57.490544 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m21:56:57.490721 [info ] [MainThread]: 
[0m21:56:57.490922 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m21:56:57.491209 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 29952) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m21:56:57.491634 [debug] [MainThread]: Command `dbt run` failed at 21:56:57.491581 after 0.20 seconds
[0m21:56:57.491842 [debug] [MainThread]: Flushing usage events


============================== 21:57:08.821139 | 1ff3fc2d-702d-4e0d-8931-990d14eb953a ==============================
[0m21:57:08.821139 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:57:08.824157 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_lead.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:57:08.824442 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:57:08.901052 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:57:08.921358 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:57:08.971660 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:57:08.971967 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:57:08.972954 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m21:57:08.986958 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:57:08.988348 [info ] [MainThread]: 
[0m21:57:08.988766 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:57:08.989303 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:57:08.997020 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:57:08.997369 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:57:08.997558 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:57:09.003534 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:57:09.003792 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m21:57:09.003978 [info ] [MainThread]: 
[0m21:57:09.004355 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m21:57:09.004683 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 29952) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m21:57:09.005061 [debug] [MainThread]: Command `dbt run` failed at 21:57:09.005014 after 0.21 seconds
[0m21:57:09.005251 [debug] [MainThread]: Flushing usage events


============================== 21:57:25.224394 | 926796f0-75e8-4bf8-afb6-e3e92489db5e ==============================
[0m21:57:25.224394 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:57:25.236392 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_lead.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m21:57:25.237144 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:57:25.367917 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:57:25.387592 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:57:25.461206 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:57:25.461496 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:57:25.462453 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m21:57:25.477086 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:57:25.478571 [info ] [MainThread]: 
[0m21:57:25.479031 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:57:25.479690 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:57:25.487311 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:57:25.487651 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:57:25.488226 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:57:25.500389 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.501700 [debug] [ThreadPool]: On list_dbt: Close
[0m21:57:25.509003 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:57:25.511703 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:57:25.524521 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:25.526772 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:57:25.527069 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:25.540296 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.540601 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:25.540774 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:57:25.541008 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.541521 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:57:25.541689 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:25.541835 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:57:25.542036 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.542195 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:57:25.545230 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m21:57:25.549070 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:57:25.549378 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:57:25.549543 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:25.556011 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.556344 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:57:25.556518 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:57:25.573463 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.577618 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:57:25.578408 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:57:25.578692 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:57:25.581219 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m21:57:25.583746 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:57:25.583977 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:57:25.584150 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:25.591180 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.591470 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:57:25.591646 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:57:25.610010 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.611159 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:57:25.611519 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:57:25.611699 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:57:25.614193 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m21:57:25.616662 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:57:25.616865 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:57:25.617019 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:25.623387 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.623637 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:57:25.623825 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:57:25.642315 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.643086 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:57:25.643316 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:57:25.643471 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:57:25.646192 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m21:57:25.647889 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:57:25.648092 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:57:25.648250 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:25.654573 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.654764 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:57:25.654927 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:57:25.670080 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:25.673397 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:57:25.673614 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:57:25.673784 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:57:25.676418 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:25.676600 [debug] [MainThread]: On master: BEGIN
[0m21:57:25.676746 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:57:25.682875 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:25.683136 [debug] [MainThread]: On master: COMMIT
[0m21:57:25.683297 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:25.683445 [debug] [MainThread]: On master: COMMIT
[0m21:57:25.683650 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:25.683807 [debug] [MainThread]: On master: Close
[0m21:57:25.685347 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:57:25.685552 [info ] [MainThread]: 
[0m21:57:25.689169 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m21:57:25.689544 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_lead ...................................... [RUN]
[0m21:57:25.689943 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_lead)
[0m21:57:25.690140 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m21:57:25.695958 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.697635 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 21:57:25.690271 => 21:57:25.697449
[0m21:57:25.697919 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m21:57:25.717312 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.718155 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.718381 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m21:57:25.718596 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:25.724933 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:25.725226 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.725492 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

SELECT
    lead_id,
    masterrecordid AS master_record_id,
    salutation AS lead_salutation,
    firstname AS first_name,
    lastname AS last_name,
    title AS lead_title,
    company AS lead_company,
    street AS lead_street,
    city AS lead_city,
    state AS lead_state,
    postalcode AS lead_postal_code,
    country AS lead_country,
    latitude AS lead_latitude,
    longitude AS lead_longitude,
    geocodeaccuracy AS geocode_accuracy,
    phone AS lead_phone,
    mobilephone AS lead_mobile_phone,
    fax AS lead_fax,
    email AS lead_email,
    website AS lead_website,
    description AS lead_description,
    leadsource AS lead_source,
    status AS lead_status,
    industry AS lead_industry,
    rating AS lead_rating,
    annualrevenue AS annual_revenue,
    numberofemployees AS number_of_employees,
    ownerid AS owner_id,
    hasoptedoutofemail AS has_opted_out_of_email,
    isconverted AS is_converted,
    converteddate AS converted_date,
    convertedaccountid AS converted_account_id,
    convertedcontactid AS converted_contact_id,
    convertedopportunityid AS converted_opportunity_id,
    isunreadbyowner AS is_unread_by_owner,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    donotcall AS do_not_call,
    hasoptedoutoffax AS has_opted_out_of_fax,
    lasttransferdate AS last_transfer_date,
    jigsaw AS jigsaw_id,
    jigsawcontactid AS jigsaw_contact_id,
    cleanstatus AS clean_status,
    companydunsnumber AS company_duns_number,
    dandbcompanyid AS dandb_company_id,
    emailbouncedreason AS email_bounced_reason,
    emailbounceddate AS email_bounced_date,
    individualid AS individual_id,
    pronouns AS lead_pronouns,
    genderidentity AS gender_identity,
    siccode__c AS sic_code,
    productinterest__c AS product_interest,
    primary__c AS is_primary,
    currentgenerators__c AS current_generators,
    numberoflocations__c AS number_of_locations
FROM
    "dbt"."staging"."stg_salesforce__lead"
WHERE
    isdeleted = FALSE  -- Exclude deleted leads
ORDER BY
    last_name, first_name
    );
  
  
[0m21:57:25.730108 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:25.734252 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.734554 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m21:57:25.734932 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:25.745050 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m21:57:25.745366 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.745562 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m21:57:25.747306 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:25.750399 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m21:57:25.750622 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m21:57:25.750927 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:25.751751 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 21:57:25.698089 => 21:57:25.751648
[0m21:57:25.751960 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m21:57:25.783079 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_lead ................................. [[32mOK[0m in 0.09s]
[0m21:57:25.783513 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m21:57:25.784310 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:25.784531 [debug] [MainThread]: On master: BEGIN
[0m21:57:25.784708 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:57:25.791101 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:25.791386 [debug] [MainThread]: On master: COMMIT
[0m21:57:25.791551 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:25.791704 [debug] [MainThread]: On master: COMMIT
[0m21:57:25.791909 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:25.792077 [debug] [MainThread]: On master: Close
[0m21:57:25.793904 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:57:25.794177 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_lead' was properly closed.
[0m21:57:25.794379 [info ] [MainThread]: 
[0m21:57:25.794591 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m21:57:25.794965 [debug] [MainThread]: Command end result
[0m21:57:25.803211 [info ] [MainThread]: 
[0m21:57:25.803503 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:57:25.803673 [info ] [MainThread]: 
[0m21:57:25.803860 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:57:25.804261 [debug] [MainThread]: Command `dbt run` succeeded at 21:57:25.804205 after 0.61 seconds
[0m21:57:25.804464 [debug] [MainThread]: Flushing usage events


============================== 21:57:42.577042 | 21e5a38d-08de-48c1-a030-0e03878b5007 ==============================
[0m21:57:42.577042 [info ] [MainThread]: Running with dbt=1.6.18
[0m21:57:42.579434 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_contact.sql', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m21:57:42.579707 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m21:57:42.656063 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m21:57:42.674410 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m21:57:42.711681 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:57:42.711977 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:57:42.712897 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m21:57:42.726006 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m21:57:42.727421 [info ] [MainThread]: 
[0m21:57:42.727849 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m21:57:42.728570 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m21:57:42.736383 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m21:57:42.736856 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m21:57:42.737096 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:57:42.745435 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.746376 [debug] [ThreadPool]: On list_dbt: Close
[0m21:57:42.748375 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m21:57:42.748881 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m21:57:42.751885 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:42.752092 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m21:57:42.752249 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:42.759103 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.759313 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:42.759477 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m21:57:42.759728 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.760217 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:57:42.760382 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m21:57:42.760529 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m21:57:42.760735 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.760891 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m21:57:42.763872 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m21:57:42.767346 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:57:42.767544 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m21:57:42.767703 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:42.774318 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.774600 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m21:57:42.774808 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m21:57:42.792753 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.793757 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m21:57:42.794194 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m21:57:42.794364 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m21:57:42.796582 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m21:57:42.799052 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:57:42.799264 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m21:57:42.799423 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:42.805390 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.805644 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m21:57:42.805814 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m21:57:42.820976 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.824247 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m21:57:42.824477 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m21:57:42.824635 [debug] [ThreadPool]: On list_dbt_main: Close
[0m21:57:42.827042 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m21:57:42.829885 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:57:42.830124 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m21:57:42.830278 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:42.836289 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.836559 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m21:57:42.836738 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m21:57:42.856851 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.857730 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m21:57:42.857971 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m21:57:42.858134 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m21:57:42.860698 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m21:57:42.862262 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:57:42.862448 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m21:57:42.862604 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:57:42.869184 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.869424 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m21:57:42.869610 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m21:57:42.886192 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m21:57:42.889744 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m21:57:42.889979 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m21:57:42.890141 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m21:57:42.894514 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:42.894785 [debug] [MainThread]: On master: BEGIN
[0m21:57:42.894969 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:57:42.901890 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:42.902143 [debug] [MainThread]: On master: COMMIT
[0m21:57:42.902312 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:42.902468 [debug] [MainThread]: On master: COMMIT
[0m21:57:42.902674 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:42.902838 [debug] [MainThread]: On master: Close
[0m21:57:42.904619 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:57:42.904867 [info ] [MainThread]: 
[0m21:57:42.906215 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m21:57:42.906514 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_contact ................................... [RUN]
[0m21:57:42.906884 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_contact)
[0m21:57:42.907083 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m21:57:42.912501 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.914010 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 21:57:42.907221 => 21:57:42.913900
[0m21:57:42.914215 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m21:57:42.933193 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.933781 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.933979 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m21:57:42.934383 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:42.940876 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:42.941187 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.941445 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    contact_id,
    masterrecordid AS master_record_id,
    accountid AS account_id,
    reportstoid AS reports_to_id,
    ownerid AS owner_id,
    jigsawcontactid AS jigsaw_contact_id,
    individualid AS individual_id,

    /* Dates */
    birthdate AS birth_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    lastcurequestdate AS last_cu_request_date,
    lastcuupdatedate AS last_cu_update_date,
    emailbounceddate AS email_bounced_date,

    /* Dimensions */
    salutation,
    firstname AS first_name,
    lastname AS last_name,
    otherstreet AS other_street,
    othercity AS other_city,
    otherstate AS other_state,
    otherpostalcode AS other_postal_code,
    othercountry AS other_country,
    otherlatitude AS other_latitude,
    otherlongitude AS other_longitude,
    othergeocodeaccuracy AS other_geocode_accuracy,
    mailingstreet AS mailing_street,
    mailingcity AS mailing_city,
    mailingstate AS mailing_state,
    mailingpostalcode AS mailing_postal_code,
    mailingcountry AS mailing_country,
    mailinglatitude AS mailing_latitude,
    mailinglongitude AS mailing_longitude,
    mailinggeocodeaccuracy AS mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone AS mobile_phone,
    homephone AS home_phone,
    otherphone AS other_phone,
    assistantphone AS assistant_phone,
    email,
    title,
    department,
    assistantname AS assistant_name,
    leadsource AS lead_source,
    description,
    pronouns,
    genderidentity AS gender_identity,
    cleanstatus AS clean_status,
    emailbouncedreason AS email_bounced_reason,
    level__c AS level,
    languages__c AS languages,

    /* Metrics */
    hasoptedoutofemail AS has_opted_out_of_email,
    hasoptedoutoffax AS has_opted_out_of_fax,
    donotcall AS do_not_call

FROM "dbt"."staging"."stg_salesforce__contact"
WHERE isdeleted = FALSE
    );
  
  
[0m21:57:42.945433 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:42.949638 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.949942 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m21:57:42.950341 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:42.960396 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m21:57:42.960702 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.960904 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m21:57:42.962347 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:42.966137 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m21:57:42.966403 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m21:57:42.966723 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m21:57:42.967531 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 21:57:42.914350 => 21:57:42.967437
[0m21:57:42.967734 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m21:57:42.986963 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_contact .............................. [[32mOK[0m in 0.08s]
[0m21:57:42.987403 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m21:57:42.988154 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:42.988346 [debug] [MainThread]: On master: BEGIN
[0m21:57:42.988501 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:57:42.995350 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:42.995710 [debug] [MainThread]: On master: COMMIT
[0m21:57:42.995903 [debug] [MainThread]: Using duckdb connection "master"
[0m21:57:42.996083 [debug] [MainThread]: On master: COMMIT
[0m21:57:42.996325 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m21:57:42.996524 [debug] [MainThread]: On master: Close
[0m21:57:42.998351 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:57:42.998627 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_contact' was properly closed.
[0m21:57:42.998815 [info ] [MainThread]: 
[0m21:57:42.999020 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m21:57:42.999378 [debug] [MainThread]: Command end result
[0m21:57:43.007010 [info ] [MainThread]: 
[0m21:57:43.007354 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:57:43.007531 [info ] [MainThread]: 
[0m21:57:43.007721 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:57:43.008117 [debug] [MainThread]: Command `dbt run` succeeded at 21:57:43.008059 after 0.45 seconds
[0m21:57:43.008326 [debug] [MainThread]: Flushing usage events


============================== 22:02:30.318193 | 001af2e4-d0ef-44a8-a24e-15d648d24bae ==============================
[0m22:02:30.318193 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:02:30.322038 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_account.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:02:30.322397 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:02:30.410194 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:02:30.429858 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:02:30.465990 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:02:30.466308 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:02:30.467283 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:02:30.481862 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:02:30.483477 [info ] [MainThread]: 
[0m22:02:30.484009 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:02:30.484716 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:02:30.492035 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:02:30.492300 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:02:30.492477 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:02:30.506807 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.507908 [debug] [ThreadPool]: On list_dbt: Close
[0m22:02:30.509988 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:02:30.510448 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:02:30.513445 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:30.513659 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:02:30.513823 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:30.520722 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.521007 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:30.521177 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:02:30.521434 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.521970 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:02:30.522138 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:30.522289 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:02:30.522496 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.522662 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:02:30.525684 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m22:02:30.529287 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:02:30.529535 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:02:30.529704 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:30.536348 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.536613 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:02:30.536786 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:02:30.554928 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.555895 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:02:30.556519 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:02:30.556766 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:02:30.559200 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:02:30.561758 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:02:30.562009 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:02:30.562173 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:30.568609 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.568877 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:02:30.569063 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:02:30.589214 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.590020 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:02:30.590263 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:02:30.590431 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:02:30.592817 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:02:30.594690 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:02:30.594954 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:02:30.595131 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:30.601863 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.602167 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:02:30.602358 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:02:30.618326 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.621832 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:02:30.622064 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:02:30.622220 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:02:30.624691 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:02:30.626979 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:02:30.627197 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:02:30.627356 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:30.633891 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.634172 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:02:30.634351 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:02:30.649873 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:30.653655 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:02:30.653912 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:02:30.654083 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:02:30.657979 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:30.658192 [debug] [MainThread]: On master: BEGIN
[0m22:02:30.658352 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:02:30.664747 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:30.665014 [debug] [MainThread]: On master: COMMIT
[0m22:02:30.665183 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:30.665334 [debug] [MainThread]: On master: COMMIT
[0m22:02:30.665532 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:30.665692 [debug] [MainThread]: On master: Close
[0m22:02:30.667216 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:02:30.667466 [info ] [MainThread]: 
[0m22:02:30.668855 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m22:02:30.669139 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_account ................................... [RUN]
[0m22:02:30.669544 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_account)
[0m22:02:30.669769 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m22:02:30.675168 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m22:02:30.676781 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 22:02:30.669922 => 22:02:30.676627
[0m22:02:30.677031 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m22:02:30.696360 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m22:02:30.697121 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:02:30.697436 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m22:02:30.697632 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:02:30.704106 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:30.704338 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:02:30.704605 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

SELECT
    account_id,
    masterrecordid AS master_record_id,
    name AS account_name,
    type AS account_type,
    parentid AS parent_account_id,
    billingstreet AS billing_street,
    billingcity AS billing_city,
    billingstate AS billing_state,
    billingpostalcode AS billing_postal_code,
    billingcountry AS billing_country,
    billinglatitude AS billing_latitude,
    billinglongitude AS billing_longitude,
    billinggeocodeaccuracy AS billing_geocode_accuracy,
    shippingstreet AS shipping_street,
    shippingcity AS shipping_city,
    shippingstate AS shipping_state,
    shippingpostalcode AS shipping_postal_code,
    shippingcountry AS shipping_country,
    shippinglatitude AS shipping_latitude,
    shippinglongitude AS shipping_longitude,
    shippinggeocodeaccuracy AS shipping_geocode_accuracy,
    phone AS account_phone,
    fax AS account_fax,
    accountnumber AS account_number,
    website AS account_website,
    sic AS sic_code,
    industry AS account_industry,
    annualrevenue AS annual_revenue,
    numberofemployees AS number_of_employees,
    ownership AS account_ownership,
    tickersymbol AS ticker_symbol,
    description AS account_description,
    rating AS account_rating,
    site AS account_site,
    ownerid AS owner_id,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,
    jigsaw AS jigsaw_id,
    jigsawcompanyid AS jigsaw_company_id,
    cleanstatus AS clean_status,
    accountsource AS account_source,
    dunsnumber AS duns_number,
    tradestyle AS trade_style,
    naicscode AS naics_code,
    naicsdesc AS naics_description,
    yearstarted AS year_started,
    sicdesc AS sic_description,
    dandbcompanyid AS dandb_company_id,
    operatinghoursid AS operating_hours_id,
    customerpriority__c AS customer_priority,
    sla__c AS sla,
    active__c AS is_active,
    numberoflocations__c AS number_of_locations,
    upsellopportunity__c AS upsell_opportunity,
    slaserialnumber__c AS sla_serial_number,
    slaexpirationdate__c AS sla_expiration_date
FROM
    "dbt"."staging"."stg_salesforce__account"
WHERE
    isdeleted = FALSE  -- Exclude deleted accounts
ORDER BY
    account_name
    );
  
  
[0m22:02:30.709131 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:30.713435 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:02:30.713701 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m22:02:30.714159 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:30.724135 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m22:02:30.724411 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:02:30.724599 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m22:02:30.726076 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:30.729971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:02:30.730262 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m22:02:30.730595 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:30.731365 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 22:02:30.677205 => 22:02:30.731272
[0m22:02:30.731573 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m22:02:30.762560 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_account .............................. [[32mOK[0m in 0.09s]
[0m22:02:30.762995 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m22:02:30.763752 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:30.763941 [debug] [MainThread]: On master: BEGIN
[0m22:02:30.764101 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:02:30.770712 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:30.770974 [debug] [MainThread]: On master: COMMIT
[0m22:02:30.771436 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:30.771931 [debug] [MainThread]: On master: COMMIT
[0m22:02:30.772273 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:30.772609 [debug] [MainThread]: On master: Close
[0m22:02:30.774464 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:02:30.774661 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_account' was properly closed.
[0m22:02:30.774861 [info ] [MainThread]: 
[0m22:02:30.775055 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m22:02:30.775457 [debug] [MainThread]: Command end result
[0m22:02:30.782926 [info ] [MainThread]: 
[0m22:02:30.783185 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:02:30.783370 [info ] [MainThread]: 
[0m22:02:30.783554 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:02:30.783958 [debug] [MainThread]: Command `dbt run` succeeded at 22:02:30.783908 after 0.49 seconds
[0m22:02:30.784176 [debug] [MainThread]: Flushing usage events


============================== 22:02:42.556862 | 9a71764e-9f50-4fa4-bbaf-17f43f05b667 ==============================
[0m22:02:42.556862 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:02:42.559557 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_campaign.sql', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:02:42.559810 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:02:42.634517 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:02:42.652667 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:02:42.688998 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:02:42.689309 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:02:42.690299 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m22:02:42.704225 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:02:42.705759 [info ] [MainThread]: 
[0m22:02:42.706195 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:02:42.706772 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:02:42.714285 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:02:42.714554 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:02:42.714734 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:02:42.723421 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.724370 [debug] [ThreadPool]: On list_dbt: Close
[0m22:02:42.726372 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:02:42.726878 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:02:42.729893 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:42.730132 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:02:42.730295 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:42.736601 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.736816 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:42.736979 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:02:42.737221 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.737706 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:02:42.737878 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:02:42.738026 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:02:42.738234 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.738387 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:02:42.741607 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m22:02:42.744933 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:02:42.745195 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:02:42.745376 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:42.752364 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.752640 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:02:42.752815 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:02:42.768397 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.772015 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:02:42.772805 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:02:42.773003 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:02:42.775248 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:02:42.777426 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:02:42.777604 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:02:42.777780 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:42.784358 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.784604 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:02:42.784775 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:02:42.800309 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.804032 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:02:42.804289 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:02:42.804458 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:02:42.806888 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:02:42.808409 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:02:42.809398 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:02:42.809565 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:42.816657 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.816941 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:02:42.817117 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:02:42.835532 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.836601 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:02:42.836839 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:02:42.836996 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:02:42.839367 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:02:42.842511 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:02:42.842706 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:02:42.842873 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:02:42.849200 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.849439 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:02:42.849620 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:02:42.867727 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:02:42.868560 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:02:42.868797 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:02:42.868963 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:02:42.871563 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:42.871835 [debug] [MainThread]: On master: BEGIN
[0m22:02:42.872012 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:02:42.878647 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:42.878941 [debug] [MainThread]: On master: COMMIT
[0m22:02:42.879115 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:42.879263 [debug] [MainThread]: On master: COMMIT
[0m22:02:42.879492 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:42.879653 [debug] [MainThread]: On master: Close
[0m22:02:42.881307 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:02:42.881527 [info ] [MainThread]: 
[0m22:02:42.883686 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m22:02:42.883968 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_campaign .................................. [RUN]
[0m22:02:42.884322 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_campaign)
[0m22:02:42.884513 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m22:02:42.890142 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m22:02:42.891262 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 22:02:42.884646 => 22:02:42.891093
[0m22:02:42.891528 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m22:02:42.911358 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m22:02:42.911988 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:02:42.912193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m22:02:42.912374 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:02:42.919505 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:02:42.919887 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:02:42.920166 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

SELECT
    /* IDs */
    id AS campaign_id,
    parentid AS parent_campaign_id,
    ownerid AS owner_id,
    createdbyid AS created_by_id,
    lastmodifiedbyid AS last_modified_by_id,
    campaignmemberrecordtypeid AS campaign_member_record_type_id,

    /* Dates */
    startdate AS start_date,
    enddate AS end_date,
    createddate AS created_at,
    lastmodifieddate AS last_modified_at,
    systemmodstamp AS system_mod_stamp,
    lastactivitydate AS last_activity_date,

    /* Dimensions */
    name AS campaign_name,
    type AS campaign_type,
    status,
    description,
    isactive AS is_active,

    /* Metrics */
    expectedrevenue AS expected_revenue,
    budgetedcost AS budgeted_cost,
    actualcost AS actual_cost,
    expectedresponse AS expected_response,
    numbersent AS number_sent,
    numberofleads AS number_of_leads,
    numberofconvertedleads AS number_of_converted_leads,
    numberofcontacts AS number_of_contacts,
    numberofresponses AS number_of_responses,
    numberofopportunities AS number_of_opportunities,
    numberofwonopportunities AS number_of_won_opportunities,
    amountallopportunities AS amount_all_opportunities,
    amountwonopportunities AS amount_won_opportunities,
    hierarchynumberofleads AS hierarchy_number_of_leads,
    hierarchynumberofconvertedleads AS hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts AS hierarchy_number_of_contacts,
    hierarchynumberofresponses AS hierarchy_number_of_responses,
    hierarchynumberofopportunities AS hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities AS hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities AS hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities AS hierarchy_amount_won_opportunities,
    hierarchynumbersent AS hierarchy_number_sent,
    hierarchyexpectedrevenue AS hierarchy_expected_revenue,
    hierarchybudgetedcost AS hierarchy_budgeted_cost,
    hierarchyactualcost AS hierarchy_actual_cost

FROM "dbt"."staging"."stg_salesforce__campaign"
WHERE isdeleted = FALSE;
    );
  
  
[0m22:02:42.920814 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 22:02:42.891675 => 22:02:42.920693
[0m22:02:42.921050 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: ROLLBACK
[0m22:02:42.925575 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_campaign'
[0m22:02:42.925997 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m22:02:42.928881 [debug] [Thread-1  ]: Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Parser Error: syntax error at or near ";"
[0m22:02:42.929416 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_campaign ......................... [[31mERROR[0m in 0.04s]
[0m22:02:42.929776 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m22:02:42.930712 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:42.930893 [debug] [MainThread]: On master: BEGIN
[0m22:02:42.931045 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:02:42.937748 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:42.938020 [debug] [MainThread]: On master: COMMIT
[0m22:02:42.938186 [debug] [MainThread]: Using duckdb connection "master"
[0m22:02:42.938340 [debug] [MainThread]: On master: COMMIT
[0m22:02:42.938539 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:02:42.938701 [debug] [MainThread]: On master: Close
[0m22:02:42.940778 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:02:42.940964 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_campaign' was properly closed.
[0m22:02:42.941151 [info ] [MainThread]: 
[0m22:02:42.941345 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m22:02:42.941727 [debug] [MainThread]: Command end result
[0m22:02:42.949591 [info ] [MainThread]: 
[0m22:02:42.949874 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:02:42.950116 [info ] [MainThread]: 
[0m22:02:42.950439 [error] [MainThread]:   Runtime Error in model dim_campaign (models/dimensions/dim_campaign.sql)
  Parser Error: syntax error at or near ";"
[0m22:02:42.950638 [info ] [MainThread]: 
[0m22:02:42.950843 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:02:42.951266 [debug] [MainThread]: Command `dbt run` failed at 22:02:42.951206 after 0.42 seconds
[0m22:02:42.951497 [debug] [MainThread]: Flushing usage events


============================== 22:05:38.406616 | dfec6857-29dc-476a-b880-0da8c78d29a0 ==============================
[0m22:05:38.406616 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:05:38.410022 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:05:38.410299 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:05:38.495109 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:05:38.512870 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:05:38.548446 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:05:38.548759 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:05:38.549729 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:05:38.565125 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:05:38.566587 [info ] [MainThread]: 
[0m22:05:38.567032 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:05:38.567701 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:05:38.575735 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:05:38.576074 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:05:38.576247 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:05:38.592517 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.593612 [debug] [ThreadPool]: On list_dbt: Close
[0m22:05:38.595590 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:05:38.595964 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:05:38.598920 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:38.599130 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:05:38.599297 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:38.606107 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.606396 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:38.606569 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:05:38.606818 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.607336 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:05:38.607503 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:38.607653 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:05:38.607872 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.608028 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:05:38.611058 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_fact)
[0m22:05:38.614434 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:05:38.614629 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:05:38.614785 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:38.620950 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.621224 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:05:38.621404 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:05:38.640433 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.641314 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:05:38.644758 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:05:38.644947 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:05:38.647473 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:05:38.649264 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:05:38.649480 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:05:38.649641 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:38.656779 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.657065 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:05:38.657262 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:05:38.673577 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.677395 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:05:38.677640 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:05:38.677806 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:05:38.680545 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:05:38.682972 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:05:38.683166 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:05:38.683320 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:38.690146 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.690427 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:05:38.690614 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:05:38.706371 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.709844 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:05:38.710089 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:05:38.710251 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:05:38.713008 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:05:38.715552 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:05:38.715769 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:05:38.715925 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:38.722408 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.722673 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:05:38.722845 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:05:38.740699 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:38.741613 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:05:38.741842 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:05:38.741999 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:05:38.745985 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:38.746185 [debug] [MainThread]: On master: BEGIN
[0m22:05:38.746343 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:05:38.752574 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:38.752831 [debug] [MainThread]: On master: COMMIT
[0m22:05:38.753000 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:38.753152 [debug] [MainThread]: On master: COMMIT
[0m22:05:38.753350 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:38.753512 [debug] [MainThread]: On master: Close
[0m22:05:38.755110 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:05:38.755345 [info ] [MainThread]: 
[0m22:05:38.756882 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m22:05:38.757166 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_solution .................................. [RUN]
[0m22:05:38.757546 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_solution)
[0m22:05:38.757745 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m22:05:38.763110 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m22:05:38.764178 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 22:05:38.757882 => 22:05:38.763998
[0m22:05:38.764486 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m22:05:38.783716 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m22:05:38.784661 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:38.784996 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m22:05:38.785183 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:05:38.791509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:38.791772 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:38.791986 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

SELECT
    id AS solution_id,
    solutionnumber AS solution_number,
    solutionname AS solution_name,
    ispublished AS is_published,
    ispublishedinpublickb AS is_published_in_public_kb,
    status AS solution_status,
    isreviewed AS is_reviewed,
    solutionnote AS solution_note,
    caseid AS case_id,
    ownerid AS owner_id,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    timesused AS times_used,
    ishtml AS is_html
FROM
    "dbt"."staging"."stg_salesforce__solution"
WHERE
    isdeleted = FALSE
ORDER BY
    solution_name
    );
  
  
[0m22:05:38.792826 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 22:05:38.764654 => 22:05:38.792726
[0m22:05:38.793040 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: ROLLBACK
[0m22:05:38.796876 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_solution'
[0m22:05:38.797174 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m22:05:38.799842 [debug] [Thread-1  ]: Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__solution.caseid", "stg_salesforce__solution.ownerid"
  LINE 14:     id AS solution_id,
               ^
[0m22:05:38.800260 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_solution ......................... [[31mERROR[0m in 0.04s]
[0m22:05:38.800587 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m22:05:38.801420 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:38.801757 [debug] [MainThread]: On master: BEGIN
[0m22:05:38.801939 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:05:38.808923 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:38.809203 [debug] [MainThread]: On master: COMMIT
[0m22:05:38.809368 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:38.809520 [debug] [MainThread]: On master: COMMIT
[0m22:05:38.809725 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:38.809882 [debug] [MainThread]: On master: Close
[0m22:05:38.811416 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:05:38.811624 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_solution' was properly closed.
[0m22:05:38.811808 [info ] [MainThread]: 
[0m22:05:38.811999 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m22:05:38.812346 [debug] [MainThread]: Command end result
[0m22:05:38.819531 [info ] [MainThread]: 
[0m22:05:38.819772 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:05:38.819931 [info ] [MainThread]: 
[0m22:05:38.820146 [error] [MainThread]:   Runtime Error in model dim_solution (models/dimensions/dim_solution.sql)
  Binder Error: Referenced column "id" not found in FROM clause!
  Candidate bindings: "stg_salesforce__solution.caseid", "stg_salesforce__solution.ownerid"
  LINE 14:     id AS solution_id,
               ^
[0m22:05:38.820527 [info ] [MainThread]: 
[0m22:05:38.820765 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:05:38.821124 [debug] [MainThread]: Command `dbt run` failed at 22:05:38.821076 after 0.43 seconds
[0m22:05:38.821329 [debug] [MainThread]: Flushing usage events


============================== 22:05:50.474338 | 7b8f473a-8ef2-4e59-af3d-697af720b93b ==============================
[0m22:05:50.474338 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:05:50.476826 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_solution.sql', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:05:50.477090 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:05:50.552857 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:05:50.571425 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:05:50.607579 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:05:50.607879 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:05:50.608923 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:05:50.622484 [info ] [MainThread]: Found 33 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:05:50.623888 [info ] [MainThread]: 
[0m22:05:50.624310 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:05:50.625086 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:05:50.634260 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:05:50.634662 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:05:50.634900 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:05:50.643754 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.644823 [debug] [ThreadPool]: On list_dbt: Close
[0m22:05:50.647683 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:05:50.648203 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:05:50.651458 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:50.651746 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:05:50.651919 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:50.659437 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.659755 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:50.659944 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:05:50.660241 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.660776 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:05:50.660951 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:05:50.661095 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:05:50.661312 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.661467 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:05:50.664732 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m22:05:50.668815 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:05:50.669060 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:05:50.669311 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:50.677238 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.677673 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:05:50.677980 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:05:50.694981 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.698955 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:05:50.699774 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:05:50.699980 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:05:50.702789 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:05:50.705233 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:05:50.705416 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:05:50.705574 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:50.712189 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.712431 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:05:50.712656 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:05:50.728889 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.732669 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:05:50.732915 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:05:50.733085 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:05:50.735539 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:05:50.737994 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:05:50.738211 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:05:50.738401 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:50.745429 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.745684 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:05:50.745858 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:05:50.764469 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.765260 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:05:50.765620 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:05:50.765797 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:05:50.768228 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:05:50.769843 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:05:50.770045 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:05:50.770194 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:50.777026 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.777319 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:05:50.777504 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:05:50.795750 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:05:50.796691 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:05:50.796927 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:05:50.797085 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:05:50.801548 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:50.801770 [debug] [MainThread]: On master: BEGIN
[0m22:05:50.801926 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:05:50.808314 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:50.808568 [debug] [MainThread]: On master: COMMIT
[0m22:05:50.808728 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:50.808875 [debug] [MainThread]: On master: COMMIT
[0m22:05:50.809073 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:50.809231 [debug] [MainThread]: On master: Close
[0m22:05:50.810826 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:05:50.811059 [info ] [MainThread]: 
[0m22:05:50.813282 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m22:05:50.813568 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_solution .................................. [RUN]
[0m22:05:50.813967 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_solution)
[0m22:05:50.814159 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m22:05:50.819714 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.820319 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 22:05:50.814294 => 22:05:50.820204
[0m22:05:50.820533 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m22:05:50.840289 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.840971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.841172 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m22:05:50.841351 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:05:50.847953 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.848261 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.848480 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

SELECT
    solution_id,
    solutionnumber AS solution_number,
    solutionname AS solution_name,
    ispublished AS is_published,
    ispublishedinpublickb AS is_published_in_public_kb,
    status AS solution_status,
    isreviewed AS is_reviewed,
    solutionnote AS solution_note,
    caseid AS case_id,
    ownerid AS owner_id,
    createddate AS created_date,
    createdbyid AS created_by_id,
    lastmodifieddate AS last_modified_date,
    lastmodifiedbyid AS last_modified_by_id,
    systemmodstamp AS system_modstamp,
    timesused AS times_used,
    ishtml AS is_html
FROM
    "dbt"."staging"."stg_salesforce__solution"
WHERE
    isdeleted = FALSE
ORDER BY
    solution_name
    );
  
  
[0m22:05:50.850572 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.854585 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.854837 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m22:05:50.855226 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.857095 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.857303 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m22:05:50.857625 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.868961 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m22:05:50.869214 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.869412 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m22:05:50.870300 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.873246 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:05:50.873459 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m22:05:50.873888 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:05:50.874681 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 22:05:50.820685 => 22:05:50.874580
[0m22:05:50.874893 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m22:05:50.900100 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_solution ............................. [[32mOK[0m in 0.09s]
[0m22:05:50.900507 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m22:05:50.901364 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:50.901540 [debug] [MainThread]: On master: BEGIN
[0m22:05:50.901695 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:05:50.908396 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:50.908663 [debug] [MainThread]: On master: COMMIT
[0m22:05:50.908835 [debug] [MainThread]: Using duckdb connection "master"
[0m22:05:50.908998 [debug] [MainThread]: On master: COMMIT
[0m22:05:50.909196 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:05:50.909355 [debug] [MainThread]: On master: Close
[0m22:05:50.911044 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:05:50.911282 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_solution' was properly closed.
[0m22:05:50.911481 [info ] [MainThread]: 
[0m22:05:50.911698 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m22:05:50.912042 [debug] [MainThread]: Command end result
[0m22:05:50.918798 [info ] [MainThread]: 
[0m22:05:50.919050 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:05:50.919218 [info ] [MainThread]: 
[0m22:05:50.919398 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:05:50.919887 [debug] [MainThread]: Command `dbt run` succeeded at 22:05:50.919787 after 0.46 seconds
[0m22:05:50.920204 [debug] [MainThread]: Flushing usage events


============================== 22:13:30.110293 | 5b992ca6-ea57-4c7b-8bbe-6ccd47d1e945 ==============================
[0m22:13:30.110293 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:13:30.113891 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_case_history.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:13:30.114186 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:13:30.200295 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:13:30.220723 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:13:30.262338 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:13:30.262703 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:13:30.263696 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:13:30.278795 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:13:30.280477 [info ] [MainThread]: 
[0m22:13:30.281009 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:13:30.281708 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:13:30.289077 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:13:30.289333 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:13:30.289504 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:13:30.304488 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.305401 [debug] [ThreadPool]: On list_dbt: Close
[0m22:13:30.307534 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:13:30.308031 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:13:30.311115 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:30.311331 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:13:30.311496 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:30.318555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.318841 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:30.319020 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:13:30.319290 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.319813 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:30.319993 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:30.320147 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:30.320668 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.320874 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:13:30.348233 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:13:30.351863 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:30.352063 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:13:30.352218 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:30.358501 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.358744 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:30.358916 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:13:30.374440 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.377943 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:13:30.378821 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:13:30.379059 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:13:30.382000 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:13:30.384698 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:30.384933 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:13:30.385105 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:30.391940 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.392270 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:30.392611 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:13:30.409217 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.412858 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:13:30.413102 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:13:30.413267 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:13:30.415828 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:13:30.418006 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:30.418187 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:13:30.418338 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:30.424902 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.425145 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:30.425312 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:13:30.443445 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.444207 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:13:30.444447 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:13:30.444610 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:13:30.446867 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:13:30.448445 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:30.448631 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:13:30.448778 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:30.454917 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.455182 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:30.455361 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:13:30.473167 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:30.474023 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:13:30.474240 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:13:30.474397 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:13:30.478173 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:30.478357 [debug] [MainThread]: On master: BEGIN
[0m22:13:30.478504 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:13:30.484612 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:30.484869 [debug] [MainThread]: On master: COMMIT
[0m22:13:30.485033 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:30.485183 [debug] [MainThread]: On master: COMMIT
[0m22:13:30.485381 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:30.485542 [debug] [MainThread]: On master: Close
[0m22:13:30.487069 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:13:30.487275 [info ] [MainThread]: 
[0m22:13:30.489547 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m22:13:30.489875 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case_history ...................... [RUN]
[0m22:13:30.490254 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_case_history)
[0m22:13:30.490461 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m22:13:30.496844 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m22:13:30.498295 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 22:13:30.490598 => 22:13:30.498180
[0m22:13:30.498502 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m22:13:30.528209 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case_history"
[0m22:13:30.528976 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:13:30.529198 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: BEGIN
[0m22:13:30.529398 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:13:30.535989 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:13:30.536290 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:13:30.536514 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case_history"
  
    as (
      

select
    case_history_id,
    h.status,
    h.priority,
    h.created_date,
    h.last_modified_date,
    c.account_id,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__case_history_2" h
left join "dbt"."fact"."fact_case" c
    on h.case_history_id = c.case_id
left join "dbt"."dim"."dim_account" a
    on c.account_id = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.owner_id = u.user_id;
    );
  
  
  
[0m22:13:30.537048 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 22:13:30.498637 => 22:13:30.536945
[0m22:13:30.537258 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: ROLLBACK
[0m22:13:30.540975 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case_history'
[0m22:13:30.541179 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: Close
[0m22:13:30.543129 [debug] [Thread-1  ]: Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Parser Error: syntax error at or near ";"
[0m22:13:30.543524 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case_history ............. [[31mERROR[0m in 0.05s]
[0m22:13:30.543870 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m22:13:30.544641 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:30.544823 [debug] [MainThread]: On master: BEGIN
[0m22:13:30.545000 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:13:30.551596 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:30.551962 [debug] [MainThread]: On master: COMMIT
[0m22:13:30.552264 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:30.552613 [debug] [MainThread]: On master: COMMIT
[0m22:13:30.552940 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:30.553122 [debug] [MainThread]: On master: Close
[0m22:13:30.554983 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:13:30.555227 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case_history' was properly closed.
[0m22:13:30.555415 [info ] [MainThread]: 
[0m22:13:30.555604 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m22:13:30.555973 [debug] [MainThread]: Command end result
[0m22:13:30.563874 [info ] [MainThread]: 
[0m22:13:30.564177 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:13:30.564341 [info ] [MainThread]: 
[0m22:13:30.564505 [error] [MainThread]:   Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Parser Error: syntax error at or near ";"
[0m22:13:30.564663 [info ] [MainThread]: 
[0m22:13:30.564852 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:13:30.565266 [debug] [MainThread]: Command `dbt run` failed at 22:13:30.565207 after 0.48 seconds
[0m22:13:30.565471 [debug] [MainThread]: Flushing usage events


============================== 22:13:37.673286 | 82436322-84de-4e14-a150-5416c0a7ddb2 ==============================
[0m22:13:37.673286 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:13:37.675561 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/facts/fact_case_history.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:13:37.675813 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:13:37.749610 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:13:37.767540 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:13:37.802357 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:13:37.802655 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:13:37.803599 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:13:37.818540 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:13:37.820054 [info ] [MainThread]: 
[0m22:13:37.820477 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:13:37.821031 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:13:37.828456 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:13:37.828710 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:13:37.828892 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:13:37.837703 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.838672 [debug] [ThreadPool]: On list_dbt: Close
[0m22:13:37.840886 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:13:37.841406 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:13:37.844500 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:37.844726 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:13:37.844890 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:37.851481 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.851752 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:37.851920 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:13:37.852193 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.852715 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:37.852964 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:37.853135 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:37.853398 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.853568 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:13:37.856502 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:13:37.860019 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:37.860309 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:13:37.860493 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:37.867229 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.867531 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:37.867711 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:13:37.883455 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.887085 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:13:37.887510 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:13:37.887692 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:13:37.889952 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:13:37.892265 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:37.892438 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:13:37.892586 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:37.898665 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.898925 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:37.899099 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:13:37.917578 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.918626 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:13:37.918884 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:13:37.919064 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:13:37.921571 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:13:37.924261 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:37.924474 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:13:37.924638 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:37.931664 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.931920 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:37.932125 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:13:37.951215 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.952379 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:13:37.952628 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:13:37.952796 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:13:37.955560 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:13:37.957295 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:37.957496 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:13:37.957665 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:37.964222 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.964477 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:37.964650 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:13:37.979870 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:37.983314 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:13:37.983542 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:13:37.983698 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:13:37.987301 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:37.987486 [debug] [MainThread]: On master: BEGIN
[0m22:13:37.987633 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:13:37.994391 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:37.994680 [debug] [MainThread]: On master: COMMIT
[0m22:13:37.994848 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:37.995000 [debug] [MainThread]: On master: COMMIT
[0m22:13:37.995206 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:37.995371 [debug] [MainThread]: On master: Close
[0m22:13:37.996988 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:13:37.997195 [info ] [MainThread]: 
[0m22:13:37.999483 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m22:13:37.999777 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case_history ...................... [RUN]
[0m22:13:38.000142 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_case_history)
[0m22:13:38.000337 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m22:13:38.006436 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m22:13:38.007016 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 22:13:38.000473 => 22:13:38.006895
[0m22:13:38.007220 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m22:13:38.036749 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case_history"
[0m22:13:38.037395 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:13:38.037604 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: BEGIN
[0m22:13:38.037783 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:13:38.044686 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:13:38.045007 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:13:38.045244 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case_history"
  
    as (
      

select
    case_history_id,
    h.status,
    h.priority,
    h.created_date,
    h.last_modified_date,
    c.account_id,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__case_history_2" h
left join "dbt"."fact"."fact_case" c
    on h.case_history_id = c.case_id
left join "dbt"."dim"."dim_account" a
    on c.account_id = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.owner_id = u.user_id
    );
  
  
  
[0m22:13:38.060176 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 22:13:38.007356 => 22:13:38.060033
[0m22:13:38.060442 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: ROLLBACK
[0m22:13:38.064184 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case_history'
[0m22:13:38.064412 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: Close
[0m22:13:38.066818 [debug] [Thread-1  ]: Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Catalog Error: Table with name fact_case does not exist!
  Did you mean "dbt.fact_case"?
  LINE 25: left join "dbt"."fact"."fact_case" c
                     ^
[0m22:13:38.067233 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case_history ............. [[31mERROR[0m in 0.07s]
[0m22:13:38.067581 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m22:13:38.068385 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:38.068560 [debug] [MainThread]: On master: BEGIN
[0m22:13:38.068716 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:13:38.075173 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:38.075452 [debug] [MainThread]: On master: COMMIT
[0m22:13:38.075614 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:38.075762 [debug] [MainThread]: On master: COMMIT
[0m22:13:38.075964 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:38.076123 [debug] [MainThread]: On master: Close
[0m22:13:38.077726 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:13:38.077955 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case_history' was properly closed.
[0m22:13:38.078144 [info ] [MainThread]: 
[0m22:13:38.078347 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:13:38.078699 [debug] [MainThread]: Command end result
[0m22:13:38.086645 [info ] [MainThread]: 
[0m22:13:38.086966 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:13:38.087148 [info ] [MainThread]: 
[0m22:13:38.087320 [error] [MainThread]:   Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Catalog Error: Table with name fact_case does not exist!
  Did you mean "dbt.fact_case"?
  LINE 25: left join "dbt"."fact"."fact_case" c
                     ^
[0m22:13:38.087489 [info ] [MainThread]: 
[0m22:13:38.087681 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:13:38.088112 [debug] [MainThread]: Command `dbt run` failed at 22:13:38.088047 after 0.43 seconds
[0m22:13:38.088314 [debug] [MainThread]: Flushing usage events


============================== 22:13:50.752708 | 733cfac9-751e-4d3d-a238-8d3d88f7bd98 ==============================
[0m22:13:50.752708 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:13:50.755173 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:13:50.755459 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:13:50.831339 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:13:50.850086 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:13:50.885520 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:13:50.885850 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:13:50.886935 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m22:13:50.901670 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:13:50.903217 [info ] [MainThread]: 
[0m22:13:50.903732 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:13:50.904391 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:13:50.912053 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:13:50.912343 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:13:50.912692 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:13:50.920792 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.921770 [debug] [ThreadPool]: On list_dbt: Close
[0m22:13:50.923992 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:13:50.924499 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:13:50.927641 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:50.927874 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:13:50.928046 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:50.935649 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.935945 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:50.936115 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:13:50.936371 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.936909 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:50.937082 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:13:50.937231 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:13:50.937456 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.937616 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:13:50.940857 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:13:50.944516 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:50.944750 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:13:50.944913 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:50.951643 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.951909 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:13:50.952096 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:13:50.967804 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.971291 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:13:50.971721 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:13:50.971890 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:13:50.974439 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:13:50.976710 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:50.976890 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:13:50.977040 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:50.983452 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:50.983702 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:13:50.983883 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:13:51.003425 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:51.004160 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:13:51.004397 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:13:51.004560 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:13:51.007042 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:13:51.008527 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:51.008708 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:13:51.008861 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:51.016023 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:51.016397 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:13:51.016633 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:13:51.032691 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:51.036348 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:13:51.036599 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:13:51.036760 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:13:51.039187 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:13:51.041750 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:51.041984 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:13:51.042137 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:13:51.048624 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:51.048866 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:13:51.049040 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:13:51.066947 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:13:51.067874 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:13:51.068114 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:13:51.068274 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:13:51.072313 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:51.072495 [debug] [MainThread]: On master: BEGIN
[0m22:13:51.072658 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:13:51.078841 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:51.079147 [debug] [MainThread]: On master: COMMIT
[0m22:13:51.079312 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:51.079468 [debug] [MainThread]: On master: COMMIT
[0m22:13:51.079682 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:51.079853 [debug] [MainThread]: On master: Close
[0m22:13:51.081778 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:13:51.081986 [info ] [MainThread]: 
[0m22:13:51.084381 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:13:51.084755 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:13:51.085209 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_case)
[0m22:13:51.085448 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:13:51.091583 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:13:51.092781 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:13:51.085602 => 22:13:51.092612
[0m22:13:51.093047 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:13:51.122982 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:13:51.123686 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:13:51.123930 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:13:51.124155 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:13:51.130984 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:13:51.131306 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:13:51.131530 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.created_date,
    c.last_modified_date,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.account_id = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.owner_id = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.created_date = d.date
where c.is_deleted = false
    );
  
  
  
[0m22:13:51.132579 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:13:51.093186 => 22:13:51.132473
[0m22:13:51.132788 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m22:13:51.136368 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m22:13:51.136588 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:13:51.138747 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "account_id"
  LINE 27:     on c.account_id = a.account_id
                  ^
[0m22:13:51.139189 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case ..................... [[31mERROR[0m in 0.05s]
[0m22:13:51.139547 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:13:51.140412 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:51.140661 [debug] [MainThread]: On master: BEGIN
[0m22:13:51.140823 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:13:51.147389 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:51.147697 [debug] [MainThread]: On master: COMMIT
[0m22:13:51.147867 [debug] [MainThread]: Using duckdb connection "master"
[0m22:13:51.148021 [debug] [MainThread]: On master: COMMIT
[0m22:13:51.148229 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:13:51.148393 [debug] [MainThread]: On master: Close
[0m22:13:51.150346 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:13:51.150604 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:13:51.150783 [info ] [MainThread]: 
[0m22:13:51.150988 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:13:51.151354 [debug] [MainThread]: Command end result
[0m22:13:51.159011 [info ] [MainThread]: 
[0m22:13:51.159341 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:13:51.159525 [info ] [MainThread]: 
[0m22:13:51.159700 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "account_id"
  LINE 27:     on c.account_id = a.account_id
                  ^
[0m22:13:51.159876 [info ] [MainThread]: 
[0m22:13:51.160187 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:13:51.160636 [debug] [MainThread]: Command `dbt run` failed at 22:13:51.160577 after 0.43 seconds
[0m22:13:51.160850 [debug] [MainThread]: Flushing usage events


============================== 22:15:06.220494 | 7efc0044-244c-4491-9354-af4295a555d5 ==============================
[0m22:15:06.220494 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:15:06.228958 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/opportunity.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:15:06.229245 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:15:08.073107 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:15:08.454975 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:15:08.543400 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:15:08.543704 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:15:08.544677 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:15:08.565720 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:15:08.566843 [warn ] [MainThread]: The selection criterion 'models/facts/opportunity.sql' does not match any nodes
[0m22:15:08.567622 [info ] [MainThread]: 
[0m22:15:08.567840 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:15:08.568132 [debug] [MainThread]: Command end result
[0m22:15:08.576388 [debug] [MainThread]: Command `dbt run` succeeded at 22:15:08.576253 after 2.39 seconds
[0m22:15:08.576711 [debug] [MainThread]: Flushing usage events


============================== 22:15:19.603660 | 4b646931-29bf-4a93-95a7-8fce904e6bb4 ==============================
[0m22:15:19.603660 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:15:19.606304 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact-opportunity.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:15:19.606568 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:15:19.697775 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:15:19.717537 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:15:19.765092 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:15:19.765404 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:15:19.766379 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:15:19.780509 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:15:19.781395 [warn ] [MainThread]: The selection criterion 'models/facts/fact-opportunity.sql' does not match any nodes
[0m22:15:19.782051 [info ] [MainThread]: 
[0m22:15:19.782248 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:15:19.782510 [debug] [MainThread]: Command end result
[0m22:15:19.789685 [debug] [MainThread]: Command `dbt run` succeeded at 22:15:19.789581 after 0.21 seconds
[0m22:15:19.789983 [debug] [MainThread]: Flushing usage events


============================== 22:15:24.636304 | 3ac96365-a212-4f2e-85ad-de155b1f4bc3 ==============================
[0m22:15:24.636304 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:15:24.638774 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:15:24.639021 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:15:24.713435 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:15:24.731785 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:15:24.767940 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:15:24.768275 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:15:24.769272 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:15:24.783145 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:15:24.784574 [info ] [MainThread]: 
[0m22:15:24.785035 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:15:24.785712 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:15:24.792991 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:15:24.793309 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:15:24.793488 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:15:24.808409 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.809375 [debug] [ThreadPool]: On list_dbt: Close
[0m22:15:24.811534 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:15:24.812014 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:15:24.815039 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:24.815276 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:15:24.815431 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:24.822176 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.822453 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:24.822634 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:15:24.822889 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.823393 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:15:24.823557 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:24.823702 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:15:24.823908 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.824064 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:15:24.827417 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:15:24.831083 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:15:24.831346 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:15:24.831503 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:24.838384 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.838672 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:15:24.838852 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:15:24.855010 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.858536 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:15:24.860211 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:15:24.860460 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:15:24.863219 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m22:15:24.865603 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:15:24.865801 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:15:24.865955 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:24.872290 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.872562 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:15:24.872762 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:15:24.892628 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.893438 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:15:24.893685 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:15:24.893855 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:15:24.896370 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:15:24.898055 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:15:24.898330 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:15:24.898493 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:24.904991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.905260 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:15:24.905430 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:15:24.923750 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.924768 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:15:24.925011 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:15:24.925170 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:15:24.927588 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:15:24.929990 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:15:24.930185 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:15:24.930335 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:24.936814 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.937070 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:15:24.937238 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:15:24.952469 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:24.956117 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:15:24.956380 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:15:24.956542 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:15:24.960356 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:24.960548 [debug] [MainThread]: On master: BEGIN
[0m22:15:24.960701 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:15:24.966754 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:24.966997 [debug] [MainThread]: On master: COMMIT
[0m22:15:24.967156 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:24.967308 [debug] [MainThread]: On master: COMMIT
[0m22:15:24.967505 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:24.967663 [debug] [MainThread]: On master: Close
[0m22:15:24.969363 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:15:24.969573 [info ] [MainThread]: 
[0m22:15:24.971992 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:15:24.972273 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:15:24.972632 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity)
[0m22:15:24.972823 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:15:24.978419 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:24.979625 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:15:24.972956 => 22:15:24.979458
[0m22:15:24.979895 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:15:25.009240 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:25.009818 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:25.010029 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:15:25.010214 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:15:25.016870 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:15:25.017209 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:25.017427 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

select
    o.opportunity_id,
    o.amount,
    o.probability,
    o.close_date,
    o.created_date,
    o.last_modified_date,
    a.account_name,
    u.username as owner_username,
    d.date_key as close_date_key
from "dbt"."staging"."stg_salesforce__opportunity" o
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
left join "dbt"."dim"."dim_date" d
    on o.close_date = d.date
where o.is_deleted = false
    );
  
  
  
[0m22:15:25.018518 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:15:24.980034 => 22:15:25.018412
[0m22:15:25.018731 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:15:25.022889 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:15:25.023198 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:15:25.025306 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Values list "o" does not have a column named "account_id"
  LINE 25:     on o.account_id = a.account_id
                  ^
[0m22:15:25.025717 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:15:25.026079 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:15:25.026974 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:25.027221 [debug] [MainThread]: On master: BEGIN
[0m22:15:25.027388 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:15:25.034240 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:25.034555 [debug] [MainThread]: On master: COMMIT
[0m22:15:25.034758 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:25.034911 [debug] [MainThread]: On master: COMMIT
[0m22:15:25.035140 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:25.035304 [debug] [MainThread]: On master: Close
[0m22:15:25.037166 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:15:25.037408 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:15:25.037593 [info ] [MainThread]: 
[0m22:15:25.037794 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:15:25.038159 [debug] [MainThread]: Command end result
[0m22:15:25.046229 [info ] [MainThread]: 
[0m22:15:25.046528 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:15:25.046691 [info ] [MainThread]: 
[0m22:15:25.046852 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Values list "o" does not have a column named "account_id"
  LINE 25:     on o.account_id = a.account_id
                  ^
[0m22:15:25.047011 [info ] [MainThread]: 
[0m22:15:25.047185 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:15:25.047553 [debug] [MainThread]: Command `dbt run` failed at 22:15:25.047502 after 0.43 seconds
[0m22:15:25.047756 [debug] [MainThread]: Flushing usage events


============================== 22:15:55.774723 | bcc294c5-a8a9-4ea4-9185-f990c131d39b ==============================
[0m22:15:55.774723 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:15:55.777704 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:15:55.777976 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:15:55.858555 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:15:55.877457 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:15:55.925293 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:15:55.925813 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m22:15:55.954569 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:15:55.968335 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:15:55.969806 [info ] [MainThread]: 
[0m22:15:55.970281 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:15:55.970846 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:15:55.978007 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:15:55.978321 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:15:55.978520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:15:55.996889 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:55.998078 [debug] [ThreadPool]: On list_dbt: Close
[0m22:15:56.000315 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:15:56.000827 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:15:56.004812 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:56.005110 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:15:56.005286 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:56.012714 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.013009 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:56.013178 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:15:56.013423 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.014143 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:15:56.014369 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:15:56.014530 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:15:56.014775 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.014950 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:15:56.018085 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:15:56.021685 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:15:56.021967 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:15:56.022134 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:56.028881 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.029113 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:15:56.029294 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:15:56.045543 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.049684 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:15:56.051375 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:15:56.051671 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:15:56.054633 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:15:56.058831 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:15:56.059206 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:15:56.059517 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:56.066333 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.066617 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:15:56.066797 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:15:56.084799 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.085799 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:15:56.086034 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:15:56.086199 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:15:56.088600 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:15:56.092022 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:15:56.092225 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:15:56.092379 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:56.098526 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.098788 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:15:56.098957 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:15:56.117801 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.118548 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:15:56.118772 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:15:56.118934 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:15:56.121890 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:15:56.123848 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:15:56.124117 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:15:56.124287 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:15:56.131040 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.131336 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:15:56.131518 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:15:56.147411 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:15:56.150995 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:15:56.151235 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:15:56.151396 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:15:56.154549 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:56.154781 [debug] [MainThread]: On master: BEGIN
[0m22:15:56.154954 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:15:56.161688 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:56.161997 [debug] [MainThread]: On master: COMMIT
[0m22:15:56.162173 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:56.162324 [debug] [MainThread]: On master: COMMIT
[0m22:15:56.162537 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:56.162695 [debug] [MainThread]: On master: Close
[0m22:15:56.164375 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:15:56.164591 [info ] [MainThread]: 
[0m22:15:56.166883 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:15:56.167322 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:15:56.167726 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m22:15:56.167929 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:15:56.173644 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:56.174172 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:15:56.168067 => 22:15:56.174065
[0m22:15:56.174371 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:15:56.204924 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:56.205542 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:56.205761 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:15:56.206040 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:15:56.213338 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:15:56.213659 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:15:56.213942 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

select
    o.opportunity_id,
    o.amount,
    o.probability,
    o.close_date,
    o.created_date,
    o.last_modified_date,
    a.account_name,
    u.username as owner_username,
    d.date_key as close_date_key
from "dbt"."dim"."dim_opportunity" o
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
left join "dbt"."dim"."dim_date" d
    on o.close_date = d.date
where o.is_deleted = false
    );
  
  
  
[0m22:15:56.229453 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:15:56.174504 => 22:15:56.229287
[0m22:15:56.229743 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:15:56.233949 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:15:56.234260 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:15:56.237174 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Catalog Error: Table with name dim_date does not exist!
  Did you mean "dbt.dim_date"?
  LINE 28: left join "dbt"."dim"."dim_date" d
                     ^
[0m22:15:56.237625 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.07s]
[0m22:15:56.238036 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:15:56.238943 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:56.239161 [debug] [MainThread]: On master: BEGIN
[0m22:15:56.239318 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:15:56.245801 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:56.246094 [debug] [MainThread]: On master: COMMIT
[0m22:15:56.246496 [debug] [MainThread]: Using duckdb connection "master"
[0m22:15:56.246751 [debug] [MainThread]: On master: COMMIT
[0m22:15:56.247032 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:15:56.247209 [debug] [MainThread]: On master: Close
[0m22:15:56.249005 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:15:56.249245 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:15:56.249432 [info ] [MainThread]: 
[0m22:15:56.249624 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m22:15:56.249974 [debug] [MainThread]: Command end result
[0m22:15:56.283921 [info ] [MainThread]: 
[0m22:15:56.284242 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:15:56.284419 [info ] [MainThread]: 
[0m22:15:56.284596 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Catalog Error: Table with name dim_date does not exist!
  Did you mean "dbt.dim_date"?
  LINE 28: left join "dbt"."dim"."dim_date" d
                     ^
[0m22:15:56.284767 [info ] [MainThread]: 
[0m22:15:56.284941 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:15:56.285280 [debug] [MainThread]: Command `dbt run` failed at 22:15:56.285229 after 0.53 seconds
[0m22:15:56.285494 [debug] [MainThread]: Flushing usage events


============================== 22:16:25.375455 | 0f4504a3-b249-4f99-b34b-fdef200cb7f1 ==============================
[0m22:16:25.375455 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:16:25.378514 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/dimensions/dim_date.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:16:25.378788 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:16:25.460025 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:16:25.479674 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:16:25.527335 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:16:25.527648 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:16:25.528647 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:16:25.542792 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:16:25.544156 [info ] [MainThread]: 
[0m22:16:25.544578 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:16:25.545267 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:16:25.552739 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:16:25.553041 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:16:25.553239 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:16:25.568009 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.569063 [debug] [ThreadPool]: On list_dbt: Close
[0m22:16:25.571225 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:16:25.571658 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:16:25.574606 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:16:25.574818 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:16:25.574980 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:25.581610 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.581844 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:16:25.582014 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:16:25.582256 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.582728 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:16:25.582896 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:16:25.583052 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:16:25.583265 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.583432 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:16:25.586980 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_fact)
[0m22:16:25.591079 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:16:25.591392 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:16:25.591560 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:25.598178 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.598384 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:16:25.598551 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:16:25.616573 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.617309 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:16:25.620961 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:16:25.621145 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:16:25.623472 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:16:25.625224 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:16:25.625465 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:16:25.625634 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:25.632451 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.632724 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:16:25.632905 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:16:25.648389 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.651952 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:16:25.652223 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:16:25.652408 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:16:25.654727 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:16:25.657073 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:16:25.657255 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:16:25.657407 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:25.664562 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.664903 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:16:25.665107 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:16:25.683994 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.685085 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:16:25.685328 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:16:25.685493 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:16:25.687944 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:16:25.689541 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:16:25.689751 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:16:25.689921 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:25.697822 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.698057 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:16:25.698231 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:16:25.713722 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:25.718057 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:16:25.718299 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:16:25.718459 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:16:25.721649 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:25.721850 [debug] [MainThread]: On master: BEGIN
[0m22:16:25.722010 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:16:25.728451 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:25.728711 [debug] [MainThread]: On master: COMMIT
[0m22:16:25.728876 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:25.729037 [debug] [MainThread]: On master: COMMIT
[0m22:16:25.729242 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:25.729409 [debug] [MainThread]: On master: Close
[0m22:16:25.730956 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:16:25.731170 [info ] [MainThread]: 
[0m22:16:25.733177 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:16:25.733453 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_date ...................................... [RUN]
[0m22:16:25.733834 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_date)
[0m22:16:25.734044 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:16:25.759518 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.759850 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:16:25.760042 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:16:25.766437 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.766725 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.766925 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:16:25.767310 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.807772 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:16:25.810090 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:16:25.734188 => 22:16:25.809974
[0m22:16:25.810308 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:16:25.828258 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m22:16:25.828716 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.829137 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

with date_range as (
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1

)

select
    *,
    case when date_day = current_date then true else false end as is_current_date,

    case
        when
            extract(year from date_day) = extract(year from current_date)
            and extract(month from date_day) = extract(month from current_date)
        then true
        else false
    end as is_current_month,

    case
        when
            extract(year from date_day) = extract(year from current_date)
            and extract(quarter from date_day) = extract(quarter from current_date)
        then true
        else false
    end as is_current_quarter,

    case
        when extract(year from date_day) = extract(year from current_date)
        then true
        else false
    end as is_current_year,

    concat(month_name, ' ', year_number) as year_qualified_month_name
from date_range
    );
  
  
[0m22:16:25.909230 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.913219 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.913457 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m22:16:25.913879 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.923120 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:16:25.923343 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.923532 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:16:25.928330 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.931286 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:16:25.931512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m22:16:25.931818 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:25.932565 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:16:25.810452 => 22:16:25.932470
[0m22:16:25.932771 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:16:25.969075 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_date ................................. [[32mOK[0m in 0.24s]
[0m22:16:25.969511 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:16:25.970270 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:25.970469 [debug] [MainThread]: On master: BEGIN
[0m22:16:25.970632 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:16:25.977843 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:25.978142 [debug] [MainThread]: On master: COMMIT
[0m22:16:25.978333 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:25.978502 [debug] [MainThread]: On master: COMMIT
[0m22:16:25.978719 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:25.978886 [debug] [MainThread]: On master: Close
[0m22:16:25.980645 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:16:25.980909 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_date' was properly closed.
[0m22:16:25.981100 [info ] [MainThread]: 
[0m22:16:25.981311 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.44 seconds (0.44s).
[0m22:16:25.981674 [debug] [MainThread]: Command end result
[0m22:16:26.015741 [info ] [MainThread]: 
[0m22:16:26.016038 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:16:26.016213 [info ] [MainThread]: 
[0m22:16:26.016395 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:16:26.016717 [debug] [MainThread]: Command `dbt run` succeeded at 22:16:26.016668 after 0.66 seconds
[0m22:16:26.016908 [debug] [MainThread]: Flushing usage events


============================== 22:16:29.675769 | 2649339f-bd28-439e-9aa0-e99d6238b3d9 ==============================
[0m22:16:29.675769 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:16:29.678158 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:16:29.678400 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:16:29.753353 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:16:29.772552 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:16:29.808335 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:16:29.808663 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:16:29.809691 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:16:29.824995 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:16:29.826536 [info ] [MainThread]: 
[0m22:16:29.827197 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:16:29.827965 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:16:29.835540 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:16:29.835808 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:16:29.835987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:16:29.844714 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.845661 [debug] [ThreadPool]: On list_dbt: Close
[0m22:16:29.848005 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:16:29.848548 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:16:29.851761 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:16:29.851998 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:16:29.852175 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:29.859133 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.859390 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:16:29.859557 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:16:29.859801 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.860312 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:16:29.860477 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:16:29.860623 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:16:29.860838 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.860993 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:16:29.864103 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m22:16:29.867696 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:16:29.867912 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:16:29.868073 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:29.875292 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.875607 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:16:29.875785 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:16:29.895132 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.895981 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:16:29.896408 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:16:29.896572 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:16:29.899033 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:16:29.900781 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:16:29.901016 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:16:29.901189 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:29.907705 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.907969 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:16:29.908145 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:16:29.924230 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.927892 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:16:29.928182 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:16:29.928349 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:16:29.930684 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:16:29.933403 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:16:29.933608 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:16:29.933765 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:29.940563 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.940853 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:16:29.941033 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:16:29.959209 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.960144 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:16:29.960369 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:16:29.960531 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:16:29.962898 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:16:29.965319 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:16:29.965510 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:16:29.965676 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:29.972041 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.972442 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:16:29.972669 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:16:29.988798 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:16:29.992269 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:16:29.992499 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:16:29.992659 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:16:29.996924 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:29.997209 [debug] [MainThread]: On master: BEGIN
[0m22:16:29.997390 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:16:30.004002 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:30.004275 [debug] [MainThread]: On master: COMMIT
[0m22:16:30.004435 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:30.004590 [debug] [MainThread]: On master: COMMIT
[0m22:16:30.004795 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:30.004985 [debug] [MainThread]: On master: Close
[0m22:16:30.006727 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:16:30.006978 [info ] [MainThread]: 
[0m22:16:30.008151 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:16:30.008453 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:16:30.008880 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m22:16:30.009131 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:16:30.014789 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:16:30.015350 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:16:30.009271 => 22:16:30.015237
[0m22:16:30.015564 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:16:30.045091 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:16:30.045784 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:16:30.046000 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:16:30.046197 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:16:30.052659 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:16:30.052959 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:16:30.053167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

select
    o.opportunity_id,
    o.amount,
    o.probability,
    o.close_date,
    o.created_date,
    o.last_modified_date,
    a.account_name,
    u.username as owner_username,
    d.date_key as close_date_key
from "dbt"."dim"."dim_opportunity" o
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
left join "dbt"."dim"."dim_date" d
    on o.close_date = d.date
where o.is_deleted = false
    );
  
  
  
[0m22:16:30.054008 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:16:30.015712 => 22:16:30.053886
[0m22:16:30.054211 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:16:30.057915 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:16:30.058140 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:16:30.060133 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 29:     on o.close_date = d.date
                                 ^
[0m22:16:30.060542 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:16:30.060890 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:16:30.061685 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:30.061872 [debug] [MainThread]: On master: BEGIN
[0m22:16:30.062026 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:16:30.068724 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:30.069026 [debug] [MainThread]: On master: COMMIT
[0m22:16:30.069204 [debug] [MainThread]: Using duckdb connection "master"
[0m22:16:30.069360 [debug] [MainThread]: On master: COMMIT
[0m22:16:30.069574 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:16:30.069739 [debug] [MainThread]: On master: Close
[0m22:16:30.071593 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:16:30.071866 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:16:30.072065 [info ] [MainThread]: 
[0m22:16:30.072278 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:16:30.072646 [debug] [MainThread]: Command end result
[0m22:16:30.080784 [info ] [MainThread]: 
[0m22:16:30.081207 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:16:30.081463 [info ] [MainThread]: 
[0m22:16:30.081637 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 29:     on o.close_date = d.date
                                 ^
[0m22:16:30.081802 [info ] [MainThread]: 
[0m22:16:30.081980 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:16:30.082357 [debug] [MainThread]: Command `dbt run` failed at 22:16:30.082307 after 0.42 seconds
[0m22:16:30.082567 [debug] [MainThread]: Flushing usage events


============================== 22:19:32.078516 | baca8b82-a742-4cc4-9b06-d59abfd69865 ==============================
[0m22:19:32.078516 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:19:32.081731 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:19:32.082017 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:19:32.168954 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:19:32.187080 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:19:32.231133 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:19:32.231448 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:19:32.232408 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:19:32.249930 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:19:32.251488 [info ] [MainThread]: 
[0m22:19:32.251973 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:19:32.252551 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:19:32.260445 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:19:32.260727 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:19:32.260903 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:19:32.277161 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.278188 [debug] [ThreadPool]: On list_dbt: Close
[0m22:19:32.280387 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:19:32.280840 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:19:32.283846 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:32.284079 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:19:32.284240 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:32.292744 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.293055 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:32.293238 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:19:32.293586 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.294170 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:19:32.294359 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:32.294524 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:19:32.294752 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.294925 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:19:32.298491 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:19:32.302067 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:19:32.302335 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:19:32.302537 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:32.309770 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.310070 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:19:32.310244 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:19:32.325775 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.329380 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:19:32.329985 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:19:32.330160 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:19:32.333059 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:19:32.335766 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:19:32.336002 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:19:32.336385 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:32.343624 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.343918 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:19:32.344104 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:19:32.360930 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.364747 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:19:32.364983 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:19:32.365141 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:19:32.367993 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:19:32.370355 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:19:32.370541 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:19:32.370697 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:32.377462 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.377731 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:19:32.377908 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:19:32.396744 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.397713 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:19:32.397938 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:19:32.398096 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:19:32.400570 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:19:32.404107 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:19:32.404367 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:19:32.404529 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:32.411355 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.411603 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:19:32.411778 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:19:32.430069 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:32.430835 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:19:32.431074 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:19:32.431232 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:19:32.433701 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:32.433891 [debug] [MainThread]: On master: BEGIN
[0m22:19:32.434040 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:19:32.440681 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:32.440939 [debug] [MainThread]: On master: COMMIT
[0m22:19:32.441098 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:32.441244 [debug] [MainThread]: On master: COMMIT
[0m22:19:32.441444 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:32.441604 [debug] [MainThread]: On master: Close
[0m22:19:32.443408 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:19:32.443678 [info ] [MainThread]: 
[0m22:19:32.445221 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:19:32.445578 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:19:32.446027 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_opportunity)
[0m22:19:32.446252 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:19:32.454169 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:32.455283 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:19:32.446415 => 22:19:32.455081
[0m22:19:32.455581 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:19:32.484785 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:32.485335 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:32.485546 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:19:32.485725 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:19:32.492373 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:19:32.492671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:32.492890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.amount,
        o.probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.username as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date
    WHERE o.is_deleted = false

    
)

SELECT * FROM latest_data;
    );
  
  
  
[0m22:19:32.493396 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:19:32.455723 => 22:19:32.493295
[0m22:19:32.493600 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:19:32.497247 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:19:32.497449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:19:32.499515 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:19:32.499933 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:19:32.500278 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:19:32.501060 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:32.501253 [debug] [MainThread]: On master: BEGIN
[0m22:19:32.501399 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:19:32.508029 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:32.508538 [debug] [MainThread]: On master: COMMIT
[0m22:19:32.508739 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:32.508907 [debug] [MainThread]: On master: COMMIT
[0m22:19:32.509159 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:32.509356 [debug] [MainThread]: On master: Close
[0m22:19:32.511157 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:19:32.511421 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:19:32.511623 [info ] [MainThread]: 
[0m22:19:32.511827 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:19:32.512213 [debug] [MainThread]: Command end result
[0m22:19:32.519689 [info ] [MainThread]: 
[0m22:19:32.520033 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:19:32.520224 [info ] [MainThread]: 
[0m22:19:32.520399 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:19:32.520568 [info ] [MainThread]: 
[0m22:19:32.520755 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:19:32.521232 [debug] [MainThread]: Command `dbt run` failed at 22:19:32.521134 after 0.46 seconds
[0m22:19:32.521530 [debug] [MainThread]: Flushing usage events


============================== 22:19:40.323557 | 7cd3c86b-acc6-41d1-a4d4-0e7479b29b5d ==============================
[0m22:19:40.323557 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:19:40.326119 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:19:40.326380 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:19:40.404079 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:19:40.422883 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:19:40.458434 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:19:40.458735 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:19:40.459666 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:19:40.473225 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:19:40.474626 [info ] [MainThread]: 
[0m22:19:40.475068 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:19:40.475745 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:19:40.483260 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:19:40.483542 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:19:40.483715 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:19:40.492437 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.493362 [debug] [ThreadPool]: On list_dbt: Close
[0m22:19:40.495565 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:19:40.496053 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:19:40.498923 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:40.499122 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:19:40.499281 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:40.506710 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.507018 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:40.507194 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:19:40.507469 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.508028 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:19:40.508273 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:19:40.508436 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:19:40.508669 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.508836 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:19:40.511849 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:19:40.515405 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:19:40.515621 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:19:40.515774 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:40.522874 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.523148 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:19:40.523327 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:19:40.541361 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.542369 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:19:40.543117 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:19:40.543319 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:19:40.545917 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:19:40.548539 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:19:40.548749 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:19:40.548902 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:40.555945 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.556219 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:19:40.556391 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:19:40.575700 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.576537 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:19:40.576771 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:19:40.576929 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:19:40.579338 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:19:40.581091 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:19:40.581343 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:19:40.581503 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:40.588500 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.588793 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:19:40.588984 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:19:40.604732 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.608090 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:19:40.608330 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:19:40.608490 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:19:40.610854 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:19:40.613407 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:19:40.613594 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:19:40.613744 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:19:40.620160 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.620411 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:19:40.620588 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:19:40.637479 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:19:40.641829 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:19:40.642080 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:19:40.642240 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:19:40.646353 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:40.646648 [debug] [MainThread]: On master: BEGIN
[0m22:19:40.646827 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:19:40.653889 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:40.654183 [debug] [MainThread]: On master: COMMIT
[0m22:19:40.654343 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:40.654492 [debug] [MainThread]: On master: COMMIT
[0m22:19:40.654695 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:40.654857 [debug] [MainThread]: On master: Close
[0m22:19:40.656539 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:19:40.656745 [info ] [MainThread]: 
[0m22:19:40.658339 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:19:40.658707 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:19:40.659134 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m22:19:40.659338 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:19:40.667160 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:40.667680 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:19:40.659470 => 22:19:40.667577
[0m22:19:40.667886 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:19:40.697052 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:40.697732 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:40.697948 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:19:40.698137 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:19:40.704863 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:19:40.705156 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:19:40.705366 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.amount,
        o.probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.username as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date
    WHERE o.is_deleted = false

    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:19:40.706187 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:19:40.668016 => 22:19:40.706081
[0m22:19:40.706390 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:19:40.710087 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:19:40.710310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:19:40.712255 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 30:         ON o.close_date = d.date
                                     ^
[0m22:19:40.712648 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:19:40.712985 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:19:40.713745 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:40.713917 [debug] [MainThread]: On master: BEGIN
[0m22:19:40.714061 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:19:40.720752 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:40.721149 [debug] [MainThread]: On master: COMMIT
[0m22:19:40.721373 [debug] [MainThread]: Using duckdb connection "master"
[0m22:19:40.721591 [debug] [MainThread]: On master: COMMIT
[0m22:19:40.721897 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:19:40.722083 [debug] [MainThread]: On master: Close
[0m22:19:40.724122 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:19:40.724353 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:19:40.724541 [info ] [MainThread]: 
[0m22:19:40.724728 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:19:40.725063 [debug] [MainThread]: Command end result
[0m22:19:40.732815 [info ] [MainThread]: 
[0m22:19:40.733139 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:19:40.733460 [info ] [MainThread]: 
[0m22:19:40.733664 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 30:         ON o.close_date = d.date
                                     ^
[0m22:19:40.733833 [info ] [MainThread]: 
[0m22:19:40.734006 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:19:40.734373 [debug] [MainThread]: Command `dbt run` failed at 22:19:40.734323 after 0.43 seconds
[0m22:19:40.734571 [debug] [MainThread]: Flushing usage events


============================== 22:20:11.402543 | 41446852-1e43-45d3-b12c-b916cd445400 ==============================
[0m22:20:11.402543 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:20:11.405551 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:20:11.405811 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:20:11.482596 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:20:11.500310 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:20:11.535621 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:20:11.535926 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:20:11.536888 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m22:20:11.553857 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:20:11.555362 [info ] [MainThread]: 
[0m22:20:11.555792 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:20:11.556448 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:20:11.563742 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:20:11.564013 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:20:11.564194 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:20:11.579840 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.580735 [debug] [ThreadPool]: On list_dbt: Close
[0m22:20:11.582828 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:20:11.583314 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:20:11.586173 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:11.586380 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:20:11.586539 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:11.593532 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.593776 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:11.593945 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:20:11.594193 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.594709 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:20:11.594874 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:11.595025 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:20:11.595238 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.595613 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:20:11.599326 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:20:11.602992 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:20:11.603255 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:20:11.603423 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:11.610192 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.610450 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:20:11.610623 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:20:11.626051 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.629759 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:20:11.630392 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:20:11.630565 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:20:11.633184 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:20:11.635716 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:20:11.635926 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:20:11.636082 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:11.675769 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.677624 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:20:11.677831 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:20:11.694972 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.698486 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:20:11.698724 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:20:11.698879 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:20:11.701302 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:20:11.702886 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:20:11.703795 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:20:11.703952 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:11.710288 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.710551 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:20:11.710721 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:20:11.729044 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.730091 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:20:11.730337 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:20:11.730495 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:20:11.732854 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:20:11.734478 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:20:11.734667 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:20:11.734822 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:11.742749 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.742968 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:20:11.743138 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:20:11.761012 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:11.761716 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:20:11.761951 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:20:11.762107 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:20:11.764160 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:11.764364 [debug] [MainThread]: On master: BEGIN
[0m22:20:11.764527 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:20:11.770416 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:11.770667 [debug] [MainThread]: On master: COMMIT
[0m22:20:11.770829 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:11.770977 [debug] [MainThread]: On master: COMMIT
[0m22:20:11.771177 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:11.771336 [debug] [MainThread]: On master: Close
[0m22:20:11.772847 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:20:11.773069 [info ] [MainThread]: 
[0m22:20:11.775403 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:20:11.775682 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:20:11.776040 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_opportunity)
[0m22:20:11.776231 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:20:11.784129 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:11.784706 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:20:11.776366 => 22:20:11.784590
[0m22:20:11.784910 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:20:11.815085 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:11.815718 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:11.815930 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:20:11.816130 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:11.823027 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:20:11.823344 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:11.823561 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.amount,
        o.probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.username as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here
    WHERE o.is_deleted = false

    
)

SELECT * FROM latest_data;
    );
  
  
  
[0m22:20:11.824099 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:20:11.785044 => 22:20:11.824000
[0m22:20:11.824302 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:20:11.827971 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:20:11.828226 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:20:11.830258 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:20:11.830672 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:20:11.831018 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:20:11.831838 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:11.832014 [debug] [MainThread]: On master: BEGIN
[0m22:20:11.832171 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:20:11.838554 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:11.838880 [debug] [MainThread]: On master: COMMIT
[0m22:20:11.839108 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:11.839273 [debug] [MainThread]: On master: COMMIT
[0m22:20:11.839498 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:11.839659 [debug] [MainThread]: On master: Close
[0m22:20:11.841487 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:20:11.841777 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:20:11.841982 [info ] [MainThread]: 
[0m22:20:11.842244 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m22:20:11.842650 [debug] [MainThread]: Command end result
[0m22:20:11.850381 [info ] [MainThread]: 
[0m22:20:11.850746 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:20:11.850948 [info ] [MainThread]: 
[0m22:20:11.851125 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:20:11.851300 [info ] [MainThread]: 
[0m22:20:11.851484 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:20:11.851901 [debug] [MainThread]: Command `dbt run` failed at 22:20:11.851849 after 0.47 seconds
[0m22:20:11.852120 [debug] [MainThread]: Flushing usage events


============================== 22:20:18.135840 | 73c1150a-1133-4167-9e3a-f9adf004690e ==============================
[0m22:20:18.135840 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:20:18.138601 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:20:18.138854 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:20:18.221994 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:20:18.240326 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:20:18.276826 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:20:18.277353 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m22:20:18.310284 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m22:20:18.323158 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:20:18.324697 [info ] [MainThread]: 
[0m22:20:18.325170 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:20:18.325784 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:20:18.333447 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:20:18.334278 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:20:18.334714 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:20:18.344205 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.345291 [debug] [ThreadPool]: On list_dbt: Close
[0m22:20:18.347759 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:20:18.348362 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:20:18.351416 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:18.351655 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:20:18.351838 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:18.358902 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.359202 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:18.359385 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:20:18.359642 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.360173 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:20:18.360351 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:20:18.360507 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:20:18.360733 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.360903 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:20:18.363775 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m22:20:18.367235 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:20:18.367496 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:20:18.368258 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:18.374748 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.375040 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:20:18.375232 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:20:18.394422 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.395277 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:20:18.396270 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:20:18.396560 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:20:18.399295 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:20:18.400973 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:20:18.401173 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:20:18.401322 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:18.408497 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.408812 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:20:18.409025 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:20:18.426003 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.430443 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:20:18.430825 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:20:18.431000 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:20:18.433858 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:20:18.436589 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:20:18.436799 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:20:18.436950 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:18.443991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.444197 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:20:18.444373 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:20:18.462605 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.463523 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:20:18.463758 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:20:18.463920 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:20:18.466476 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:20:18.468223 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:20:18.469376 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:20:18.469539 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:18.475907 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.476160 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:20:18.476330 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:20:18.491582 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:20:18.495250 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:20:18.495502 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:20:18.495664 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:20:18.498874 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:18.499084 [debug] [MainThread]: On master: BEGIN
[0m22:20:18.499250 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:20:18.506450 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:18.506749 [debug] [MainThread]: On master: COMMIT
[0m22:20:18.506912 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:18.507067 [debug] [MainThread]: On master: COMMIT
[0m22:20:18.507272 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:18.507430 [debug] [MainThread]: On master: Close
[0m22:20:18.509254 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:20:18.509544 [info ] [MainThread]: 
[0m22:20:18.512253 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:20:18.512608 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:20:18.513017 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m22:20:18.513214 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:20:18.519396 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:18.519943 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:20:18.513350 => 22:20:18.519833
[0m22:20:18.520149 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:20:18.577087 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:18.577763 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:18.577973 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:20:18.578167 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:18.584676 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:20:18.584933 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:20:18.585147 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.amount,
        o.probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.username as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here
    WHERE o.is_deleted = false

    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:20:18.586130 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:20:18.520281 => 22:20:18.586024
[0m22:20:18.586361 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:20:18.590273 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:20:18.590521 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:20:18.592726 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "o" does not have a column named "is_deleted"
  LINE 31:     WHERE o.is_deleted = false
                     ^
[0m22:20:18.593189 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.08s]
[0m22:20:18.593523 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:20:18.594319 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:18.594575 [debug] [MainThread]: On master: BEGIN
[0m22:20:18.594738 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:20:18.601938 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:18.602232 [debug] [MainThread]: On master: COMMIT
[0m22:20:18.602467 [debug] [MainThread]: Using duckdb connection "master"
[0m22:20:18.602698 [debug] [MainThread]: On master: COMMIT
[0m22:20:18.602961 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:20:18.603137 [debug] [MainThread]: On master: Close
[0m22:20:18.604845 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:20:18.605083 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:20:18.605277 [info ] [MainThread]: 
[0m22:20:18.605453 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m22:20:18.605796 [debug] [MainThread]: Command end result
[0m22:20:18.612633 [info ] [MainThread]: 
[0m22:20:18.612883 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:20:18.613044 [info ] [MainThread]: 
[0m22:20:18.613223 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "o" does not have a column named "is_deleted"
  LINE 31:     WHERE o.is_deleted = false
                     ^
[0m22:20:18.613466 [info ] [MainThread]: 
[0m22:20:18.613812 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:20:18.614202 [debug] [MainThread]: Command `dbt run` failed at 22:20:18.614147 after 0.50 seconds
[0m22:20:18.614415 [debug] [MainThread]: Flushing usage events


============================== 22:22:01.641134 | c84f3c9c-f9ba-4161-a8ea-9c5c7ed2747c ==============================
[0m22:22:01.641134 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:22:01.644339 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:22:01.644584 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:22:01.728819 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:22:01.749912 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:22:01.799921 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:22:01.800207 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:22:01.801179 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:22:01.825995 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:22:01.839187 [info ] [MainThread]: 
[0m22:22:01.848252 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:22:01.848956 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:22:01.860533 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:22:01.860871 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:22:01.861209 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:22:01.876761 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.877850 [debug] [ThreadPool]: On list_dbt: Close
[0m22:22:01.879932 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:22:01.880349 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:22:01.883368 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:01.883570 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:22:01.883731 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:01.890319 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.890584 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:01.890759 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:22:01.891020 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.891536 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:22:01.891705 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:01.891849 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:22:01.892056 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.892217 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:22:01.895170 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:22:01.898535 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:22:01.898729 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:22:01.898878 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:01.904891 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.905138 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:22:01.905311 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:22:01.920949 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.924511 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:22:01.925262 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:22:01.925484 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:22:01.927779 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:22:01.929353 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:22:01.930387 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:22:01.930568 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:01.937425 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.937723 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:22:01.937917 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:22:01.958376 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.959270 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:22:01.959525 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:22:01.959694 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:22:01.962068 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:22:01.963783 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:22:01.964090 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:22:01.964259 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:01.971345 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.971634 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:22:01.971818 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:22:01.991098 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:01.992019 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:22:01.992250 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:22:01.992409 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:22:01.995018 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:22:01.998066 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:22:01.998304 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:22:01.998474 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:02.004651 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:02.004903 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:22:02.005078 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:22:02.020932 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:02.024347 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:22:02.024582 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:22:02.024734 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:22:02.028854 [debug] [MainThread]: Using duckdb connection "master"
[0m22:22:02.029113 [debug] [MainThread]: On master: BEGIN
[0m22:22:02.029276 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:22:02.036237 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:22:02.036511 [debug] [MainThread]: On master: COMMIT
[0m22:22:02.036685 [debug] [MainThread]: Using duckdb connection "master"
[0m22:22:02.036838 [debug] [MainThread]: On master: COMMIT
[0m22:22:02.037047 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:22:02.037208 [debug] [MainThread]: On master: Close
[0m22:22:02.038770 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:22:02.038977 [info ] [MainThread]: 
[0m22:22:02.040552 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:22:02.040866 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:22:02.041253 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m22:22:02.041448 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:22:02.049164 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:22:02.050355 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:22:02.041586 => 22:22:02.050168
[0m22:22:02.050729 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:22:02.079964 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:22:02.080634 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:22:02.080840 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:22:02.081018 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:22:02.087719 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:22:02.088009 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:22:02.088225 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.amount,
        o.probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.username as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here

    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:22:02.089207 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:22:02.050970 => 22:22:02.089102
[0m22:22:02.089418 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:22:02.093119 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:22:02.093354 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:22:02.095336 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "o" does not have a column named "amount"
  LINE 16:         o.amount,
                   ^
[0m22:22:02.095756 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:22:02.096100 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:22:02.096844 [debug] [MainThread]: Using duckdb connection "master"
[0m22:22:02.097026 [debug] [MainThread]: On master: BEGIN
[0m22:22:02.097196 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:22:02.103859 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:22:02.104145 [debug] [MainThread]: On master: COMMIT
[0m22:22:02.104309 [debug] [MainThread]: Using duckdb connection "master"
[0m22:22:02.104464 [debug] [MainThread]: On master: COMMIT
[0m22:22:02.104663 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:22:02.104824 [debug] [MainThread]: On master: Close
[0m22:22:02.106404 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:22:02.106611 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:22:02.106985 [info ] [MainThread]: 
[0m22:22:02.107209 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:22:02.107582 [debug] [MainThread]: Command end result
[0m22:22:02.115395 [info ] [MainThread]: 
[0m22:22:02.115671 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:22:02.115840 [info ] [MainThread]: 
[0m22:22:02.116005 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "o" does not have a column named "amount"
  LINE 16:         o.amount,
                   ^
[0m22:22:02.116168 [info ] [MainThread]: 
[0m22:22:02.116343 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:22:02.116713 [debug] [MainThread]: Command `dbt run` failed at 22:22:02.116663 after 0.50 seconds
[0m22:22:02.117061 [debug] [MainThread]: Flushing usage events


============================== 22:22:59.719705 | 9b3d8d86-cf7a-4ec8-bf10-48aaa854e30c ==============================
[0m22:22:59.719705 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:22:59.723041 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:22:59.723342 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:22:59.807093 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:22:59.827271 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:22:59.863188 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:22:59.863473 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:22:59.864460 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:22:59.882029 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:22:59.883660 [info ] [MainThread]: 
[0m22:22:59.884161 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:22:59.884950 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:22:59.892007 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:22:59.892289 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:22:59.892483 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:22:59.908851 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.909934 [debug] [ThreadPool]: On list_dbt: Close
[0m22:22:59.912185 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:22:59.912629 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:22:59.915634 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:59.915844 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:22:59.916006 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:59.922813 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.923081 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:59.923252 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:22:59.923494 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.924013 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:22:59.924178 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:22:59.924326 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:22:59.924542 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.924703 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:22:59.927760 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:22:59.931383 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:22:59.931590 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:22:59.931743 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:59.938492 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.938759 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:22:59.938933 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:22:59.957196 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.958233 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:22:59.960258 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:22:59.960439 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:22:59.963101 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:22:59.965825 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:22:59.966038 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:22:59.966194 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:59.973128 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.973444 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:22:59.973638 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:22:59.992530 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:22:59.993399 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:22:59.993639 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:22:59.993799 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:22:59.997369 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:22:59.999110 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:22:59.999326 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:22:59.999486 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:23:00.006631 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:23:00.006917 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:23:00.007095 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:23:00.023191 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:23:00.026814 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:23:00.027065 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:23:00.027229 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:23:00.029816 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:23:00.032217 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:23:00.032420 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:23:00.032570 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:23:00.039432 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:23:00.039723 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:23:00.039908 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:23:00.055364 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:23:00.058998 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:23:00.059256 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:23:00.059427 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:23:00.063423 [debug] [MainThread]: Using duckdb connection "master"
[0m22:23:00.063636 [debug] [MainThread]: On master: BEGIN
[0m22:23:00.063793 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:23:00.070228 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:23:00.070486 [debug] [MainThread]: On master: COMMIT
[0m22:23:00.070647 [debug] [MainThread]: Using duckdb connection "master"
[0m22:23:00.070795 [debug] [MainThread]: On master: COMMIT
[0m22:23:00.071001 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:23:00.071158 [debug] [MainThread]: On master: Close
[0m22:23:00.072700 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:23:00.072910 [info ] [MainThread]: 
[0m22:23:00.075610 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:23:00.075894 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:23:00.076272 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity)
[0m22:23:00.076472 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:23:00.084461 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:23:00.085525 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:23:00.076609 => 22:23:00.085349
[0m22:23:00.085815 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:23:00.115896 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:23:00.116511 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:23:00.116853 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:23:00.117071 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:23:00.123532 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:23:00.123854 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:23:00.124092 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here

    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:23:00.125024 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:23:00.085974 => 22:23:00.124920
[0m22:23:00.125239 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: ROLLBACK
[0m22:23:00.129574 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity'
[0m22:23:00.129781 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:23:00.131897 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date_key"
  LINE 23:         d.date_key as close_date_key
                   ^
[0m22:23:00.132329 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.06s]
[0m22:23:00.132659 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:23:00.133463 [debug] [MainThread]: Using duckdb connection "master"
[0m22:23:00.133715 [debug] [MainThread]: On master: BEGIN
[0m22:23:00.133876 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:23:00.140544 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:23:00.141014 [debug] [MainThread]: On master: COMMIT
[0m22:23:00.141216 [debug] [MainThread]: Using duckdb connection "master"
[0m22:23:00.141373 [debug] [MainThread]: On master: COMMIT
[0m22:23:00.141575 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:23:00.141774 [debug] [MainThread]: On master: Close
[0m22:23:00.143710 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:23:00.143880 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:23:00.144060 [info ] [MainThread]: 
[0m22:23:00.144243 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:23:00.144575 [debug] [MainThread]: Command end result
[0m22:23:00.152212 [info ] [MainThread]: 
[0m22:23:00.152552 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:23:00.152731 [info ] [MainThread]: 
[0m22:23:00.152909 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "d" does not have a column named "date_key"
  LINE 23:         d.date_key as close_date_key
                   ^
[0m22:23:00.153078 [info ] [MainThread]: 
[0m22:23:00.153262 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:23:00.153632 [debug] [MainThread]: Command `dbt run` failed at 22:23:00.153585 after 0.46 seconds
[0m22:23:00.153839 [debug] [MainThread]: Flushing usage events


============================== 22:24:17.916404 | ad35447f-57f5-453b-b979-34ed2ce9a0f7 ==============================
[0m22:24:17.916404 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:24:17.919568 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/dim_date.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:24:17.919835 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:24:18.005303 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:24:18.024356 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:24:18.059487 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:24:18.059768 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:24:18.060770 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:24:18.076115 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:24:18.077718 [info ] [MainThread]: 
[0m22:24:18.078203 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:24:18.078878 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:24:18.086427 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:24:18.086687 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:24:18.086855 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:18.104079 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.105114 [debug] [ThreadPool]: On list_dbt: Close
[0m22:24:18.107587 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:24:18.108089 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:24:18.111038 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:18.111236 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:24:18.111395 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:18.120514 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.120760 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:18.120922 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:24:18.121164 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.121669 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:24:18.121837 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:18.121985 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:24:18.122193 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.122361 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:24:18.125539 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m22:24:18.129172 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:18.129435 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:24:18.129635 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:18.136810 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.137092 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:18.137269 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:24:18.153063 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.156749 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:24:18.157438 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:24:18.157635 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:24:18.159964 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:24:18.162427 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:18.162653 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:24:18.162802 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:18.169538 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.169832 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:18.170019 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:24:18.190138 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.191216 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:24:18.191455 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:24:18.191620 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:24:18.194124 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:24:18.196544 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:18.196728 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:24:18.196885 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:18.203836 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.204090 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:18.204261 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:24:18.220891 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.226130 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:24:18.226375 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:24:18.226542 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:24:18.231349 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:24:18.234550 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:18.234809 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:24:18.234968 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:18.242286 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.242565 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:18.242741 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:24:18.261292 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:18.262054 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:24:18.262312 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:24:18.262484 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:24:18.264988 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:18.265188 [debug] [MainThread]: On master: BEGIN
[0m22:24:18.265336 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:18.271810 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:18.272072 [debug] [MainThread]: On master: COMMIT
[0m22:24:18.272227 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:18.272373 [debug] [MainThread]: On master: COMMIT
[0m22:24:18.272571 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:18.272729 [debug] [MainThread]: On master: Close
[0m22:24:18.274269 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:18.274471 [info ] [MainThread]: 
[0m22:24:18.277197 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:24:18.277475 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_date ...................................... [RUN]
[0m22:24:18.277836 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_date)
[0m22:24:18.278026 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:24:18.304152 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:18.304512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:24:18.304708 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:18.311575 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:18.311863 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:18.312059 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:24:18.312430 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:18.353689 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:24:18.355962 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:24:18.278158 => 22:24:18.355827
[0m22:24:18.356197 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:24:18.374489 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m22:24:18.374969 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:18.375367 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day;
    );
  
  
[0m22:24:18.376238 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:24:18.356344 => 22:24:18.376145
[0m22:24:18.376458 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: ROLLBACK
[0m22:24:18.380149 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_date'
[0m22:24:18.380360 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:24:18.382393 [debug] [Thread-1  ]: Runtime Error in model dim_date (models/dimensions/dim_date.sql)
  Parser Error: syntax error at or near ";"
[0m22:24:18.382774 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dim.dim_date ............................. [[31mERROR[0m in 0.10s]
[0m22:24:18.383111 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:24:18.383893 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:18.384107 [debug] [MainThread]: On master: BEGIN
[0m22:24:18.384286 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:18.390697 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:18.391030 [debug] [MainThread]: On master: COMMIT
[0m22:24:18.391213 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:18.391375 [debug] [MainThread]: On master: COMMIT
[0m22:24:18.391638 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:18.391813 [debug] [MainThread]: On master: Close
[0m22:24:18.396497 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:18.397733 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_date' was properly closed.
[0m22:24:18.399854 [info ] [MainThread]: 
[0m22:24:18.400498 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m22:24:18.406943 [debug] [MainThread]: Command end result
[0m22:24:18.434483 [info ] [MainThread]: 
[0m22:24:18.435034 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:24:18.435476 [info ] [MainThread]: 
[0m22:24:18.435727 [error] [MainThread]:   Runtime Error in model dim_date (models/dimensions/dim_date.sql)
  Parser Error: syntax error at or near ";"
[0m22:24:18.435905 [info ] [MainThread]: 
[0m22:24:18.436107 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:24:18.436585 [debug] [MainThread]: Command `dbt run` failed at 22:24:18.436511 after 0.55 seconds
[0m22:24:18.436835 [debug] [MainThread]: Flushing usage events


============================== 22:24:24.624780 | 82cc520c-2307-43e8-9861-f4c541fdadf7 ==============================
[0m22:24:24.624780 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:24:24.628024 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_date.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:24:24.628332 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:24:24.710393 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:24:24.729610 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:24:24.765262 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:24:24.765768 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/dimensions/dim_date.sql
[0m22:24:24.853460 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:24:24.866584 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:24:24.867947 [info ] [MainThread]: 
[0m22:24:24.868370 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:24:24.869083 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:24:24.873401 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:24:24.873644 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:24:24.873829 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:24.884583 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.885685 [debug] [ThreadPool]: On list_dbt: Close
[0m22:24:24.887992 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:24:24.888464 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:24:24.891318 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:24.891507 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:24:24.891662 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:24.898471 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.898764 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:24.898933 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:24:24.899184 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.899714 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:24:24.899888 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:24:24.900035 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:24:24.900379 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.900650 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:24:24.904130 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_dim)
[0m22:24:24.907619 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:24.907851 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:24:24.908011 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:24.914804 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.915096 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:24.915272 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:24:24.932084 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.936104 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:24:24.937523 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:24:24.937871 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:24:24.940156 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:24:24.941691 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:24.942776 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:24:24.943071 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:24.950097 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.950377 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:24.950563 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:24:24.972359 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:24.973379 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:24:24.973629 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:24:24.973800 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:24:24.976282 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:24:25.006410 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:25.006868 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:24:25.007070 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:25.014501 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:25.014805 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:25.014979 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:24:25.033998 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:25.034879 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:24:25.035145 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:24:25.035319 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:24:25.037771 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:24:25.039507 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:25.039784 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:24:25.040005 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:25.046683 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:25.046888 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:25.047064 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:24:25.064165 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:25.068270 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:24:25.068607 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:24:25.068783 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:24:25.072396 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:25.072663 [debug] [MainThread]: On master: BEGIN
[0m22:24:25.072824 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:25.081571 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:25.081855 [debug] [MainThread]: On master: COMMIT
[0m22:24:25.082023 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:25.082182 [debug] [MainThread]: On master: COMMIT
[0m22:24:25.082389 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:25.082558 [debug] [MainThread]: On master: Close
[0m22:24:25.085018 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:25.085221 [info ] [MainThread]: 
[0m22:24:25.086776 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:24:25.087146 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_date ...................................... [RUN]
[0m22:24:25.087560 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_date)
[0m22:24:25.087770 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:24:25.099946 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.100312 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:24:25.100501 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:25.107247 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.107572 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.107768 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:24:25.108154 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.118908 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:24:25.119454 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:24:25.087908 => 22:24:25.119351
[0m22:24:25.119765 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:24:25.138508 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m22:24:25.139120 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.139527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m22:24:25.188466 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.192524 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.192776 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m22:24:25.193227 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.195071 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.195284 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m22:24:25.195617 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.207317 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:24:25.207740 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.208078 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:24:25.212401 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.215929 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:24:25.216253 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m22:24:25.217066 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:25.217950 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:24:25.119905 => 22:24:25.217849
[0m22:24:25.218299 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:24:25.260914 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_date ................................. [[32mOK[0m in 0.17s]
[0m22:24:25.261333 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:24:25.262214 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:25.262418 [debug] [MainThread]: On master: BEGIN
[0m22:24:25.262581 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:25.269333 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:25.269629 [debug] [MainThread]: On master: COMMIT
[0m22:24:25.269810 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:25.269978 [debug] [MainThread]: On master: COMMIT
[0m22:24:25.270230 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:25.270410 [debug] [MainThread]: On master: Close
[0m22:24:25.272306 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:25.272570 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_date' was properly closed.
[0m22:24:25.272774 [info ] [MainThread]: 
[0m22:24:25.272969 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.40 seconds (0.40s).
[0m22:24:25.273422 [debug] [MainThread]: Command end result
[0m22:24:25.282059 [info ] [MainThread]: 
[0m22:24:25.282374 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:24:25.282554 [info ] [MainThread]: 
[0m22:24:25.282739 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:24:25.283081 [debug] [MainThread]: Command `dbt run` succeeded at 22:24:25.283029 after 0.69 seconds
[0m22:24:25.283278 [debug] [MainThread]: Flushing usage events


============================== 22:24:30.255554 | cc0e5a11-c6a5-406c-966d-98c1b6222f62 ==============================
[0m22:24:30.255554 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:24:30.258034 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:24:30.258281 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:24:30.364511 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:24:30.382989 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:24:30.418080 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:24:30.418361 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:24:30.419288 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m22:24:30.432493 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:24:30.433917 [info ] [MainThread]: 
[0m22:24:30.434356 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:24:30.435104 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:24:30.442348 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:24:30.442641 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:24:30.442817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:30.451042 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.452009 [debug] [ThreadPool]: On list_dbt: Close
[0m22:24:30.454194 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:24:30.454697 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:24:30.457863 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:30.458111 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:24:30.458273 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:30.465215 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.465469 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:30.465633 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:24:30.465886 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.466395 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:30.466570 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:30.466717 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:30.466924 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.467083 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:24:30.470383 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:24:30.473898 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:30.474103 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:24:30.474258 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:30.481366 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.481641 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:30.481818 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:24:30.497799 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.501289 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:24:30.501711 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:24:30.501884 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:24:30.504117 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:24:30.506417 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:30.506598 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:24:30.506746 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:30.512913 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.513157 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:30.513343 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:24:30.529043 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.532584 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:24:30.532900 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:24:30.533062 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:24:30.535449 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:24:30.538011 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:30.538291 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:24:30.538505 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:30.545243 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.545524 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:30.545703 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:24:30.563822 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.564940 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:24:30.565232 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:24:30.565412 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:24:30.567842 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:24:30.571085 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:30.571288 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:24:30.571448 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:30.578220 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.578486 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:30.578661 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:24:30.596589 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:30.597291 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:24:30.597518 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:24:30.597675 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:24:30.600184 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:30.600393 [debug] [MainThread]: On master: BEGIN
[0m22:24:30.600548 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:30.606916 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:30.607177 [debug] [MainThread]: On master: COMMIT
[0m22:24:30.607336 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:30.607491 [debug] [MainThread]: On master: COMMIT
[0m22:24:30.607692 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:30.607851 [debug] [MainThread]: On master: Close
[0m22:24:30.609643 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:30.609913 [info ] [MainThread]: 
[0m22:24:30.611010 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:24:30.611356 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:24:30.611782 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_opportunity)
[0m22:24:30.611983 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:24:30.619937 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:24:30.621573 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:24:30.612121 => 22:24:30.621414
[0m22:24:30.621790 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:24:30.650999 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:24:30.651638 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:24:30.651843 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:24:30.652019 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:30.658800 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:30.659097 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:24:30.659307 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here

    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:24:30.662906 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:30.673115 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:24:30.673392 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:24:30.673584 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:24:30.674303 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:30.674785 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:24:30.621930 => 22:24:30.674697
[0m22:24:30.674989 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:24:30.689712 [info ] [Thread-1  ]: 1 of 1 OK created sql incremental model fact.fact_opportunity .................. [[32mOK[0m in 0.08s]
[0m22:24:30.690137 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:24:30.690902 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:30.691085 [debug] [MainThread]: On master: BEGIN
[0m22:24:30.691236 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:30.697812 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:30.698089 [debug] [MainThread]: On master: COMMIT
[0m22:24:30.698258 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:30.698420 [debug] [MainThread]: On master: COMMIT
[0m22:24:30.698719 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:30.698909 [debug] [MainThread]: On master: Close
[0m22:24:30.700719 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:30.700991 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:24:30.701179 [info ] [MainThread]: 
[0m22:24:30.701375 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m22:24:30.701825 [debug] [MainThread]: Command end result
[0m22:24:30.709400 [info ] [MainThread]: 
[0m22:24:30.709688 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:24:30.709866 [info ] [MainThread]: 
[0m22:24:30.710058 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:24:30.710511 [debug] [MainThread]: Command `dbt run` succeeded at 22:24:30.710445 after 0.47 seconds
[0m22:24:30.710738 [debug] [MainThread]: Flushing usage events


============================== 22:24:46.561706 | 1a7050c5-8a81-46a0-a0ac-841542910fab ==============================
[0m22:24:46.561706 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:24:46.565399 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:24:46.565935 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:24:46.642187 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:24:46.662842 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:24:46.709344 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:24:46.709652 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:24:46.710605 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:24:46.725442 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:24:46.726850 [info ] [MainThread]: 
[0m22:24:46.727284 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:24:46.728002 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:24:46.735351 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:24:46.735627 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:24:46.735802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:46.751690 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.752726 [debug] [ThreadPool]: On list_dbt: Close
[0m22:24:46.754854 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:24:46.755404 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:24:46.758387 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:46.758605 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:24:46.758758 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.766054 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.766359 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:46.766527 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:24:46.766767 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.767295 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:46.767468 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:46.767613 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:46.767827 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.767992 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:24:46.771284 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:24:46.775100 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:46.775423 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:24:46.775673 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.783027 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.783335 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:46.783532 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:24:46.801409 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.802572 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:24:46.803249 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:24:46.803470 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:24:46.806359 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:24:46.809216 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:46.809438 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:24:46.809596 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.816541 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.816804 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:46.816976 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:24:46.832615 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.836265 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:24:46.836616 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:24:46.836798 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:24:46.839165 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:24:46.840739 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:46.840931 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:24:46.841090 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.848626 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.848875 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:46.849053 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:24:46.864543 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.867877 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:24:46.868111 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:24:46.868271 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:24:46.870756 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:24:46.873165 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:46.873375 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:24:46.873529 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:46.880242 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.880500 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:46.880669 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:24:46.896243 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:46.900549 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:24:46.900855 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:24:46.901039 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:24:46.904025 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:46.904209 [debug] [MainThread]: On master: BEGIN
[0m22:24:46.904363 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:46.911180 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:46.911436 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.911601 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:46.911753 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.911950 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:46.912110 [debug] [MainThread]: On master: Close
[0m22:24:46.914245 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:46.914472 [info ] [MainThread]: 
[0m22:24:46.916104 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:46.916470 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:24:46.918280 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:24:46.918518 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:46.924606 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:46.925813 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:24:46.918678 => 22:24:46.925639
[0m22:24:46.926100 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:46.954606 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:46.955183 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:46.955390 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:24:46.955572 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:46.962236 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:46.962600 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:46.962865 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id;
    );
  
  
  
[0m22:24:46.963396 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:24:46.926245 => 22:24:46.963289
[0m22:24:46.963605 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:24:46.967661 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:24:46.967968 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:24:46.970050 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Parser Error: syntax error at or near ";"
[0m22:24:46.970484 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:24:46.970843 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:46.971651 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:46.971860 [debug] [MainThread]: On master: BEGIN
[0m22:24:46.972021 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:46.978963 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:46.979252 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.979417 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:46.979570 [debug] [MainThread]: On master: COMMIT
[0m22:24:46.979777 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:46.979945 [debug] [MainThread]: On master: Close
[0m22:24:46.981770 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:46.981944 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:24:46.982120 [info ] [MainThread]: 
[0m22:24:46.982305 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:24:46.982636 [debug] [MainThread]: Command end result
[0m22:24:46.990008 [info ] [MainThread]: 
[0m22:24:46.990274 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:24:46.990441 [info ] [MainThread]: 
[0m22:24:46.990596 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Parser Error: syntax error at or near ";"
[0m22:24:46.990758 [info ] [MainThread]: 
[0m22:24:46.990943 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:24:46.991342 [debug] [MainThread]: Command `dbt run` failed at 22:24:46.991287 after 0.45 seconds
[0m22:24:46.991546 [debug] [MainThread]: Flushing usage events


============================== 22:24:52.267034 | a4fc9014-414b-461f-a27c-8eece2e72c51 ==============================
[0m22:24:52.267034 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:24:52.269428 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:24:52.269680 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:24:52.354065 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:24:52.374040 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:24:52.417458 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:24:52.417931 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m22:24:52.446319 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m22:24:52.460130 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:24:52.461766 [info ] [MainThread]: 
[0m22:24:52.462264 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:24:52.462882 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:24:52.470335 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:24:52.470663 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:24:52.470840 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:52.479238 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.480202 [debug] [ThreadPool]: On list_dbt: Close
[0m22:24:52.482519 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:24:52.483038 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:24:52.485933 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:52.486127 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:24:52.486288 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:52.493369 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.493678 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:52.493852 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:24:52.494109 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.494618 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:52.494793 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:24:52.494942 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:24:52.495153 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.495318 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:24:52.498480 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m22:24:52.502108 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:52.502318 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:24:52.502473 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:52.509638 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.509903 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:24:52.510078 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:24:52.526187 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.529854 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:24:52.531310 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:24:52.531515 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:24:52.534030 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:24:52.535937 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:52.536159 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:24:52.536315 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:52.542526 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.542806 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:24:52.542998 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:24:52.559282 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.563047 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:24:52.563298 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:24:52.563463 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:24:52.566114 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:24:52.568430 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:52.569436 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:24:52.569612 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:52.576248 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.576497 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:24:52.576672 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:24:52.595124 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.596096 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:24:52.596347 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:24:52.596518 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:24:52.598884 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:24:52.600404 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:52.601558 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:24:52.601717 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:52.608611 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.608876 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:24:52.609053 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:24:52.624343 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:24:52.627739 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:24:52.627974 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:24:52.628131 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:24:52.631241 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:52.631443 [debug] [MainThread]: On master: BEGIN
[0m22:24:52.631597 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:52.637670 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:52.637919 [debug] [MainThread]: On master: COMMIT
[0m22:24:52.638081 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:52.638232 [debug] [MainThread]: On master: COMMIT
[0m22:24:52.638439 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:52.638602 [debug] [MainThread]: On master: Close
[0m22:24:52.640235 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:52.640442 [info ] [MainThread]: 
[0m22:24:52.642048 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:52.642339 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:24:52.642712 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:24:52.642910 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:52.648723 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:52.649306 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:24:52.643047 => 22:24:52.649190
[0m22:24:52.649513 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:52.679023 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:52.679680 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:52.679898 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:24:52.680088 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:52.686978 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:24:52.687231 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:24:52.687448 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:24:52.688308 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:24:52.649651 => 22:24:52.688210
[0m22:24:52.688514 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:24:52.692245 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:24:52.692562 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:24:52.694673 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:24:52.695124 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:24:52.695483 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:24:52.696277 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:52.696482 [debug] [MainThread]: On master: BEGIN
[0m22:24:52.696646 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:24:52.703194 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:52.703456 [debug] [MainThread]: On master: COMMIT
[0m22:24:52.703617 [debug] [MainThread]: Using duckdb connection "master"
[0m22:24:52.703770 [debug] [MainThread]: On master: COMMIT
[0m22:24:52.703971 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:24:52.704132 [debug] [MainThread]: On master: Close
[0m22:24:52.705888 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:52.706176 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:24:52.706367 [info ] [MainThread]: 
[0m22:24:52.706575 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m22:24:52.706955 [debug] [MainThread]: Command end result
[0m22:24:52.739248 [info ] [MainThread]: 
[0m22:24:52.739541 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:24:52.739770 [info ] [MainThread]: 
[0m22:24:52.740079 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:24:52.740275 [info ] [MainThread]: 
[0m22:24:52.740464 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:24:52.740831 [debug] [MainThread]: Command `dbt run` failed at 22:24:52.740776 after 0.49 seconds
[0m22:24:52.741040 [debug] [MainThread]: Flushing usage events


============================== 22:25:23.765101 | c842815c-afb6-4159-8971-7429760f0a16 ==============================
[0m22:25:23.765101 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:25:23.768395 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:25:23.768686 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:25:23.844763 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:25:23.863346 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:25:23.898905 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:25:23.899188 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:25:23.900139 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:25:23.915319 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:25:23.916916 [info ] [MainThread]: 
[0m22:25:23.917397 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:25:23.918055 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:25:23.925556 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:25:23.925873 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:25:23.926045 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:25:23.942270 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.943461 [debug] [ThreadPool]: On list_dbt: Close
[0m22:25:23.945712 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:25:23.946122 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:25:23.948962 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:25:23.949235 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:25:23.949396 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:23.956499 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.956750 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:25:23.956926 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:25:23.957187 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.957699 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:25:23.957864 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:25:23.958013 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:25:23.958220 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.958377 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:25:23.961733 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:25:23.965258 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:25:23.965474 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:25:23.965627 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:23.972637 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.972930 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:25:23.973121 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:25:23.991186 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:23.992132 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:25:23.993480 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:25:23.993673 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:25:23.996183 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:25:23.998607 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:25:23.998792 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:25:23.998941 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:24.005837 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.006130 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:25:24.006311 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:25:24.022333 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.025846 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:25:24.026090 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:25:24.026254 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:25:24.028840 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:25:24.031416 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:25:24.031631 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:25:24.031791 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:24.038423 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.038671 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:25:24.038846 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:25:24.054549 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.058037 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:25:24.058276 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:25:24.058440 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:25:24.061024 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:25:24.062806 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:25:24.063016 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:25:24.063166 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:24.069747 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.069995 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:25:24.070171 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:25:24.085624 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:25:24.088989 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:25:24.089215 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:25:24.089369 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:25:24.093256 [debug] [MainThread]: Using duckdb connection "master"
[0m22:25:24.093437 [debug] [MainThread]: On master: BEGIN
[0m22:25:24.093582 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:25:24.099599 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:25:24.099842 [debug] [MainThread]: On master: COMMIT
[0m22:25:24.099998 [debug] [MainThread]: Using duckdb connection "master"
[0m22:25:24.100142 [debug] [MainThread]: On master: COMMIT
[0m22:25:24.100341 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:25:24.100501 [debug] [MainThread]: On master: Close
[0m22:25:24.102118 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:25:24.102333 [info ] [MainThread]: 
[0m22:25:24.104662 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:25:24.104952 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:25:24.105333 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:25:24.105530 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:25:24.111231 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:25:24.111776 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:25:24.105673 => 22:25:24.111660
[0m22:25:24.111976 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:25:24.141334 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:25:24.141881 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:25:24.142088 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:25:24.142269 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:25:24.148936 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:25:24.149255 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:25:24.149490 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:25:24.150377 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:25:24.112108 => 22:25:24.150264
[0m22:25:24.150603 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:25:24.154263 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:25:24.154511 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:25:24.156707 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:25:24.157143 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:25:24.157496 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:25:24.158277 [debug] [MainThread]: Using duckdb connection "master"
[0m22:25:24.158470 [debug] [MainThread]: On master: BEGIN
[0m22:25:24.158623 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:25:24.165636 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:25:24.165911 [debug] [MainThread]: On master: COMMIT
[0m22:25:24.166075 [debug] [MainThread]: Using duckdb connection "master"
[0m22:25:24.166225 [debug] [MainThread]: On master: COMMIT
[0m22:25:24.166421 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:25:24.166584 [debug] [MainThread]: On master: Close
[0m22:25:24.168687 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:25:24.169040 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:25:24.169286 [info ] [MainThread]: 
[0m22:25:24.169614 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:25:24.169999 [debug] [MainThread]: Command end result
[0m22:25:24.178215 [info ] [MainThread]: 
[0m22:25:24.178498 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:25:24.178664 [info ] [MainThread]: 
[0m22:25:24.178831 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:25:24.178994 [info ] [MainThread]: 
[0m22:25:24.179168 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:25:24.179513 [debug] [MainThread]: Command `dbt run` failed at 22:25:24.179464 after 0.44 seconds
[0m22:25:24.179713 [debug] [MainThread]: Flushing usage events


============================== 22:26:40.708348 | 81526282-a5a3-4b43-8049-eabc2f93b027 ==============================
[0m22:26:40.708348 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:26:40.711428 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select models/staging/stg_salesforce__opportunity_history.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:26:40.711692 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:26:40.795601 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:26:40.815768 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:26:40.863519 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:26:40.863849 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:26:40.864870 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:26:40.879090 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:26:40.880733 [info ] [MainThread]: 
[0m22:26:40.881222 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:26:40.881905 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:26:40.889593 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:26:40.889900 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:26:40.890081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:26:40.906228 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.907277 [debug] [ThreadPool]: On list_dbt: Close
[0m22:26:40.909487 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m22:26:40.909947 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m22:26:40.912855 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:26:40.913073 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m22:26:40.913255 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:40.920186 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.920474 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:26:40.920643 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m22:26:40.920896 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.921419 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m22:26:40.921604 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:26:40.921753 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m22:26:40.921965 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.922124 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m22:26:40.925341 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_staging)
[0m22:26:40.928816 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:40.929091 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:26:40.929282 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:40.936038 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.936323 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:40.936500 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:26:40.955330 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.956369 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:26:40.958268 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:26:40.958468 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:26:40.960996 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:26:40.963687 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:40.963912 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:26:40.964068 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:40.970968 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.971236 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:40.971414 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:26:40.987424 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:40.991321 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:26:40.991672 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:26:40.991852 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:26:40.994807 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:26:40.996658 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:40.996879 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:26:40.997037 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:41.004269 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:41.004532 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:41.004720 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:26:41.021822 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:41.025631 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:26:41.025997 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:26:41.026166 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:26:41.028887 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:26:41.031274 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:41.031477 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:26:41.031626 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:41.038554 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:41.038799 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:41.038968 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:26:41.054722 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:41.059161 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:26:41.059438 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:26:41.059628 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:26:41.062872 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:41.063105 [debug] [MainThread]: On master: BEGIN
[0m22:26:41.063280 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:26:41.070157 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:41.070436 [debug] [MainThread]: On master: COMMIT
[0m22:26:41.070605 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:41.070756 [debug] [MainThread]: On master: COMMIT
[0m22:26:41.070951 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:41.071117 [debug] [MainThread]: On master: Close
[0m22:26:41.072731 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:26:41.072943 [info ] [MainThread]: 
[0m22:26:41.075793 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:26:41.076086 [info ] [Thread-1  ]: 1 of 1 START sql view model staging.stg_salesforce__opportunity_history ........ [RUN]
[0m22:26:41.076459 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m22:26:41.076663 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:26:41.082046 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.083442 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 22:26:41.076796 => 22:26:41.083243
[0m22:26:41.083732 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:26:41.102188 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.103081 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.103318 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m22:26:41.103512 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:26:41.110127 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.110427 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.110649 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m22:26:41.111286 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.115350 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.115602 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m22:26:41.115972 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.117803 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.118006 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m22:26:41.118301 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.127852 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m22:26:41.128105 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.128299 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m22:26:41.128961 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.131842 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:26:41.132101 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m22:26:41.132510 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:41.133288 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 22:26:41.083877 => 22:26:41.133190
[0m22:26:41.133500 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m22:26:41.162888 [info ] [Thread-1  ]: 1 of 1 OK created sql view model staging.stg_salesforce__opportunity_history ... [[32mOK[0m in 0.09s]
[0m22:26:41.163310 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:26:41.164073 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:41.164268 [debug] [MainThread]: On master: BEGIN
[0m22:26:41.164422 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:26:41.170999 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:41.171281 [debug] [MainThread]: On master: COMMIT
[0m22:26:41.171447 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:41.171599 [debug] [MainThread]: On master: COMMIT
[0m22:26:41.171796 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:41.171957 [debug] [MainThread]: On master: Close
[0m22:26:41.173668 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:26:41.174001 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.stg_salesforce__opportunity_history' was properly closed.
[0m22:26:41.174253 [info ] [MainThread]: 
[0m22:26:41.174467 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m22:26:41.174846 [debug] [MainThread]: Command end result
[0m22:26:41.182197 [info ] [MainThread]: 
[0m22:26:41.182501 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:26:41.182673 [info ] [MainThread]: 
[0m22:26:41.182865 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:26:41.183242 [debug] [MainThread]: Command `dbt run` succeeded at 22:26:41.183193 after 0.50 seconds
[0m22:26:41.183476 [debug] [MainThread]: Flushing usage events


============================== 22:26:46.030295 | e00cd055-a8a9-4601-ae97-67b9cd4ee0b0 ==============================
[0m22:26:46.030295 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:26:46.032692 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:26:46.032948 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:26:46.107277 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:26:46.125682 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:26:46.162507 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:26:46.162785 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:26:46.163742 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:26:46.177923 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:26:46.179558 [info ] [MainThread]: 
[0m22:26:46.180049 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:26:46.180689 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:26:46.188064 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:26:46.188350 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:26:46.188525 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:26:46.197443 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.198466 [debug] [ThreadPool]: On list_dbt: Close
[0m22:26:46.200794 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:26:46.201316 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:26:46.204321 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:46.204558 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:26:46.204725 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:46.211781 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.212056 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:46.212230 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:26:46.212484 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.212992 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:26:46.213163 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:46.213311 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:26:46.213519 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.213682 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:26:46.216780 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:26:46.220278 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:46.220495 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:26:46.220656 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:46.228081 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.228417 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:46.228617 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:26:46.246004 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.250055 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:26:46.250528 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:26:46.250722 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:26:46.253470 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:26:46.256260 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:46.256531 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:26:46.256705 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:46.263934 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.264277 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:46.264472 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:26:46.282678 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.283859 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:26:46.284106 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:26:46.284273 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:26:46.286789 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:26:46.289244 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:46.289435 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:26:46.289589 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:46.296283 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.296536 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:46.296714 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:26:46.312643 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.316380 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:26:46.316635 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:26:46.316805 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:26:46.319180 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:26:46.320850 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:46.321048 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:26:46.321201 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:46.328095 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.328362 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:46.328540 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:26:46.344006 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:46.347491 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:26:46.347794 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:26:46.347966 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:26:46.352203 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:46.352453 [debug] [MainThread]: On master: BEGIN
[0m22:26:46.352611 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:26:46.358943 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:46.359262 [debug] [MainThread]: On master: COMMIT
[0m22:26:46.359432 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:46.359584 [debug] [MainThread]: On master: COMMIT
[0m22:26:46.359820 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:46.359977 [debug] [MainThread]: On master: Close
[0m22:26:46.361607 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:26:46.361829 [info ] [MainThread]: 
[0m22:26:46.362970 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:46.363246 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:26:46.363625 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:26:46.363825 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:46.369567 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:46.371559 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:26:46.363962 => 22:26:46.371374
[0m22:26:46.371824 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:46.401603 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:46.402261 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:46.402461 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:26:46.402643 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:26:46.409601 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:46.409911 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:46.410128 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:26:46.411004 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:26:46.371973 => 22:26:46.410894
[0m22:26:46.411213 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:26:46.415470 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:26:46.415669 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:26:46.417652 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:26:46.418085 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:26:46.418449 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:46.419216 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:46.419460 [debug] [MainThread]: On master: BEGIN
[0m22:26:46.419623 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:26:46.426111 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:46.426480 [debug] [MainThread]: On master: COMMIT
[0m22:26:46.426753 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:46.426974 [debug] [MainThread]: On master: COMMIT
[0m22:26:46.427299 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:46.427585 [debug] [MainThread]: On master: Close
[0m22:26:46.429378 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:26:46.429612 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:26:46.429797 [info ] [MainThread]: 
[0m22:26:46.429990 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:26:46.430346 [debug] [MainThread]: Command end result
[0m22:26:46.438785 [info ] [MainThread]: 
[0m22:26:46.439134 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:26:46.439334 [info ] [MainThread]: 
[0m22:26:46.439518 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "opportunity_id"
  LINE 25:     on h.opportunity_id = o.opportunity_id
                  ^
[0m22:26:46.439697 [info ] [MainThread]: 
[0m22:26:46.439900 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:26:46.440351 [debug] [MainThread]: Command `dbt run` failed at 22:26:46.440296 after 0.43 seconds
[0m22:26:46.440568 [debug] [MainThread]: Flushing usage events


============================== 22:26:59.347372 | 0f108e18-f8d7-4251-8b9d-d8a47906ba1b ==============================
[0m22:26:59.347372 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:26:59.350552 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:26:59.350856 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:26:59.428555 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:26:59.448819 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:26:59.497600 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:26:59.498100 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m22:26:59.526087 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:26:59.539571 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:26:59.541249 [info ] [MainThread]: 
[0m22:26:59.541736 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:26:59.542418 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:26:59.549641 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:26:59.549916 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:26:59.550094 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:26:59.565501 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.566568 [debug] [ThreadPool]: On list_dbt: Close
[0m22:26:59.568965 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:26:59.569393 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:26:59.572385 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:59.572595 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:26:59.572755 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:59.579782 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.580059 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:59.580226 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:26:59.580474 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.580998 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:26:59.581169 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:26:59.581316 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:26:59.581536 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.581698 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:26:59.584636 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m22:26:59.588092 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:59.588361 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:26:59.588517 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:59.594837 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.595094 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:26:59.595264 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:26:59.611334 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.614857 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:26:59.616842 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:26:59.617101 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:26:59.619938 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:26:59.621760 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:59.621985 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:26:59.622156 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:59.629216 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.629500 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:26:59.629678 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:26:59.649351 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.650582 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:26:59.650873 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:26:59.651041 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:26:59.653625 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:26:59.656931 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:59.657155 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:26:59.657317 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:59.664211 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.664466 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:26:59.664646 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:26:59.680143 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.683826 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:26:59.684084 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:26:59.684252 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:26:59.686554 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m22:26:59.688772 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:59.688953 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:26:59.689106 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:59.695489 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.695747 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:26:59.695925 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:26:59.711024 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:26:59.714464 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:26:59.714695 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:26:59.714851 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:26:59.717801 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:59.717977 [debug] [MainThread]: On master: BEGIN
[0m22:26:59.718124 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:26:59.724242 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:59.724502 [debug] [MainThread]: On master: COMMIT
[0m22:26:59.724668 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:59.724828 [debug] [MainThread]: On master: COMMIT
[0m22:26:59.725035 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:59.725195 [debug] [MainThread]: On master: Close
[0m22:26:59.726785 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:26:59.726992 [info ] [MainThread]: 
[0m22:26:59.729510 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:59.729792 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:26:59.730156 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:26:59.730351 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:59.736057 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:59.736627 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:26:59.730488 => 22:26:59.736514
[0m22:26:59.736837 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:59.767105 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:59.767767 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:59.767980 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:26:59.768170 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:26:59.775055 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:26:59.775357 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:26:59.775572 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:26:59.776481 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:26:59.736972 => 22:26:59.776371
[0m22:26:59.776699 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:26:59.780281 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:26:59.780535 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:26:59.782545 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:26:59.782967 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:26:59.783327 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:26:59.784084 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:59.784265 [debug] [MainThread]: On master: BEGIN
[0m22:26:59.784415 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:26:59.790897 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:59.791335 [debug] [MainThread]: On master: COMMIT
[0m22:26:59.791564 [debug] [MainThread]: Using duckdb connection "master"
[0m22:26:59.791746 [debug] [MainThread]: On master: COMMIT
[0m22:26:59.792059 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:26:59.792245 [debug] [MainThread]: On master: Close
[0m22:26:59.794301 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:26:59.794588 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:26:59.794791 [info ] [MainThread]: 
[0m22:26:59.795209 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:26:59.795763 [debug] [MainThread]: Command end result
[0m22:26:59.857056 [info ] [MainThread]: 
[0m22:26:59.857366 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:26:59.857545 [info ] [MainThread]: 
[0m22:26:59.857764 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:26:59.857974 [info ] [MainThread]: 
[0m22:26:59.858167 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:26:59.858520 [debug] [MainThread]: Command `dbt run` failed at 22:26:59.858466 after 0.54 seconds
[0m22:26:59.858723 [debug] [MainThread]: Flushing usage events


============================== 22:27:44.942210 | a425a544-ef67-4d70-a805-ac82a24a5160 ==============================
[0m22:27:44.942210 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:27:44.945111 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dim/dim_opportunity.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:27:44.945366 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:27:45.021633 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:27:45.041471 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:27:45.088319 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:27:45.088610 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:27:45.089553 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:27:45.104365 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:27:45.105361 [warn ] [MainThread]: The selection criterion 'models/dim/dim_opportunity.sql' does not match any nodes
[0m22:27:45.106039 [info ] [MainThread]: 
[0m22:27:45.106232 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:27:45.106517 [debug] [MainThread]: Command end result
[0m22:27:45.113965 [debug] [MainThread]: Command `dbt run` succeeded at 22:27:45.113867 after 0.20 seconds
[0m22:27:45.114245 [debug] [MainThread]: Flushing usage events


============================== 22:27:53.969018 | 561587ce-a144-4787-9072-995849635f82 ==============================
[0m22:27:53.969018 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:27:53.972046 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/dimensions/dim_opportunity.sql', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:27:53.972301 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:27:54.056326 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:27:54.075123 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:27:54.118924 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:27:54.119233 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:27:54.120257 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:27:54.134210 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:27:54.135602 [info ] [MainThread]: 
[0m22:27:54.136020 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:27:54.136555 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:27:54.143845 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:27:54.144126 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:27:54.144309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:27:54.162820 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.163948 [debug] [ThreadPool]: On list_dbt: Close
[0m22:27:54.166112 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m22:27:54.166501 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:27:54.169412 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:27:54.169601 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:27:54.169753 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:54.176647 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.176929 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:27:54.177098 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:27:54.177347 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.177852 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:27:54.178014 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:27:54.178159 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:27:54.178371 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.178530 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:27:54.181630 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m22:27:54.185222 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:27:54.185419 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:27:54.185574 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:54.192741 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.192996 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:27:54.193179 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:27:54.211183 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.212135 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:27:54.214601 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:27:54.214769 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:27:54.217282 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:27:54.219694 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:27:54.219867 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:27:54.220013 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:54.226823 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.227119 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:27:54.227301 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:27:54.246430 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.251052 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:27:54.251323 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:27:54.251488 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:27:54.254396 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:27:54.256847 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:27:54.257033 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:27:54.257188 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:54.263799 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.264047 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:27:54.264222 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:27:54.279686 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.282969 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:27:54.283262 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:27:54.283514 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:27:54.285790 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:27:54.287408 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:27:54.287600 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:27:54.287755 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:54.294353 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.294597 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:27:54.294767 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:27:54.310210 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:54.313672 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:27:54.313899 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:27:54.314057 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:27:54.317854 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:54.318031 [debug] [MainThread]: On master: BEGIN
[0m22:27:54.318175 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:27:54.324465 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:54.324713 [debug] [MainThread]: On master: COMMIT
[0m22:27:54.324875 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:54.325029 [debug] [MainThread]: On master: COMMIT
[0m22:27:54.325224 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:54.325382 [debug] [MainThread]: On master: Close
[0m22:27:54.326923 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:27:54.327145 [info ] [MainThread]: 
[0m22:27:54.328804 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m22:27:54.329082 [info ] [Thread-1  ]: 1 of 1 START sql table model dim.dim_opportunity ............................... [RUN]
[0m22:27:54.329459 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_opportunity)
[0m22:27:54.329651 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m22:27:54.335384 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.336677 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 22:27:54.329787 => 22:27:54.336560
[0m22:27:54.336889 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m22:27:54.356210 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.356829 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.357036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m22:27:54.357220 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:27:54.363933 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.364192 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.364439 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m22:27:54.368291 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.372095 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.372342 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m22:27:54.372749 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.374522 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.374719 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m22:27:54.375038 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.385954 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m22:27:54.386197 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.386377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m22:27:54.387732 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.390658 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:27:54.390879 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m22:27:54.391367 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:54.392135 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 22:27:54.337024 => 22:27:54.392037
[0m22:27:54.392333 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m22:27:54.426347 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dim.dim_opportunity .......................... [[32mOK[0m in 0.10s]
[0m22:27:54.426777 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m22:27:54.427595 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:54.427787 [debug] [MainThread]: On master: BEGIN
[0m22:27:54.427960 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:27:54.434642 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:54.434966 [debug] [MainThread]: On master: COMMIT
[0m22:27:54.435141 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:54.435318 [debug] [MainThread]: On master: COMMIT
[0m22:27:54.435585 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:54.435823 [debug] [MainThread]: On master: Close
[0m22:27:54.437710 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:27:54.437903 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.dim_opportunity' was properly closed.
[0m22:27:54.438088 [info ] [MainThread]: 
[0m22:27:54.438282 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.30 seconds (0.30s).
[0m22:27:54.438645 [debug] [MainThread]: Command end result
[0m22:27:54.445845 [info ] [MainThread]: 
[0m22:27:54.446118 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:27:54.446290 [info ] [MainThread]: 
[0m22:27:54.446478 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:27:54.446865 [debug] [MainThread]: Command `dbt run` succeeded at 22:27:54.446815 after 0.50 seconds
[0m22:27:54.447072 [debug] [MainThread]: Flushing usage events


============================== 22:27:59.455544 | 82ae3549-6468-4506-a2e0-8766ed27f9dd ==============================
[0m22:27:59.455544 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:27:59.457901 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:27:59.458163 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:27:59.533846 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:27:59.551920 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:27:59.589161 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:27:59.589450 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:27:59.590416 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:27:59.604773 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:27:59.606350 [info ] [MainThread]: 
[0m22:27:59.606830 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:27:59.607536 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:27:59.615229 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:27:59.615580 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:27:59.615767 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:27:59.624605 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.625547 [debug] [ThreadPool]: On list_dbt: Close
[0m22:27:59.627631 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:27:59.628149 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:27:59.631322 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:27:59.631564 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:27:59.631723 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:59.638586 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.638850 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:27:59.639021 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:27:59.639273 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.639762 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:27:59.639925 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:27:59.640071 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:27:59.640289 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.640452 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:27:59.643433 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:27:59.646923 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:27:59.647126 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:27:59.647279 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:59.654222 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.654506 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:27:59.654692 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:27:59.670113 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.673662 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:27:59.674105 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:27:59.674281 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:27:59.676761 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:27:59.678988 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:27:59.679165 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:27:59.679309 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:59.685567 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.685828 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:27:59.686000 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:27:59.704412 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.705491 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:27:59.705757 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:27:59.705934 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:27:59.709097 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:27:59.711707 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:27:59.711911 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:27:59.712073 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:59.718817 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.719091 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:27:59.719273 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:27:59.734970 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.738451 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:27:59.738700 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:27:59.738863 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:27:59.741353 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:27:59.742914 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:27:59.743110 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:27:59.743270 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:27:59.750496 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.750726 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:27:59.750904 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:27:59.766129 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:27:59.769482 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:27:59.769710 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:27:59.769868 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:27:59.773733 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:59.773958 [debug] [MainThread]: On master: BEGIN
[0m22:27:59.774115 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:27:59.780354 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:59.780605 [debug] [MainThread]: On master: COMMIT
[0m22:27:59.780761 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:59.780908 [debug] [MainThread]: On master: COMMIT
[0m22:27:59.781128 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:59.781285 [debug] [MainThread]: On master: Close
[0m22:27:59.783187 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:27:59.783456 [info ] [MainThread]: 
[0m22:27:59.784641 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:27:59.784964 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m22:27:59.785416 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:27:59.785628 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:27:59.791459 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:27:59.793003 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:27:59.785763 => 22:27:59.792827
[0m22:27:59.793262 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:27:59.822856 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:27:59.823533 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:27:59.823747 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:27:59.823931 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:27:59.830546 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:27:59.830853 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:27:59.831072 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:27:59.832028 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:27:59.793408 => 22:27:59.831927
[0m22:27:59.832244 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:27:59.835852 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:27:59.836090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:27:59.838275 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:27:59.838731 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m22:27:59.839076 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:27:59.839888 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:59.840104 [debug] [MainThread]: On master: BEGIN
[0m22:27:59.840269 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:27:59.846813 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:59.847080 [debug] [MainThread]: On master: COMMIT
[0m22:27:59.847246 [debug] [MainThread]: Using duckdb connection "master"
[0m22:27:59.847420 [debug] [MainThread]: On master: COMMIT
[0m22:27:59.847638 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:27:59.847798 [debug] [MainThread]: On master: Close
[0m22:27:59.849700 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:27:59.850050 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:27:59.850312 [info ] [MainThread]: 
[0m22:27:59.850522 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m22:27:59.850890 [debug] [MainThread]: Command end result
[0m22:27:59.858645 [info ] [MainThread]: 
[0m22:27:59.858932 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:27:59.859102 [info ] [MainThread]: 
[0m22:27:59.859270 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:27:59.859434 [info ] [MainThread]: 
[0m22:27:59.859612 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:27:59.859993 [debug] [MainThread]: Command `dbt run` failed at 22:27:59.859932 after 0.42 seconds
[0m22:27:59.860192 [debug] [MainThread]: Flushing usage events


============================== 22:28:43.341582 | f6368a7d-8755-42cb-b805-4bb412ded9df ==============================
[0m22:28:43.341582 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:28:43.344813 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/dimensions/fact_opportunity.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:28:43.345085 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:28:43.430266 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:28:43.450753 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:28:43.502343 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:28:43.502666 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:28:43.503697 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:28:43.519464 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:28:43.520485 [warn ] [MainThread]: The selection criterion 'models/dimensions/fact_opportunity.sql' does not match any nodes
[0m22:28:43.521205 [info ] [MainThread]: 
[0m22:28:43.521415 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:28:43.521801 [debug] [MainThread]: Command end result
[0m22:28:43.529791 [debug] [MainThread]: Command `dbt run` succeeded at 22:28:43.529686 after 0.21 seconds
[0m22:28:43.530091 [debug] [MainThread]: Flushing usage events


============================== 22:28:51.869377 | 3e23db34-bec6-485a-8dce-26d6be658111 ==============================
[0m22:28:51.869377 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:28:51.872012 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:28:51.872268 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:28:51.948866 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:28:51.967471 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:28:52.011756 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:28:52.012082 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:28:52.013155 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:28:52.027750 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:28:52.029402 [info ] [MainThread]: 
[0m22:28:52.029859 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:28:52.030516 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:28:52.037982 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:28:52.038270 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:28:52.038450 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:28:52.054328 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.055402 [debug] [ThreadPool]: On list_dbt: Close
[0m22:28:52.057536 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:28:52.057981 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:28:52.060879 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:28:52.061114 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:28:52.061268 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:52.068350 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.068587 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:28:52.068799 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:28:52.069201 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.069806 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:28:52.070003 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:28:52.070159 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:28:52.070413 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.070575 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:28:52.074099 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:28:52.078058 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:28:52.078290 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:28:52.078452 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:52.088641 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.088912 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:28:52.089084 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:28:52.106597 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.110796 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:28:52.111378 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:28:52.111555 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:28:52.114128 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:28:52.116864 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:28:52.117103 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:28:52.117264 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:52.124710 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.125070 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:28:52.125274 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:28:52.145752 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.146816 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:28:52.147059 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:28:52.147230 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:28:52.149993 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:28:52.152785 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:28:52.152978 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:28:52.153131 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:52.160145 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.160411 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:28:52.160588 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:28:52.176876 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.181457 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:28:52.181698 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:28:52.181857 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:28:52.184369 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m22:28:52.186019 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:28:52.186387 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:28:52.186574 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:52.193446 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.193753 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:28:52.193948 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:28:52.209579 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:28:52.213277 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:28:52.213539 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:28:52.213706 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:28:52.217871 [debug] [MainThread]: Using duckdb connection "master"
[0m22:28:52.218063 [debug] [MainThread]: On master: BEGIN
[0m22:28:52.218214 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:28:52.224590 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:28:52.224847 [debug] [MainThread]: On master: COMMIT
[0m22:28:52.225006 [debug] [MainThread]: Using duckdb connection "master"
[0m22:28:52.225153 [debug] [MainThread]: On master: COMMIT
[0m22:28:52.225349 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:28:52.225514 [debug] [MainThread]: On master: Close
[0m22:28:52.227177 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:28:52.227407 [info ] [MainThread]: 
[0m22:28:52.228962 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:28:52.229240 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:28:52.229602 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity)
[0m22:28:52.229796 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:28:52.238530 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:28:52.239594 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:28:52.229929 => 22:28:52.239429
[0m22:28:52.239877 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:28:52.268958 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:28:52.269350 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821222852255418"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name as owner_username,
        d.date_key as close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Use the correct column name here

    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:28:52.269580 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:28:52.277724 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:28:52.240030 => 22:28:52.277539
[0m22:28:52.278020 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:28:52.283271 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Not implemented Error: Cannot perform non-inner join on subquery!
[0m22:28:52.283717 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.05s]
[0m22:28:52.284064 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:28:52.284941 [debug] [MainThread]: Using duckdb connection "master"
[0m22:28:52.285177 [debug] [MainThread]: On master: BEGIN
[0m22:28:52.285368 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:28:52.291789 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:28:52.292074 [debug] [MainThread]: On master: COMMIT
[0m22:28:52.292245 [debug] [MainThread]: Using duckdb connection "master"
[0m22:28:52.292399 [debug] [MainThread]: On master: COMMIT
[0m22:28:52.292616 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:28:52.292777 [debug] [MainThread]: On master: Close
[0m22:28:52.294584 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:28:52.294884 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:28:52.295142 [info ] [MainThread]: 
[0m22:28:52.295367 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m22:28:52.295739 [debug] [MainThread]: Command end result
[0m22:28:52.303181 [info ] [MainThread]: 
[0m22:28:52.303540 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:28:52.303724 [info ] [MainThread]: 
[0m22:28:52.303885 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Not implemented Error: Cannot perform non-inner join on subquery!
[0m22:28:52.304053 [info ] [MainThread]: 
[0m22:28:52.304232 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:28:52.304659 [debug] [MainThread]: Command `dbt run` failed at 22:28:52.304605 after 0.46 seconds
[0m22:28:52.305015 [debug] [MainThread]: Flushing usage events


============================== 22:29:53.216856 | 96456032-53b3-4d1f-b5cd-2a3005dd98fa ==============================
[0m22:29:53.216856 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:29:53.220064 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:29:53.220337 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:29:53.308516 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:29:53.327497 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:29:53.397046 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:29:53.397570 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m22:29:53.429869 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m22:29:53.445498 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:29:53.447200 [info ] [MainThread]: 
[0m22:29:53.447844 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:29:53.448478 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:29:53.455903 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:29:53.456245 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:29:53.456435 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:29:53.481178 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.482190 [debug] [ThreadPool]: On list_dbt: Close
[0m22:29:53.484396 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:29:53.484825 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:29:53.487737 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:53.487931 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:29:53.488087 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:53.494815 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.495084 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:53.495245 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:29:53.495489 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.496031 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:29:53.496202 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:53.496351 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:29:53.496556 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.496718 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:29:53.499764 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:29:53.503171 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:29:53.503417 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:29:53.504310 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:53.511129 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.511410 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:29:53.511593 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:29:53.527995 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.531770 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:29:53.532419 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:29:53.532606 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:29:53.535321 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:29:53.537789 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:29:53.537989 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:29:53.538140 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:53.544643 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.544903 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:29:53.545071 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:29:53.563443 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.564447 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:29:53.564690 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:29:53.564852 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:29:53.567271 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:29:53.569719 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:29:53.569909 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:29:53.570072 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:53.576586 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.576846 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:29:53.577015 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:29:53.592400 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.595797 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:29:53.596025 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:29:53.596179 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:29:53.598773 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m22:29:53.601195 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:29:53.601407 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:29:53.601564 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:53.608228 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.608443 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:29:53.608615 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:29:53.625365 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:53.628988 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:29:53.629238 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:29:53.629406 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:29:53.632382 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:53.632650 [debug] [MainThread]: On master: BEGIN
[0m22:29:53.632834 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:29:53.639395 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:53.639641 [debug] [MainThread]: On master: COMMIT
[0m22:29:53.639805 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:53.639956 [debug] [MainThread]: On master: COMMIT
[0m22:29:53.640160 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:53.640322 [debug] [MainThread]: On master: Close
[0m22:29:53.642294 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:29:53.642576 [info ] [MainThread]: 
[0m22:29:53.645018 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:29:53.645339 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:29:53.645757 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_opportunity)
[0m22:29:53.645956 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:29:53.653735 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:53.654743 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:29:53.646100 => 22:29:53.654572
[0m22:29:53.655027 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:29:53.707835 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:53.708206 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821222953695300"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data;
    );
  
  
  
[0m22:29:53.708432 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:29:53.715250 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:29:53.655208 => 22:29:53.715092
[0m22:29:53.715517 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:29:53.722037 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:29:53.722463 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity .............. [[31mERROR[0m in 0.08s]
[0m22:29:53.722813 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:29:53.723577 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:53.723768 [debug] [MainThread]: On master: BEGIN
[0m22:29:53.723921 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:29:53.730721 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:53.731008 [debug] [MainThread]: On master: COMMIT
[0m22:29:53.731168 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:53.731319 [debug] [MainThread]: On master: COMMIT
[0m22:29:53.731527 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:53.731686 [debug] [MainThread]: On master: Close
[0m22:29:53.733416 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:29:53.733613 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:29:53.733794 [info ] [MainThread]: 
[0m22:29:53.733975 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m22:29:53.734308 [debug] [MainThread]: Command end result
[0m22:29:53.741268 [info ] [MainThread]: 
[0m22:29:53.741648 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:29:53.741916 [info ] [MainThread]: 
[0m22:29:53.742096 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Parser Error: syntax error at or near ";"
[0m22:29:53.742268 [info ] [MainThread]: 
[0m22:29:53.742447 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:29:53.742789 [debug] [MainThread]: Command `dbt run` failed at 22:29:53.742739 after 0.55 seconds
[0m22:29:53.742993 [debug] [MainThread]: Flushing usage events


============================== 22:29:59.249932 | 35fabaae-e0b8-49fb-a668-feb7d019f103 ==============================
[0m22:29:59.249932 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:29:59.252606 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:29:59.252867 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:29:59.331372 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:29:59.350718 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:29:59.387404 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:29:59.387979 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m22:29:59.419524 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:29:59.432814 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:29:59.434246 [info ] [MainThread]: 
[0m22:29:59.434706 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:29:59.435377 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:29:59.442359 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:29:59.442670 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:29:59.442852 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:29:59.451442 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.452420 [debug] [ThreadPool]: On list_dbt: Close
[0m22:29:59.454811 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:29:59.455400 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:29:59.458254 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:59.458458 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:29:59.458614 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:59.465211 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.465499 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:59.465674 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:29:59.465928 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.466458 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:29:59.466629 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:29:59.466778 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:29:59.466989 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.467152 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:29:59.470455 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:29:59.474281 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:29:59.475025 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:29:59.475186 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:59.481505 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.481775 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:29:59.481950 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:29:59.497806 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.501389 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:29:59.502829 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:29:59.503010 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:29:59.505428 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:29:59.507670 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:29:59.507858 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:29:59.508012 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:59.515451 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.515751 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:29:59.515946 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:29:59.532623 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.536303 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:29:59.536685 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:29:59.536872 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:29:59.539543 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:29:59.541314 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:29:59.541521 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:29:59.541687 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:59.548619 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.548885 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:29:59.549074 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:29:59.566063 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.569964 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:29:59.570205 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:29:59.570365 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:29:59.573010 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:29:59.575416 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:29:59.575619 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:29:59.575776 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:59.582374 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.582625 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:29:59.582801 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:29:59.601178 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:29:59.602118 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:29:59.602349 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:29:59.602506 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:29:59.605640 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:59.605817 [debug] [MainThread]: On master: BEGIN
[0m22:29:59.605963 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:29:59.612186 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:59.612432 [debug] [MainThread]: On master: COMMIT
[0m22:29:59.612592 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:59.612749 [debug] [MainThread]: On master: COMMIT
[0m22:29:59.612945 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:59.613103 [debug] [MainThread]: On master: Close
[0m22:29:59.614762 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:29:59.615008 [info ] [MainThread]: 
[0m22:29:59.617166 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:29:59.617462 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m22:29:59.617834 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_opportunity)
[0m22:29:59.618032 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:29:59.625163 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.625749 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:29:59.618170 => 22:29:59.625638
[0m22:29:59.625958 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:29:59.678572 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.678966 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821222959666127"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:29:59.679198 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:29:59.690131 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.694581 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.694831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:29:59.695116 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.695305 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.695508 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240821222959666127'
      
      
      order by ordinal_position

  
[0m22:29:59.722912 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.726103 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.726354 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:29:59.740251 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.745289 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.745613 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:29:59.759573 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.768623 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.769233 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.769476 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240821222959666127"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240821222959666127"
    )
  
[0m22:29:59.771268 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.808539 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:29:59.809438 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:29:59.809973 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:29:59.811066 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:29:59.811990 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:29:59.626098 => 22:29:59.811726
[0m22:29:59.812343 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:29:59.815307 [info ] [Thread-1  ]: 1 of 1 OK created sql incremental model fact.fact_opportunity .................. [[32mOK[0m in 0.20s]
[0m22:29:59.815728 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:29:59.816594 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:59.816870 [debug] [MainThread]: On master: BEGIN
[0m22:29:59.817030 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:29:59.824180 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:59.824504 [debug] [MainThread]: On master: COMMIT
[0m22:29:59.824694 [debug] [MainThread]: Using duckdb connection "master"
[0m22:29:59.824990 [debug] [MainThread]: On master: COMMIT
[0m22:29:59.825245 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:29:59.825436 [debug] [MainThread]: On master: Close
[0m22:29:59.827540 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:29:59.827825 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m22:29:59.828020 [info ] [MainThread]: 
[0m22:29:59.828218 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.39 seconds (0.39s).
[0m22:29:59.828642 [debug] [MainThread]: Command end result
[0m22:29:59.836579 [info ] [MainThread]: 
[0m22:29:59.836874 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:29:59.837051 [info ] [MainThread]: 
[0m22:29:59.837232 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:29:59.837569 [debug] [MainThread]: Command `dbt run` succeeded at 22:29:59.837520 after 0.61 seconds
[0m22:29:59.837765 [debug] [MainThread]: Flushing usage events


============================== 22:30:24.595388 | eb85a8e9-c3e2-4489-b7e4-0f35d1189317 ==============================
[0m22:30:24.595388 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:30:24.598200 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:30:24.598490 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:30:24.673075 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:30:24.690467 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:30:24.725771 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:30:24.726077 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:30:24.727026 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:30:24.741091 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:30:24.742535 [info ] [MainThread]: 
[0m22:30:24.742953 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:30:24.743597 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:30:24.750822 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:30:24.751130 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:30:24.751308 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:30:24.759926 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.760898 [debug] [ThreadPool]: On list_dbt: Close
[0m22:30:24.763006 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:30:24.763467 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:30:24.766512 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:30:24.766717 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:30:24.766866 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:24.773801 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.774103 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:30:24.774274 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:30:24.774522 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.775038 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:30:24.775203 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:30:24.775350 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:30:24.775557 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.775716 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:30:24.778839 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:30:24.782780 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:30:24.783023 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:30:24.783178 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:24.789617 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.789849 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:30:24.790023 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:30:24.806334 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.809971 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:30:24.811314 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:30:24.811485 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:30:24.814041 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m22:30:24.815519 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:30:24.815703 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:30:24.815854 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:24.823559 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.823801 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:30:24.823972 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:30:24.839600 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.843544 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:30:24.843851 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:30:24.844014 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:30:24.846304 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:30:24.847819 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:30:24.848247 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:30:24.848490 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:24.855529 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.855755 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:30:24.855926 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:30:24.877496 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.879112 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:30:24.879401 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:30:24.879579 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:30:24.882723 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:30:24.885565 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:30:24.885797 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:30:24.885965 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:24.892838 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.893096 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:30:24.893274 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:30:24.908858 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:30:24.912489 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:30:24.912723 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:30:24.912882 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:30:24.916728 [debug] [MainThread]: Using duckdb connection "master"
[0m22:30:24.916908 [debug] [MainThread]: On master: BEGIN
[0m22:30:24.917059 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:30:24.923419 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:30:24.923689 [debug] [MainThread]: On master: COMMIT
[0m22:30:24.923853 [debug] [MainThread]: Using duckdb connection "master"
[0m22:30:24.924008 [debug] [MainThread]: On master: COMMIT
[0m22:30:24.924212 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:30:24.924371 [debug] [MainThread]: On master: Close
[0m22:30:24.926270 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:30:24.926565 [info ] [MainThread]: 
[0m22:30:24.928687 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:30:24.928989 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:30:24.929388 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_case)
[0m22:30:24.929600 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:30:24.935598 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:30:24.937075 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:30:24.929750 => 22:30:24.936924
[0m22:30:24.937325 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:30:24.966333 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:30:24.966882 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:30:24.967091 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:30:24.967270 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:30:24.973638 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:30:24.973918 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:30:24.974138 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.created_date,
    c.last_modified_date,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.account_id = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.owner_id = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.created_date = d.date
where c.is_deleted = false
    );
  
  
  
[0m22:30:24.975238 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:30:24.937466 => 22:30:24.975099
[0m22:30:24.975484 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m22:30:24.979095 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m22:30:24.979336 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:30:24.981580 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "account_id"
  LINE 27:     on c.account_id = a.account_id
                  ^
[0m22:30:24.981990 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case ..................... [[31mERROR[0m in 0.05s]
[0m22:30:24.982319 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:30:24.983166 [debug] [MainThread]: Using duckdb connection "master"
[0m22:30:24.983367 [debug] [MainThread]: On master: BEGIN
[0m22:30:24.983520 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:30:24.990579 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:30:24.990876 [debug] [MainThread]: On master: COMMIT
[0m22:30:24.991042 [debug] [MainThread]: Using duckdb connection "master"
[0m22:30:24.991192 [debug] [MainThread]: On master: COMMIT
[0m22:30:24.991470 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:30:24.991745 [debug] [MainThread]: On master: Close
[0m22:30:24.993605 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:30:24.993796 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:30:24.993974 [info ] [MainThread]: 
[0m22:30:24.994162 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:30:24.994501 [debug] [MainThread]: Command end result
[0m22:30:25.001752 [info ] [MainThread]: 
[0m22:30:25.002026 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:30:25.002194 [info ] [MainThread]: 
[0m22:30:25.002357 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "account_id"
  LINE 27:     on c.account_id = a.account_id
                  ^
[0m22:30:25.002530 [info ] [MainThread]: 
[0m22:30:25.002716 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:30:25.003097 [debug] [MainThread]: Command `dbt run` failed at 22:30:25.003049 after 0.42 seconds
[0m22:30:25.003299 [debug] [MainThread]: Flushing usage events


============================== 22:31:08.594212 | 640a9ac1-3a90-4e29-af5d-aefd1d846f23 ==============================
[0m22:31:08.594212 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:31:08.597199 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:31:08.597470 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:31:08.673535 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:31:08.691627 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:31:08.726659 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:31:08.726949 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:31:08.727920 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m22:31:08.742130 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:31:08.743522 [info ] [MainThread]: 
[0m22:31:08.743940 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:31:08.744597 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:31:08.751574 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:31:08.751864 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:31:08.752037 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:31:08.768362 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.769585 [debug] [ThreadPool]: On list_dbt: Close
[0m22:31:08.772269 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:31:08.772751 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:31:08.775687 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:08.775876 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:31:08.776032 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:08.783592 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.783864 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:08.784026 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:31:08.784265 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.784781 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:31:08.784945 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:08.785095 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:31:08.785299 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.785457 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:31:08.788836 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:31:08.792740 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:31:08.793055 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:31:08.793320 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:08.799899 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.800178 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:31:08.800361 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:31:08.816239 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.819860 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:31:08.821333 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:31:08.821541 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:31:08.823911 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:31:08.826172 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:31:08.826366 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:31:08.826513 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:08.832873 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.833140 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:31:08.833325 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:31:08.852722 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.853756 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:31:08.853990 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:31:08.854147 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:31:08.856559 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:31:08.859241 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:31:08.859481 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:31:08.859644 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:08.866686 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.866935 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:31:08.867106 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:31:08.882446 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.885845 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:31:08.886089 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:31:08.886252 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:31:08.888904 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:31:08.890541 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:31:08.890733 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:31:08.890885 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:08.897578 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.897828 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:31:08.898000 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:31:08.913318 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:08.917516 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:31:08.917756 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:31:08.917914 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:31:08.920954 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:08.921143 [debug] [MainThread]: On master: BEGIN
[0m22:31:08.921296 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:31:08.927426 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:08.927672 [debug] [MainThread]: On master: COMMIT
[0m22:31:08.927838 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:08.927985 [debug] [MainThread]: On master: COMMIT
[0m22:31:08.928181 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:08.928334 [debug] [MainThread]: On master: Close
[0m22:31:08.929906 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:31:08.930128 [info ] [MainThread]: 
[0m22:31:08.932392 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:31:08.932668 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:31:08.933022 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_case)
[0m22:31:08.933214 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:31:08.939509 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:31:08.940123 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:31:08.933349 => 22:31:08.940015
[0m22:31:08.940328 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:31:08.970354 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:31:08.970994 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:31:08.971201 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:31:08.971389 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:31:08.978313 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:31:08.978595 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:31:08.978805 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.created_date,
    c.last_modified_date,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.accountid = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.ownerid = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.created_date = d.date
where c.is_deleted = false
    );
  
  
  
[0m22:31:08.979952 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:31:08.940474 => 22:31:08.979838
[0m22:31:08.980161 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m22:31:08.984009 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m22:31:08.984325 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:31:08.986538 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "contact_id"
  LINE 29:     on c.contact_id = ct.contact_id
                  ^
[0m22:31:08.986995 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case ..................... [[31mERROR[0m in 0.05s]
[0m22:31:08.987343 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:31:08.988164 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:08.988384 [debug] [MainThread]: On master: BEGIN
[0m22:31:08.988538 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:31:08.995505 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:08.995751 [debug] [MainThread]: On master: COMMIT
[0m22:31:08.995910 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:08.996060 [debug] [MainThread]: On master: COMMIT
[0m22:31:08.996255 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:08.996410 [debug] [MainThread]: On master: Close
[0m22:31:08.998275 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:31:08.998579 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:31:08.998771 [info ] [MainThread]: 
[0m22:31:08.998985 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:31:08.999364 [debug] [MainThread]: Command end result
[0m22:31:09.007862 [info ] [MainThread]: 
[0m22:31:09.008251 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:31:09.008437 [info ] [MainThread]: 
[0m22:31:09.008601 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Values list "c" does not have a column named "contact_id"
  LINE 29:     on c.contact_id = ct.contact_id
                  ^
[0m22:31:09.008763 [info ] [MainThread]: 
[0m22:31:09.008951 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:31:09.009431 [debug] [MainThread]: Command `dbt run` failed at 22:31:09.009346 after 0.44 seconds
[0m22:31:09.009718 [debug] [MainThread]: Flushing usage events


============================== 22:31:57.961928 | 4d144952-48c7-4b81-acfd-7a16510ccc39 ==============================
[0m22:31:57.961928 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:31:57.965015 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:31:57.965282 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:31:58.049038 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:31:58.066939 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:31:58.102537 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:31:58.102844 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:31:58.103902 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:31:58.119472 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:31:58.121141 [info ] [MainThread]: 
[0m22:31:58.121719 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:31:58.122412 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:31:58.129902 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:31:58.130184 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:31:58.130359 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:31:58.146404 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.147298 [debug] [ThreadPool]: On list_dbt: Close
[0m22:31:58.149656 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:31:58.150187 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:31:58.153198 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:58.153403 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:31:58.153556 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:58.160469 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.160780 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:58.160961 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:31:58.161226 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.161864 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:31:58.162155 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:31:58.162328 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:31:58.162606 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.162794 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:31:58.166257 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m22:31:58.170025 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:31:58.170278 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:31:58.170439 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:58.177530 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.177814 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:31:58.177991 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:31:58.193567 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.197020 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:31:58.198371 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:31:58.198652 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:31:58.201072 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m22:31:58.203531 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:31:58.203717 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:31:58.203866 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:58.210668 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.210933 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:31:58.211113 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:31:58.230248 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.231313 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:31:58.231573 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:31:58.231751 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:31:58.234372 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m22:31:58.236943 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:31:58.237142 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:31:58.237288 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:58.244192 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.244450 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:31:58.244621 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:31:58.260210 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.263794 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:31:58.264033 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:31:58.264201 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:31:58.266727 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:31:58.269701 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:31:58.269889 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:31:58.270048 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:58.276792 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.277044 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:31:58.277217 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:31:58.293097 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:31:58.296625 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:31:58.296868 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:31:58.297029 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:31:58.299605 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:58.299803 [debug] [MainThread]: On master: BEGIN
[0m22:31:58.299957 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:31:58.306472 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:58.306730 [debug] [MainThread]: On master: COMMIT
[0m22:31:58.306894 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:58.307045 [debug] [MainThread]: On master: COMMIT
[0m22:31:58.307252 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:58.307408 [debug] [MainThread]: On master: Close
[0m22:31:58.309002 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:31:58.309207 [info ] [MainThread]: 
[0m22:31:58.311728 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:31:58.312004 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:31:58.312368 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_case)
[0m22:31:58.312563 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:31:58.318837 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:31:58.319740 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:31:58.312696 => 22:31:58.319565
[0m22:31:58.320012 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:31:58.349538 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:31:58.350175 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:31:58.350381 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:31:58.350560 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:31:58.357150 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:31:58.357442 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:31:58.357650 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.createddate,
    c.lastmodifieddate,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.accountid = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contactid = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.ownerid = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.createddate = d.date
where c.isdeleted = false
    );
  
  
  
[0m22:31:58.358836 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:31:58.320152 => 22:31:58.358732
[0m22:31:58.359041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m22:31:58.362863 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m22:31:58.363100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:31:58.365152 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 33:     on c.createddate = d.date
                                  ^
[0m22:31:58.365573 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case ..................... [[31mERROR[0m in 0.05s]
[0m22:31:58.365922 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:31:58.366807 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:58.367058 [debug] [MainThread]: On master: BEGIN
[0m22:31:58.367228 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:31:58.373693 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:58.373987 [debug] [MainThread]: On master: COMMIT
[0m22:31:58.374186 [debug] [MainThread]: Using duckdb connection "master"
[0m22:31:58.374345 [debug] [MainThread]: On master: COMMIT
[0m22:31:58.374544 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:31:58.374705 [debug] [MainThread]: On master: Close
[0m22:31:58.376583 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:31:58.376846 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:31:58.377045 [info ] [MainThread]: 
[0m22:31:58.377250 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m22:31:58.377612 [debug] [MainThread]: Command end result
[0m22:31:58.385610 [info ] [MainThread]: 
[0m22:31:58.385915 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:31:58.386095 [info ] [MainThread]: 
[0m22:31:58.386266 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Table "d" does not have a column named "date"
  LINE 33:     on c.createddate = d.date
                                  ^
[0m22:31:58.386433 [info ] [MainThread]: 
[0m22:31:58.386621 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:31:58.387015 [debug] [MainThread]: Command `dbt run` failed at 22:31:58.386959 after 0.45 seconds
[0m22:31:58.387232 [debug] [MainThread]: Flushing usage events


============================== 22:32:26.160937 | 5705276b-49cf-4779-a8cd-343aab949b4e ==============================
[0m22:32:26.160937 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:32:26.163806 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:32:26.164070 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:32:26.239680 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:32:26.258350 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:32:26.293485 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:32:26.293782 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:32:26.294694 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:32:26.309919 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:32:26.311607 [info ] [MainThread]: 
[0m22:32:26.312158 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:32:26.312922 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:32:26.320483 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:32:26.320775 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:32:26.320953 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:32:26.337290 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.338314 [debug] [ThreadPool]: On list_dbt: Close
[0m22:32:26.340459 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:32:26.340879 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:32:26.343846 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:26.344046 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:32:26.344203 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:26.350885 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.351163 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:26.351335 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:32:26.351577 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.352079 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:26.352250 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:26.352398 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:26.352610 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.352774 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:32:26.355803 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:32:26.359300 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:26.359515 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:32:26.359682 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:26.366400 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.366675 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:26.366851 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:32:26.385034 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.386041 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:32:26.386751 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:32:26.386983 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:32:26.389712 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:32:26.392243 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:26.392428 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:32:26.392583 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:26.399235 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.399626 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:26.399826 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:32:26.416292 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.420218 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:32:26.420569 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:32:26.420781 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:32:26.423548 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:32:26.426099 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:26.426324 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:32:26.426478 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:26.433076 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.433325 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:26.433500 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:32:26.449268 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.453118 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:32:26.453425 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:32:26.453600 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:32:26.455954 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:32:26.458966 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:26.459182 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:32:26.459344 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:26.465780 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.466037 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:26.466215 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:32:26.481763 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:26.485235 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:32:26.485462 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:32:26.485622 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:32:26.488246 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:26.488461 [debug] [MainThread]: On master: BEGIN
[0m22:32:26.488625 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:32:26.495216 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:26.495482 [debug] [MainThread]: On master: COMMIT
[0m22:32:26.495646 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:26.495798 [debug] [MainThread]: On master: COMMIT
[0m22:32:26.496001 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:26.496175 [debug] [MainThread]: On master: Close
[0m22:32:26.497741 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:32:26.497978 [info ] [MainThread]: 
[0m22:32:26.500041 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:32:26.500321 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:32:26.500694 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_case)
[0m22:32:26.500886 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:32:26.506757 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:32:26.507306 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:32:26.501016 => 22:32:26.507199
[0m22:32:26.507510 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:32:26.536935 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:32:26.537534 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:26.537740 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:32:26.537925 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:26.544739 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:26.545030 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:26.545245 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.createddate,
    c.lastmodifieddate,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.accountid = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contactid = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.ownerid = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.createddate = d.date_day
where c.isdeleted = false
    );
  
  
  
[0m22:32:26.546485 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:32:26.507644 => 22:32:26.546381
[0m22:32:26.546696 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: ROLLBACK
[0m22:32:26.550727 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case'
[0m22:32:26.550930 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:32:26.552945 [debug] [Thread-1  ]: Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Table "u" does not have a column named "username"
  LINE 23:     u.username as owner_username,
               ^
[0m22:32:26.553349 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_case ..................... [[31mERROR[0m in 0.05s]
[0m22:32:26.553701 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:32:26.554449 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:26.554628 [debug] [MainThread]: On master: BEGIN
[0m22:32:26.554778 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:32:26.561316 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:26.561564 [debug] [MainThread]: On master: COMMIT
[0m22:32:26.561726 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:26.561878 [debug] [MainThread]: On master: COMMIT
[0m22:32:26.562086 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:26.562244 [debug] [MainThread]: On master: Close
[0m22:32:26.563925 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:32:26.564119 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:32:26.564310 [info ] [MainThread]: 
[0m22:32:26.564499 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m22:32:26.564838 [debug] [MainThread]: Command end result
[0m22:32:26.572152 [info ] [MainThread]: 
[0m22:32:26.572423 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:32:26.572792 [info ] [MainThread]: 
[0m22:32:26.573065 [error] [MainThread]:   Runtime Error in model fact_case (models/facts/fact_case.sql)
  Binder Error: Table "u" does not have a column named "username"
  LINE 23:     u.username as owner_username,
               ^
[0m22:32:26.573268 [info ] [MainThread]: 
[0m22:32:26.573466 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:32:26.573852 [debug] [MainThread]: Command `dbt run` failed at 22:32:26.573799 after 0.43 seconds
[0m22:32:26.574059 [debug] [MainThread]: Flushing usage events


============================== 22:32:42.960984 | 4db7511c-f3c6-4a8c-86a3-16390f540927 ==============================
[0m22:32:42.960984 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:32:42.964361 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_case.sql', 'send_anonymous_usage_stats': 'False'}
[0m22:32:42.964706 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:32:43.046854 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:32:43.066770 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:32:43.101226 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:32:43.101533 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:32:43.102496 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m22:32:43.118492 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:32:43.120046 [info ] [MainThread]: 
[0m22:32:43.120565 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:32:43.121226 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:32:43.129372 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:32:43.129687 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:32:43.129883 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:32:43.146561 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.147571 [debug] [ThreadPool]: On list_dbt: Close
[0m22:32:43.149608 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m22:32:43.149986 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:32:43.153043 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:43.153338 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:32:43.153503 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:43.160421 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.160645 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:43.160808 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:32:43.161056 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.161540 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:43.161702 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:43.161846 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:43.162050 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.162207 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:32:43.165321 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m22:32:43.168624 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:43.168844 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:32:43.169005 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:43.175863 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.176163 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:43.176338 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:32:43.191618 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.195157 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:32:43.198923 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:32:43.199100 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:32:43.201352 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:32:43.203607 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:43.203781 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:32:43.203930 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:43.211284 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.211545 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:43.211718 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:32:43.227618 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.231201 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:32:43.231430 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:32:43.231588 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:32:43.234029 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m22:32:43.235694 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:43.235867 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:32:43.236012 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:43.242304 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.242559 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:43.242729 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:32:43.261603 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.262663 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:32:43.262914 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:32:43.263080 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:32:43.265737 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:32:43.268418 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:43.268625 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:32:43.268795 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:43.275610 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.275883 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:43.276059 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:32:43.292233 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:43.295725 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:32:43.295995 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:32:43.296167 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:32:43.300143 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:43.300363 [debug] [MainThread]: On master: BEGIN
[0m22:32:43.300526 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:32:43.306972 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:43.307244 [debug] [MainThread]: On master: COMMIT
[0m22:32:43.307404 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:43.307552 [debug] [MainThread]: On master: COMMIT
[0m22:32:43.307754 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:43.307911 [debug] [MainThread]: On master: Close
[0m22:32:43.309580 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:32:43.309799 [info ] [MainThread]: 
[0m22:32:43.311257 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:32:43.311557 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_case .............................. [RUN]
[0m22:32:43.311959 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_case)
[0m22:32:43.312162 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:32:43.318261 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:32:43.318838 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:32:43.312306 => 22:32:43.318726
[0m22:32:43.319045 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:32:43.348408 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:32:43.349070 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:43.349281 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:32:43.349463 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:43.356260 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:43.356495 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:43.356710 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.createddate,
    c.lastmodifieddate,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.user_name as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.accountid = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contactid = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.ownerid = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.createddate = d.date_day
where c.isdeleted = false
    );
  
  
  
[0m22:32:43.361398 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:43.371171 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: COMMIT
[0m22:32:43.371428 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:43.371614 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: COMMIT
[0m22:32:43.372432 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:43.372907 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:32:43.319186 => 22:32:43.372815
[0m22:32:43.373108 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:32:43.403656 [info ] [Thread-1  ]: 1 of 1 OK created sql incremental model fact.fact_case ......................... [[32mOK[0m in 0.09s]
[0m22:32:43.404109 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:32:43.404909 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:43.405112 [debug] [MainThread]: On master: BEGIN
[0m22:32:43.405270 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:32:43.412444 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:43.412739 [debug] [MainThread]: On master: COMMIT
[0m22:32:43.412920 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:43.413080 [debug] [MainThread]: On master: COMMIT
[0m22:32:43.413286 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:43.413458 [debug] [MainThread]: On master: Close
[0m22:32:43.415352 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:32:43.415613 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_case' was properly closed.
[0m22:32:43.415798 [info ] [MainThread]: 
[0m22:32:43.415991 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.30 seconds (0.30s).
[0m22:32:43.416429 [debug] [MainThread]: Command end result
[0m22:32:43.423659 [info ] [MainThread]: 
[0m22:32:43.424046 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:32:43.424232 [info ] [MainThread]: 
[0m22:32:43.424427 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:32:43.425093 [debug] [MainThread]: Command `dbt run` succeeded at 22:32:43.424986 after 0.49 seconds
[0m22:32:43.425522 [debug] [MainThread]: Flushing usage events


============================== 22:32:50.484547 | 7e1350ab-00aa-47cd-8d12-3450257b134c ==============================
[0m22:32:50.484547 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:32:50.487123 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:32:50.489151 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:32:50.562510 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:32:50.580875 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:32:50.619822 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:32:50.620150 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:32:50.621095 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:32:50.635053 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:32:50.636847 [info ] [MainThread]: 
[0m22:32:50.637343 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:32:50.638876 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m22:32:50.646001 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:32:50.646279 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:32:50.646469 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:32:50.654833 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.655728 [debug] [ThreadPool]: On list_dbt: Close
[0m22:32:50.659559 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:32:50.659956 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:32:50.660213 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.667665 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.668462 [debug] [ThreadPool]: On list_dbt: Close
[0m22:32:50.671971 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m22:32:50.672231 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m22:32:50.672402 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.679454 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.680161 [debug] [ThreadPool]: On list_dbt: Close
[0m22:32:50.682322 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m22:32:50.682818 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m22:32:50.686007 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:32:50.686292 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m22:32:50.686480 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.694718 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.695041 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:32:50.695322 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m22:32:50.696092 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.696678 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m22:32:50.696874 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m22:32:50.697041 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m22:32:50.697329 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.697504 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m22:32:50.699986 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now create_dbt_dim)
[0m22:32:50.700529 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m22:32:50.702321 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:32:50.702584 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m22:32:50.702759 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.709817 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.710103 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:32:50.710378 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m22:32:50.710953 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.711529 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:32:50.711825 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m22:32:50.712105 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m22:32:50.712421 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.712615 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m22:32:50.714949 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now create_dbt_fact)
[0m22:32:50.715566 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m22:32:50.717869 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:50.718149 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m22:32:50.718371 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.726799 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.727257 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:50.727525 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m22:32:50.727923 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.728640 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:50.728884 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m22:32:50.729085 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m22:32:50.729392 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.730269 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m22:32:50.734431 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m22:32:50.740109 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:50.740383 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:32:50.740543 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.747593 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.747944 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:32:50.748147 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:32:50.766059 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.766945 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:32:50.767663 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:32:50.767841 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:32:50.770246 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m22:32:50.772682 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:50.772887 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:32:50.773051 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.779642 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.779902 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:32:50.780079 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:32:50.795430 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.798847 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:32:50.799077 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:32:50.799252 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:32:50.801708 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:32:50.803904 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:50.804078 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:32:50.804223 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.810810 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.811067 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:32:50.811240 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:32:50.826664 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.830077 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:32:50.830303 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:32:50.830463 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:32:50.832747 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m22:32:50.834887 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:50.835058 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:32:50.835205 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:32:50.841638 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.841903 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:32:50.842088 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:32:50.857423 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:32:50.860784 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:32:50.861016 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:32:50.861171 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:32:50.863652 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:50.863857 [debug] [MainThread]: On master: BEGIN
[0m22:32:50.864012 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:32:50.870216 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:50.870480 [debug] [MainThread]: On master: COMMIT
[0m22:32:50.870638 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:50.870789 [debug] [MainThread]: On master: COMMIT
[0m22:32:50.870994 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:50.871160 [debug] [MainThread]: On master: Close
[0m22:32:50.872764 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:32:50.872972 [info ] [MainThread]: 
[0m22:32:50.875417 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:32:50.875697 [info ] [Thread-1  ]: 1 of 32 START sql table model dim.dim_date ..................................... [RUN]
[0m22:32:50.876055 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_date)
[0m22:32:50.876243 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:32:50.902734 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:50.903079 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:32:50.903274 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:50.910055 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:50.910379 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:50.910640 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:32:50.911082 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:50.951562 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:32:50.954341 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:32:50.876379 => 22:32:50.954222
[0m22:32:50.954561 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:32:50.996244 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m22:32:50.996892 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:50.997293 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m22:32:51.044101 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.048086 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:51.048334 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m22:32:51.048765 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.050523 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:51.050726 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m22:32:51.051040 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.061335 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:32:51.061552 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:51.061751 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m22:32:51.064625 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.067629 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:32:51.067851 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m22:32:51.068362 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.069106 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:32:50.954704 => 22:32:51.069011
[0m22:32:51.069310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:32:51.097160 [info ] [Thread-1  ]: 1 of 32 OK created sql table model dim.dim_date ................................ [[32mOK[0m in 0.22s]
[0m22:32:51.097579 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:32:51.097834 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m22:32:51.098113 [info ] [Thread-1  ]: 2 of 32 START sql table model fact.fact_product_sales .......................... [RUN]
[0m22:32:51.098468 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.fact_product_sales)
[0m22:32:51.098666 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m22:32:51.100840 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m22:32:51.101916 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 22:32:51.098798 => 22:32:51.101802
[0m22:32:51.102128 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m22:32:51.105622 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_product_sales"
[0m22:32:51.106158 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m22:32:51.106360 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: BEGIN
[0m22:32:51.106541 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.112920 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.113211 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_product_sales"
[0m22:32:51.113430 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_product_sales"} */

  
    
    

    create  table
      "dbt"."fact"."fact_product_sales__dbt_tmp"
  
    as (
      

WITH source AS (
    SELECT * 
    FROM "dbt"."raw"."pricebook_entry"
)

SELECT
    ROW_NUMBER() OVER (ORDER BY createddate) AS product_sales_id,  -- Surrogate Key
    opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
    product2id AS product_fk,                                      -- Foreign Key to dim_product
    pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
    unitprice AS unit_price,
    isactive AS is_active,
    createddate AS product_sales_created_at,
    lastmodifieddate AS product_sales_last_modified_date
FROM source
    );
  
  
[0m22:32:51.114394 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 22:32:51.102262 => 22:32:51.114224
[0m22:32:51.114638 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: ROLLBACK
[0m22:32:51.118456 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_product_sales'
[0m22:32:51.118799 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_product_sales: Close
[0m22:32:51.120901 [debug] [Thread-1  ]: Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m22:32:51.121343 [error] [Thread-1  ]: 2 of 32 ERROR creating sql table model fact.fact_product_sales ................. [[31mERROR[0m in 0.02s]
[0m22:32:51.121689 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m22:32:51.121914 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:32:51.122276 [info ] [Thread-1  ]: 3 of 32 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m22:32:51.122757 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m22:32:51.122988 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m22:32:51.125142 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.125851 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 22:32:51.123125 => 22:32:51.125739
[0m22:32:51.126059 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m22:32:51.136506 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.137279 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.137573 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m22:32:51.137756 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.144153 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.144460 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.144715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m22:32:51.145533 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.147903 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.148246 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m22:32:51.148720 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.150865 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.151109 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m22:32:51.151441 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.152450 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m22:32:51.152669 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.152847 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m22:32:51.153468 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.155790 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:32:51.156011 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m22:32:51.156378 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.157098 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 22:32:51.126191 => 22:32:51.157006
[0m22:32:51.157299 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m22:32:51.170848 [info ] [Thread-1  ]: 3 of 32 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.05s]
[0m22:32:51.171267 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:32:51.171513 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:32:51.171839 [info ] [Thread-1  ]: 4 of 32 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m22:32:51.172210 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m22:32:51.172417 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:32:51.174482 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.174958 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 22:32:51.172558 => 22:32:51.174858
[0m22:32:51.175176 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:32:51.178036 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.178671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.178914 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m22:32:51.179092 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.185757 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.186043 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.186279 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m22:32:51.187041 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.189193 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.189404 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m22:32:51.189731 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.191436 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.191641 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m22:32:51.191929 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.192764 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m22:32:51.192955 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.193130 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m22:32:51.193830 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.196432 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:32:51.196677 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m22:32:51.197106 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.197948 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 22:32:51.175318 => 22:32:51.197839
[0m22:32:51.198206 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m22:32:51.211796 [info ] [Thread-1  ]: 4 of 32 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.04s]
[0m22:32:51.212232 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:32:51.212481 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:32:51.212722 [info ] [Thread-1  ]: 5 of 32 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m22:32:51.213164 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m22:32:51.213367 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m22:32:51.215458 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.215930 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 22:32:51.213501 => 22:32:51.215829
[0m22:32:51.216127 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m22:32:51.224763 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.225412 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.226651 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m22:32:51.231317 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.259164 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.259536 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.259840 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m22:32:51.260706 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.263093 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.263344 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m22:32:51.263703 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.265448 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.265653 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m22:32:51.265943 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.266845 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m22:32:51.267036 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.267208 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m22:32:51.267756 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.269329 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:32:51.269531 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m22:32:51.269956 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.270745 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 22:32:51.216256 => 22:32:51.270649
[0m22:32:51.270956 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m22:32:51.284873 [info ] [Thread-1  ]: 5 of 32 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.07s]
[0m22:32:51.285293 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:32:51.285533 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:32:51.285905 [info ] [Thread-1  ]: 6 of 32 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m22:32:51.286395 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m22:32:51.286608 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:32:51.289399 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.290199 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 22:32:51.286761 => 22:32:51.290081
[0m22:32:51.290417 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:32:51.293163 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.293636 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.293836 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m22:32:51.294019 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.300953 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.301254 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.301464 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m22:32:51.301910 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.304084 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.304296 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m22:32:51.304601 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.306291 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.306516 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m22:32:51.306854 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.307793 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m22:32:51.308159 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.308367 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m22:32:51.308986 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.310809 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:32:51.311014 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m22:32:51.311385 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.312186 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 22:32:51.290548 => 22:32:51.312086
[0m22:32:51.312448 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m22:32:51.326613 [info ] [Thread-1  ]: 6 of 32 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.04s]
[0m22:32:51.327027 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:32:51.327308 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:32:51.327665 [info ] [Thread-1  ]: 7 of 32 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m22:32:51.328036 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m22:32:51.328231 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:32:51.330314 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.330808 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 22:32:51.328370 => 22:32:51.330709
[0m22:32:51.331010 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:32:51.334518 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.335020 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.335223 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m22:32:51.335407 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.342527 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.342834 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.343101 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m22:32:51.343914 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.346343 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.346582 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m22:32:51.346941 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.348754 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.348981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m22:32:51.349290 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.350170 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m22:32:51.350365 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.350538 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m22:32:51.351154 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.353104 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:32:51.353372 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m22:32:51.353823 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.354672 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 22:32:51.331138 => 22:32:51.354568
[0m22:32:51.354880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m22:32:51.373364 [info ] [Thread-1  ]: 7 of 32 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.05s]
[0m22:32:51.373793 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:32:51.374042 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:32:51.374369 [info ] [Thread-1  ]: 8 of 32 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m22:32:51.374737 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m22:32:51.374931 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:32:51.377030 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.377587 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 22:32:51.375065 => 22:32:51.377476
[0m22:32:51.377810 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:32:51.381463 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.382037 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.382243 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m22:32:51.382424 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.388966 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.389260 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.389492 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m22:32:51.390310 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.392700 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.393013 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m22:32:51.393476 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.395513 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.395777 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m22:32:51.396100 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.396998 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m22:32:51.397188 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.397365 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m22:32:51.398029 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.399971 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:32:51.400196 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m22:32:51.400610 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.401537 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 22:32:51.377946 => 22:32:51.401406
[0m22:32:51.401816 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m22:32:51.415080 [info ] [Thread-1  ]: 8 of 32 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.04s]
[0m22:32:51.415507 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:32:51.415757 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:32:51.416086 [info ] [Thread-1  ]: 9 of 32 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m22:32:51.416462 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m22:32:51.416656 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:32:51.418693 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.419177 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 22:32:51.416793 => 22:32:51.419070
[0m22:32:51.419378 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:32:51.422076 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.422645 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.422875 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m22:32:51.423073 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.429930 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.430229 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.430464 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m22:32:51.431233 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.434339 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.434664 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m22:32:51.435119 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.437142 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.437359 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m22:32:51.437675 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.438582 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m22:32:51.438884 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.439089 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m22:32:51.439804 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.441620 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:32:51.441850 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m22:32:51.442240 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.443011 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 22:32:51.419505 => 22:32:51.442906
[0m22:32:51.443220 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m22:32:51.457071 [info ] [Thread-1  ]: 9 of 32 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.04s]
[0m22:32:51.457491 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:32:51.457740 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:32:51.458082 [info ] [Thread-1  ]: 10 of 32 START sql view model staging.stg_salesforce__opportunity_history ...... [RUN]
[0m22:32:51.458442 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m22:32:51.458641 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:32:51.460650 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.461143 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 22:32:51.458777 => 22:32:51.461037
[0m22:32:51.461342 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:32:51.464026 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.464524 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.464781 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m22:32:51.464963 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.471585 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.471873 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.472090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m22:32:51.472655 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.475538 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.475765 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m22:32:51.476098 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.477796 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.478018 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m22:32:51.478320 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.479161 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m22:32:51.479354 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.479529 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m22:32:51.480173 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.481946 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:32:51.482180 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m22:32:51.482572 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.483478 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 22:32:51.461470 => 22:32:51.483372
[0m22:32:51.483714 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m22:32:51.497251 [info ] [Thread-1  ]: 10 of 32 OK created sql view model staging.stg_salesforce__opportunity_history . [[32mOK[0m in 0.04s]
[0m22:32:51.497730 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:32:51.497986 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:32:51.498327 [info ] [Thread-1  ]: 11 of 32 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m22:32:51.498758 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m22:32:51.498994 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:32:51.501107 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.501615 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 22:32:51.499134 => 22:32:51.501504
[0m22:32:51.501835 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:32:51.504609 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.505051 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.505253 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m22:32:51.505442 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.512270 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.512608 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.512825 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m22:32:51.513459 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.515750 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.515990 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m22:32:51.516333 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.518747 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.518956 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m22:32:51.519259 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.520112 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m22:32:51.520302 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.520478 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m22:32:51.521080 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.523043 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:32:51.523281 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m22:32:51.523691 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.524491 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 22:32:51.501975 => 22:32:51.524385
[0m22:32:51.524710 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m22:32:51.538778 [info ] [Thread-1  ]: 11 of 32 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.04s]
[0m22:32:51.539253 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:32:51.539512 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:32:51.539852 [info ] [Thread-1  ]: 12 of 32 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m22:32:51.540261 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m22:32:51.540460 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:32:51.542476 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.543374 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 22:32:51.540591 => 22:32:51.543272
[0m22:32:51.543580 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:32:51.546461 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.546935 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.547182 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m22:32:51.547364 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.553830 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.554156 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.554385 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m22:32:51.555063 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.557552 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.557814 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m22:32:51.558144 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.560543 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.560765 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m22:32:51.561064 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.561932 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m22:32:51.562135 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.562310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m22:32:51.562891 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.564709 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:32:51.564937 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m22:32:51.565321 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.566219 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 22:32:51.543718 => 22:32:51.566120
[0m22:32:51.566441 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m22:32:51.580090 [info ] [Thread-1  ]: 12 of 32 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.04s]
[0m22:32:51.580566 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:32:51.580832 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:32:51.581081 [info ] [Thread-1  ]: 13 of 32 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m22:32:51.581575 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m22:32:51.581802 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:32:51.583813 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.584319 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 22:32:51.581946 => 22:32:51.584213
[0m22:32:51.584526 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:32:51.587262 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.588054 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.588380 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m22:32:51.588641 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.594987 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.595284 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.595495 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m22:32:51.595972 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.598185 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.598538 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m22:32:51.598953 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.601083 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.601300 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m22:32:51.601633 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.602646 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m22:32:51.602860 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.603047 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m22:32:51.603608 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.605924 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:32:51.606248 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m22:32:51.606668 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.607504 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 22:32:51.584670 => 22:32:51.607401
[0m22:32:51.607728 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m22:32:51.621999 [info ] [Thread-1  ]: 13 of 32 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.04s]
[0m22:32:51.622429 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:32:51.622671 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:32:51.622997 [info ] [Thread-1  ]: 14 of 32 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m22:32:51.623368 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m22:32:51.623564 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:32:51.625659 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.626340 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 22:32:51.623696 => 22:32:51.626238
[0m22:32:51.626544 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:32:51.629289 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.629796 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.629990 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m22:32:51.630170 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.636668 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.636965 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.637175 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m22:32:51.637683 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.640238 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.640549 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m22:32:51.640948 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.642823 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.643036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m22:32:51.643364 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.644227 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m22:32:51.644422 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.644599 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m22:32:51.645133 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.646676 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:32:51.646880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m22:32:51.647249 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.648886 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 22:32:51.626676 => 22:32:51.648785
[0m22:32:51.649106 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m22:32:51.661887 [info ] [Thread-1  ]: 14 of 32 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.04s]
[0m22:32:51.662276 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:32:51.662508 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:32:51.662949 [info ] [Thread-1  ]: 15 of 32 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m22:32:51.663430 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m22:32:51.663649 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m22:32:51.665800 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.666394 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 22:32:51.663860 => 22:32:51.666261
[0m22:32:51.666637 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m22:32:51.669468 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.670157 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.670438 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m22:32:51.670752 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.677096 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.677391 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.677645 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m22:32:51.678831 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.681322 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.681600 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m22:32:51.681986 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.683742 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.683949 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m22:32:51.684254 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.685194 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m22:32:51.685403 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.685579 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m22:32:51.686198 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.687749 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:32:51.687942 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m22:32:51.688284 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.689024 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 22:32:51.666778 => 22:32:51.688931
[0m22:32:51.689310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m22:32:51.712758 [info ] [Thread-1  ]: 15 of 32 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.05s]
[0m22:32:51.713221 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:32:51.713487 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:32:51.713783 [info ] [Thread-1  ]: 16 of 32 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m22:32:51.714176 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m22:32:51.714393 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:32:51.717306 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.717773 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 22:32:51.714543 => 22:32:51.717675
[0m22:32:51.717972 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:32:51.720805 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.721346 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.721549 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m22:32:51.721731 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.728495 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.728717 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.728927 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m22:32:51.729459 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.731603 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.731831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m22:32:51.732144 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.733933 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.734271 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m22:32:51.734714 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.735705 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m22:32:51.735918 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.736101 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m22:32:51.736749 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.738621 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:32:51.738845 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m22:32:51.739248 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.740071 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 22:32:51.718106 => 22:32:51.739973
[0m22:32:51.740293 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m22:32:51.753972 [info ] [Thread-1  ]: 16 of 32 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.04s]
[0m22:32:51.754377 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:32:51.754622 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m22:32:51.754947 [info ] [Thread-1  ]: 17 of 32 START sql table model dim.dim_account ................................. [RUN]
[0m22:32:51.755302 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m22:32:51.755511 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m22:32:51.757791 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m22:32:51.758966 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 22:32:51.755649 => 22:32:51.758853
[0m22:32:51.759182 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m22:32:51.788813 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m22:32:51.789383 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.789576 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m22:32:51.789755 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.796422 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.796714 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.797008 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

select
    account_id,
    masterrecordid as master_record_id,
    name as account_name,
    type as account_type,
    parentid as parent_account_id,
    billingstreet as billing_street,
    billingcity as billing_city,
    billingstate as billing_state,
    billingpostalcode as billing_postal_code,
    billingcountry as billing_country,
    billinglatitude as billing_latitude,
    billinglongitude as billing_longitude,
    billinggeocodeaccuracy as billing_geocode_accuracy,
    shippingstreet as shipping_street,
    shippingcity as shipping_city,
    shippingstate as shipping_state,
    shippingpostalcode as shipping_postal_code,
    shippingcountry as shipping_country,
    shippinglatitude as shipping_latitude,
    shippinglongitude as shipping_longitude,
    shippinggeocodeaccuracy as shipping_geocode_accuracy,
    phone as account_phone,
    fax as account_fax,
    accountnumber as account_number,
    website as account_website,
    sic as sic_code,
    industry as account_industry,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownership as account_ownership,
    tickersymbol as ticker_symbol,
    description as account_description,
    rating as account_rating,
    site as account_site,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    jigsaw as jigsaw_id,
    jigsawcompanyid as jigsaw_company_id,
    cleanstatus as clean_status,
    accountsource as account_source,
    dunsnumber as duns_number,
    tradestyle as trade_style,
    naicscode as naics_code,
    naicsdesc as naics_description,
    yearstarted as year_started,
    sicdesc as sic_description,
    dandbcompanyid as dandb_company_id,
    operatinghoursid as operating_hours_id,
    customerpriority__c as customer_priority,
    sla__c as sla,
    active__c as is_active,
    numberoflocations__c as number_of_locations,
    upsellopportunity__c as upsell_opportunity,
    slaserialnumber__c as sla_serial_number,
    slaexpirationdate__c as sla_expiration_date
from "dbt"."staging"."stg_salesforce__account"
where isdeleted = false  -- Exclude deleted accounts
order by account_name
    );
  
  
[0m22:32:51.801460 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.803591 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.803810 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account" rename to "dim_account__dbt_backup"
[0m22:32:51.804194 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.805964 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.806178 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m22:32:51.806506 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.807499 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m22:32:51.807699 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.807884 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m22:32:51.809382 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.810948 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m22:32:51.811143 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m22:32:51.811637 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.812340 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 22:32:51.759319 => 22:32:51.812251
[0m22:32:51.812545 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m22:32:51.833848 [info ] [Thread-1  ]: 17 of 32 OK created sql table model dim.dim_account ............................ [[32mOK[0m in 0.08s]
[0m22:32:51.834256 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m22:32:51.834488 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m22:32:51.834871 [info ] [Thread-1  ]: 18 of 32 START sql table model dim.dim_campaign ................................ [RUN]
[0m22:32:51.835353 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m22:32:51.835579 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m22:32:51.837928 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.838857 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 22:32:51.835723 => 22:32:51.838717
[0m22:32:51.839082 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m22:32:51.842742 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.843248 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.843452 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m22:32:51.843627 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.849976 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.850266 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.850628 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

select
    /* IDs */
    campaign_id,
    parentid as parent_campaign_id,
    ownerid as owner_id,
    createdbyid as created_by_id,
    lastmodifiedbyid as last_modified_by_id,
    campaignmemberrecordtypeid as campaign_member_record_type_id,

    /* Dates */
    startdate as start_date,
    enddate as end_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,

    /* Dimensions */
    name as campaign_name,
    type as campaign_type,
    status,
    description,
    isactive as is_active,

    /* Metrics */
    expectedrevenue as expected_revenue,
    budgetedcost as budgeted_cost,
    actualcost as actual_cost,
    expectedresponse as expected_response,
    numbersent as number_sent,
    numberofleads as number_of_leads,
    numberofconvertedleads as number_of_converted_leads,
    numberofcontacts as number_of_contacts,
    numberofresponses as number_of_responses,
    numberofopportunities as number_of_opportunities,
    numberofwonopportunities as number_of_won_opportunities,
    amountallopportunities as amount_all_opportunities,
    amountwonopportunities as amount_won_opportunities,
    hierarchynumberofleads as hierarchy_number_of_leads,
    hierarchynumberofconvertedleads as hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts as hierarchy_number_of_contacts,
    hierarchynumberofresponses as hierarchy_number_of_responses,
    hierarchynumberofopportunities as hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities as hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities as hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities as hierarchy_amount_won_opportunities,
    hierarchynumbersent as hierarchy_number_sent,
    hierarchyexpectedrevenue as hierarchy_expected_revenue,
    hierarchybudgetedcost as hierarchy_budgeted_cost,
    hierarchyactualcost as hierarchy_actual_cost

from "dbt"."staging"."stg_salesforce__campaign"
where isdeleted = false
    );
  
  
[0m22:32:51.854239 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.856325 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.856544 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m22:32:51.856909 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.857775 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m22:32:51.857968 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.858141 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m22:32:51.859332 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.861256 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m22:32:51.861505 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."dim"."dim_campaign__dbt_backup" cascade
[0m22:32:51.861835 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.862626 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 22:32:51.839223 => 22:32:51.862528
[0m22:32:51.862834 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m22:32:51.884922 [info ] [Thread-1  ]: 18 of 32 OK created sql table model dim.dim_campaign ........................... [[32mOK[0m in 0.05s]
[0m22:32:51.885356 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m22:32:51.885602 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m22:32:51.885876 [info ] [Thread-1  ]: 19 of 32 START sql table model dim.dim_case_status ............................. [RUN]
[0m22:32:51.886249 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m22:32:51.886437 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m22:32:51.888639 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.896548 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 22:32:51.886572 => 22:32:51.896380
[0m22:32:51.896802 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m22:32:51.899455 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.900222 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.900487 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m22:32:51.900671 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.907487 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.907766 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.907981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."dim"."dim_case_status__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct status as status_name, status as status_description  -- Adjust as needed, typically a description field should be separate
        from "dbt"."staging"."stg_salesforce__case_history_2"
    )

select
    row_number() over (order by status_name) as status_id,  -- Surrogate Key
    status_name,
    status_description
from source
    );
  
  
[0m22:32:51.909906 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.912198 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.912452 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m22:32:51.912863 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.913892 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m22:32:51.914171 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.914387 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m22:32:51.914948 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.917369 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m22:32:51.917598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."dim"."dim_case_status__dbt_backup" cascade
[0m22:32:51.917988 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.918851 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 22:32:51.896941 => 22:32:51.918749
[0m22:32:51.919078 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m22:32:51.934671 [info ] [Thread-1  ]: 19 of 32 OK created sql table model dim.dim_case_status ........................ [[32mOK[0m in 0.05s]
[0m22:32:51.935113 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m22:32:51.935359 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m22:32:51.935626 [info ] [Thread-1  ]: 20 of 32 START sql table model dim.dim_contact ................................. [RUN]
[0m22:32:51.935974 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m22:32:51.936164 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m22:32:51.938380 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.938885 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 22:32:51.936296 => 22:32:51.938781
[0m22:32:51.939089 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m22:32:51.941719 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.942157 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.942350 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m22:32:51.942523 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:51.949131 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.949405 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.949659 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

select
    /* IDs */
    contact_id,
    masterrecordid as master_record_id,
    accountid as account_id,
    reportstoid as reports_to_id,
    ownerid as owner_id,
    jigsawcontactid as jigsaw_contact_id,
    individualid as individual_id,

    /* Dates */
    birthdate as birth_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    lastcurequestdate as last_cu_request_date,
    lastcuupdatedate as last_cu_update_date,
    emailbounceddate as email_bounced_date,

    /* Dimensions */
    salutation,
    firstname as first_name,
    lastname as last_name,
    otherstreet as other_street,
    othercity as other_city,
    otherstate as other_state,
    otherpostalcode as other_postal_code,
    othercountry as other_country,
    otherlatitude as other_latitude,
    otherlongitude as other_longitude,
    othergeocodeaccuracy as other_geocode_accuracy,
    mailingstreet as mailing_street,
    mailingcity as mailing_city,
    mailingstate as mailing_state,
    mailingpostalcode as mailing_postal_code,
    mailingcountry as mailing_country,
    mailinglatitude as mailing_latitude,
    mailinglongitude as mailing_longitude,
    mailinggeocodeaccuracy as mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone as mobile_phone,
    homephone as home_phone,
    otherphone as other_phone,
    assistantphone as assistant_phone,
    email,
    title,
    department,
    assistantname as assistant_name,
    leadsource as lead_source,
    description,
    pronouns,
    genderidentity as gender_identity,
    cleanstatus as clean_status,
    emailbouncedreason as email_bounced_reason,
    level__c as level,
    languages__c as languages,

    /* Metrics */
    hasoptedoutofemail as has_opted_out_of_email,
    hasoptedoutoffax as has_opted_out_of_fax,
    donotcall as do_not_call

from "dbt"."staging"."stg_salesforce__contact"
where isdeleted = false
    );
  
  
[0m22:32:51.953561 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.955920 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.956173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact" rename to "dim_contact__dbt_backup"
[0m22:32:51.956520 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.958499 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.958710 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m22:32:51.959035 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.959974 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m22:32:51.960162 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.960331 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m22:32:51.961607 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.963455 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m22:32:51.963676 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m22:32:51.964335 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:51.965160 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 22:32:51.939228 => 22:32:51.965063
[0m22:32:51.965391 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m22:32:51.989655 [info ] [Thread-1  ]: 20 of 32 OK created sql table model dim.dim_contact ............................ [[32mOK[0m in 0.05s]
[0m22:32:51.990090 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m22:32:51.990335 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m22:32:51.990606 [info ] [Thread-1  ]: 21 of 32 START sql table model dim.dim_lead .................................... [RUN]
[0m22:32:51.990969 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m22:32:51.991160 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m22:32:51.994450 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m22:32:51.994952 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 22:32:51.991291 => 22:32:51.994844
[0m22:32:51.995151 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m22:32:51.998273 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m22:32:51.998718 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:51.998901 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m22:32:51.999071 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.005617 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.005897 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:52.006167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

select
    lead_id,
    masterrecordid as master_record_id,
    salutation as lead_salutation,
    firstname as first_name,
    lastname as last_name,
    title as lead_title,
    company as lead_company,
    street as lead_street,
    city as lead_city,
    state as lead_state,
    postalcode as lead_postal_code,
    country as lead_country,
    latitude as lead_latitude,
    longitude as lead_longitude,
    geocodeaccuracy as geocode_accuracy,
    phone as lead_phone,
    mobilephone as lead_mobile_phone,
    fax as lead_fax,
    email as lead_email,
    website as lead_website,
    description as lead_description,
    leadsource as lead_source,
    status as lead_status,
    industry as lead_industry,
    rating as lead_rating,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownerid as owner_id,
    hasoptedoutofemail as has_opted_out_of_email,
    isconverted as is_converted,
    converteddate as converted_date,
    convertedaccountid as converted_account_id,
    convertedcontactid as converted_contact_id,
    convertedopportunityid as converted_opportunity_id,
    isunreadbyowner as is_unread_by_owner,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    donotcall as do_not_call,
    hasoptedoutoffax as has_opted_out_of_fax,
    lasttransferdate as last_transfer_date,
    jigsaw as jigsaw_id,
    jigsawcontactid as jigsaw_contact_id,
    cleanstatus as clean_status,
    companydunsnumber as company_duns_number,
    dandbcompanyid as dandb_company_id,
    emailbouncedreason as email_bounced_reason,
    emailbounceddate as email_bounced_date,
    individualid as individual_id,
    pronouns as lead_pronouns,
    genderidentity as gender_identity,
    siccode__c as sic_code,
    productinterest__c as product_interest,
    primary__c as is_primary,
    currentgenerators__c as current_generators,
    numberoflocations__c as number_of_locations
from "dbt"."staging"."stg_salesforce__lead"
where isdeleted = false  -- Exclude deleted leads
order by last_name, first_name
    );
  
  
[0m22:32:52.010300 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.012722 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:52.012981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead" rename to "dim_lead__dbt_backup"
[0m22:32:52.013440 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.015529 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:52.015760 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m22:32:52.016107 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.017102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m22:32:52.017299 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:52.017473 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m22:32:52.018738 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.020618 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m22:32:52.020868 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m22:32:52.021436 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.022267 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 22:32:51.995281 => 22:32:52.022165
[0m22:32:52.022486 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m22:32:52.043938 [info ] [Thread-1  ]: 21 of 32 OK created sql table model dim.dim_lead ............................... [[32mOK[0m in 0.05s]
[0m22:32:52.044388 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m22:32:52.044640 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m22:32:52.044923 [info ] [Thread-1  ]: 22 of 32 START sql table model dim.dim_opportunity ............................. [RUN]
[0m22:32:52.045289 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m22:32:52.045510 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m22:32:52.048822 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.049399 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 22:32:52.045653 => 22:32:52.049283
[0m22:32:52.049616 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m22:32:52.052464 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.052968 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.053171 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m22:32:52.053347 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.060359 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.060644 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.060893 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m22:32:52.064151 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.066577 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.066873 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m22:32:52.067340 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.069351 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.069568 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m22:32:52.069896 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.070858 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m22:32:52.071053 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.071311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m22:32:52.072827 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.074732 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m22:32:52.074968 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m22:32:52.075508 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.076397 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 22:32:52.049758 => 22:32:52.076294
[0m22:32:52.076613 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m22:32:52.097125 [info ] [Thread-1  ]: 22 of 32 OK created sql table model dim.dim_opportunity ........................ [[32mOK[0m in 0.05s]
[0m22:32:52.097592 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m22:32:52.097842 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:32:52.098163 [info ] [Thread-1  ]: 23 of 32 START sql table model dim.dim_opportunity_stage ....................... [RUN]
[0m22:32:52.098667 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m22:32:52.098928 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:32:52.101167 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.104210 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 22:32:52.099073 => 22:32:52.104065
[0m22:32:52.104432 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:32:52.108869 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.109508 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.109753 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m22:32:52.109944 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.117262 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.117589 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.117810 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct stagename as stage_name, stagesortorder as stage_sort_order
        from "dbt"."staging"."stg_salesforce__opportunity"
    )

select
    row_number() over (order by stage_sort_order) as stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
from source
    );
  
  
[0m22:32:52.121939 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.124311 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.124565 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m22:32:52.124926 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.126890 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.127152 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m22:32:52.127458 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.128449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m22:32:52.128647 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.128822 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m22:32:52.129493 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.131556 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:32:52.131820 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m22:32:52.132280 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.133172 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 22:32:52.104574 => 22:32:52.133017
[0m22:32:52.133420 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m22:32:52.148800 [info ] [Thread-1  ]: 23 of 32 OK created sql table model dim.dim_opportunity_stage .................. [[32mOK[0m in 0.05s]
[0m22:32:52.149187 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:32:52.149417 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m22:32:52.149786 [info ] [Thread-1  ]: 24 of 32 START sql table model dim.dim_pricebook ............................... [RUN]
[0m22:32:52.150305 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m22:32:52.150527 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m22:32:52.152847 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.155040 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 22:32:52.150670 => 22:32:52.154811
[0m22:32:52.155351 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m22:32:52.159667 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.160383 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.160611 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m22:32:52.160793 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.168218 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.168614 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.168987 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

select
    pricebook_entry_id,
    pricebook2id as pricebook_id,
    product2id as product_id,
    unitprice as unit_price,
    isactive as is_active,
    usestandardprice as use_standard_price,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__pricebook_entry"
where isdeleted = false  -- Exclude deleted entries
order by pricebook_entry_id
    );
  
  
[0m22:32:52.170474 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.172494 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.172718 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook" rename to "dim_pricebook__dbt_backup"
[0m22:32:52.173116 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.175356 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.175611 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m22:32:52.175974 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.177036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m22:32:52.177250 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.177434 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m22:32:52.178187 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.179850 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m22:32:52.180059 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m22:32:52.180493 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.181398 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 22:32:52.155523 => 22:32:52.181304
[0m22:32:52.181608 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m22:32:52.197488 [info ] [Thread-1  ]: 24 of 32 OK created sql table model dim.dim_pricebook .......................... [[32mOK[0m in 0.05s]
[0m22:32:52.197957 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m22:32:52.198212 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m22:32:52.198521 [info ] [Thread-1  ]: 25 of 32 START sql table model dim.dim_product ................................. [RUN]
[0m22:32:52.198940 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m22:32:52.199161 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m22:32:52.201420 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m22:32:52.201910 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 22:32:52.199313 => 22:32:52.201808
[0m22:32:52.202112 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m22:32:52.204822 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m22:32:52.205517 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.205751 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m22:32:52.206002 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.212455 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.212746 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.212966 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    productcode as product_code,
    description as product_description,
    isactive as is_active,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    family as product_family,
    externaldatasourceid as external_datasource_id,
    externalid as external_id,
    displayurl as display_url,
    quantityunitofmeasure as quantity_unit_of_measure,
    stockkeepingunit as stock_keeping_unit,
    type as product_type,
    productclass as product_class,
    sourceproductid as source_product_id,
    sellerid as seller_id
from "dbt"."staging"."stg_salesforce__product_2"
where isdeleted = false
-- isactive =1 ?
order by product_name
    );
  
  
[0m22:32:52.215218 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.218422 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.218678 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product" rename to "dim_product__dbt_backup"
[0m22:32:52.219102 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.220995 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.221216 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m22:32:52.221551 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.222656 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m22:32:52.222892 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.223087 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m22:32:52.223979 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.225756 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m22:32:52.225976 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m22:32:52.226424 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.227198 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 22:32:52.202249 => 22:32:52.227099
[0m22:32:52.227410 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m22:32:52.245175 [info ] [Thread-1  ]: 25 of 32 OK created sql table model dim.dim_product ............................ [[32mOK[0m in 0.05s]
[0m22:32:52.245625 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m22:32:52.245889 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m22:32:52.246162 [info ] [Thread-1  ]: 26 of 32 START sql table model dim.dim_record_type ............................. [RUN]
[0m22:32:52.246529 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m22:32:52.246724 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m22:32:52.249072 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.249877 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 22:32:52.246860 => 22:32:52.249776
[0m22:32:52.250080 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m22:32:52.253255 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.253887 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.254145 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m22:32:52.254335 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.261162 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.261449 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.261805 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

select
    record_type_id,
    name as record_type_name,
    modulenamespace as module_namespace,
    description as record_type_description,
    businessprocessid as business_process_id,
    sobjecttype as sobject_type,
    isactive as is_active,
    createdbyid as created_by_id,
    createddate as created_date,
    lastmodifiedbyid as last_modified_by_id,
    lastmodifieddate as last_modified_date,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__record_type"
where isdeleted = false
order by record_type_name
    );
  
  
[0m22:32:52.263745 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.265792 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.266013 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m22:32:52.266346 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.268835 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.269037 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m22:32:52.269403 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.270641 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m22:32:52.270857 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.271030 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m22:32:52.271727 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.273663 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m22:32:52.273880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m22:32:52.274388 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.275166 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 22:32:52.250231 => 22:32:52.275065
[0m22:32:52.275375 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m22:32:52.292026 [info ] [Thread-1  ]: 26 of 32 OK created sql table model dim.dim_record_type ........................ [[32mOK[0m in 0.05s]
[0m22:32:52.292466 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m22:32:52.292708 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m22:32:52.293016 [info ] [Thread-1  ]: 27 of 32 START sql table model dim.dim_solution ................................ [RUN]
[0m22:32:52.293428 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m22:32:52.293649 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m22:32:52.295943 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.296402 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 22:32:52.293799 => 22:32:52.296301
[0m22:32:52.296603 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m22:32:52.299739 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.300214 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.300612 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m22:32:52.300857 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.308641 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.308941 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.309152 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solutionnumber as solution_number,
    solutionname as solution_name,
    ispublished as is_published,
    ispublishedinpublickb as is_published_in_public_kb,
    status as solution_status,
    isreviewed as is_reviewed,
    solutionnote as solution_note,
    caseid as case_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    timesused as times_used,
    ishtml as is_html
from "dbt"."staging"."stg_salesforce__solution"
where isdeleted = false
order by solution_name
    );
  
  
[0m22:32:52.310883 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.313038 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.313273 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m22:32:52.313628 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.316004 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.316214 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m22:32:52.316522 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.317444 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m22:32:52.317628 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.317803 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m22:32:52.318621 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.320161 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m22:32:52.320361 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m22:32:52.320833 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.321821 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 22:32:52.296734 => 22:32:52.321718
[0m22:32:52.322060 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m22:32:52.340291 [info ] [Thread-1  ]: 27 of 32 OK created sql table model dim.dim_solution ........................... [[32mOK[0m in 0.05s]
[0m22:32:52.340745 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m22:32:52.340993 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m22:32:52.341290 [info ] [Thread-1  ]: 28 of 32 START sql table model dim.dim_user .................................... [RUN]
[0m22:32:52.341699 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m22:32:52.341917 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m22:32:52.344315 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m22:32:52.344983 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 22:32:52.342061 => 22:32:52.344884
[0m22:32:52.345184 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m22:32:52.348019 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m22:32:52.348569 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.348890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m22:32:52.349142 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.356116 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.356411 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.356640 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m22:32:52.360509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.362973 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.363262 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user" rename to "dim_user__dbt_backup"
[0m22:32:52.363718 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.365757 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.366002 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m22:32:52.366391 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.367406 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m22:32:52.367595 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.367765 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m22:32:52.368655 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.371460 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m22:32:52.371674 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m22:32:52.372182 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.373108 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 22:32:52.345324 => 22:32:52.373003
[0m22:32:52.373333 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m22:32:52.392690 [info ] [Thread-1  ]: 28 of 32 OK created sql table model dim.dim_user ............................... [[32mOK[0m in 0.05s]
[0m22:32:52.393102 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m22:32:52.393644 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:32:52.393925 [info ] [Thread-1  ]: 29 of 32 START sql incremental model fact.fact_case ............................ [RUN]
[0m22:32:52.394288 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_case)
[0m22:32:52.394490 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:32:52.397055 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:32:52.397648 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:32:52.394622 => 22:32:52.397553
[0m22:32:52.397843 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:32:52.417896 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.418229 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

    
  
    
    

    create temporary table
      "fact_case__dbt_tmp20240821223252414129"
  
    as (
      

select
    c.case_id,
    c.status,
    c.priority,
    c.origin,
    c.createddate,
    c.lastmodifieddate,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.user_name as owner_username,
    d.date_key as created_date_key
from "dbt"."staging"."stg_salesforce__case" c
left join "dbt"."dim"."dim_account" a
    on c.accountid = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contactid = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.ownerid = u.user_id
left join "dbt"."dim"."dim_date" d
    on c.createddate = d.date_day
where c.isdeleted = false
    );
  
  
  
[0m22:32:52.419324 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.432480 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.437067 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.437339 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: BEGIN
[0m22:32:52.437678 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.437877 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.438136 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_case__dbt_tmp20240821223252414129'
      
      
      order by ordinal_position

  
[0m22:32:52.467586 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.471793 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.472071 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_case'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:32:52.487329 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.493675 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.493976 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_case'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:32:52.508987 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.518617 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case"
[0m22:32:52.519211 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.519476 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case"} */

        
            delete from "dbt"."fact"."fact_case"
            where (
                case_id) in (
                select (case_id)
                from "fact_case__dbt_tmp20240821223252414129"
            );

        
    

    insert into "dbt"."fact"."fact_case" ("case_id", "status", "priority", "origin", "createddate", "lastmodifieddate", "account_name", "contact_first_name", "contact_last_name", "owner_username", "created_date_key")
    (
        select "case_id", "status", "priority", "origin", "createddate", "lastmodifieddate", "account_name", "contact_first_name", "contact_last_name", "owner_username", "created_date_key"
        from "fact_case__dbt_tmp20240821223252414129"
    )
  
[0m22:32:52.522104 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.523050 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: COMMIT
[0m22:32:52.523258 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case"
[0m22:32:52.523451 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: COMMIT
[0m22:32:52.524260 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.524689 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:32:52.397980 => 22:32:52.524600
[0m22:32:52.524880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case: Close
[0m22:32:52.542951 [info ] [Thread-1  ]: 29 of 32 OK created sql incremental model fact.fact_case ....................... [[32mOK[0m in 0.15s]
[0m22:32:52.543416 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:32:52.543662 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:32:52.544026 [info ] [Thread-1  ]: 30 of 32 START sql incremental model fact.fact_opportunity ..................... [RUN]
[0m22:32:52.544441 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.fact_opportunity)
[0m22:32:52.544653 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:32:52.549347 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.550230 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:32:52.544788 => 22:32:52.550061
[0m22:32:52.550517 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:32:52.553695 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.553977 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821223252552570"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m22:32:52.554195 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.563672 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.565949 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.566190 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m22:32:52.566482 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.566711 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.566929 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240821223252552570'
      
      
      order by ordinal_position

  
[0m22:32:52.596010 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.598372 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.598625 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:32:52.612551 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.615453 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.615686 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m22:32:52.630142 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.632111 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.632752 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.633070 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240821223252552570"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240821223252552570"
    )
  
[0m22:32:52.634496 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.635602 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:32:52.635828 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m22:32:52.636019 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m22:32:52.636342 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.636795 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:32:52.550735 => 22:32:52.636701
[0m22:32:52.636992 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m22:32:52.639770 [info ] [Thread-1  ]: 30 of 32 OK created sql incremental model fact.fact_opportunity ................ [[32mOK[0m in 0.10s]
[0m22:32:52.640175 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:32:52.640420 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m22:32:52.640770 [info ] [Thread-1  ]: 31 of 32 START sql incremental model fact.fact_case_history .................... [RUN]
[0m22:32:52.641196 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_case_history)
[0m22:32:52.641416 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m22:32:52.644176 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m22:32:52.644743 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 22:32:52.641560 => 22:32:52.644625
[0m22:32:52.644966 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m22:32:52.647797 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_case_history"
[0m22:32:52.648343 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:32:52.648547 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: BEGIN
[0m22:32:52.648730 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.655633 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.655927 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_case_history"
[0m22:32:52.656155 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_case_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_case_history"
  
    as (
      

select
    case_history_id,
    h.status,
    h.priority,
    h.created_date,
    h.last_modified_date,
    c.account_id,
    a.account_name,
    ct.first_name as contact_first_name,
    ct.last_name as contact_last_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__case_history_2" h
left join "dbt"."fact"."fact_case" c
    on h.case_history_id = c.case_id
left join "dbt"."dim"."dim_account" a
    on c.account_id = a.account_id
left join "dbt"."dim"."dim_contact" ct
    on c.contact_id = ct.contact_id
left join "dbt"."dim"."dim_user" u
    on c.owner_id = u.user_id
    );
  
  
  
[0m22:32:52.656901 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 22:32:52.645110 => 22:32:52.656800
[0m22:32:52.657106 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: ROLLBACK
[0m22:32:52.657640 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_case_history'
[0m22:32:52.657823 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_case_history: Close
[0m22:32:52.660537 [debug] [Thread-1  ]: Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Binder Error: Table "c" does not have a column named "account_id"
  LINE 28:     on c.account_id = a.account_id
                  ^
[0m22:32:52.661067 [error] [Thread-1  ]: 31 of 32 ERROR creating sql incremental model fact.fact_case_history ........... [[31mERROR[0m in 0.02s]
[0m22:32:52.661422 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m22:32:52.661668 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:32:52.661907 [info ] [Thread-1  ]: 32 of 32 START sql incremental model fact.fact_opportunity_history ............. [RUN]
[0m22:32:52.662316 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:32:52.662611 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:32:52.665172 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:32:52.665737 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:32:52.662778 => 22:32:52.665625
[0m22:32:52.665940 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:32:52.669038 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:32:52.669612 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:32:52.669812 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m22:32:52.669992 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:32:52.676894 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:32:52.677262 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:32:52.677522 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m22:32:52.678485 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:32:52.666074 => 22:32:52.678369
[0m22:32:52.678713 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m22:32:52.679251 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m22:32:52.679454 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m22:32:52.681753 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:32:52.682210 [error] [Thread-1  ]: 32 of 32 ERROR creating sql incremental model fact.fact_opportunity_history .... [[31mERROR[0m in 0.02s]
[0m22:32:52.682554 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:32:52.683403 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:52.683621 [debug] [MainThread]: On master: BEGIN
[0m22:32:52.683791 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:32:52.690697 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:52.690962 [debug] [MainThread]: On master: COMMIT
[0m22:32:52.691121 [debug] [MainThread]: Using duckdb connection "master"
[0m22:32:52.691269 [debug] [MainThread]: On master: COMMIT
[0m22:32:52.691467 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:32:52.691625 [debug] [MainThread]: On master: Close
[0m22:32:52.693617 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:32:52.694000 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:32:52.694353 [info ] [MainThread]: 
[0m22:32:52.694618 [info ] [MainThread]: Finished running 14 table models, 14 view models, 4 incremental models in 0 hours 0 minutes and 2.06 seconds (2.06s).
[0m22:32:52.697067 [debug] [MainThread]: Command end result
[0m22:32:52.706573 [info ] [MainThread]: 
[0m22:32:52.706876 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m22:32:52.707048 [info ] [MainThread]: 
[0m22:32:52.707237 [error] [MainThread]:   Runtime Error in model fact_product_sales (models/facts/fact_product_sales.sql)
  Binder Error: Referenced column "opportunityid" not found in FROM clause!
  Candidate bindings: "source.id"
  LINE 20:     opportunityid AS opportunity_fk,                               -- Foreign Key to fact_opportunity
      product2id AS product_fk,                                      -- Foreign Key to dim_product
      pricebook2id AS pricebook_fk,                                  -- Foreign Key to dim_pricebook (if applicable)
      unitprice AS unit_price,
      isactive AS is_active,
      createddate AS product_sales_created_at,
      lastmodifieddate AS product_sales_last_modified_date
  FROM source
      );
    
    ...
               ^
[0m22:32:52.707424 [info ] [MainThread]: 
[0m22:32:52.707585 [error] [MainThread]:   Runtime Error in model fact_case_history (models/facts/fact_case_history.sql)
  Binder Error: Table "c" does not have a column named "account_id"
  LINE 28:     on c.account_id = a.account_id
                  ^
[0m22:32:52.707749 [info ] [MainThread]: 
[0m22:32:52.707904 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m22:32:52.708084 [info ] [MainThread]: 
[0m22:32:52.708369 [info ] [MainThread]: Done. PASS=29 WARN=0 ERROR=3 SKIP=0 TOTAL=32
[0m22:32:52.708773 [debug] [MainThread]: Command `dbt run` failed at 22:32:52.708717 after 2.24 seconds
[0m22:32:52.708976 [debug] [MainThread]: Flushing usage events


============================== 22:33:31.551890 | e997c118-97c2-4988-9ac5-3c03fd28b728 ==============================
[0m22:33:31.551890 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:33:31.554941 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt test', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m22:33:31.555209 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:33:31.637117 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:33:31.657109 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:33:31.704926 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:33:31.705235 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:33:31.706212 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m22:33:31.720748 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:33:31.722052 [info ] [MainThread]: 
[0m22:33:31.722248 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:33:31.722514 [debug] [MainThread]: Command end result
[0m22:33:31.729777 [debug] [MainThread]: Command `dbt test` succeeded at 22:33:31.729663 after 0.20 seconds
[0m22:33:31.730057 [debug] [MainThread]: Flushing usage events


============================== 22:33:41.117768 | 5ecd156b-4f54-4fc0-bf99-140341fc8039 ==============================
[0m22:33:41.117768 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:33:41.120624 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'False'}
[0m22:33:41.120898 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:33:41.198140 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:33:41.216177 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:33:41.262236 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:33:41.262552 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:33:41.263534 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m22:33:41.278657 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:33:41.279942 [info ] [MainThread]: 
[0m22:33:41.280140 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:33:41.280404 [debug] [MainThread]: Command end result
[0m22:33:41.287188 [debug] [MainThread]: Command `dbt test` succeeded at 22:33:41.287117 after 0.20 seconds
[0m22:33:41.287422 [debug] [MainThread]: Flushing usage events


============================== 22:35:01.848626 | 0a7ad9a5-3c07-49a3-bf3f-fa713255c0f5 ==============================
[0m22:35:01.848626 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:35:01.851903 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt test', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:35:01.852168 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:35:01.936300 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:35:01.956998 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:35:02.008585 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:35:02.008920 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:35:02.009911 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
[0m22:35:02.024200 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:35:02.025532 [info ] [MainThread]: 
[0m22:35:02.025753 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:35:02.026030 [debug] [MainThread]: Command end result
[0m22:35:02.032686 [debug] [MainThread]: Command `dbt test` succeeded at 22:35:02.032598 after 0.20 seconds
[0m22:35:02.032949 [debug] [MainThread]: Flushing usage events


============================== 22:35:09.294044 | 90d3cc0c-6659-4911-a59f-a1f49d499be2 ==============================
[0m22:35:09.294044 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:35:09.296533 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:35:09.296800 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:35:09.370852 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:35:09.388244 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:35:09.424662 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:35:09.424968 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:35:09.425954 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:35:09.441395 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:35:09.443202 [info ] [MainThread]: 
[0m22:35:09.443650 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:35:09.445034 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt_main'
[0m22:35:09.451816 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:35:09.452105 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:35:09.452274 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:35:09.468078 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.468317 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:35:09.468490 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:35:09.484539 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.488352 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:35:09.489099 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:35:09.489386 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:35:09.491829 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:35:09.494298 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:35:09.494493 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:35:09.494646 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:09.502358 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.502641 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:35:09.502823 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:35:09.519247 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.523377 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:35:09.523711 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:35:09.523889 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:35:09.526746 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:35:09.529567 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:35:09.529764 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:35:09.529912 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:09.537206 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.537494 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:35:09.537682 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:35:09.557360 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.558535 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:35:09.558880 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:35:09.559069 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:35:09.561922 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m22:35:09.564533 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:35:09.564725 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:35:09.564882 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:09.571748 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.572005 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:35:09.572176 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:35:09.587710 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:09.591149 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:35:09.591390 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:35:09.591547 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:35:09.594306 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:35:09.594564 [info ] [MainThread]: 
[0m22:35:09.597430 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:35:09.597793 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_date)
[0m22:35:09.597990 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:35:09.625183 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:35:09.625550 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:35:09.625736 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:35:09.632644 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:35:09.632938 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:35:09.633136 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:35:09.633509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:35:09.674401 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:35:09.675526 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:35:09.598132 => 22:35:09.675339
[0m22:35:09.675807 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:35:09.676069 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:35:09.675955 => 22:35:09.675976
[0m22:35:09.676276 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: ROLLBACK
[0m22:35:09.676518 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_date'
[0m22:35:09.676705 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:35:09.678618 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:35:09.678893 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m22:35:09.679360 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.fact_product_sales)
[0m22:35:09.679622 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m22:35:09.682097 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m22:35:09.683050 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 22:35:09.679768 => 22:35:09.682930
[0m22:35:09.683268 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m22:35:09.683501 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 22:35:09.683408 => 22:35:09.683415
[0m22:35:09.684048 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m22:35:09.684337 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:09.684774 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m22:35:09.685000 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:09.687733 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:35:09.688570 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 22:35:09.685146 => 22:35:09.688407
[0m22:35:09.688840 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:09.689094 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 22:35:09.688996 => 22:35:09.689003
[0m22:35:09.689615 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:09.689862 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:09.690421 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m22:35:09.690829 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:09.693207 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:35:09.694277 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 22:35:09.691036 => 22:35:09.694143
[0m22:35:09.694524 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:09.694776 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 22:35:09.694675 => 22:35:09.694683
[0m22:35:09.695426 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:09.695713 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:09.696121 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m22:35:09.696345 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:09.699070 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:35:09.699676 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 22:35:09.696764 => 22:35:09.699556
[0m22:35:09.699903 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:09.700273 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 22:35:09.700148 => 22:35:09.700157
[0m22:35:09.700839 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:09.701109 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:09.701499 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m22:35:09.701715 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:09.703881 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:35:09.705255 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 22:35:09.701854 => 22:35:09.705078
[0m22:35:09.705558 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:09.705817 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 22:35:09.705713 => 22:35:09.705722
[0m22:35:09.706361 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:09.706603 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:09.706951 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m22:35:09.707367 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:09.709615 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:35:09.710150 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 22:35:09.707559 => 22:35:09.710040
[0m22:35:09.710363 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:09.710593 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 22:35:09.710503 => 22:35:09.710509
[0m22:35:09.711064 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:09.711295 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:09.711632 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m22:35:09.711852 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:09.714016 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:35:09.714531 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 22:35:09.712168 => 22:35:09.714423
[0m22:35:09.714749 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:09.714981 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 22:35:09.714892 => 22:35:09.714897
[0m22:35:09.715469 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:09.715699 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:09.716034 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m22:35:09.716243 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:09.744983 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:35:09.745696 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 22:35:09.716452 => 22:35:09.745585
[0m22:35:09.745908 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:09.746135 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 22:35:09.746045 => 22:35:09.746051
[0m22:35:09.746600 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:09.746823 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:09.747274 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m22:35:09.747518 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:09.749297 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:35:09.749748 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 22:35:09.747663 => 22:35:09.749637
[0m22:35:09.749954 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:09.750181 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 22:35:09.750090 => 22:35:09.750094
[0m22:35:09.750664 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:09.750881 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:09.751217 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m22:35:09.751407 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:09.753072 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:35:09.753461 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 22:35:09.751532 => 22:35:09.753364
[0m22:35:09.753653 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:09.753867 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 22:35:09.753783 => 22:35:09.753787
[0m22:35:09.754280 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:09.754489 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:09.754847 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m22:35:09.755036 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:09.756699 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:35:09.757056 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 22:35:09.755164 => 22:35:09.756963
[0m22:35:09.757246 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:09.757456 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 22:35:09.757375 => 22:35:09.757379
[0m22:35:09.757863 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:09.758085 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:09.758430 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m22:35:09.758660 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:09.760330 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:35:09.760700 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 22:35:09.758793 => 22:35:09.760607
[0m22:35:09.760892 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:09.761109 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 22:35:09.761025 => 22:35:09.761029
[0m22:35:09.761522 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:09.761727 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:09.762080 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m22:35:09.762282 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:09.763973 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:35:09.764516 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 22:35:09.762412 => 22:35:09.764420
[0m22:35:09.764704 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:09.764925 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 22:35:09.764840 => 22:35:09.764844
[0m22:35:09.765338 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:09.765543 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:09.765888 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m22:35:09.766081 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:09.768622 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:35:09.768997 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 22:35:09.766208 => 22:35:09.768899
[0m22:35:09.769189 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:09.769401 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 22:35:09.769320 => 22:35:09.769324
[0m22:35:09.769852 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:09.770076 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:09.770405 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m22:35:09.770592 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:09.772277 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:35:09.772655 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 22:35:09.770718 => 22:35:09.772557
[0m22:35:09.772856 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:09.773069 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 22:35:09.772987 => 22:35:09.772991
[0m22:35:09.773482 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:09.773698 [debug] [Thread-1  ]: Began running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:09.774059 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now seed.elastic_dbt_interview.dbt_project_evaluator_exceptions)
[0m22:35:09.774265 [debug] [Thread-1  ]: Began compiling node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:09.775487 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (compile): 22:35:09.774400 => 22:35:09.775392
[0m22:35:09.775677 [debug] [Thread-1  ]: Began executing node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:09.775891 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (execute): 22:35:09.775808 => 22:35:09.775812
[0m22:35:09.776304 [debug] [Thread-1  ]: Finished running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:09.776508 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m22:35:09.776827 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly seed.elastic_dbt_interview.dbt_project_evaluator_exceptions, now model.elastic_dbt_interview.dim_account)
[0m22:35:09.777011 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m22:35:09.778947 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m22:35:09.779613 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 22:35:09.777136 => 22:35:09.779517
[0m22:35:09.779806 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m22:35:09.780025 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 22:35:09.779940 => 22:35:09.779944
[0m22:35:09.780442 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m22:35:09.780663 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m22:35:09.781021 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m22:35:09.781222 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m22:35:09.783242 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m22:35:09.783652 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 22:35:09.781369 => 22:35:09.783560
[0m22:35:09.783840 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m22:35:09.784053 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 22:35:09.783972 => 22:35:09.783977
[0m22:35:09.784473 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m22:35:09.784680 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m22:35:09.785001 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m22:35:09.785184 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m22:35:09.787004 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m22:35:09.787644 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 22:35:09.785308 => 22:35:09.787548
[0m22:35:09.787849 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m22:35:09.788068 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 22:35:09.787984 => 22:35:09.787988
[0m22:35:09.788481 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m22:35:09.788684 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m22:35:09.788998 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m22:35:09.789181 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m22:35:09.791791 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m22:35:09.792175 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 22:35:09.789307 => 22:35:09.792083
[0m22:35:09.792368 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m22:35:09.792581 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 22:35:09.792499 => 22:35:09.792503
[0m22:35:09.792992 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m22:35:09.793196 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m22:35:09.793520 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m22:35:09.793707 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m22:35:09.795628 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m22:35:09.795987 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 22:35:09.793833 => 22:35:09.795897
[0m22:35:09.796190 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m22:35:09.796417 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 22:35:09.796330 => 22:35:09.796334
[0m22:35:09.796842 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m22:35:09.797046 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m22:35:09.797363 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m22:35:09.797545 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m22:35:09.799426 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:35:09.799803 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 22:35:09.797672 => 22:35:09.799705
[0m22:35:09.799990 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m22:35:09.800202 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 22:35:09.800120 => 22:35:09.800124
[0m22:35:09.800609 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m22:35:09.800814 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:09.801132 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m22:35:09.801316 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:09.803094 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:35:09.803478 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 22:35:09.801440 => 22:35:09.803387
[0m22:35:09.803668 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:09.803896 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 22:35:09.803814 => 22:35:09.803818
[0m22:35:09.804303 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:09.804506 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m22:35:09.804819 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m22:35:09.805001 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m22:35:09.806782 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m22:35:09.807133 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 22:35:09.805124 => 22:35:09.807043
[0m22:35:09.807320 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m22:35:09.807536 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 22:35:09.807453 => 22:35:09.807457
[0m22:35:09.807943 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m22:35:09.808143 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m22:35:09.808462 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m22:35:09.808654 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m22:35:09.810500 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m22:35:09.810865 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 22:35:09.808795 => 22:35:09.810777
[0m22:35:09.811051 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m22:35:09.811261 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 22:35:09.811180 => 22:35:09.811184
[0m22:35:09.811672 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m22:35:09.811878 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m22:35:09.812192 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m22:35:09.812375 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m22:35:09.814850 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m22:35:09.815234 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 22:35:09.812497 => 22:35:09.815134
[0m22:35:09.815425 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m22:35:09.815640 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 22:35:09.815557 => 22:35:09.815561
[0m22:35:09.816057 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m22:35:09.816261 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m22:35:09.816576 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m22:35:09.816763 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m22:35:09.818603 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m22:35:09.818971 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 22:35:09.816889 => 22:35:09.818882
[0m22:35:09.819160 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m22:35:09.819373 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 22:35:09.819292 => 22:35:09.819295
[0m22:35:09.819810 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m22:35:09.820033 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m22:35:09.820356 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m22:35:09.820537 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m22:35:09.822425 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m22:35:09.822787 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 22:35:09.820660 => 22:35:09.822696
[0m22:35:09.822975 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m22:35:09.823184 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 22:35:09.823104 => 22:35:09.823108
[0m22:35:09.823588 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m22:35:09.823961 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:35:09.824363 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_case)
[0m22:35:09.824566 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:35:09.826802 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:35:09.827182 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:35:09.824696 => 22:35:09.827090
[0m22:35:09.827375 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:35:09.827592 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:35:09.827507 => 22:35:09.827511
[0m22:35:09.828004 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:35:09.828209 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:35:09.828557 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.fact_opportunity)
[0m22:35:09.828740 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:35:09.833974 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:35:09.834424 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:35:09.828864 => 22:35:09.834323
[0m22:35:09.834619 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:35:09.834835 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:35:09.834751 => 22:35:09.834756
[0m22:35:09.835254 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:35:09.835466 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m22:35:09.835818 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_case_history)
[0m22:35:09.836006 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m22:35:09.838901 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m22:35:09.839305 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 22:35:09.836133 => 22:35:09.839212
[0m22:35:09.839503 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m22:35:09.839718 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 22:35:09.839636 => 22:35:09.839640
[0m22:35:09.840133 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m22:35:09.840340 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:09.840656 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:35:09.840840 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:09.842963 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:35:09.843332 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:35:09.840966 => 22:35:09.843240
[0m22:35:09.843522 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:09.843738 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:35:09.843654 => 22:35:09.843658
[0m22:35:09.844148 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:09.844678 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:35:09.844865 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:35:09.847010 [debug] [MainThread]: Command end result
[0m22:35:09.854837 [debug] [MainThread]: Command `dbt compile` succeeded at 22:35:09.854761 after 0.58 seconds
[0m22:35:09.855063 [debug] [MainThread]: Flushing usage events


============================== 22:35:12.526581 | a6c36d18-17a7-4ff2-aedc-89847ac81036 ==============================
[0m22:35:12.526581 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:35:12.528876 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt compile', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:35:12.529120 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:35:12.603443 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:35:12.621280 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:35:12.658970 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:35:12.659293 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:35:12.660287 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m22:35:12.674009 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:35:12.675751 [info ] [MainThread]: 
[0m22:35:12.676186 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m22:35:12.677852 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt_fact'
[0m22:35:12.684460 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:35:12.684735 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m22:35:12.684904 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:35:12.693167 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.693499 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m22:35:12.693699 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m22:35:12.710137 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.714098 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m22:35:12.714660 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m22:35:12.714840 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m22:35:12.717409 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m22:35:12.719294 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:35:12.719505 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m22:35:12.719665 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:12.727006 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.727282 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m22:35:12.727464 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m22:35:12.743041 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.746509 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m22:35:12.746750 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m22:35:12.747007 [debug] [ThreadPool]: On list_dbt_main: Close
[0m22:35:12.757577 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m22:35:12.760413 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:35:12.760741 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m22:35:12.760973 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:12.774534 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.774846 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m22:35:12.775047 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m22:35:12.791547 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.795689 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m22:35:12.796033 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m22:35:12.796210 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m22:35:12.798807 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m22:35:12.801212 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:35:12.801412 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m22:35:12.801564 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:35:12.808703 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.808970 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m22:35:12.809148 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m22:35:12.827725 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m22:35:12.828824 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m22:35:12.829057 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m22:35:12.829216 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m22:35:12.832715 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:35:12.832932 [info ] [MainThread]: 
[0m22:35:12.834205 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m22:35:12.834562 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_date)
[0m22:35:12.834755 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m22:35:12.861618 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:35:12.861975 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m22:35:12.862173 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:35:12.869056 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:35:12.869338 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m22:35:12.869536 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m22:35:12.869908 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m22:35:12.912082 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m22:35:12.912778 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 22:35:12.834889 => 22:35:12.912653
[0m22:35:12.912996 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m22:35:12.913244 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 22:35:12.913137 => 22:35:12.913159
[0m22:35:12.913449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: ROLLBACK
[0m22:35:12.913681 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.dim_date'
[0m22:35:12.913858 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m22:35:12.915867 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m22:35:12.916167 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_product_sales
[0m22:35:12.916517 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.fact_product_sales)
[0m22:35:12.916733 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_product_sales
[0m22:35:12.919051 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_product_sales"
[0m22:35:12.919573 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (compile): 22:35:12.916870 => 22:35:12.919449
[0m22:35:12.919805 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_product_sales
[0m22:35:12.920023 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_product_sales (execute): 22:35:12.919942 => 22:35:12.919946
[0m22:35:12.920513 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_product_sales
[0m22:35:12.920739 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:12.921054 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_product_sales, now model.elastic_dbt_interview.stg_salesforce__account)
[0m22:35:12.921250 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:12.923788 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m22:35:12.924276 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 22:35:12.921387 => 22:35:12.924174
[0m22:35:12.924478 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:12.924700 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 22:35:12.924610 => 22:35:12.924618
[0m22:35:12.925135 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m22:35:12.925367 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:12.925758 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m22:35:12.925950 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:12.927746 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m22:35:12.928138 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 22:35:12.926076 => 22:35:12.928045
[0m22:35:12.928335 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:12.928546 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 22:35:12.928464 => 22:35:12.928468
[0m22:35:12.928962 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m22:35:12.929170 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:12.929526 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m22:35:12.929717 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:12.931608 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m22:35:12.932068 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 22:35:12.929873 => 22:35:12.931955
[0m22:35:12.932280 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:12.932499 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 22:35:12.932415 => 22:35:12.932419
[0m22:35:12.932932 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m22:35:12.933146 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:12.933504 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m22:35:12.933697 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:12.935399 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m22:35:12.935782 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 22:35:12.933826 => 22:35:12.935681
[0m22:35:12.935978 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:12.936223 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 22:35:12.936134 => 22:35:12.936138
[0m22:35:12.936679 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m22:35:12.936901 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:12.937289 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m22:35:12.937522 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:12.939334 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m22:35:12.939738 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 22:35:12.937660 => 22:35:12.939634
[0m22:35:12.939935 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:12.940149 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 22:35:12.940066 => 22:35:12.940070
[0m22:35:12.940576 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m22:35:12.940782 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:12.941139 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m22:35:12.941331 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:12.943099 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m22:35:12.943486 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 22:35:12.941455 => 22:35:12.943390
[0m22:35:12.943678 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:12.943893 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 22:35:12.943807 => 22:35:12.943810
[0m22:35:12.944305 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m22:35:12.944512 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:12.944856 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m22:35:12.945044 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:12.971274 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m22:35:12.971892 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 22:35:12.945172 => 22:35:12.971787
[0m22:35:12.972115 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:12.972351 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 22:35:12.972262 => 22:35:12.972267
[0m22:35:12.972822 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m22:35:12.973048 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:12.973367 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m22:35:12.973644 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:12.975388 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m22:35:12.975822 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 22:35:12.973786 => 22:35:12.975727
[0m22:35:12.976015 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:12.976234 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 22:35:12.976146 => 22:35:12.976150
[0m22:35:12.976708 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m22:35:12.976923 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:12.977248 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m22:35:12.977434 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:12.979111 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m22:35:12.979474 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 22:35:12.977559 => 22:35:12.979377
[0m22:35:12.979658 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:12.979877 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 22:35:12.979791 => 22:35:12.979795
[0m22:35:12.980315 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m22:35:12.980531 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:12.980892 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m22:35:12.981103 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:12.982808 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m22:35:12.983205 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 22:35:12.981255 => 22:35:12.983111
[0m22:35:12.983400 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:12.983621 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 22:35:12.983537 => 22:35:12.983541
[0m22:35:12.984045 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m22:35:12.984275 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:12.984655 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m22:35:12.984851 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:12.986539 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m22:35:12.986927 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 22:35:12.984987 => 22:35:12.986824
[0m22:35:12.987117 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:12.987332 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 22:35:12.987248 => 22:35:12.987252
[0m22:35:12.987744 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m22:35:12.987950 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:12.988300 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m22:35:12.988488 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:12.990148 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m22:35:12.990522 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 22:35:12.988615 => 22:35:12.990425
[0m22:35:12.990708 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:12.990926 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 22:35:12.990843 => 22:35:12.990846
[0m22:35:12.991339 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m22:35:12.991539 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:12.991880 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m22:35:12.992066 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:12.994544 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m22:35:12.994920 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 22:35:12.992189 => 22:35:12.994830
[0m22:35:12.995125 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:12.995353 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 22:35:12.995267 => 22:35:12.995271
[0m22:35:12.995783 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m22:35:12.995991 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:12.996316 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m22:35:12.996499 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:12.998153 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m22:35:12.998530 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 22:35:12.996624 => 22:35:12.998430
[0m22:35:12.998719 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:12.998930 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 22:35:12.998847 => 22:35:12.998851
[0m22:35:12.999351 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m22:35:12.999571 [debug] [Thread-1  ]: Began running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:12.999915 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now seed.elastic_dbt_interview.dbt_project_evaluator_exceptions)
[0m22:35:13.000099 [debug] [Thread-1  ]: Began compiling node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:13.001317 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (compile): 22:35:13.000227 => 22:35:13.001224
[0m22:35:13.001505 [debug] [Thread-1  ]: Began executing node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:13.001713 [debug] [Thread-1  ]: Timing info for seed.elastic_dbt_interview.dbt_project_evaluator_exceptions (execute): 22:35:13.001630 => 22:35:13.001633
[0m22:35:13.002119 [debug] [Thread-1  ]: Finished running node seed.elastic_dbt_interview.dbt_project_evaluator_exceptions
[0m22:35:13.002318 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m22:35:13.002638 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly seed.elastic_dbt_interview.dbt_project_evaluator_exceptions, now model.elastic_dbt_interview.dim_account)
[0m22:35:13.002815 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m22:35:13.004748 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m22:35:13.005145 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 22:35:13.002939 => 22:35:13.005048
[0m22:35:13.005332 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m22:35:13.005545 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 22:35:13.005464 => 22:35:13.005468
[0m22:35:13.005958 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m22:35:13.006178 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m22:35:13.006500 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m22:35:13.006710 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m22:35:13.008639 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m22:35:13.009028 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 22:35:13.006841 => 22:35:13.008930
[0m22:35:13.009214 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m22:35:13.009424 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 22:35:13.009344 => 22:35:13.009348
[0m22:35:13.009846 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m22:35:13.010050 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m22:35:13.010368 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m22:35:13.010555 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m22:35:13.012351 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m22:35:13.012712 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 22:35:13.010680 => 22:35:13.012612
[0m22:35:13.012905 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m22:35:13.013115 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 22:35:13.013034 => 22:35:13.013037
[0m22:35:13.013521 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m22:35:13.013720 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m22:35:13.014052 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m22:35:13.014243 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m22:35:13.016912 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m22:35:13.017360 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 22:35:13.014368 => 22:35:13.017268
[0m22:35:13.017554 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m22:35:13.017798 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 22:35:13.017719 => 22:35:13.017723
[0m22:35:13.018220 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m22:35:13.018429 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m22:35:13.018758 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m22:35:13.018939 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m22:35:13.020887 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m22:35:13.021256 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 22:35:13.019063 => 22:35:13.021167
[0m22:35:13.021462 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m22:35:13.021684 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 22:35:13.021599 => 22:35:13.021603
[0m22:35:13.022122 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m22:35:13.022328 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m22:35:13.022647 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m22:35:13.022831 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m22:35:13.024716 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m22:35:13.025102 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 22:35:13.022954 => 22:35:13.025007
[0m22:35:13.025309 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m22:35:13.025522 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 22:35:13.025441 => 22:35:13.025445
[0m22:35:13.025936 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m22:35:13.026142 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:13.026460 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m22:35:13.026647 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:13.028429 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m22:35:13.028813 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 22:35:13.026773 => 22:35:13.028723
[0m22:35:13.029004 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:13.029218 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 22:35:13.029135 => 22:35:13.029138
[0m22:35:13.029627 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m22:35:13.029827 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m22:35:13.030140 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m22:35:13.030322 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m22:35:13.032120 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m22:35:13.032466 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 22:35:13.030445 => 22:35:13.032378
[0m22:35:13.032655 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m22:35:13.032869 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 22:35:13.032789 => 22:35:13.032792
[0m22:35:13.033276 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m22:35:13.033479 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m22:35:13.033799 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m22:35:13.033999 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m22:35:13.035823 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m22:35:13.036189 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 22:35:13.034130 => 22:35:13.036096
[0m22:35:13.036373 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m22:35:13.036583 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 22:35:13.036500 => 22:35:13.036504
[0m22:35:13.036991 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m22:35:13.037196 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m22:35:13.037521 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m22:35:13.037708 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m22:35:13.040189 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m22:35:13.040569 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 22:35:13.037833 => 22:35:13.040468
[0m22:35:13.040759 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m22:35:13.040979 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 22:35:13.040894 => 22:35:13.040898
[0m22:35:13.041390 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m22:35:13.041592 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m22:35:13.041910 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m22:35:13.042094 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m22:35:13.043911 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m22:35:13.044247 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 22:35:13.042219 => 22:35:13.044160
[0m22:35:13.044434 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m22:35:13.044645 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 22:35:13.044565 => 22:35:13.044569
[0m22:35:13.045059 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m22:35:13.045283 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m22:35:13.045607 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m22:35:13.045789 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m22:35:13.047670 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m22:35:13.048040 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 22:35:13.045914 => 22:35:13.047953
[0m22:35:13.048225 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m22:35:13.048434 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 22:35:13.048356 => 22:35:13.048360
[0m22:35:13.048841 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m22:35:13.049179 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case
[0m22:35:13.049527 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_case)
[0m22:35:13.049732 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case
[0m22:35:13.051974 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case"
[0m22:35:13.052348 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (compile): 22:35:13.049861 => 22:35:13.052257
[0m22:35:13.052539 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case
[0m22:35:13.052753 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case (execute): 22:35:13.052671 => 22:35:13.052675
[0m22:35:13.053162 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case
[0m22:35:13.053367 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m22:35:13.053712 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case, now model.elastic_dbt_interview.fact_opportunity)
[0m22:35:13.053898 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m22:35:13.059077 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m22:35:13.059505 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 22:35:13.054024 => 22:35:13.059400
[0m22:35:13.059701 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m22:35:13.059918 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 22:35:13.059833 => 22:35:13.059837
[0m22:35:13.060334 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m22:35:13.060542 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_case_history
[0m22:35:13.060888 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_case_history)
[0m22:35:13.061072 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_case_history
[0m22:35:13.063915 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_case_history"
[0m22:35:13.064273 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (compile): 22:35:13.061196 => 22:35:13.064180
[0m22:35:13.064466 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_case_history
[0m22:35:13.064677 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_case_history (execute): 22:35:13.064597 => 22:35:13.064600
[0m22:35:13.065089 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_case_history
[0m22:35:13.065296 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:13.065630 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_case_history, now model.elastic_dbt_interview.fact_opportunity_history)
[0m22:35:13.065817 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:13.067920 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m22:35:13.068287 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 22:35:13.065940 => 22:35:13.068194
[0m22:35:13.068483 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:13.068696 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 22:35:13.068613 => 22:35:13.068617
[0m22:35:13.069109 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m22:35:13.069604 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:35:13.069782 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m22:35:13.071889 [debug] [MainThread]: Command end result
[0m22:35:13.080020 [debug] [MainThread]: Command `dbt compile` succeeded at 22:35:13.079945 after 0.57 seconds
[0m22:35:13.080233 [debug] [MainThread]: Flushing usage events


============================== 22:35:15.147930 | a5482c8a-b0f3-4931-ba48-865b5979c1ee ==============================
[0m22:35:15.147930 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:35:15.150577 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt test', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:35:15.150829 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:35:15.225234 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:35:15.244419 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:35:15.280409 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:35:15.280716 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:35:15.281666 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m22:35:15.295847 [info ] [MainThread]: Found 32 models, 1 seed, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m22:35:15.297170 [info ] [MainThread]: 
[0m22:35:15.297381 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m22:35:15.297656 [debug] [MainThread]: Command end result
[0m22:35:15.304904 [debug] [MainThread]: Command `dbt test` succeeded at 22:35:15.304798 after 0.17 seconds
[0m22:35:15.305260 [debug] [MainThread]: Flushing usage events


============================== 22:37:32.787385 | faa2669b-b68e-467f-b5c6-83c539ab0e98 ==============================
[0m22:37:32.787385 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:37:32.790587 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt test', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m22:37:32.790836 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:37:32.875316 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:37:32.896143 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:37:32.914822 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error
    Syntax error near line 25
    ------------------------------
    22 |         tests:
    23 |           - not_null
    24 | 
    25 |   - name: dim_account
    26 |     description: "Dimension table containing account data."
    27 |     columns:
    28 |       - name: account_id
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 1, column 1
    did not find expected key
      in "<unicode string>", line 25, column 3
[0m22:37:32.915514 [debug] [MainThread]: Command `dbt test` failed at 22:37:32.915424 after 0.16 seconds
[0m22:37:32.915807 [debug] [MainThread]: Flushing usage events


============================== 22:37:51.978348 | ccb220f5-d970-447a-9b16-9795cbaf7597 ==============================
[0m22:37:51.978348 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:37:51.980739 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:37:51.981005 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:37:52.063336 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:37:52.081492 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:37:52.097047 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error
    Syntax error near line 25
    ------------------------------
    22 |         tests:
    23 |           - not_null
    24 | 
    25 |   - name: dim_account
    26 |     description: "Dimension table containing account data."
    27 |     columns:
    28 |       - name: account_id
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 1, column 1
    did not find expected key
      in "<unicode string>", line 25, column 3
[0m22:37:52.097629 [debug] [MainThread]: Command `dbt test` failed at 22:37:52.097556 after 0.14 seconds
[0m22:37:52.097884 [debug] [MainThread]: Flushing usage events


============================== 22:38:46.477058 | 1a3c332c-fca4-43aa-9805-05a6270fec2e ==============================
[0m22:38:46.477058 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:38:46.480398 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt test', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:38:46.480653 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:38:46.562662 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:38:46.582155 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:38:46.600433 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error
    Syntax error near line 39
    ------------------------------
    36 |     columns:
    37 |       - name: account_id
    38 |         description
    
    Raw Error:
    ------------------------------
    while scanning a simple key
      in "<unicode string>", line 38, column 9
    could not find expected ':'
      in "<unicode string>", line 39, column 1
[0m22:38:46.600956 [debug] [MainThread]: Command `dbt test` failed at 22:38:46.600887 after 0.14 seconds
[0m22:38:46.601185 [debug] [MainThread]: Flushing usage events


============================== 22:39:24.032634 | 7c7598e6-2fb8-4a1b-bde2-4227efd2d53d ==============================
[0m22:39:24.032634 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:39:24.035611 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt test', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m22:39:24.035940 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m22:39:24.119138 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m22:39:24.138247 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m22:39:24.154234 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error
    Syntax error near line 65
    ------------------------------
    62 |     columns:
    63 |       - name: account_id
    64 |         description
    
    Raw Error:
    ------------------------------
    while scanning a simple key
      in "<unicode string>", line 64, column 9
    could not find expected ':'
      in "<unicode string>", line 65, column 1
[0m22:39:24.154782 [debug] [MainThread]: Command `dbt test` failed at 22:39:24.154710 after 0.15 seconds
[0m22:39:24.155022 [debug] [MainThread]: Flushing usage events
{"data": {"log_version": 3, "version": "=1.6.18"}, "info": {"category": "", "code": "A001", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "info", "msg": "Running with dbt=1.6.18", "name": "MainReportVersion", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.752659Z"}}
{"data": {"args": {"cache_selected_only": "False", "debug": "False", "fail_fast": "False", "indirect_selection": "eager", "introspect": "True", "invocation_command": "dbt --log-format json ls --select elastic_dbt_interview --resource-type model --resource-type seed --resource-type snapshot --resource-type analysis --project-dir /Users/sunjay.nair/Documents/GitHub/dbt_sunjay --output json", "log_cache_events": "False", "log_format": "json", "log_path": "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs", "no_print": "None", "partial_parse": "True", "printer_width": "80", "profiles_dir": ".", "quiet": "False", "send_anonymous_usage_stats": "False", "static_parser": "True", "target_path": "None", "use_colors": "True", "use_experimental_parser": "False", "version_check": "True", "warn_error": "None", "warn_error_options": "WarnErrorOptions(include=[], exclude=[])", "write_json": "True"}}, "info": {"category": "", "code": "A002", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "debug", "msg": "running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'json', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt --log-format json ls --select elastic_dbt_interview --resource-type model --resource-type seed --resource-type snapshot --resource-type analysis --project-dir /Users/sunjay.nair/Documents/GitHub/dbt_sunjay --output json', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}", "name": "MainReportArgs", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.756084Z"}}
{"data": {}, "info": {"category": "", "code": "D013", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "warn", "msg": "[\u001b[33mWARNING\u001b[0m]: Deprecated functionality\n\nUser config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.", "name": "ProjectFlagsMovedDeprecation", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.756443Z"}}
{"data": {"adapter_name": "duckdb", "adapter_version": "=1.6.2"}, "info": {"category": "", "code": "E034", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "info", "msg": "Registered adapter: duckdb=1.6.2", "name": "AdapterRegistered", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.836935Z"}}
{"data": {"checksum": "7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189", "profile": "", "target": "", "vars": "{}", "version": "1.6.18"}, "info": {"category": "", "code": "I025", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "debug", "msg": "checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18", "name": "StateCheckVarsHash", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.855189Z"}}
{"data": {"exc": "Parsing Error\n  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error\n    Syntax error near line 65\n    ------------------------------\n    62 |     columns:\n    63 |       - name: account_id\n    64 |         description\n    \n    Raw Error:\n    ------------------------------\n    while scanning a simple key\n      in \"<unicode string>\", line 64, column 9\n    could not find expected ':'\n      in \"<unicode string>\", line 65, column 1"}, "info": {"category": "", "code": "Z002", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "error", "msg": "Encountered an error:\nParsing Error\n  Error reading elastic_dbt_interview: facts/schema.yml - Runtime Error\n    Syntax error near line 65\n    ------------------------------\n    62 |     columns:\n    63 |       - name: account_id\n    64 |         description\n    \n    Raw Error:\n    ------------------------------\n    while scanning a simple key\n      in \"<unicode string>\", line 64, column 9\n    could not find expected ':'\n      in \"<unicode string>\", line 65, column 1", "name": "MainEncounteredError", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.874671Z"}}
{"data": {"command": "dbt ls", "completed_at": "2024-08-21T20:40:12.875195Z", "elapsed": 0.14122829, "success": false}, "info": {"category": "", "code": "Q039", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "debug", "msg": "Command `dbt ls` failed at 22:40:12.875195 after 0.14 seconds", "name": "CommandCompleted", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.875270Z"}}
{"data": {}, "info": {"category": "", "code": "Z042", "extra": {}, "invocation_id": "84bf5404-cd2f-40eb-bf9d-dae4e48fbc3d", "level": "debug", "msg": "Flushing usage events", "name": "FlushEvents", "pid": 50123, "thread": "MainThread", "ts": "2024-08-21T20:40:12.875607Z"}}


============================== 23:44:38.726135 | 27136a1b-e7e4-49c5-9bba-69bd583208fb ==============================
[0m23:44:38.726135 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:44:38.728898 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'False'}
[0m23:44:38.729158 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:44:38.810747 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:44:38.831566 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:44:38.885926 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:44:38.886228 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:44:38.887142 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m23:44:38.900911 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:44:38.902819 [info ] [MainThread]: 
[0m23:44:38.903378 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:44:38.904931 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:44:38.912132 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:44:38.912434 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:44:38.912616 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:44:38.921209 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.922162 [debug] [ThreadPool]: On list_dbt: Close
[0m23:44:38.925587 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:44:38.925902 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:44:38.926211 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:38.933286 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.934113 [debug] [ThreadPool]: On list_dbt: Close
[0m23:44:38.937385 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:44:38.937653 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:44:38.937875 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:38.946068 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.946892 [debug] [ThreadPool]: On list_dbt: Close
[0m23:44:38.949204 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m23:44:38.949599 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m23:44:38.952555 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m23:44:38.952768 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m23:44:38.952928 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:38.960549 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.960844 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m23:44:38.961015 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m23:44:38.961273 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.961808 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m23:44:38.961982 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m23:44:38.962129 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m23:44:38.962331 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.962491 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m23:44:38.964677 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now create_dbt_fact)
[0m23:44:38.965183 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:44:38.966804 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:44:38.966992 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:44:38.967146 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:38.975705 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.976086 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:44:38.976255 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:44:38.976625 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.977292 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:44:38.977466 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:44:38.977620 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:44:38.977837 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.978000 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:44:38.980326 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now create_dbt_dim)
[0m23:44:38.980830 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m23:44:38.983117 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m23:44:38.983307 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m23:44:38.983462 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:38.990263 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.990512 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m23:44:38.990779 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m23:44:38.991164 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.991864 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m23:44:38.992091 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m23:44:38.992273 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m23:44:38.992558 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:38.992731 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m23:44:38.995930 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m23:44:38.999498 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:44:38.999727 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:44:38.999897 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:39.007129 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:39.007389 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:44:39.007563 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:44:39.026567 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:39.027626 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:44:39.028270 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:44:39.028463 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:44:39.030975 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m23:44:39.033393 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:44:39.033566 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:44:39.033709 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:39.040479 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:39.040731 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:44:39.040899 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:44:39.057015 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:39.060739 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:44:39.060978 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:44:39.061134 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:44:39.063833 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m23:44:39.066364 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:44:39.066622 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:44:39.066777 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:39.073880 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:39.074178 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:44:39.074360 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:44:39.089718 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:39.093182 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:44:39.093427 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:44:39.093591 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:44:39.095858 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m23:44:39.097345 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:44:39.097524 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:44:39.097673 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:39.104543 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:39.104763 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:44:39.104929 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:44:39.120373 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:44:39.123801 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:44:39.124025 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:44:39.124178 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:44:39.127334 [debug] [MainThread]: Using duckdb connection "master"
[0m23:44:39.127525 [debug] [MainThread]: On master: BEGIN
[0m23:44:39.127679 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:44:39.134179 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:44:39.134442 [debug] [MainThread]: On master: COMMIT
[0m23:44:39.134605 [debug] [MainThread]: Using duckdb connection "master"
[0m23:44:39.134762 [debug] [MainThread]: On master: COMMIT
[0m23:44:39.134961 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:44:39.135124 [debug] [MainThread]: On master: Close
[0m23:44:39.137070 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:44:39.137338 [info ] [MainThread]: 
[0m23:44:39.139925 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m23:44:39.140260 [info ] [Thread-1  ]: 1 of 30 START sql table model dim.dim_date ..................................... [RUN]
[0m23:44:39.140663 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_date)
[0m23:44:39.140849 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m23:44:39.167873 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:44:39.168297 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m23:44:39.168495 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.175219 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.175499 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:44:39.175693 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m23:44:39.176067 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.217896 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m23:44:39.219783 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 23:44:39.140983 => 23:44:39.219642
[0m23:44:39.220067 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m23:44:39.264282 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m23:44:39.264957 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:44:39.265365 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m23:44:39.312228 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.316358 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:44:39.316603 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m23:44:39.317069 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.318873 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:44:39.319067 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m23:44:39.319380 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.329691 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m23:44:39.329918 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:44:39.330097 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m23:44:39.333265 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.336260 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:44:39.336480 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m23:44:39.336999 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.337731 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 23:44:39.220209 => 23:44:39.337639
[0m23:44:39.337926 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m23:44:39.362556 [info ] [Thread-1  ]: 1 of 30 OK created sql table model dim.dim_date ................................ [[32mOK[0m in 0.22s]
[0m23:44:39.363001 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m23:44:39.363248 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m23:44:39.363527 [info ] [Thread-1  ]: 2 of 30 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m23:44:39.363887 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.stg_salesforce__account)
[0m23:44:39.364092 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m23:44:39.366853 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:44:39.367382 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 23:44:39.364236 => 23:44:39.367280
[0m23:44:39.367582 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m23:44:39.378893 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:44:39.379492 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:44:39.379694 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m23:44:39.379876 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.386909 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.387230 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:44:39.387476 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m23:44:39.388327 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.390633 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:44:39.390861 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m23:44:39.391205 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.393014 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:44:39.393236 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m23:44:39.393546 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.394548 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m23:44:39.394872 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:44:39.395106 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m23:44:39.395692 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.397423 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:44:39.397635 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m23:44:39.397998 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.398763 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 23:44:39.367712 => 23:44:39.398662
[0m23:44:39.398974 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m23:44:39.413550 [info ] [Thread-1  ]: 2 of 30 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.05s]
[0m23:44:39.413935 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m23:44:39.414161 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:44:39.414564 [info ] [Thread-1  ]: 3 of 30 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m23:44:39.415074 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m23:44:39.415313 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:44:39.418192 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:44:39.418718 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 23:44:39.415450 => 23:44:39.418610
[0m23:44:39.418926 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:44:39.421616 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:44:39.422061 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:44:39.422250 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m23:44:39.422422 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.429469 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.429751 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:44:39.429980 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m23:44:39.430744 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.432983 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:44:39.433196 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m23:44:39.433499 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.435495 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:44:39.435793 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m23:44:39.436180 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.437090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m23:44:39.437283 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:44:39.437458 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m23:44:39.438043 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.439847 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:44:39.440060 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m23:44:39.440430 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.441166 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 23:44:39.419057 => 23:44:39.441071
[0m23:44:39.441369 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m23:44:39.455199 [info ] [Thread-1  ]: 3 of 30 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.04s]
[0m23:44:39.455641 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:44:39.455888 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m23:44:39.456215 [info ] [Thread-1  ]: 4 of 30 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m23:44:39.456575 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m23:44:39.456766 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m23:44:39.458785 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:44:39.459299 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 23:44:39.456901 => 23:44:39.459197
[0m23:44:39.459491 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m23:44:39.462962 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:44:39.463456 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:44:39.463655 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m23:44:39.463832 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.470505 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.470776 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:44:39.470999 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m23:44:39.471687 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.473897 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:44:39.474106 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m23:44:39.474408 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.476119 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:44:39.476443 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m23:44:39.476747 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.477600 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m23:44:39.477796 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:44:39.477974 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m23:44:39.478661 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.480654 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:44:39.480888 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m23:44:39.481330 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.482117 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 23:44:39.459617 => 23:44:39.482023
[0m23:44:39.482329 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m23:44:39.496352 [info ] [Thread-1  ]: 4 of 30 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.04s]
[0m23:44:39.496778 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m23:44:39.497024 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:44:39.497289 [info ] [Thread-1  ]: 5 of 30 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m23:44:39.497653 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m23:44:39.497847 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:44:39.499809 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:44:39.500316 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 23:44:39.497976 => 23:44:39.500203
[0m23:44:39.500510 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:44:39.504093 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:44:39.504555 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:44:39.504750 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m23:44:39.504926 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.511780 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.512053 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:44:39.512256 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m23:44:39.512685 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.514914 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:44:39.515129 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m23:44:39.515440 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.517163 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:44:39.517514 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m23:44:39.518001 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.519130 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m23:44:39.519358 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:44:39.519548 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m23:44:39.520160 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.522002 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:44:39.522263 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m23:44:39.522710 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.523565 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 23:44:39.500640 => 23:44:39.523464
[0m23:44:39.523801 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m23:44:39.538131 [info ] [Thread-1  ]: 5 of 30 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.04s]
[0m23:44:39.538566 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:44:39.538816 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:44:39.539168 [info ] [Thread-1  ]: 6 of 30 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m23:44:39.539582 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m23:44:39.539794 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:44:39.541849 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:44:39.542334 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 23:44:39.539929 => 23:44:39.542229
[0m23:44:39.542530 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:44:39.545389 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:44:39.545909 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:44:39.546112 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m23:44:39.546283 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.552727 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.553005 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:44:39.553247 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m23:44:39.554211 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.557113 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:44:39.557337 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m23:44:39.557660 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.559415 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:44:39.559618 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m23:44:39.560010 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.561135 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m23:44:39.561374 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:44:39.561556 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m23:44:39.562235 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.564251 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:44:39.564477 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m23:44:39.564873 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.565757 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 23:44:39.542655 => 23:44:39.565660
[0m23:44:39.565991 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m23:44:39.580713 [info ] [Thread-1  ]: 6 of 30 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.04s]
[0m23:44:39.581136 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:44:39.581381 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:44:39.581711 [info ] [Thread-1  ]: 7 of 30 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m23:44:39.582070 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m23:44:39.582268 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:44:39.584379 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:44:39.584873 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 23:44:39.582398 => 23:44:39.584773
[0m23:44:39.585066 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:44:39.587694 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:44:39.588152 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:44:39.588344 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m23:44:39.588518 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.595418 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.595703 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:44:39.595936 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m23:44:39.596713 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.598987 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:44:39.599198 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m23:44:39.599495 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.601999 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:44:39.602221 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m23:44:39.602509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.603548 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m23:44:39.603861 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:44:39.604068 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m23:44:39.605152 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.607544 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:44:39.607833 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m23:44:39.608298 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.609171 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 23:44:39.585194 => 23:44:39.609066
[0m23:44:39.609457 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m23:44:39.623393 [info ] [Thread-1  ]: 7 of 30 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.04s]
[0m23:44:39.623792 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:44:39.624022 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:44:39.624460 [info ] [Thread-1  ]: 8 of 30 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m23:44:39.625002 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m23:44:39.625264 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:44:39.627438 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:44:39.627974 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 23:44:39.625419 => 23:44:39.627863
[0m23:44:39.628180 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:44:39.630942 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:44:39.631402 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:44:39.631599 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m23:44:39.631772 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.638472 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.638765 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:44:39.638995 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m23:44:39.639707 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.641920 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:44:39.642130 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m23:44:39.642430 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.645057 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:44:39.645391 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m23:44:39.645815 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.646898 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m23:44:39.647123 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:44:39.647327 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m23:44:39.648050 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.649859 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:44:39.650072 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m23:44:39.650465 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.651212 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 23:44:39.628317 => 23:44:39.651116
[0m23:44:39.651490 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m23:44:39.666059 [info ] [Thread-1  ]: 8 of 30 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.04s]
[0m23:44:39.666507 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:44:39.666757 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:44:39.667093 [info ] [Thread-1  ]: 9 of 30 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m23:44:39.667466 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m23:44:39.667726 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:44:39.669875 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:44:39.670375 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 23:44:39.667900 => 23:44:39.670271
[0m23:44:39.670576 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:44:39.673359 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:44:39.673798 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:44:39.674004 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m23:44:39.674186 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.681147 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.681445 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:44:39.681660 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m23:44:39.682251 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.684472 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:44:39.684686 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m23:44:39.685008 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.686702 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:44:39.686920 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m23:44:39.687217 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.688076 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m23:44:39.688269 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:44:39.688606 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m23:44:39.689200 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.691800 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:44:39.692021 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m23:44:39.692405 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.693312 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 23:44:39.670706 => 23:44:39.693217
[0m23:44:39.693551 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m23:44:39.706975 [info ] [Thread-1  ]: 9 of 30 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.04s]
[0m23:44:39.707374 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:44:39.707615 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:44:39.707984 [info ] [Thread-1  ]: 10 of 30 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m23:44:39.708479 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m23:44:39.708689 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:44:39.710791 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:44:39.711270 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 23:44:39.708828 => 23:44:39.711163
[0m23:44:39.711470 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:44:39.714352 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:44:39.714822 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:44:39.715024 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m23:44:39.715199 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.721993 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.722301 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:44:39.722509 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m23:44:39.722968 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.725127 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:44:39.725343 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m23:44:39.725638 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.727342 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:44:39.727539 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m23:44:39.727815 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.729000 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m23:44:39.729260 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:44:39.729461 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m23:44:39.730112 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.732590 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:44:39.732805 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m23:44:39.733181 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.733896 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 23:44:39.711600 => 23:44:39.733800
[0m23:44:39.734104 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m23:44:39.748808 [info ] [Thread-1  ]: 10 of 30 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.04s]
[0m23:44:39.749235 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:44:39.749475 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:44:39.749789 [info ] [Thread-1  ]: 11 of 30 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m23:44:39.750202 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m23:44:39.750423 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:44:39.752493 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:44:39.752979 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 23:44:39.750564 => 23:44:39.752874
[0m23:44:39.753181 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:44:39.755658 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:44:39.756032 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:44:39.756227 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m23:44:39.756555 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.763583 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.763875 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:44:39.764088 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m23:44:39.764644 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.766873 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:44:39.767081 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m23:44:39.767381 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.769093 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:44:39.769306 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m23:44:39.769590 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.770651 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m23:44:39.771046 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:44:39.771245 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m23:44:39.771963 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.773659 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:44:39.773885 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m23:44:39.774282 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.775148 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 23:44:39.753312 => 23:44:39.775046
[0m23:44:39.775372 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m23:44:39.789575 [info ] [Thread-1  ]: 11 of 30 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.04s]
[0m23:44:39.789996 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:44:39.790243 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:44:39.790579 [info ] [Thread-1  ]: 12 of 30 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m23:44:39.790940 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m23:44:39.791136 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:44:39.793916 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:44:39.794390 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 23:44:39.791266 => 23:44:39.794288
[0m23:44:39.794591 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:44:39.797303 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:44:39.797762 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:44:39.797964 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m23:44:39.798142 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.805192 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.805484 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:44:39.805694 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m23:44:39.806187 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.808397 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:44:39.808608 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m23:44:39.808909 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.811185 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:44:39.811493 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m23:44:39.811883 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.812842 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m23:44:39.813044 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:44:39.813221 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m23:44:39.813808 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.815442 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:44:39.815647 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m23:44:39.816003 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.816725 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 23:44:39.794720 => 23:44:39.816628
[0m23:44:39.816932 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m23:44:39.831826 [info ] [Thread-1  ]: 12 of 30 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.04s]
[0m23:44:39.832265 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:44:39.832512 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:44:39.832843 [info ] [Thread-1  ]: 13 of 30 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m23:44:39.833206 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m23:44:39.833399 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:44:39.835491 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:44:39.836005 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 23:44:39.833529 => 23:44:39.835902
[0m23:44:39.836207 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:44:39.839679 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:44:39.840165 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:44:39.840361 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m23:44:39.840542 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.847448 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.847721 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:44:39.847925 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m23:44:39.848422 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.850566 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:44:39.850786 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m23:44:39.851087 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.852809 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:44:39.853015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m23:44:39.853305 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.854134 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m23:44:39.854315 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:44:39.854485 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m23:44:39.855170 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.856976 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:44:39.857285 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m23:44:39.857735 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.858573 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 23:44:39.836340 => 23:44:39.858475
[0m23:44:39.858787 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m23:44:39.873036 [info ] [Thread-1  ]: 13 of 30 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.04s]
[0m23:44:39.873439 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:44:39.873666 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m23:44:39.874110 [info ] [Thread-1  ]: 14 of 30 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m23:44:39.874621 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m23:44:39.874834 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m23:44:39.877064 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:44:39.877623 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 23:44:39.874970 => 23:44:39.877520
[0m23:44:39.877827 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m23:44:39.881463 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:44:39.881925 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:44:39.882112 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m23:44:39.882286 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.889107 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.889405 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:44:39.889654 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m23:44:39.890644 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.892879 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:44:39.893099 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m23:44:39.893413 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.895238 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:44:39.895465 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m23:44:39.895797 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.896683 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m23:44:39.896887 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:44:39.897065 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m23:44:39.897701 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.899403 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:44:39.899608 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m23:44:39.900061 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.901083 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 23:44:39.877962 => 23:44:39.900980
[0m23:44:39.901358 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m23:44:39.915074 [info ] [Thread-1  ]: 14 of 30 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.04s]
[0m23:44:39.915515 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m23:44:39.915764 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:44:39.916038 [info ] [Thread-1  ]: 15 of 30 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m23:44:39.916400 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m23:44:39.916599 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:44:39.918720 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:44:39.919248 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 23:44:39.916728 => 23:44:39.919146
[0m23:44:39.919453 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:44:39.922425 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:44:39.922929 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:44:39.923148 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m23:44:39.923337 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.929953 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.930236 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:44:39.930443 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m23:44:39.930963 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.933912 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:44:39.934125 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m23:44:39.934474 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.936197 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:44:39.936391 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m23:44:39.936756 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.937977 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m23:44:39.938261 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:44:39.938490 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m23:44:39.939084 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.940891 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:44:39.941102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m23:44:39.941509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.942277 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 23:44:39.919592 => 23:44:39.942183
[0m23:44:39.942486 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m23:44:39.957509 [info ] [Thread-1  ]: 15 of 30 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.04s]
[0m23:44:39.957927 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:44:39.958176 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m23:44:39.958498 [info ] [Thread-1  ]: 16 of 30 START sql table model dim.dim_account ................................. [RUN]
[0m23:44:39.958896 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m23:44:39.959103 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m23:44:39.961434 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m23:44:39.961947 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 23:44:39.959237 => 23:44:39.961844
[0m23:44:39.962154 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m23:44:39.964903 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m23:44:39.965365 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:44:39.965573 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m23:44:39.965757 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:39.972305 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.972582 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:44:39.972839 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

select
    account_id,
    masterrecordid as master_record_id,
    name as account_name,
    type as account_type,
    parentid as parent_account_id,
    billingstreet as billing_street,
    billingcity as billing_city,
    billingstate as billing_state,
    billingpostalcode as billing_postal_code,
    billingcountry as billing_country,
    billinglatitude as billing_latitude,
    billinglongitude as billing_longitude,
    billinggeocodeaccuracy as billing_geocode_accuracy,
    shippingstreet as shipping_street,
    shippingcity as shipping_city,
    shippingstate as shipping_state,
    shippingpostalcode as shipping_postal_code,
    shippingcountry as shipping_country,
    shippinglatitude as shipping_latitude,
    shippinglongitude as shipping_longitude,
    shippinggeocodeaccuracy as shipping_geocode_accuracy,
    phone as account_phone,
    fax as account_fax,
    accountnumber as account_number,
    website as account_website,
    sic as sic_code,
    industry as account_industry,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownership as account_ownership,
    tickersymbol as ticker_symbol,
    description as account_description,
    rating as account_rating,
    site as account_site,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    jigsaw as jigsaw_id,
    jigsawcompanyid as jigsaw_company_id,
    cleanstatus as clean_status,
    accountsource as account_source,
    dunsnumber as duns_number,
    tradestyle as trade_style,
    naicscode as naics_code,
    naicsdesc as naics_description,
    yearstarted as year_started,
    sicdesc as sic_description,
    dandbcompanyid as dandb_company_id,
    operatinghoursid as operating_hours_id,
    customerpriority__c as customer_priority,
    sla__c as sla,
    active__c as is_active,
    numberoflocations__c as number_of_locations,
    upsellopportunity__c as upsell_opportunity,
    slaserialnumber__c as sla_serial_number,
    slaexpirationdate__c as sla_expiration_date
from "dbt"."staging"."stg_salesforce__account"
where isdeleted = false  -- Exclude deleted accounts
order by account_name
    );
  
  
[0m23:44:39.977139 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:39.980353 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:44:39.980597 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account" rename to "dim_account__dbt_backup"
[0m23:44:39.980956 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.010106 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:44:40.010449 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m23:44:40.010920 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.012014 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m23:44:40.012220 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:44:40.012402 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m23:44:40.013693 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.015293 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:44:40.015493 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m23:44:40.016119 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.017018 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 23:44:39.962290 => 23:44:40.016918
[0m23:44:40.017226 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m23:44:40.038872 [info ] [Thread-1  ]: 16 of 30 OK created sql table model dim.dim_account ............................ [[32mOK[0m in 0.08s]
[0m23:44:40.039328 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m23:44:40.039576 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m23:44:40.039848 [info ] [Thread-1  ]: 17 of 30 START sql table model dim.dim_campaign ................................ [RUN]
[0m23:44:40.040201 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m23:44:40.040394 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m23:44:40.042661 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m23:44:40.043158 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 23:44:40.040530 => 23:44:40.043051
[0m23:44:40.043371 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m23:44:40.045766 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m23:44:40.046130 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:44:40.046327 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m23:44:40.046679 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.054161 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.054454 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:44:40.054708 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

select
    /* IDs */
    campaign_id,
    parentid as parent_campaign_id,
    ownerid as owner_id,
    createdbyid as created_by_id,
    lastmodifiedbyid as last_modified_by_id,
    campaignmemberrecordtypeid as campaign_member_record_type_id,

    /* Dates */
    startdate as start_date,
    enddate as end_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,

    /* Dimensions */
    name as campaign_name,
    type as campaign_type,
    status,
    description,
    isactive as is_active,

    /* Metrics */
    expectedrevenue as expected_revenue,
    budgetedcost as budgeted_cost,
    actualcost as actual_cost,
    expectedresponse as expected_response,
    numbersent as number_sent,
    numberofleads as number_of_leads,
    numberofconvertedleads as number_of_converted_leads,
    numberofcontacts as number_of_contacts,
    numberofresponses as number_of_responses,
    numberofopportunities as number_of_opportunities,
    numberofwonopportunities as number_of_won_opportunities,
    amountallopportunities as amount_all_opportunities,
    amountwonopportunities as amount_won_opportunities,
    hierarchynumberofleads as hierarchy_number_of_leads,
    hierarchynumberofconvertedleads as hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts as hierarchy_number_of_contacts,
    hierarchynumberofresponses as hierarchy_number_of_responses,
    hierarchynumberofopportunities as hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities as hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities as hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities as hierarchy_amount_won_opportunities,
    hierarchynumbersent as hierarchy_number_sent,
    hierarchyexpectedrevenue as hierarchy_expected_revenue,
    hierarchybudgetedcost as hierarchy_budgeted_cost,
    hierarchyactualcost as hierarchy_actual_cost

from "dbt"."staging"."stg_salesforce__campaign"
where isdeleted = false
    );
  
  
[0m23:44:40.057666 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.060005 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:44:40.060262 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign" rename to "dim_campaign__dbt_backup"
[0m23:44:40.060688 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.063190 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:44:40.063406 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m23:44:40.063786 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.064779 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m23:44:40.064982 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:44:40.065165 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m23:44:40.066211 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.067791 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:44:40.068011 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."dim"."dim_campaign__dbt_backup" cascade
[0m23:44:40.068651 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.069508 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 23:44:40.043498 => 23:44:40.069412
[0m23:44:40.069722 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m23:44:40.086226 [info ] [Thread-1  ]: 17 of 30 OK created sql table model dim.dim_campaign ........................... [[32mOK[0m in 0.05s]
[0m23:44:40.086672 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m23:44:40.086927 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m23:44:40.087203 [info ] [Thread-1  ]: 18 of 30 START sql table model dim.dim_case_status ............................. [RUN]
[0m23:44:40.087565 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m23:44:40.087760 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m23:44:40.089950 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m23:44:40.090448 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 23:44:40.087898 => 23:44:40.090344
[0m23:44:40.090644 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m23:44:40.093238 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m23:44:40.093695 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:44:40.093897 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m23:44:40.094078 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.101779 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.102090 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:44:40.102305 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."dim"."dim_case_status__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct status as status_name, status as status_description  -- Adjust as needed, typically a description field should be separate
        from "dbt"."staging"."stg_salesforce__case_history_2"
    )

select
    row_number() over (order by status_name) as status_id,  -- Surrogate Key
    status_name,
    status_description
from source
    );
  
  
[0m23:44:40.104166 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.106374 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:44:40.106671 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m23:44:40.107116 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.109520 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:44:40.109818 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m23:44:40.110195 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.111258 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m23:44:40.111462 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:44:40.111647 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m23:44:40.112385 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.115458 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:44:40.115775 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."dim"."dim_case_status__dbt_backup" cascade
[0m23:44:40.116362 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.117262 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 23:44:40.090773 => 23:44:40.117124
[0m23:44:40.117519 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m23:44:40.134041 [info ] [Thread-1  ]: 18 of 30 OK created sql table model dim.dim_case_status ........................ [[32mOK[0m in 0.05s]
[0m23:44:40.134452 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m23:44:40.134681 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m23:44:40.135089 [info ] [Thread-1  ]: 19 of 30 START sql table model dim.dim_contact ................................. [RUN]
[0m23:44:40.135586 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m23:44:40.135798 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m23:44:40.138112 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m23:44:40.138609 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 23:44:40.135936 => 23:44:40.138506
[0m23:44:40.138810 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m23:44:40.141479 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m23:44:40.141957 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:44:40.142142 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m23:44:40.142316 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.149344 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.149648 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:44:40.149907 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

select
    /* IDs */
    contact_id,
    masterrecordid as master_record_id,
    accountid as account_id,
    reportstoid as reports_to_id,
    ownerid as owner_id,
    jigsawcontactid as jigsaw_contact_id,
    individualid as individual_id,

    /* Dates */
    birthdate as birth_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    lastcurequestdate as last_cu_request_date,
    lastcuupdatedate as last_cu_update_date,
    emailbounceddate as email_bounced_date,

    /* Dimensions */
    salutation,
    firstname as first_name,
    lastname as last_name,
    otherstreet as other_street,
    othercity as other_city,
    otherstate as other_state,
    otherpostalcode as other_postal_code,
    othercountry as other_country,
    otherlatitude as other_latitude,
    otherlongitude as other_longitude,
    othergeocodeaccuracy as other_geocode_accuracy,
    mailingstreet as mailing_street,
    mailingcity as mailing_city,
    mailingstate as mailing_state,
    mailingpostalcode as mailing_postal_code,
    mailingcountry as mailing_country,
    mailinglatitude as mailing_latitude,
    mailinglongitude as mailing_longitude,
    mailinggeocodeaccuracy as mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone as mobile_phone,
    homephone as home_phone,
    otherphone as other_phone,
    assistantphone as assistant_phone,
    email,
    title,
    department,
    assistantname as assistant_name,
    leadsource as lead_source,
    description,
    pronouns,
    genderidentity as gender_identity,
    cleanstatus as clean_status,
    emailbouncedreason as email_bounced_reason,
    level__c as level,
    languages__c as languages,

    /* Metrics */
    hasoptedoutofemail as has_opted_out_of_email,
    hasoptedoutoffax as has_opted_out_of_fax,
    donotcall as do_not_call

from "dbt"."staging"."stg_salesforce__contact"
where isdeleted = false
    );
  
  
[0m23:44:40.153469 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.155640 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:44:40.155867 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact" rename to "dim_contact__dbt_backup"
[0m23:44:40.156348 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.158697 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:44:40.159052 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m23:44:40.159515 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.160542 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m23:44:40.160755 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:44:40.160937 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m23:44:40.162291 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.164786 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:44:40.165038 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m23:44:40.165603 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.166388 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 23:44:40.138939 => 23:44:40.166290
[0m23:44:40.166602 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m23:44:40.188770 [info ] [Thread-1  ]: 19 of 30 OK created sql table model dim.dim_contact ............................ [[32mOK[0m in 0.05s]
[0m23:44:40.189179 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m23:44:40.189427 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m23:44:40.189692 [info ] [Thread-1  ]: 20 of 30 START sql table model dim.dim_lead .................................... [RUN]
[0m23:44:40.190047 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m23:44:40.190240 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m23:44:40.192570 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m23:44:40.193029 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 23:44:40.190374 => 23:44:40.192927
[0m23:44:40.193331 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m23:44:40.195989 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m23:44:40.196547 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:44:40.196753 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m23:44:40.196929 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.204129 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.204403 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:44:40.204664 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

select
    lead_id,
    masterrecordid as master_record_id,
    salutation as lead_salutation,
    firstname as first_name,
    lastname as last_name,
    title as lead_title,
    company as lead_company,
    street as lead_street,
    city as lead_city,
    state as lead_state,
    postalcode as lead_postal_code,
    country as lead_country,
    latitude as lead_latitude,
    longitude as lead_longitude,
    geocodeaccuracy as geocode_accuracy,
    phone as lead_phone,
    mobilephone as lead_mobile_phone,
    fax as lead_fax,
    email as lead_email,
    website as lead_website,
    description as lead_description,
    leadsource as lead_source,
    status as lead_status,
    industry as lead_industry,
    rating as lead_rating,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownerid as owner_id,
    hasoptedoutofemail as has_opted_out_of_email,
    isconverted as is_converted,
    converteddate as converted_date,
    convertedaccountid as converted_account_id,
    convertedcontactid as converted_contact_id,
    convertedopportunityid as converted_opportunity_id,
    isunreadbyowner as is_unread_by_owner,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    donotcall as do_not_call,
    hasoptedoutoffax as has_opted_out_of_fax,
    lasttransferdate as last_transfer_date,
    jigsaw as jigsaw_id,
    jigsawcontactid as jigsaw_contact_id,
    cleanstatus as clean_status,
    companydunsnumber as company_duns_number,
    dandbcompanyid as dandb_company_id,
    emailbouncedreason as email_bounced_reason,
    emailbounceddate as email_bounced_date,
    individualid as individual_id,
    pronouns as lead_pronouns,
    genderidentity as gender_identity,
    siccode__c as sic_code,
    productinterest__c as product_interest,
    primary__c as is_primary,
    currentgenerators__c as current_generators,
    numberoflocations__c as number_of_locations
from "dbt"."staging"."stg_salesforce__lead"
where isdeleted = false  -- Exclude deleted leads
order by last_name, first_name
    );
  
  
[0m23:44:40.208968 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.211313 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:44:40.211560 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead" rename to "dim_lead__dbt_backup"
[0m23:44:40.211938 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.213739 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:44:40.213944 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m23:44:40.214273 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.215238 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m23:44:40.215423 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:44:40.215604 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m23:44:40.217142 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.219064 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:44:40.219357 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m23:44:40.219942 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.220829 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 23:44:40.193570 => 23:44:40.220721
[0m23:44:40.221052 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m23:44:40.243371 [info ] [Thread-1  ]: 20 of 30 OK created sql table model dim.dim_lead ............................... [[32mOK[0m in 0.05s]
[0m23:44:40.243803 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m23:44:40.244057 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m23:44:40.244323 [info ] [Thread-1  ]: 21 of 30 START sql table model dim.dim_opportunity ............................. [RUN]
[0m23:44:40.244711 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m23:44:40.244897 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m23:44:40.248112 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m23:44:40.248630 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 23:44:40.245028 => 23:44:40.248523
[0m23:44:40.248833 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m23:44:40.251466 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m23:44:40.252090 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:44:40.252341 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m23:44:40.252514 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.260579 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.260855 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:44:40.261098 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m23:44:40.264499 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.266716 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:44:40.266926 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m23:44:40.267305 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.269035 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:44:40.269229 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m23:44:40.269538 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.270894 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m23:44:40.271220 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:44:40.271434 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m23:44:40.272755 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.274550 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:44:40.274775 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m23:44:40.275306 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.276090 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 23:44:40.248962 => 23:44:40.275994
[0m23:44:40.276301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m23:44:40.296125 [info ] [Thread-1  ]: 21 of 30 OK created sql table model dim.dim_opportunity ........................ [[32mOK[0m in 0.05s]
[0m23:44:40.296563 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m23:44:40.296807 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:44:40.297126 [info ] [Thread-1  ]: 22 of 30 START sql table model dim.dim_opportunity_stage ....................... [RUN]
[0m23:44:40.297484 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m23:44:40.297679 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:44:40.300614 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:44:40.301126 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 23:44:40.297806 => 23:44:40.301010
[0m23:44:40.301464 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:44:40.304484 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:44:40.305008 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:44:40.305221 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m23:44:40.305568 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.314248 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.314756 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:44:40.315032 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct stagename as stage_name, stagesortorder as stage_sort_order
        from "dbt"."staging"."stg_salesforce__opportunity"
    )

select
    row_number() over (order by stage_sort_order) as stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
from source
    );
  
  
[0m23:44:40.318000 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.320555 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:44:40.320854 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m23:44:40.321377 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.323613 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:44:40.323905 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m23:44:40.324300 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.325358 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m23:44:40.325565 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:44:40.325746 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m23:44:40.326630 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.328762 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:44:40.329034 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m23:44:40.329569 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.330767 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 23:44:40.301607 => 23:44:40.330660
[0m23:44:40.331001 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m23:44:40.352733 [info ] [Thread-1  ]: 22 of 30 OK created sql table model dim.dim_opportunity_stage .................. [[32mOK[0m in 0.06s]
[0m23:44:40.353177 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:44:40.353421 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m23:44:40.353868 [info ] [Thread-1  ]: 23 of 30 START sql table model dim.dim_pricebook ............................... [RUN]
[0m23:44:40.354512 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m23:44:40.354750 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m23:44:40.357110 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m23:44:40.357653 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 23:44:40.354898 => 23:44:40.357543
[0m23:44:40.357862 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m23:44:40.361351 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m23:44:40.361864 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:44:40.362074 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m23:44:40.362256 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.369457 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.369758 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:44:40.369968 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

select
    pricebook_entry_id,
    pricebook2id as pricebook_id,
    product2id as product_id,
    unitprice as unit_price,
    isactive as is_active,
    usestandardprice as use_standard_price,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__pricebook_entry"
where isdeleted = false  -- Exclude deleted entries
order by pricebook_entry_id
    );
  
  
[0m23:44:40.371552 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.373609 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:44:40.373832 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook" rename to "dim_pricebook__dbt_backup"
[0m23:44:40.374158 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.375901 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:44:40.376097 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m23:44:40.376381 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.377324 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m23:44:40.377511 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:44:40.377680 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m23:44:40.378470 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.380502 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:44:40.380757 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m23:44:40.381227 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.382037 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 23:44:40.357996 => 23:44:40.381940
[0m23:44:40.382250 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m23:44:40.398649 [info ] [Thread-1  ]: 23 of 30 OK created sql table model dim.dim_pricebook .......................... [[32mOK[0m in 0.04s]
[0m23:44:40.399056 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m23:44:40.399279 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m23:44:40.399653 [info ] [Thread-1  ]: 24 of 30 START sql table model dim.dim_product ................................. [RUN]
[0m23:44:40.400147 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m23:44:40.400358 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m23:44:40.402639 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m23:44:40.403169 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 23:44:40.400503 => 23:44:40.403062
[0m23:44:40.403381 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m23:44:40.406009 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m23:44:40.406446 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:44:40.406655 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m23:44:40.406830 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.413468 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.413749 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:44:40.413961 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    productcode as product_code,
    description as product_description,
    isactive as is_active,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    family as product_family,
    externaldatasourceid as external_datasource_id,
    externalid as external_id,
    displayurl as display_url,
    quantityunitofmeasure as quantity_unit_of_measure,
    stockkeepingunit as stock_keeping_unit,
    type as product_type,
    productclass as product_class,
    sourceproductid as source_product_id,
    sellerid as seller_id
from "dbt"."staging"."stg_salesforce__product_2"
where isdeleted = false
-- isactive =1 ?
order by product_name
    );
  
  
[0m23:44:40.416432 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.419665 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:44:40.419923 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product" rename to "dim_product__dbt_backup"
[0m23:44:40.420315 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.422239 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:44:40.422491 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m23:44:40.422804 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.423948 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m23:44:40.424149 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:44:40.424324 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m23:44:40.425138 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.426957 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:44:40.427201 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m23:44:40.427669 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.428462 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 23:44:40.403518 => 23:44:40.428363
[0m23:44:40.428688 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m23:44:40.453217 [info ] [Thread-1  ]: 24 of 30 OK created sql table model dim.dim_product ............................ [[32mOK[0m in 0.05s]
[0m23:44:40.453624 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m23:44:40.453861 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m23:44:40.454235 [info ] [Thread-1  ]: 25 of 30 START sql table model dim.dim_record_type ............................. [RUN]
[0m23:44:40.454786 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m23:44:40.455055 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m23:44:40.457446 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m23:44:40.457924 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 23:44:40.455215 => 23:44:40.457829
[0m23:44:40.458117 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m23:44:40.462515 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m23:44:40.463082 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:44:40.463301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m23:44:40.463567 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.470866 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.471183 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:44:40.471415 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

select
    record_type_id,
    name as record_type_name,
    modulenamespace as module_namespace,
    description as record_type_description,
    businessprocessid as business_process_id,
    sobjecttype as sobject_type,
    isactive as is_active,
    createdbyid as created_by_id,
    createddate as created_date,
    lastmodifiedbyid as last_modified_by_id,
    lastmodifieddate as last_modified_date,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__record_type"
where isdeleted = false
order by record_type_name
    );
  
  
[0m23:44:40.472933 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.475945 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:44:40.476206 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m23:44:40.476597 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.478504 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:44:40.478731 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m23:44:40.479176 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.480258 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m23:44:40.480491 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:44:40.480695 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m23:44:40.481515 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.483338 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:44:40.483598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m23:44:40.484092 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.484926 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 23:44:40.458253 => 23:44:40.484829
[0m23:44:40.485147 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m23:44:40.501648 [info ] [Thread-1  ]: 25 of 30 OK created sql table model dim.dim_record_type ........................ [[32mOK[0m in 0.05s]
[0m23:44:40.502089 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m23:44:40.502341 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m23:44:40.502729 [info ] [Thread-1  ]: 26 of 30 START sql table model dim.dim_solution ................................ [RUN]
[0m23:44:40.503122 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m23:44:40.503330 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m23:44:40.505615 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m23:44:40.506200 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 23:44:40.503481 => 23:44:40.506093
[0m23:44:40.506425 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m23:44:40.509067 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m23:44:40.509504 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:44:40.509703 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m23:44:40.509883 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.519087 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.519409 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:44:40.519644 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solutionnumber as solution_number,
    solutionname as solution_name,
    ispublished as is_published,
    ispublishedinpublickb as is_published_in_public_kb,
    status as solution_status,
    isreviewed as is_reviewed,
    solutionnote as solution_note,
    caseid as case_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    timesused as times_used,
    ishtml as is_html
from "dbt"."staging"."stg_salesforce__solution"
where isdeleted = false
order by solution_name
    );
  
  
[0m23:44:40.521471 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.523658 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:44:40.523887 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m23:44:40.524219 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.526991 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:44:40.527331 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m23:44:40.527749 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.528856 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m23:44:40.529057 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:44:40.529235 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m23:44:40.530058 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.531829 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:44:40.532035 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m23:44:40.532450 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.533190 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 23:44:40.506581 => 23:44:40.533094
[0m23:44:40.533393 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m23:44:40.556003 [info ] [Thread-1  ]: 26 of 30 OK created sql table model dim.dim_solution ........................... [[32mOK[0m in 0.05s]
[0m23:44:40.556372 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m23:44:40.556594 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m23:44:40.556816 [info ] [Thread-1  ]: 27 of 30 START sql table model dim.dim_user .................................... [RUN]
[0m23:44:40.557449 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m23:44:40.557746 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m23:44:40.560256 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m23:44:40.560805 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 23:44:40.557905 => 23:44:40.560692
[0m23:44:40.561024 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m23:44:40.563636 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m23:44:40.564067 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:44:40.564270 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m23:44:40.564455 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.571042 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.571333 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:44:40.571561 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m23:44:40.575577 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.577667 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:44:40.577876 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user" rename to "dim_user__dbt_backup"
[0m23:44:40.578226 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.579912 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:44:40.580106 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m23:44:40.580406 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.582576 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m23:44:40.582883 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:44:40.583075 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m23:44:40.584135 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.585888 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:44:40.586099 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m23:44:40.586597 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.587423 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 23:44:40.561166 => 23:44:40.587326
[0m23:44:40.587642 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m23:44:40.605854 [info ] [Thread-1  ]: 27 of 30 OK created sql table model dim.dim_user ............................... [[32mOK[0m in 0.05s]
[0m23:44:40.606281 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m23:44:40.606527 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_lead_conversion
[0m23:44:40.606889 [info ] [Thread-1  ]: 28 of 30 START sql view model fact.fact_lead_conversion ........................ [RUN]
[0m23:44:40.607303 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_lead_conversion)
[0m23:44:40.607514 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_lead_conversion
[0m23:44:40.609835 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:44:40.610357 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (compile): 23:44:40.607651 => 23:44:40.610253
[0m23:44:40.610561 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_lead_conversion
[0m23:44:40.613207 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:44:40.613688 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:44:40.613880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: BEGIN
[0m23:44:40.614056 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.621123 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.621414 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:44:40.621643 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */

  
  create view "dbt"."fact"."fact_lead_conversion__dbt_tmp" as (
    -- fact_lead_conversion.sql

with lead_data as (
    select 
        lead_id,
        lead_salutation,
        first_name,
        last_name,
        lead_company,
        lead_source,
        lead_status,
        is_converted,
        converted_date,
        converted_account_id,
        converted_contact_id,
        converted_opportunity_id,
        created_date as lead_created_date,
        last_modified_date as lead_last_modified_date
    from 
        "dbt"."dim"."dim_lead"
    where 
        is_converted = true
),

account_data as (
    select 
        account_id,
        account_name
    from 
        "dbt"."dim"."dim_account"
),

opportunity_data as (
    select 
        opportunity_id,
        opportunity_name,
        opportunity_amount
    from 
        "dbt"."dim"."dim_opportunity"
)

select 
    l.lead_id,
    l.first_name,
    l.last_name,
    l.lead_company,
    l.lead_source,
    l.lead_status,
    l.converted_date,
    a.account_name as converted_account_name,
    o.opportunity_name as converted_opportunity_name,
    o.opportunity_amount as converted_opportunity_amount,
    l.lead_created_date,
    l.lead_last_modified_date
from 
    lead_data l
left join 
    account_data a on l.converted_account_id = a.account_id
left join 
    opportunity_data o on l.converted_opportunity_id = o.opportunity_id
  );

[0m23:44:40.622300 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.624553 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:44:40.624776 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter view "dbt"."fact"."fact_lead_conversion" rename to "fact_lead_conversion__dbt_backup"
[0m23:44:40.625084 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.626784 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:44:40.627115 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter view "dbt"."fact"."fact_lead_conversion__dbt_tmp" rename to "fact_lead_conversion"
[0m23:44:40.627577 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.628642 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m23:44:40.628851 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:44:40.629032 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m23:44:40.629563 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.632138 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:44:40.632374 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
drop view if exists "dbt"."fact"."fact_lead_conversion__dbt_backup" cascade
[0m23:44:40.632790 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.633647 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (execute): 23:44:40.610693 => 23:44:40.633545
[0m23:44:40.633920 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: Close
[0m23:44:40.648159 [info ] [Thread-1  ]: 28 of 30 OK created sql view model fact.fact_lead_conversion ................... [[32mOK[0m in 0.04s]
[0m23:44:40.648552 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_lead_conversion
[0m23:44:40.648777 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m23:44:40.649153 [info ] [Thread-1  ]: 29 of 30 START sql incremental model fact.fact_opportunity ..................... [RUN]
[0m23:44:40.649648 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_lead_conversion, now model.elastic_dbt_interview.fact_opportunity)
[0m23:44:40.649860 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m23:44:40.655851 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m23:44:40.656330 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 23:44:40.649994 => 23:44:40.656227
[0m23:44:40.656529 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m23:44:40.673819 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:44:40.674197 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821234440671089"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        a.account_id,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m23:44:40.674426 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.684532 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.688887 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:44:40.689122 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m23:44:40.689390 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.689568 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:44:40.689751 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240821234440671089'
      
      
      order by ordinal_position

  
[0m23:44:40.718033 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.721219 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:44:40.721467 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m23:44:40.735731 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.741485 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:44:40.741759 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m23:44:40.756637 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.765795 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m23:44:40.766291 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:44:40.766515 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240821234440671089"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240821234440671089"
    )
  
[0m23:44:40.767611 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.768568 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m23:44:40.768775 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:44:40.768957 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m23:44:40.769219 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.769644 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 23:44:40.656660 => 23:44:40.769554
[0m23:44:40.769832 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m23:44:40.772661 [info ] [Thread-1  ]: 29 of 30 OK created sql incremental model fact.fact_opportunity ................ [[32mOK[0m in 0.12s]
[0m23:44:40.772997 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m23:44:40.773446 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:44:40.773792 [info ] [Thread-1  ]: 30 of 30 START sql incremental model fact.fact_opportunity_history ............. [RUN]
[0m23:44:40.774245 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_opportunity, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:44:40.774463 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:44:40.776961 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:44:40.777540 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:44:40.774598 => 23:44:40.777420
[0m23:44:40.777772 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:44:40.780621 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:44:40.781116 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:44:40.781305 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:44:40.781479 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:40.788184 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:44:40.788454 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:44:40.788678 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:44:40.789574 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:44:40.777900 => 23:44:40.789471
[0m23:44:40.789803 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:44:40.793683 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:44:40.793976 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:44:40.797013 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m23:44:40.797390 [error] [Thread-1  ]: 30 of 30 ERROR creating sql incremental model fact.fact_opportunity_history .... [[31mERROR[0m in 0.02s]
[0m23:44:40.797690 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:44:40.798504 [debug] [MainThread]: Using duckdb connection "master"
[0m23:44:40.798737 [debug] [MainThread]: On master: BEGIN
[0m23:44:40.798896 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:44:40.805810 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:44:40.806086 [debug] [MainThread]: On master: COMMIT
[0m23:44:40.806255 [debug] [MainThread]: Using duckdb connection "master"
[0m23:44:40.806408 [debug] [MainThread]: On master: COMMIT
[0m23:44:40.806611 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:44:40.806770 [debug] [MainThread]: On master: Close
[0m23:44:40.809254 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:44:40.809527 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:44:40.809879 [info ] [MainThread]: 
[0m23:44:40.810131 [info ] [MainThread]: Finished running 13 table models, 15 view models, 2 incremental models in 0 hours 0 minutes and 1.91 seconds (1.91s).
[0m23:44:40.812393 [debug] [MainThread]: Command end result
[0m23:44:40.821005 [info ] [MainThread]: 
[0m23:44:40.821276 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:44:40.821445 [info ] [MainThread]: 
[0m23:44:40.821609 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m23:44:40.821791 [info ] [MainThread]: 
[0m23:44:40.821966 [info ] [MainThread]: Done. PASS=29 WARN=0 ERROR=1 SKIP=0 TOTAL=30
[0m23:44:40.822308 [debug] [MainThread]: Command `dbt run` failed at 23:44:40.822261 after 2.12 seconds
[0m23:44:40.822507 [debug] [MainThread]: Flushing usage events


============================== 23:45:32.292148 | c78cd805-1a78-4aa0-8e13-910463d8f799 ==============================
[0m23:45:32.292148 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:45:32.294922 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity.sql', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m23:45:32.295188 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:45:32.373779 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:45:32.392245 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:45:32.433417 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:45:32.433712 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:45:32.434701 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m23:45:32.448260 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:45:32.449635 [info ] [MainThread]: 
[0m23:45:32.450056 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:45:32.450590 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:45:32.480215 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:45:32.480606 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:45:32.480800 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:32.491123 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.492189 [debug] [ThreadPool]: On list_dbt: Close
[0m23:45:32.494530 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:45:32.494936 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:45:32.497893 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:45:32.498091 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:45:32.498246 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:32.506001 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.506282 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:45:32.506469 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:45:32.506745 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.507298 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:45:32.507466 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:45:32.507614 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:45:32.507833 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.507995 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:45:32.511193 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m23:45:32.514712 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:45:32.514914 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:45:32.515071 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:32.522344 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.522606 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:45:32.522782 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:45:32.538581 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.542074 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:45:32.543409 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:45:32.543586 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:45:32.546562 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m23:45:32.548306 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:45:32.548816 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:45:32.548977 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:32.556558 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.556849 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:45:32.557050 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:45:32.573176 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.577048 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:45:32.577384 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:45:32.577556 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:45:32.580306 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m23:45:32.582643 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:45:32.582824 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:45:32.582975 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:32.589881 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.590128 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:45:32.590304 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:45:32.608475 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.609396 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:45:32.609652 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:45:32.609804 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:45:32.612336 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m23:45:32.615736 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:45:32.615945 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:45:32.616102 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:32.622968 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.623231 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:45:32.623401 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:45:32.639142 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:45:32.642702 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:45:32.642932 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:45:32.643093 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:45:32.646470 [debug] [MainThread]: Using duckdb connection "master"
[0m23:45:32.646690 [debug] [MainThread]: On master: BEGIN
[0m23:45:32.646848 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:45:32.653932 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:45:32.654215 [debug] [MainThread]: On master: COMMIT
[0m23:45:32.654378 [debug] [MainThread]: Using duckdb connection "master"
[0m23:45:32.654525 [debug] [MainThread]: On master: COMMIT
[0m23:45:32.654726 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:45:32.654880 [debug] [MainThread]: On master: Close
[0m23:45:32.656548 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:45:32.656760 [info ] [MainThread]: 
[0m23:45:32.658913 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m23:45:32.659203 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity ....................... [RUN]
[0m23:45:32.659556 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity)
[0m23:45:32.659746 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m23:45:32.669083 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m23:45:32.669810 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 23:45:32.659884 => 23:45:32.669658
[0m23:45:32.670188 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m23:45:32.698724 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:45:32.699120 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821234532685558"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        a.account_id,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m23:45:32.699360 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:45:32.708522 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:45:32.713410 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:45:32.713665 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m23:45:32.714055 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:45:32.714280 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:45:32.714482 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240821234532685558'
      
      
      order by ordinal_position

  
[0m23:45:32.741911 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:45:32.745072 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:45:32.745314 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m23:45:32.759308 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:45:32.763651 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:45:32.763882 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m23:45:32.778345 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:45:32.787339 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m23:45:32.787866 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:45:32.788117 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240821234532685558"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240821234532685558"
    )
  
[0m23:45:32.789126 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:45:32.797400 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m23:45:32.797726 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:45:32.797921 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m23:45:32.798275 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:45:32.798777 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 23:45:32.670377 => 23:45:32.798673
[0m23:45:32.798981 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m23:45:32.801634 [info ] [Thread-1  ]: 1 of 1 OK created sql incremental model fact.fact_opportunity .................. [[32mOK[0m in 0.14s]
[0m23:45:32.802042 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m23:45:32.802846 [debug] [MainThread]: Using duckdb connection "master"
[0m23:45:32.803102 [debug] [MainThread]: On master: BEGIN
[0m23:45:32.803262 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:45:32.810376 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:45:32.810640 [debug] [MainThread]: On master: COMMIT
[0m23:45:32.810801 [debug] [MainThread]: Using duckdb connection "master"
[0m23:45:32.810956 [debug] [MainThread]: On master: COMMIT
[0m23:45:32.811149 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:45:32.811306 [debug] [MainThread]: On master: Close
[0m23:45:32.813248 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:45:32.813505 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m23:45:32.813701 [info ] [MainThread]: 
[0m23:45:32.813894 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.36 seconds (0.36s).
[0m23:45:32.814240 [debug] [MainThread]: Command end result
[0m23:45:32.821908 [info ] [MainThread]: 
[0m23:45:32.822191 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:45:32.822360 [info ] [MainThread]: 
[0m23:45:32.822540 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:45:32.822894 [debug] [MainThread]: Command `dbt run` succeeded at 23:45:32.822846 after 0.55 seconds
[0m23:45:32.823080 [debug] [MainThread]: Flushing usage events


============================== 23:46:10.552868 | 9d3848f3-093c-4c78-9def-73cab467b8b0 ==============================
[0m23:46:10.552868 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:46:10.555306 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m23:46:10.555559 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:46:10.629940 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:46:10.647826 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:46:10.683308 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:46:10.683592 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:46:10.684569 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m23:46:10.698621 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:46:10.700048 [info ] [MainThread]: 
[0m23:46:10.700473 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:46:10.701185 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:46:10.708681 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:46:10.708942 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:46:10.709114 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:10.717793 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.718825 [debug] [ThreadPool]: On list_dbt: Close
[0m23:46:10.721078 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:46:10.721600 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:46:10.724772 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:46:10.725033 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:46:10.725193 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:10.732331 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.732605 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:46:10.732771 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:46:10.733019 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.733594 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:46:10.733893 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:46:10.734061 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:46:10.734345 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.734514 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:46:10.737698 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m23:46:10.741153 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:46:10.741361 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:46:10.741516 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:10.748930 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.749218 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:46:10.749403 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:46:10.765092 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.768789 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:46:10.769241 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:46:10.769415 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:46:10.771942 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m23:46:10.774283 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:46:10.774464 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:46:10.774612 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:10.781623 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.781896 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:46:10.782078 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:46:10.800582 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.801680 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:46:10.801984 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:46:10.802156 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:46:10.804804 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m23:46:10.807504 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:46:10.807721 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:46:10.807884 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:10.814874 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.815184 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:46:10.815362 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:46:10.835554 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.839422 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:46:10.839673 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:46:10.839837 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:46:10.842132 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m23:46:10.844882 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:46:10.845080 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:46:10.845229 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:10.851869 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.852135 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:46:10.852309 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:46:10.868346 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:10.871900 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:46:10.872146 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:46:10.872301 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:46:10.876126 [debug] [MainThread]: Using duckdb connection "master"
[0m23:46:10.876409 [debug] [MainThread]: On master: BEGIN
[0m23:46:10.876573 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:46:10.883418 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:46:10.883734 [debug] [MainThread]: On master: COMMIT
[0m23:46:10.883903 [debug] [MainThread]: Using duckdb connection "master"
[0m23:46:10.884059 [debug] [MainThread]: On master: COMMIT
[0m23:46:10.884265 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:46:10.884425 [debug] [MainThread]: On master: Close
[0m23:46:10.886327 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:46:10.886588 [info ] [MainThread]: 
[0m23:46:10.887731 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:46:10.888070 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:46:10.888474 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:46:10.888686 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:46:10.894339 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:46:10.894916 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:46:10.888826 => 23:46:10.894802
[0m23:46:10.895116 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:46:10.924594 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:46:10.925393 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:46:10.925685 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:46:10.925886 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:46:10.932580 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:46:10.932851 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:46:10.933065 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:46:10.933959 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:46:10.895250 => 23:46:10.933852
[0m23:46:10.934166 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:46:10.938163 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:46:10.938465 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:46:10.940750 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m23:46:10.941193 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.05s]
[0m23:46:10.941556 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:46:10.942304 [debug] [MainThread]: Using duckdb connection "master"
[0m23:46:10.942495 [debug] [MainThread]: On master: BEGIN
[0m23:46:10.942649 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:46:10.949437 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:46:10.949680 [debug] [MainThread]: On master: COMMIT
[0m23:46:10.949840 [debug] [MainThread]: Using duckdb connection "master"
[0m23:46:10.950225 [debug] [MainThread]: On master: COMMIT
[0m23:46:10.950623 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:46:10.950855 [debug] [MainThread]: On master: Close
[0m23:46:10.952759 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:46:10.953014 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:46:10.953212 [info ] [MainThread]: 
[0m23:46:10.953421 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m23:46:10.953788 [debug] [MainThread]: Command end result
[0m23:46:10.961749 [info ] [MainThread]: 
[0m23:46:10.962038 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:46:10.962199 [info ] [MainThread]: 
[0m23:46:10.962361 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 27:     on o.account_id = a.account_id
                  ^
[0m23:46:10.962519 [info ] [MainThread]: 
[0m23:46:10.962690 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:46:10.963043 [debug] [MainThread]: Command `dbt run` failed at 23:46:10.962993 after 0.43 seconds
[0m23:46:10.963236 [debug] [MainThread]: Flushing usage events


============================== 23:46:56.802546 | 9af9a82e-11c3-4c0d-9ea9-415d4df60b6d ==============================
[0m23:46:56.802546 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:46:56.804907 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m23:46:56.805142 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:46:56.879234 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:46:56.898035 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:46:56.934359 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:46:56.934821 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m23:46:56.962488 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m23:46:56.976692 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:46:56.978361 [info ] [MainThread]: 
[0m23:46:56.978876 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:46:56.979557 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:46:56.986655 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:46:56.986916 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:46:56.987098 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:56.995509 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:56.996477 [debug] [ThreadPool]: On list_dbt: Close
[0m23:46:56.998897 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:46:56.999372 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:46:57.002498 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:46:57.002723 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:46:57.002898 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:57.009866 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.010092 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:46:57.010250 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:46:57.010482 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.010950 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:46:57.011115 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:46:57.011259 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:46:57.011468 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.011619 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:46:57.014770 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m23:46:57.018330 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:46:57.018603 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:46:57.018840 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:57.025681 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.025951 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:46:57.026132 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:46:57.042245 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.045760 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:46:57.046199 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:46:57.046377 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:46:57.048538 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m23:46:57.050873 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:46:57.051062 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:46:57.051216 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:57.058146 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.058419 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:46:57.058611 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:46:57.078737 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.079915 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:46:57.080320 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:46:57.080593 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:46:57.083253 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m23:46:57.085882 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:46:57.086110 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:46:57.086267 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:57.093318 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.093623 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:46:57.093806 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:46:57.111378 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.115317 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:46:57.115657 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:46:57.115821 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:46:57.118496 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m23:46:57.121124 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:46:57.121353 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:46:57.121520 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:46:57.128178 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.128448 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:46:57.128624 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:46:57.145802 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:46:57.149773 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:46:57.150069 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:46:57.150245 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:46:57.153606 [debug] [MainThread]: Using duckdb connection "master"
[0m23:46:57.153878 [debug] [MainThread]: On master: BEGIN
[0m23:46:57.154047 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:46:57.162323 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:46:57.162646 [debug] [MainThread]: On master: COMMIT
[0m23:46:57.162826 [debug] [MainThread]: Using duckdb connection "master"
[0m23:46:57.162995 [debug] [MainThread]: On master: COMMIT
[0m23:46:57.163217 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:46:57.163389 [debug] [MainThread]: On master: Close
[0m23:46:57.165328 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:46:57.165615 [info ] [MainThread]: 
[0m23:46:57.167076 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:46:57.167425 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:46:57.167845 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:46:57.168058 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:46:57.174476 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:46:57.175140 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:46:57.168207 => 23:46:57.175003
[0m23:46:57.175384 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:46:57.229180 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:46:57.229864 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:46:57.230071 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:46:57.230258 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:46:57.237307 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:46:57.237618 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:46:57.237829 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    o.account_id,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:46:57.238727 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:46:57.175536 => 23:46:57.238621
[0m23:46:57.238936 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:46:57.240233 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:46:57.240425 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:46:57.242509 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:46:57.242879 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.08s]
[0m23:46:57.243207 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:46:57.243939 [debug] [MainThread]: Using duckdb connection "master"
[0m23:46:57.244132 [debug] [MainThread]: On master: BEGIN
[0m23:46:57.244285 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:46:57.251395 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:46:57.251689 [debug] [MainThread]: On master: COMMIT
[0m23:46:57.251851 [debug] [MainThread]: Using duckdb connection "master"
[0m23:46:57.251998 [debug] [MainThread]: On master: COMMIT
[0m23:46:57.252195 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:46:57.252350 [debug] [MainThread]: On master: Close
[0m23:46:57.254131 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:46:57.254304 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:46:57.254482 [info ] [MainThread]: 
[0m23:46:57.254659 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m23:46:57.254985 [debug] [MainThread]: Command end result
[0m23:46:57.261775 [info ] [MainThread]: 
[0m23:46:57.262008 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:46:57.262170 [info ] [MainThread]: 
[0m23:46:57.262329 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:46:57.262484 [info ] [MainThread]: 
[0m23:46:57.262898 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:46:57.263370 [debug] [MainThread]: Command `dbt run` failed at 23:46:57.263302 after 0.48 seconds
[0m23:46:57.263626 [debug] [MainThread]: Flushing usage events


============================== 23:47:31.857346 | 18650976-14d5-4c68-a6c4-69bef5cad4ff ==============================
[0m23:47:31.857346 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:47:31.859731 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m23:47:31.859979 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:47:31.957627 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:47:31.975629 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:47:32.010889 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:47:32.011383 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m23:47:32.068521 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m23:47:32.081273 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:47:32.082644 [info ] [MainThread]: 
[0m23:47:32.083067 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:47:32.083607 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:47:32.090647 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:47:32.090876 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:47:32.091054 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:47:32.100691 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.101723 [debug] [ThreadPool]: On list_dbt: Close
[0m23:47:32.103909 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:47:32.104284 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:47:32.107070 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:47:32.107254 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:47:32.107403 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:47:32.114885 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.115219 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:47:32.115393 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:47:32.115644 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.116173 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:47:32.116337 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:47:32.116481 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:47:32.116698 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.116852 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:47:32.120062 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m23:47:32.146901 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:47:32.147252 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:47:32.147415 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:47:32.154657 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.154987 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:47:32.155184 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:47:32.172162 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.175919 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:47:32.176341 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:47:32.176517 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:47:32.179593 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m23:47:32.182539 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:47:32.182835 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:47:32.183002 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:47:32.190410 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.190685 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:47:32.190861 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:47:32.206685 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.210181 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:47:32.210422 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:47:32.210587 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:47:32.213300 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m23:47:32.215218 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:47:32.215427 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:47:32.215573 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:47:32.222832 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.223108 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:47:32.223283 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:47:32.241215 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.242110 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:47:32.242340 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:47:32.242504 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:47:32.244904 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m23:47:32.247349 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:47:32.247526 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:47:32.247669 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:47:32.254360 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.254606 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:47:32.254776 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:47:32.270032 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:47:32.273468 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:47:32.273699 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:47:32.273870 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:47:32.277062 [debug] [MainThread]: Using duckdb connection "master"
[0m23:47:32.277255 [debug] [MainThread]: On master: BEGIN
[0m23:47:32.277412 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:47:32.284518 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:47:32.284785 [debug] [MainThread]: On master: COMMIT
[0m23:47:32.284945 [debug] [MainThread]: Using duckdb connection "master"
[0m23:47:32.285094 [debug] [MainThread]: On master: COMMIT
[0m23:47:32.285301 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:47:32.285466 [debug] [MainThread]: On master: Close
[0m23:47:32.287075 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:47:32.287286 [info ] [MainThread]: 
[0m23:47:32.288340 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:47:32.288608 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:47:32.288977 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:47:32.289170 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:47:32.295002 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:47:32.295638 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:47:32.289305 => 23:47:32.295522
[0m23:47:32.295850 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:47:32.328815 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:47:32.329440 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:47:32.329769 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:47:32.330078 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:47:32.337536 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:47:32.337871 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:47:32.338108 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    o.account_id,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:47:32.339057 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:47:32.295991 => 23:47:32.338941
[0m23:47:32.339285 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:47:32.344334 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:47:32.344669 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:47:32.347382 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:47:32.347860 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.06s]
[0m23:47:32.348243 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:47:32.349228 [debug] [MainThread]: Using duckdb connection "master"
[0m23:47:32.349517 [debug] [MainThread]: On master: BEGIN
[0m23:47:32.349693 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:47:32.357270 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:47:32.357642 [debug] [MainThread]: On master: COMMIT
[0m23:47:32.357809 [debug] [MainThread]: Using duckdb connection "master"
[0m23:47:32.357964 [debug] [MainThread]: On master: COMMIT
[0m23:47:32.358172 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:47:32.358346 [debug] [MainThread]: On master: Close
[0m23:47:32.360430 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:47:32.360725 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:47:32.360934 [info ] [MainThread]: 
[0m23:47:32.361212 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m23:47:32.361583 [debug] [MainThread]: Command end result
[0m23:47:32.369011 [info ] [MainThread]: 
[0m23:47:32.369403 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:47:32.369588 [info ] [MainThread]: 
[0m23:47:32.369818 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:47:32.370031 [info ] [MainThread]: 
[0m23:47:32.370217 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:47:32.370594 [debug] [MainThread]: Command `dbt run` failed at 23:47:32.370535 after 0.53 seconds
[0m23:47:32.370814 [debug] [MainThread]: Flushing usage events


============================== 23:48:27.912694 | 568808e6-c73f-4bc3-8ac3-009e34ef1c9f ==============================
[0m23:48:27.912694 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:48:27.915835 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m23:48:27.916098 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:48:27.999415 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:48:28.020167 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:48:28.072021 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:48:28.072537 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m23:48:28.129752 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m23:48:28.142870 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:48:28.144257 [info ] [MainThread]: 
[0m23:48:28.144685 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:48:28.145377 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:48:28.152908 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:48:28.153200 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:48:28.153381 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:48:28.169419 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.170369 [debug] [ThreadPool]: On list_dbt: Close
[0m23:48:28.172533 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:48:28.173013 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:48:28.176046 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:48:28.176267 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:48:28.176441 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:48:28.183879 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.184157 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:48:28.184328 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:48:28.184579 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.185127 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:48:28.185298 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:48:28.185449 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:48:28.185657 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.185815 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:48:28.188701 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_fact)
[0m23:48:28.215139 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:48:28.215458 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:48:28.215618 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:48:28.222567 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.222818 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:48:28.223106 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:48:28.240544 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.244421 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:48:28.245838 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:48:28.246070 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:48:28.250209 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m23:48:28.251924 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:48:28.252139 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:48:28.252312 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:48:28.261511 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.261772 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:48:28.261958 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:48:28.279065 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.282898 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:48:28.283172 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:48:28.283337 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:48:28.286159 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m23:48:28.287879 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:48:28.288120 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:48:28.289188 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:48:28.296324 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.296597 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:48:28.296771 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:48:28.314859 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.315804 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:48:28.316050 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:48:28.316213 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:48:28.318534 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m23:48:28.321065 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:48:28.321250 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:48:28.321402 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:48:28.328487 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.328760 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:48:28.328932 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:48:28.344366 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:28.347865 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:48:28.348094 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:48:28.348254 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:48:28.351613 [debug] [MainThread]: Using duckdb connection "master"
[0m23:48:28.351807 [debug] [MainThread]: On master: BEGIN
[0m23:48:28.351959 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:48:28.358351 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:48:28.358599 [debug] [MainThread]: On master: COMMIT
[0m23:48:28.358761 [debug] [MainThread]: Using duckdb connection "master"
[0m23:48:28.358908 [debug] [MainThread]: On master: COMMIT
[0m23:48:28.359106 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:48:28.359268 [debug] [MainThread]: On master: Close
[0m23:48:28.361022 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:48:28.361239 [info ] [MainThread]: 
[0m23:48:28.363579 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:48:28.363846 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:48:28.364204 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:48:28.364390 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:48:28.370134 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:48:28.370690 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:48:28.364519 => 23:48:28.370582
[0m23:48:28.370891 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:48:28.402533 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:48:28.403172 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:48:28.403380 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:48:28.403567 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:48:28.410984 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:48:28.411309 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:48:28.411534 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    o.account_id,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:48:28.412466 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:48:28.371027 => 23:48:28.412348
[0m23:48:28.412686 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:48:28.416591 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:48:28.416818 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:48:28.419083 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:48:28.419469 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.06s]
[0m23:48:28.419785 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:48:28.420550 [debug] [MainThread]: Using duckdb connection "master"
[0m23:48:28.420808 [debug] [MainThread]: On master: BEGIN
[0m23:48:28.420976 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:48:28.427907 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:48:28.428225 [debug] [MainThread]: On master: COMMIT
[0m23:48:28.428391 [debug] [MainThread]: Using duckdb connection "master"
[0m23:48:28.428542 [debug] [MainThread]: On master: COMMIT
[0m23:48:28.428752 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:48:28.428911 [debug] [MainThread]: On master: Close
[0m23:48:28.430662 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:48:28.430859 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:48:28.431046 [info ] [MainThread]: 
[0m23:48:28.431257 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m23:48:28.431620 [debug] [MainThread]: Command end result
[0m23:48:28.439410 [info ] [MainThread]: 
[0m23:48:28.439709 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:48:28.439886 [info ] [MainThread]: 
[0m23:48:28.440176 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:48:28.440422 [info ] [MainThread]: 
[0m23:48:28.440658 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:48:28.441063 [debug] [MainThread]: Command `dbt run` failed at 23:48:28.440980 after 0.55 seconds
[0m23:48:28.441343 [debug] [MainThread]: Flushing usage events


============================== 23:48:43.907515 | 3728f7e3-d420-471a-8cef-029abd564415 ==============================
[0m23:48:43.907515 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:48:43.910009 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m23:48:43.910260 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:48:43.984923 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:48:44.004431 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:48:44.005143 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m23:48:44.871012 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m23:48:44.883842 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:48:44.885165 [info ] [MainThread]: 
[0m23:48:44.885593 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:48:44.886258 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:48:44.890718 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:48:44.890940 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:48:44.891113 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:48:44.899644 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:44.900496 [debug] [ThreadPool]: On list_dbt: Close
[0m23:48:44.902701 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:48:44.903073 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:48:44.906034 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:48:44.906227 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:48:44.906381 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:48:44.913244 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:44.913543 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:48:44.913711 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:48:44.913957 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:44.914466 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:48:44.914634 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:48:44.914782 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:48:44.914987 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:44.915146 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:48:44.918210 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m23:48:44.921926 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:48:44.922154 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:48:44.922315 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:48:44.929175 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:44.929382 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:48:44.929546 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:48:44.945613 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:44.949251 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:48:44.949638 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:48:44.949802 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:48:44.952249 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m23:48:44.954722 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:48:44.954914 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:48:44.955069 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:48:44.962223 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:44.962509 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:48:44.962689 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:48:44.978504 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:44.982172 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:48:44.982435 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:48:44.982609 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:48:44.985562 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m23:48:44.987956 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:48:44.988149 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:48:44.988296 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:48:44.995021 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:44.995267 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:48:44.995441 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:48:45.014442 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:45.015478 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:48:45.015717 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:48:45.015877 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:48:45.018729 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m23:48:45.021270 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:48:45.021464 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:48:45.021615 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:48:45.028667 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:45.028946 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:48:45.029130 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:48:45.045284 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:48:45.048850 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:48:45.049096 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:48:45.049253 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:48:45.052146 [debug] [MainThread]: Using duckdb connection "master"
[0m23:48:45.052330 [debug] [MainThread]: On master: BEGIN
[0m23:48:45.052479 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:48:45.059933 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:48:45.060202 [debug] [MainThread]: On master: COMMIT
[0m23:48:45.060367 [debug] [MainThread]: Using duckdb connection "master"
[0m23:48:45.060519 [debug] [MainThread]: On master: COMMIT
[0m23:48:45.060723 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:48:45.060886 [debug] [MainThread]: On master: Close
[0m23:48:45.062655 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:48:45.062891 [info ] [MainThread]: 
[0m23:48:45.064091 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:48:45.064385 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:48:45.064756 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:48:45.064958 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:48:45.071292 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:48:45.072104 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:48:45.065094 => 23:48:45.071989
[0m23:48:45.072310 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:48:45.126225 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:48:45.127214 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:48:45.127442 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:48:45.127637 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:48:45.134879 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:48:45.135170 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:48:45.135391 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    o.account_id,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:48:45.136289 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:48:45.072446 => 23:48:45.136182
[0m23:48:45.136510 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:48:45.137906 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:48:45.138134 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:48:45.140247 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:48:45.140648 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.08s]
[0m23:48:45.140979 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:48:45.141843 [debug] [MainThread]: Using duckdb connection "master"
[0m23:48:45.142089 [debug] [MainThread]: On master: BEGIN
[0m23:48:45.142244 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:48:45.148818 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:48:45.149058 [debug] [MainThread]: On master: COMMIT
[0m23:48:45.149222 [debug] [MainThread]: Using duckdb connection "master"
[0m23:48:45.149376 [debug] [MainThread]: On master: COMMIT
[0m23:48:45.149570 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:48:45.149729 [debug] [MainThread]: On master: Close
[0m23:48:45.151438 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:48:45.151623 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:48:45.151798 [info ] [MainThread]: 
[0m23:48:45.151976 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m23:48:45.152296 [debug] [MainThread]: Command end result
[0m23:48:45.159553 [info ] [MainThread]: 
[0m23:48:45.159808 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:48:45.159971 [info ] [MainThread]: 
[0m23:48:45.160228 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:48:45.160559 [info ] [MainThread]: 
[0m23:48:45.160805 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:48:45.161205 [debug] [MainThread]: Command `dbt run` failed at 23:48:45.161135 after 1.27 seconds
[0m23:48:45.161450 [debug] [MainThread]: Flushing usage events


============================== 23:49:17.056199 | a726d83e-d6a1-49fc-994b-4bfbe624ca1d ==============================
[0m23:49:17.056199 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:49:17.058589 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m23:49:17.058841 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:49:17.132084 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:49:17.151258 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:49:17.187315 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:49:17.187802 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m23:49:17.215377 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m23:49:17.228332 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:49:17.229638 [info ] [MainThread]: 
[0m23:49:17.230187 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:49:17.230911 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:49:17.237633 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:49:17.237963 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:49:17.238155 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:49:17.246393 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.247321 [debug] [ThreadPool]: On list_dbt: Close
[0m23:49:17.249832 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:49:17.250294 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:49:17.253306 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:49:17.253544 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:49:17.253697 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:49:17.261421 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.261674 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:49:17.261837 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:49:17.262083 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.262571 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:49:17.262739 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:49:17.262883 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:49:17.263086 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.263245 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:49:17.266397 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m23:49:17.269911 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:49:17.270278 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:49:17.270484 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:49:17.277222 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.277496 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:49:17.277675 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:49:17.295853 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.296831 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:49:17.297264 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:49:17.297429 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:49:17.300080 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m23:49:17.303237 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:49:17.303445 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:49:17.303593 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:49:17.310770 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.311058 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:49:17.311247 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:49:17.328686 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.332339 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:49:17.332672 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:49:17.332840 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:49:17.335414 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m23:49:17.337343 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:49:17.337548 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:49:17.337701 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:49:17.345015 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.345332 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:49:17.345509 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:49:17.362049 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.365811 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:49:17.366050 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:49:17.366205 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:49:17.369082 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m23:49:17.371400 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:49:17.371586 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:49:17.371767 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:49:17.378849 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.379123 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:49:17.379307 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:49:17.395430 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:17.398989 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:49:17.399269 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:49:17.399436 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:49:17.402793 [debug] [MainThread]: Using duckdb connection "master"
[0m23:49:17.402993 [debug] [MainThread]: On master: BEGIN
[0m23:49:17.403163 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:49:17.410095 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:49:17.410371 [debug] [MainThread]: On master: COMMIT
[0m23:49:17.410537 [debug] [MainThread]: Using duckdb connection "master"
[0m23:49:17.410685 [debug] [MainThread]: On master: COMMIT
[0m23:49:17.410892 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:49:17.411050 [debug] [MainThread]: On master: Close
[0m23:49:17.412760 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:49:17.412979 [info ] [MainThread]: 
[0m23:49:17.414216 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:49:17.414508 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:49:17.414882 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:49:17.415080 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:49:17.420760 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:49:17.421344 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:49:17.415212 => 23:49:17.421224
[0m23:49:17.421555 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:49:17.477774 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:49:17.478487 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:49:17.478686 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:49:17.478864 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:49:17.485941 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:49:17.486245 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:49:17.486458 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    o.account_id,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    o.opportunity_id = a.opportunity_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:49:17.486944 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:49:17.421688 => 23:49:17.486838
[0m23:49:17.487151 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:49:17.488468 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:49:17.488663 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:49:17.491079 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Parser Error: syntax error at or near "o"
[0m23:49:17.491573 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.08s]
[0m23:49:17.491929 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:49:17.492737 [debug] [MainThread]: Using duckdb connection "master"
[0m23:49:17.492941 [debug] [MainThread]: On master: BEGIN
[0m23:49:17.493100 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:49:17.500297 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:49:17.500613 [debug] [MainThread]: On master: COMMIT
[0m23:49:17.500785 [debug] [MainThread]: Using duckdb connection "master"
[0m23:49:17.500933 [debug] [MainThread]: On master: COMMIT
[0m23:49:17.501132 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:49:17.501287 [debug] [MainThread]: On master: Close
[0m23:49:17.503257 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:49:17.503435 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:49:17.503618 [info ] [MainThread]: 
[0m23:49:17.503796 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m23:49:17.504123 [debug] [MainThread]: Command end result
[0m23:49:17.511076 [info ] [MainThread]: 
[0m23:49:17.511352 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:49:17.511633 [info ] [MainThread]: 
[0m23:49:17.511901 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Parser Error: syntax error at or near "o"
[0m23:49:17.512082 [info ] [MainThread]: 
[0m23:49:17.512263 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:49:17.512585 [debug] [MainThread]: Command `dbt run` failed at 23:49:17.512538 after 0.47 seconds
[0m23:49:17.512775 [debug] [MainThread]: Flushing usage events


============================== 23:49:43.103421 | c1f0388c-8090-423d-82a2-e98264732b9e ==============================
[0m23:49:43.103421 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:49:43.105886 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m23:49:43.106131 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:49:43.180290 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:49:43.198459 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:49:43.234438 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:49:43.234909 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m23:49:43.262397 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m23:49:43.275673 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:49:43.277238 [info ] [MainThread]: 
[0m23:49:43.277715 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:49:43.278315 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:49:43.285327 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:49:43.285644 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:49:43.285833 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:49:43.294754 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.295753 [debug] [ThreadPool]: On list_dbt: Close
[0m23:49:43.297898 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:49:43.298393 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:49:43.301290 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:49:43.301492 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:49:43.301655 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:49:43.308989 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.309305 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:49:43.309482 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:49:43.309729 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.310250 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:49:43.310419 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:49:43.310689 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:49:43.311016 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.311200 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:49:43.314400 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m23:49:43.317948 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:49:43.318214 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:49:43.318375 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:49:43.325724 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.325996 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:49:43.326174 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:49:43.341972 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.345640 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:49:43.346086 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:49:43.346261 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:49:43.349109 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m23:49:43.352263 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:49:43.352519 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:49:43.352673 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:49:43.359355 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.359617 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:49:43.359801 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:49:43.376267 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.379810 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:49:43.380049 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:49:43.380212 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:49:43.382788 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m23:49:43.384755 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:49:43.384990 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:49:43.385157 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:49:43.392399 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.392653 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:49:43.392826 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:49:43.410733 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.411671 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:49:43.411904 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:49:43.412073 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:49:43.414495 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m23:49:43.416867 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:49:43.417047 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:49:43.417195 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:49:43.423814 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.424071 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:49:43.424245 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:49:43.439410 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:49:43.443059 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:49:43.443290 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:49:43.443447 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:49:43.446708 [debug] [MainThread]: Using duckdb connection "master"
[0m23:49:43.446912 [debug] [MainThread]: On master: BEGIN
[0m23:49:43.447070 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:49:43.453907 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:49:43.454175 [debug] [MainThread]: On master: COMMIT
[0m23:49:43.454334 [debug] [MainThread]: Using duckdb connection "master"
[0m23:49:43.454483 [debug] [MainThread]: On master: COMMIT
[0m23:49:43.454678 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:49:43.454841 [debug] [MainThread]: On master: Close
[0m23:49:43.456639 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:49:43.456923 [info ] [MainThread]: 
[0m23:49:43.458153 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:49:43.458474 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:49:43.458884 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:49:43.459081 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:49:43.464989 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:49:43.465517 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:49:43.459226 => 23:49:43.465410
[0m23:49:43.465721 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:49:43.519536 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:49:43.520237 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:49:43.520444 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:49:43.520625 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:49:43.527643 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:49:43.527921 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:49:43.528136 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    o.account_id,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.opportunity_id = a.opportunity_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:49:43.529025 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:49:43.465855 => 23:49:43.528912
[0m23:49:43.529233 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:49:43.530526 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:49:43.530720 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:49:43.532742 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "a" does not have a column named "opportunity_id"
  LINE 28:     on o.opportunity_id = a.opportunity_id
                                     ^
[0m23:49:43.533131 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.07s]
[0m23:49:43.533455 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:49:43.534155 [debug] [MainThread]: Using duckdb connection "master"
[0m23:49:43.534323 [debug] [MainThread]: On master: BEGIN
[0m23:49:43.534469 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:49:43.541176 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:49:43.541445 [debug] [MainThread]: On master: COMMIT
[0m23:49:43.541608 [debug] [MainThread]: Using duckdb connection "master"
[0m23:49:43.541762 [debug] [MainThread]: On master: COMMIT
[0m23:49:43.541964 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:49:43.542130 [debug] [MainThread]: On master: Close
[0m23:49:43.543917 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:49:43.544113 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:49:43.544304 [info ] [MainThread]: 
[0m23:49:43.544488 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m23:49:43.544820 [debug] [MainThread]: Command end result
[0m23:49:43.551692 [info ] [MainThread]: 
[0m23:49:43.552055 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:49:43.552364 [info ] [MainThread]: 
[0m23:49:43.552567 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "a" does not have a column named "opportunity_id"
  LINE 28:     on o.opportunity_id = a.opportunity_id
                                     ^
[0m23:49:43.552751 [info ] [MainThread]: 
[0m23:49:43.552932 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:49:43.553265 [debug] [MainThread]: Command `dbt run` failed at 23:49:43.553215 after 0.47 seconds
[0m23:49:43.553463 [debug] [MainThread]: Flushing usage events


============================== 23:50:13.609168 | b95eea42-6599-4e55-8c06-9b51f4297963 ==============================
[0m23:50:13.609168 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:50:13.611597 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'send_anonymous_usage_stats': 'False'}
[0m23:50:13.611842 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:50:13.687851 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:50:13.706726 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:50:13.749605 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:50:13.750128 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m23:50:13.780445 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m23:50:13.794289 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:50:13.796409 [info ] [MainThread]: 
[0m23:50:13.797054 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:50:13.797818 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:50:13.806130 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:50:13.806469 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:50:13.806662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:50:13.815406 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.816386 [debug] [ThreadPool]: On list_dbt: Close
[0m23:50:13.818326 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:50:13.818775 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:50:13.821603 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:50:13.821797 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:50:13.821951 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:13.829180 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.829555 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:50:13.829750 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:50:13.830051 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.830601 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:50:13.830766 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:50:13.830910 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:50:13.831127 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.831294 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:50:13.834596 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m23:50:13.838233 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:50:13.838541 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:50:13.838811 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:13.846033 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.846284 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:50:13.846534 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:50:13.863098 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.867071 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:50:13.867542 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:50:13.867720 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:50:13.870313 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m23:50:13.873376 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:50:13.873611 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:50:13.873802 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:13.881396 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.881722 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:50:13.881921 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:50:13.901915 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.906032 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:50:13.906459 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:50:13.906650 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:50:13.909789 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m23:50:13.912741 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:50:13.913002 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:50:13.913170 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:13.920478 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.920800 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:50:13.920994 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:50:13.938391 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.942105 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:50:13.942364 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:50:13.942614 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:50:13.945103 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m23:50:13.946878 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:50:13.947068 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:50:13.947217 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:13.954713 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.954968 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:50:13.955138 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:50:13.973380 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:13.974335 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:50:13.974592 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:50:13.974747 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:50:13.977988 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:13.978203 [debug] [MainThread]: On master: BEGIN
[0m23:50:13.978362 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:50:13.985502 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:13.985771 [debug] [MainThread]: On master: COMMIT
[0m23:50:13.985931 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:13.986087 [debug] [MainThread]: On master: COMMIT
[0m23:50:13.986292 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:13.986452 [debug] [MainThread]: On master: Close
[0m23:50:13.988114 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:50:13.988326 [info ] [MainThread]: 
[0m23:50:13.990741 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:13.991040 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:50:13.991441 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:50:13.991633 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:13.997584 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:13.998388 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:50:13.991764 => 23:50:13.998205
[0m23:50:13.998654 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:14.053727 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:14.054440 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:14.054647 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:50:14.054830 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:50:14.061722 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:50:14.061995 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:14.062207 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    o.account_id,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:50:14.063097 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:50:13.998793 => 23:50:14.062995
[0m23:50:14.063306 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:50:14.067677 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:50:14.067991 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:50:14.070110 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:50:14.070514 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.08s]
[0m23:50:14.070861 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:14.071706 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:14.071942 [debug] [MainThread]: On master: BEGIN
[0m23:50:14.072132 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:50:14.079177 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:14.079462 [debug] [MainThread]: On master: COMMIT
[0m23:50:14.079650 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:14.079801 [debug] [MainThread]: On master: COMMIT
[0m23:50:14.080025 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:14.080188 [debug] [MainThread]: On master: Close
[0m23:50:14.082182 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:50:14.082351 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:50:14.082524 [info ] [MainThread]: 
[0m23:50:14.082702 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m23:50:14.083028 [debug] [MainThread]: Command end result
[0m23:50:14.090069 [info ] [MainThread]: 
[0m23:50:14.090369 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:50:14.090543 [info ] [MainThread]: 
[0m23:50:14.090721 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:50:14.090894 [info ] [MainThread]: 
[0m23:50:14.091096 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:50:14.091438 [debug] [MainThread]: Command `dbt run` failed at 23:50:14.091388 after 0.50 seconds
[0m23:50:14.091637 [debug] [MainThread]: Flushing usage events


============================== 23:50:32.065553 | f1fa9e46-ae4b-445c-8219-0d2898725983 ==============================
[0m23:50:32.065553 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:50:32.067970 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m23:50:32.068234 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:50:32.142199 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:50:32.160847 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:50:32.195697 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:50:32.196158 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m23:50:32.223684 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m23:50:32.237265 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:50:32.238702 [info ] [MainThread]: 
[0m23:50:32.239124 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:50:32.239813 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:50:32.246967 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:50:32.247252 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:50:32.247429 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:50:32.256292 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.257290 [debug] [ThreadPool]: On list_dbt: Close
[0m23:50:32.259265 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:50:32.259652 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:50:32.262498 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:50:32.262688 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:50:32.262836 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:32.270109 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.270446 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:50:32.270618 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:50:32.270920 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.271508 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:50:32.271692 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:50:32.271851 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:50:32.272085 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.272246 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:50:32.275564 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m23:50:32.279311 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:50:32.279652 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:50:32.279816 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:32.286567 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.286775 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:50:32.286942 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:50:32.305360 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.306348 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:50:32.306786 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:50:32.306956 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:50:32.309369 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m23:50:32.312295 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:50:32.312533 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:50:32.312725 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:32.320017 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.320317 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:50:32.320504 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:50:32.336538 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.340598 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:50:32.340954 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:50:32.341121 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:50:32.343858 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m23:50:32.346206 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:50:32.346402 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:50:32.346550 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:32.353786 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.354045 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:50:32.354213 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:50:32.369711 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.373129 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:50:32.373370 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:50:32.373531 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:50:32.375875 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m23:50:32.377600 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:50:32.377776 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:50:32.377920 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:32.384398 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.384650 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:50:32.384823 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:50:32.400631 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:32.404241 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:50:32.404527 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:50:32.404697 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:50:32.408135 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:32.408350 [debug] [MainThread]: On master: BEGIN
[0m23:50:32.408510 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:50:32.415677 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:32.415949 [debug] [MainThread]: On master: COMMIT
[0m23:50:32.416113 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:32.416268 [debug] [MainThread]: On master: COMMIT
[0m23:50:32.416470 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:32.416636 [debug] [MainThread]: On master: Close
[0m23:50:32.418305 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:50:32.418511 [info ] [MainThread]: 
[0m23:50:32.419823 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:32.420132 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:50:32.420512 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:50:32.420717 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:32.426395 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:32.427021 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:50:32.420858 => 23:50:32.426897
[0m23:50:32.427241 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:32.481263 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:32.482019 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:32.482231 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:50:32.482425 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:50:32.489090 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:50:32.489362 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:32.489575 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    o.account_id,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on h.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:50:32.490442 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:50:32.427385 => 23:50:32.490343
[0m23:50:32.490673 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:50:32.492011 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:50:32.492207 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:50:32.494553 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "account_id"
  LINE 28:     on h.account_id = a.account_id
                  ^
[0m23:50:32.495009 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.07s]
[0m23:50:32.495342 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:32.496178 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:32.496392 [debug] [MainThread]: On master: BEGIN
[0m23:50:32.496542 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:50:32.503366 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:32.503597 [debug] [MainThread]: On master: COMMIT
[0m23:50:32.503754 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:32.503908 [debug] [MainThread]: On master: COMMIT
[0m23:50:32.504101 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:32.504263 [debug] [MainThread]: On master: Close
[0m23:50:32.506206 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:50:32.506489 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:50:32.506689 [info ] [MainThread]: 
[0m23:50:32.506879 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m23:50:32.507231 [debug] [MainThread]: Command end result
[0m23:50:32.514815 [info ] [MainThread]: 
[0m23:50:32.515126 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:50:32.515305 [info ] [MainThread]: 
[0m23:50:32.515477 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "account_id"
  LINE 28:     on h.account_id = a.account_id
                  ^
[0m23:50:32.515648 [info ] [MainThread]: 
[0m23:50:32.515830 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:50:32.516224 [debug] [MainThread]: Command `dbt run` failed at 23:50:32.516136 after 0.47 seconds
[0m23:50:32.516472 [debug] [MainThread]: Flushing usage events


============================== 23:50:37.812032 | e3788e71-fd14-4ebe-837a-3570d37326b9 ==============================
[0m23:50:37.812032 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:50:37.814493 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m23:50:37.814722 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:50:37.889299 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:50:37.908542 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:50:37.944879 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:50:37.945363 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m23:50:37.972671 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m23:50:37.985916 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:50:37.987341 [info ] [MainThread]: 
[0m23:50:37.987775 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:50:37.988456 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:50:37.995135 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:50:37.995392 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:50:37.995563 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:50:38.004146 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.005111 [debug] [ThreadPool]: On list_dbt: Close
[0m23:50:38.007270 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:50:38.007761 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:50:38.011031 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:50:38.011298 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:50:38.011476 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:38.018225 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.018524 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:50:38.018695 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:50:38.018946 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.019455 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:50:38.019619 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:50:38.019764 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:50:38.019981 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.020142 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:50:38.023215 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m23:50:38.026593 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:50:38.026808 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:50:38.026964 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:38.034553 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.034870 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:50:38.035052 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:50:38.053385 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.054358 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:50:38.054798 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:50:38.054979 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:50:38.057343 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m23:50:38.060320 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:50:38.060502 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:50:38.060653 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:38.067126 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.067405 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:50:38.067587 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:50:38.084918 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.088830 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:50:38.089099 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:50:38.089274 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:50:38.091985 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m23:50:38.094346 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:50:38.094531 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:50:38.094682 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:38.101906 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.102168 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:50:38.102343 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:50:38.117690 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.121052 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:50:38.121293 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:50:38.121453 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:50:38.123760 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m23:50:38.125713 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:50:38.125896 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:50:38.126044 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:50:38.132886 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.133176 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:50:38.133355 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:50:38.149128 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:50:38.152523 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:50:38.152772 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:50:38.152935 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:50:38.156140 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:38.156348 [debug] [MainThread]: On master: BEGIN
[0m23:50:38.156504 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:50:38.163276 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:38.163487 [debug] [MainThread]: On master: COMMIT
[0m23:50:38.163648 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:38.163797 [debug] [MainThread]: On master: COMMIT
[0m23:50:38.164008 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:38.164162 [debug] [MainThread]: On master: Close
[0m23:50:38.166140 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:50:38.166348 [info ] [MainThread]: 
[0m23:50:38.167684 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:38.168070 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:50:38.168493 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:50:38.168690 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:38.174516 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:38.175041 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:50:38.168840 => 23:50:38.174940
[0m23:50:38.175237 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:38.228813 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:38.229540 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:38.229751 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:50:38.229944 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:50:38.237043 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:50:38.237319 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:50:38.237535 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    o.account_id,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on h.accountid = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:50:38.238507 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:50:38.175380 => 23:50:38.238377
[0m23:50:38.238749 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:50:38.240122 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:50:38.240328 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:50:38.242413 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "accountid"
  LINE 28:     on h.accountid = a.account_id
                  ^
[0m23:50:38.242845 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.07s]
[0m23:50:38.243200 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:50:38.244001 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:38.244184 [debug] [MainThread]: On master: BEGIN
[0m23:50:38.244337 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:50:38.250830 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:38.251132 [debug] [MainThread]: On master: COMMIT
[0m23:50:38.251309 [debug] [MainThread]: Using duckdb connection "master"
[0m23:50:38.251462 [debug] [MainThread]: On master: COMMIT
[0m23:50:38.251665 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:50:38.251823 [debug] [MainThread]: On master: Close
[0m23:50:38.253787 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:50:38.254069 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:50:38.254262 [info ] [MainThread]: 
[0m23:50:38.254449 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m23:50:38.254800 [debug] [MainThread]: Command end result
[0m23:50:38.261640 [info ] [MainThread]: 
[0m23:50:38.261917 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:50:38.262082 [info ] [MainThread]: 
[0m23:50:38.262249 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Values list "h" does not have a column named "accountid"
  LINE 28:     on h.accountid = a.account_id
                  ^
[0m23:50:38.262619 [info ] [MainThread]: 
[0m23:50:38.262905 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:50:38.263231 [debug] [MainThread]: Command `dbt run` failed at 23:50:38.263179 after 0.47 seconds
[0m23:50:38.263418 [debug] [MainThread]: Flushing usage events


============================== 23:51:40.649070 | 36f9f694-10f7-4fed-8433-74ee9a1afb84 ==============================
[0m23:51:40.649070 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:51:40.651474 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select models/facts/fact_opportunity_history.sql', 'send_anonymous_usage_stats': 'False'}
[0m23:51:40.651715 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:51:40.726815 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:51:40.745352 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:51:40.781554 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:51:40.782044 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m23:51:40.808954 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m23:51:40.822828 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:51:40.824350 [info ] [MainThread]: 
[0m23:51:40.824796 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:51:40.825466 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:51:40.832325 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:51:40.832587 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:51:40.832768 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:40.842013 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.843037 [debug] [ThreadPool]: On list_dbt: Close
[0m23:51:40.845355 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m23:51:40.845778 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:51:40.848576 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:51:40.848777 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:51:40.848937 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:40.856310 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.856570 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:51:40.856732 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:51:40.856984 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.857515 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:51:40.857689 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:51:40.857834 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:51:40.858042 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.858200 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:51:40.861267 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_main)
[0m23:51:40.864965 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:51:40.865299 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:51:40.865614 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:40.873035 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.873303 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:51:40.873483 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:51:40.889910 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.893713 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:51:40.894185 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:51:40.894356 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:51:40.896929 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m23:51:40.899895 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:51:40.900089 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:51:40.900245 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:40.907183 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.907464 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:51:40.907642 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:51:40.925682 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.930524 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:51:40.930943 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:51:40.931125 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:51:40.934284 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m23:51:40.935997 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:51:40.937233 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:51:40.937466 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:40.945145 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.945423 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:51:40.945598 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:51:40.962478 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.966808 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:51:40.967264 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:51:40.967460 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:51:40.970645 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m23:51:40.972742 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:51:40.972982 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:51:40.973143 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:40.980515 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:40.980795 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:51:40.980975 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:51:40.999251 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:41.000247 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:51:41.000495 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:51:41.000663 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:51:41.003900 [debug] [MainThread]: Using duckdb connection "master"
[0m23:51:41.004086 [debug] [MainThread]: On master: BEGIN
[0m23:51:41.004233 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:51:41.011001 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:51:41.011255 [debug] [MainThread]: On master: COMMIT
[0m23:51:41.011413 [debug] [MainThread]: Using duckdb connection "master"
[0m23:51:41.011559 [debug] [MainThread]: On master: COMMIT
[0m23:51:41.011764 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:51:41.011923 [debug] [MainThread]: On master: Close
[0m23:51:41.013766 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:51:41.014047 [info ] [MainThread]: 
[0m23:51:41.016530 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:51:41.016898 [info ] [Thread-1  ]: 1 of 1 START sql incremental model fact.fact_opportunity_history ............... [RUN]
[0m23:51:41.017332 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.fact_opportunity_history)
[0m23:51:41.017532 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity_history
[0m23:51:41.023490 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:51:41.024069 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (compile): 23:51:41.017663 => 23:51:41.023951
[0m23:51:41.024271 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity_history
[0m23:51:41.078923 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:51:41.079615 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:51:41.079820 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: BEGIN
[0m23:51:41.080005 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:41.087006 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:41.087349 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity_history"
[0m23:51:41.087562 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity_history"} */

  
    
    

    create  table
      "dbt"."fact"."fact_opportunity_history"
  
    as (
      

select
    h.id as history_id,
    h.amount,
    h.probability,
    h.close_date,
    h.created_date,
    h.last_modified_date,
    o.stage_name,
    a.account_name,
    o.account_id,
    u.username as owner_username
from "dbt"."staging"."stg_salesforce__opportunity_history" h
left join "dbt"."fact"."fact_opportunity" o
    on h.opportunity_history_id = o.opportunity_id
left join "dbt"."dim"."dim_account" a
    on o.account_id = a.account_id
left join "dbt"."dim"."dim_user" u
    on o.owner_id = u.user_id
    );
  
  
  
[0m23:51:41.088463 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity_history (execute): 23:51:41.024405 => 23:51:41.088362
[0m23:51:41.088670 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: ROLLBACK
[0m23:51:41.092974 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_opportunity_history'
[0m23:51:41.093303 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity_history: Close
[0m23:51:41.095497 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:51:41.095912 [error] [Thread-1  ]: 1 of 1 ERROR creating sql incremental model fact.fact_opportunity_history ...... [[31mERROR[0m in 0.08s]
[0m23:51:41.096230 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity_history
[0m23:51:41.096994 [debug] [MainThread]: Using duckdb connection "master"
[0m23:51:41.097244 [debug] [MainThread]: On master: BEGIN
[0m23:51:41.097409 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:51:41.104391 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:51:41.104796 [debug] [MainThread]: On master: COMMIT
[0m23:51:41.105010 [debug] [MainThread]: Using duckdb connection "master"
[0m23:51:41.105165 [debug] [MainThread]: On master: COMMIT
[0m23:51:41.105423 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:51:41.105588 [debug] [MainThread]: On master: Close
[0m23:51:41.107537 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:51:41.107813 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity_history' was properly closed.
[0m23:51:41.108025 [info ] [MainThread]: 
[0m23:51:41.108273 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m23:51:41.108700 [debug] [MainThread]: Command end result
[0m23:51:41.115658 [info ] [MainThread]: 
[0m23:51:41.116054 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:51:41.116286 [info ] [MainThread]: 
[0m23:51:41.116471 [error] [MainThread]:   Runtime Error in model fact_opportunity_history (models/facts/fact_opportunity_history.sql)
  Binder Error: Table "o" does not have a column named "account_id"
  LINE 28:     on o.account_id = a.account_id
                  ^
[0m23:51:41.116676 [info ] [MainThread]: 
[0m23:51:41.116931 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:51:41.117308 [debug] [MainThread]: Command `dbt run` failed at 23:51:41.117258 after 0.49 seconds
[0m23:51:41.117506 [debug] [MainThread]: Flushing usage events


============================== 23:51:54.509832 | 3df63410-9a20-4957-a147-3a62924fb232 ==============================
[0m23:51:54.509832 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:51:54.512280 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'profiles_dir': '.', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m23:51:54.512531 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:51:54.622494 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:51:54.640345 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:51:54.675979 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 0 files changed.
[0m23:51:54.676426 [debug] [MainThread]: Partial parsing: deleted file: elastic_dbt_interview://models/facts/fact_opportunity_history.sql
[0m23:51:54.682708 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m23:51:54.696454 [info ] [MainThread]: Found 30 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:51:54.698288 [info ] [MainThread]: 
[0m23:51:54.698815 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:51:54.700256 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:51:54.707324 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:51:54.707654 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:51:54.707858 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:51:54.716543 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.717568 [debug] [ThreadPool]: On list_dbt: Close
[0m23:51:54.721140 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:51:54.721435 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:51:54.721866 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:54.730272 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.731121 [debug] [ThreadPool]: On list_dbt: Close
[0m23:51:54.735287 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:51:54.735504 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:51:54.735676 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:54.743226 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.743910 [debug] [ThreadPool]: On list_dbt: Close
[0m23:51:54.746220 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m23:51:54.746727 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m23:51:54.749646 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m23:51:54.749883 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m23:51:54.750039 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:54.756901 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.757116 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m23:51:54.757287 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m23:51:54.757525 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.758011 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m23:51:54.758173 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m23:51:54.758317 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m23:51:54.758526 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.758698 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m23:51:54.760819 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now create_dbt_fact)
[0m23:51:54.761226 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:51:54.762723 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:51:54.762998 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:51:54.763150 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:54.769962 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.770242 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:51:54.770400 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:51:54.770724 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.771361 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:51:54.771545 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:51:54.771702 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:51:54.771967 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.772130 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:51:54.774352 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now create_dbt_staging)
[0m23:51:54.774910 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m23:51:54.776670 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m23:51:54.776874 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m23:51:54.777041 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:54.783570 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.783821 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m23:51:54.783995 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m23:51:54.784231 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.784822 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m23:51:54.785092 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m23:51:54.785272 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m23:51:54.785555 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.785740 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m23:51:54.789017 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now list_dbt_main)
[0m23:51:54.792653 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:51:54.792866 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:51:54.793085 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:54.800134 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.800450 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:51:54.800641 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:51:54.817158 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.820903 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:51:54.821350 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:51:54.821519 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:51:54.824018 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_dim)
[0m23:51:54.827127 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:51:54.827327 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:51:54.827478 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:54.834258 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.834515 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:51:54.834686 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:51:54.850397 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.853954 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:51:54.854189 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:51:54.854350 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:51:54.856639 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m23:51:54.858911 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:51:54.859081 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:51:54.859224 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:54.866210 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.866456 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:51:54.866627 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:51:54.882042 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.885345 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:51:54.885581 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:51:54.885738 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:51:54.888299 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_staging)
[0m23:51:54.890032 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:51:54.890206 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:51:54.890348 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:51:54.897209 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.897461 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:51:54.897629 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:51:54.915282 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:51:54.916167 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:51:54.916392 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:51:54.916547 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:51:54.919669 [debug] [MainThread]: Using duckdb connection "master"
[0m23:51:54.919857 [debug] [MainThread]: On master: BEGIN
[0m23:51:54.920009 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:51:54.926469 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:51:54.926733 [debug] [MainThread]: On master: COMMIT
[0m23:51:54.926893 [debug] [MainThread]: Using duckdb connection "master"
[0m23:51:54.927039 [debug] [MainThread]: On master: COMMIT
[0m23:51:54.927242 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:51:54.927405 [debug] [MainThread]: On master: Close
[0m23:51:54.929062 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:51:54.929275 [info ] [MainThread]: 
[0m23:51:54.930600 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m23:51:54.930869 [info ] [Thread-1  ]: 1 of 29 START sql table model dim.dim_date ..................................... [RUN]
[0m23:51:54.931220 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_staging, now model.elastic_dbt_interview.dim_date)
[0m23:51:54.931405 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m23:51:54.958243 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:51:54.958582 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m23:51:54.958768 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:54.965857 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:54.966152 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:51:54.966348 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m23:51:54.966715 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.032654 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m23:51:55.033485 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 23:51:54.931535 => 23:51:55.033372
[0m23:51:55.033710 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m23:51:55.052207 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m23:51:55.052949 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:51:55.053361 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m23:51:55.100861 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.104917 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:51:55.105159 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m23:51:55.105601 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.107844 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:51:55.108045 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m23:51:55.108359 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.118606 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m23:51:55.118827 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:51:55.119008 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m23:51:55.122320 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.125301 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:51:55.125527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m23:51:55.126019 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.126756 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 23:51:55.033847 => 23:51:55.126658
[0m23:51:55.126963 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m23:51:55.158961 [info ] [Thread-1  ]: 1 of 29 OK created sql table model dim.dim_date ................................ [[32mOK[0m in 0.23s]
[0m23:51:55.159411 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m23:51:55.159654 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m23:51:55.159929 [info ] [Thread-1  ]: 2 of 29 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m23:51:55.160295 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.stg_salesforce__account)
[0m23:51:55.160494 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m23:51:55.162559 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:51:55.163117 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 23:51:55.160624 => 23:51:55.163021
[0m23:51:55.163309 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m23:51:55.174198 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:51:55.174846 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:51:55.175046 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m23:51:55.175227 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.181890 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.182185 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:51:55.182459 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m23:51:55.183323 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.186128 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:51:55.186412 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m23:51:55.186830 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.188776 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:51:55.189029 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m23:51:55.189352 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.190236 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m23:51:55.190434 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:51:55.190608 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m23:51:55.191166 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.192694 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:51:55.192901 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m23:51:55.193250 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.193949 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 23:51:55.163438 => 23:51:55.193855
[0m23:51:55.194165 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m23:51:55.207934 [info ] [Thread-1  ]: 2 of 29 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.05s]
[0m23:51:55.208354 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m23:51:55.208598 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:51:55.208918 [info ] [Thread-1  ]: 3 of 29 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m23:51:55.209285 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m23:51:55.209476 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:51:55.211504 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:51:55.212036 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 23:51:55.209608 => 23:51:55.211928
[0m23:51:55.212235 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:51:55.214681 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:51:55.215056 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:51:55.215255 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m23:51:55.215516 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.222524 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.222826 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:51:55.223067 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m23:51:55.223851 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.226076 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:51:55.226294 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m23:51:55.226597 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.229015 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:51:55.229224 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m23:51:55.229526 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.230389 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m23:51:55.230581 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:51:55.230977 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m23:51:55.231743 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.233682 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:51:55.233953 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m23:51:55.234419 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.235281 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 23:51:55.212372 => 23:51:55.235182
[0m23:51:55.235508 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m23:51:55.250289 [info ] [Thread-1  ]: 3 of 29 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.04s]
[0m23:51:55.250706 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:51:55.250953 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m23:51:55.251291 [info ] [Thread-1  ]: 4 of 29 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m23:51:55.251661 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m23:51:55.251859 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m23:51:55.253928 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:51:55.254439 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 23:51:55.251993 => 23:51:55.254341
[0m23:51:55.254632 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m23:51:55.257308 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:51:55.257803 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:51:55.258010 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m23:51:55.258200 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.265052 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.265345 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:51:55.265570 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m23:51:55.266272 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.268505 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:51:55.268722 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m23:51:55.269031 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.270785 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:51:55.270996 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m23:51:55.271279 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.272113 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m23:51:55.272296 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:51:55.272462 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m23:51:55.273149 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.276002 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:51:55.276252 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m23:51:55.276701 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.277687 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 23:51:55.254757 => 23:51:55.277583
[0m23:51:55.277920 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m23:51:55.292357 [info ] [Thread-1  ]: 4 of 29 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.04s]
[0m23:51:55.292797 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m23:51:55.293047 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:51:55.293314 [info ] [Thread-1  ]: 5 of 29 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m23:51:55.293690 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m23:51:55.293881 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:51:55.295976 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:51:55.296512 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 23:51:55.294021 => 23:51:55.296414
[0m23:51:55.296734 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:51:55.299490 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:51:55.300009 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:51:55.300224 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m23:51:55.300400 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.307291 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.307579 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:51:55.307785 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m23:51:55.308215 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.310442 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:51:55.310658 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m23:51:55.310962 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.312689 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:51:55.312902 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m23:51:55.313188 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.314365 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m23:51:55.314640 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:51:55.314834 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m23:51:55.315476 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.317853 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:51:55.318100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m23:51:55.318517 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.319424 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 23:51:55.296870 => 23:51:55.319318
[0m23:51:55.319649 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m23:51:55.333111 [info ] [Thread-1  ]: 5 of 29 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.04s]
[0m23:51:55.333549 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:51:55.333796 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:51:55.334135 [info ] [Thread-1  ]: 6 of 29 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m23:51:55.334522 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m23:51:55.334724 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:51:55.336887 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:51:55.337445 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 23:51:55.334859 => 23:51:55.337342
[0m23:51:55.337647 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:51:55.340476 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:51:55.341180 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:51:55.341388 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m23:51:55.341558 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.348255 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.348552 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:51:55.348789 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m23:51:55.349571 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.351766 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:51:55.351992 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m23:51:55.352287 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.353992 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:51:55.354203 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m23:51:55.354807 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.355849 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m23:51:55.356069 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:51:55.356257 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m23:51:55.356886 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.358680 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:51:55.358928 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m23:51:55.359342 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.360155 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 23:51:55.337773 => 23:51:55.360054
[0m23:51:55.360375 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m23:51:55.374215 [info ] [Thread-1  ]: 6 of 29 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.04s]
[0m23:51:55.374620 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:51:55.374846 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:51:55.375269 [info ] [Thread-1  ]: 7 of 29 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m23:51:55.375785 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m23:51:55.376000 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:51:55.378844 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:51:55.379391 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 23:51:55.376140 => 23:51:55.379277
[0m23:51:55.379592 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:51:55.382280 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:51:55.382762 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:51:55.382959 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m23:51:55.383133 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.389985 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.390262 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:51:55.390494 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m23:51:55.391432 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.393593 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:51:55.393800 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m23:51:55.394107 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.396161 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:51:55.396509 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m23:51:55.396914 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.397937 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m23:51:55.398158 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:51:55.398335 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m23:51:55.398989 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.400784 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:51:55.401002 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m23:51:55.401373 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.402106 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 23:51:55.379720 => 23:51:55.402009
[0m23:51:55.402306 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m23:51:55.416383 [info ] [Thread-1  ]: 7 of 29 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.04s]
[0m23:51:55.416820 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:51:55.417078 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:51:55.417411 [info ] [Thread-1  ]: 8 of 29 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m23:51:55.417779 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m23:51:55.417975 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:51:55.420757 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:51:55.421315 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 23:51:55.418106 => 23:51:55.421208
[0m23:51:55.421514 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:51:55.424552 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:51:55.425305 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:51:55.425549 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m23:51:55.425735 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.432751 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.433059 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:51:55.433291 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m23:51:55.434078 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.436409 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:51:55.436664 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m23:51:55.437021 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.439428 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:51:55.439710 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m23:51:55.440129 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.441306 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m23:51:55.441646 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:51:55.441852 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m23:51:55.442648 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.444517 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:51:55.444875 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m23:51:55.445445 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.446585 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 23:51:55.421648 => 23:51:55.446472
[0m23:51:55.446851 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m23:51:55.464038 [info ] [Thread-1  ]: 8 of 29 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.05s]
[0m23:51:55.464528 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:51:55.464799 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:51:55.465054 [info ] [Thread-1  ]: 9 of 29 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m23:51:55.465445 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m23:51:55.465942 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:51:55.468524 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:51:55.469102 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 23:51:55.466232 => 23:51:55.468984
[0m23:51:55.469330 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:51:55.473816 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:51:55.474494 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:51:55.474720 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m23:51:55.474914 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.482642 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.482947 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:51:55.483173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m23:51:55.483753 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.486007 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:51:55.486228 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m23:51:55.486541 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.488280 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:51:55.488493 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m23:51:55.488793 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.489641 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m23:51:55.489833 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:51:55.490007 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m23:51:55.490682 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.492702 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:51:55.492937 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m23:51:55.493363 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.494217 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 23:51:55.469473 => 23:51:55.494112
[0m23:51:55.494445 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m23:51:55.508664 [info ] [Thread-1  ]: 9 of 29 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.04s]
[0m23:51:55.509084 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:51:55.509326 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:51:55.509598 [info ] [Thread-1  ]: 10 of 29 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m23:51:55.509963 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m23:51:55.510157 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:51:55.512168 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:51:55.512679 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 23:51:55.510287 => 23:51:55.512575
[0m23:51:55.512879 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:51:55.516504 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:51:55.517055 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:51:55.517272 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m23:51:55.517455 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.524351 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.524755 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:51:55.525097 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m23:51:55.525656 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.527911 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:51:55.528131 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m23:51:55.528436 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.530167 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:51:55.530384 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m23:51:55.530666 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.531512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m23:51:55.531700 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:51:55.531876 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m23:51:55.532423 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.534363 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:51:55.534592 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m23:51:55.534995 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.535801 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 23:51:55.513008 => 23:51:55.535705
[0m23:51:55.536015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m23:51:55.549875 [info ] [Thread-1  ]: 10 of 29 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.04s]
[0m23:51:55.550272 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:51:55.550496 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:51:55.550924 [info ] [Thread-1  ]: 11 of 29 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m23:51:55.551449 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m23:51:55.551699 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:51:55.553808 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:51:55.554314 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 23:51:55.551846 => 23:51:55.554209
[0m23:51:55.554514 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:51:55.557281 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:51:55.557784 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:51:55.557996 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m23:51:55.558177 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.564932 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.565215 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:51:55.565530 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m23:51:55.566063 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.568877 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:51:55.569095 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m23:51:55.569417 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.571179 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:51:55.571379 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m23:51:55.571657 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.572490 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m23:51:55.572861 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:51:55.573091 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m23:51:55.573704 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.575568 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:51:55.575801 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m23:51:55.576214 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.576985 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 23:51:55.554644 => 23:51:55.576889
[0m23:51:55.577190 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m23:51:55.592014 [info ] [Thread-1  ]: 11 of 29 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.04s]
[0m23:51:55.592433 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:51:55.592671 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:51:55.592979 [info ] [Thread-1  ]: 12 of 29 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m23:51:55.593420 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m23:51:55.593626 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:51:55.595644 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:51:55.596156 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 23:51:55.593758 => 23:51:55.596056
[0m23:51:55.596380 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:51:55.599088 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:51:55.599575 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:51:55.599812 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m23:51:55.600001 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.606841 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.607140 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:51:55.607344 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m23:51:55.607812 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.610034 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:51:55.610255 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m23:51:55.610589 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.613184 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:51:55.613548 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m23:51:55.613990 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.615000 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m23:51:55.615229 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:51:55.615411 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m23:51:55.616108 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.617817 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:51:55.618031 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m23:51:55.618392 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.619179 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 23:51:55.596523 => 23:51:55.619080
[0m23:51:55.619382 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m23:51:55.633453 [info ] [Thread-1  ]: 12 of 29 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.04s]
[0m23:51:55.633901 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:51:55.634224 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:51:55.634618 [info ] [Thread-1  ]: 13 of 29 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m23:51:55.635140 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m23:51:55.635359 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:51:55.637492 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:51:55.638019 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 23:51:55.635491 => 23:51:55.637912
[0m23:51:55.638228 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:51:55.640975 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:51:55.641447 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:51:55.641635 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m23:51:55.641822 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.648958 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.649264 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:51:55.649475 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m23:51:55.649982 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.652244 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:51:55.652463 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m23:51:55.652770 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.655283 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:51:55.655609 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m23:51:55.655924 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.656872 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m23:51:55.657076 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:51:55.657255 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m23:51:55.657860 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.659748 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:51:55.659970 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m23:51:55.660339 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.661219 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 23:51:55.638359 => 23:51:55.661119
[0m23:51:55.661450 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m23:51:55.675462 [info ] [Thread-1  ]: 13 of 29 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.04s]
[0m23:51:55.675848 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:51:55.676078 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m23:51:55.676507 [info ] [Thread-1  ]: 14 of 29 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m23:51:55.676977 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m23:51:55.677193 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m23:51:55.679345 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:51:55.679839 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 23:51:55.677328 => 23:51:55.679728
[0m23:51:55.680045 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m23:51:55.682837 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:51:55.683420 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:51:55.683645 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m23:51:55.683832 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.690424 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.690717 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:51:55.690965 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m23:51:55.691934 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.694345 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:51:55.694692 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m23:51:55.695150 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.697262 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:51:55.697527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m23:51:55.697860 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.698747 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m23:51:55.698952 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:51:55.699125 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m23:51:55.699767 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.729038 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:51:55.729383 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m23:51:55.729928 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.730728 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 23:51:55.680178 => 23:51:55.730633
[0m23:51:55.730952 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m23:51:55.745180 [info ] [Thread-1  ]: 14 of 29 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.07s]
[0m23:51:55.745604 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m23:51:55.745851 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:51:55.746120 [info ] [Thread-1  ]: 15 of 29 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m23:51:55.746468 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m23:51:55.746662 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:51:55.748606 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:51:55.749127 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 23:51:55.746790 => 23:51:55.749021
[0m23:51:55.749326 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:51:55.752067 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:51:55.752601 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:51:55.752891 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m23:51:55.753110 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.760234 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.760525 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:51:55.760741 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m23:51:55.761264 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.763438 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:51:55.763651 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m23:51:55.763953 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.765646 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:51:55.765846 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m23:51:55.766289 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.767566 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m23:51:55.767784 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:51:55.767964 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m23:51:55.768571 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.770497 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:51:55.770762 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m23:51:55.771142 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.772037 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 23:51:55.749459 => 23:51:55.771938
[0m23:51:55.772261 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m23:51:55.785767 [info ] [Thread-1  ]: 15 of 29 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.04s]
[0m23:51:55.786173 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:51:55.786417 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m23:51:55.786867 [info ] [Thread-1  ]: 16 of 29 START sql table model dim.dim_account ................................. [RUN]
[0m23:51:55.787406 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m23:51:55.787657 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m23:51:55.790940 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m23:51:55.791466 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 23:51:55.787848 => 23:51:55.791363
[0m23:51:55.791684 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m23:51:55.794358 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m23:51:55.794842 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:51:55.795031 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m23:51:55.795206 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.802487 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.802730 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:51:55.802987 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

select
    account_id,
    masterrecordid as master_record_id,
    name as account_name,
    type as account_type,
    parentid as parent_account_id,
    billingstreet as billing_street,
    billingcity as billing_city,
    billingstate as billing_state,
    billingpostalcode as billing_postal_code,
    billingcountry as billing_country,
    billinglatitude as billing_latitude,
    billinglongitude as billing_longitude,
    billinggeocodeaccuracy as billing_geocode_accuracy,
    shippingstreet as shipping_street,
    shippingcity as shipping_city,
    shippingstate as shipping_state,
    shippingpostalcode as shipping_postal_code,
    shippingcountry as shipping_country,
    shippinglatitude as shipping_latitude,
    shippinglongitude as shipping_longitude,
    shippinggeocodeaccuracy as shipping_geocode_accuracy,
    phone as account_phone,
    fax as account_fax,
    accountnumber as account_number,
    website as account_website,
    sic as sic_code,
    industry as account_industry,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownership as account_ownership,
    tickersymbol as ticker_symbol,
    description as account_description,
    rating as account_rating,
    site as account_site,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    jigsaw as jigsaw_id,
    jigsawcompanyid as jigsaw_company_id,
    cleanstatus as clean_status,
    accountsource as account_source,
    dunsnumber as duns_number,
    tradestyle as trade_style,
    naicscode as naics_code,
    naicsdesc as naics_description,
    yearstarted as year_started,
    sicdesc as sic_description,
    dandbcompanyid as dandb_company_id,
    operatinghoursid as operating_hours_id,
    customerpriority__c as customer_priority,
    sla__c as sla,
    active__c as is_active,
    numberoflocations__c as number_of_locations,
    upsellopportunity__c as upsell_opportunity,
    slaserialnumber__c as sla_serial_number,
    slaexpirationdate__c as sla_expiration_date
from "dbt"."staging"."stg_salesforce__account"
where isdeleted = false  -- Exclude deleted accounts
order by account_name
    );
  
  
[0m23:51:55.807499 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.809935 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:51:55.810220 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account" rename to "dim_account__dbt_backup"
[0m23:51:55.810673 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.812567 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:51:55.812778 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m23:51:55.813128 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.814111 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m23:51:55.814298 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:51:55.814473 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m23:51:55.816051 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.817748 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:51:55.817959 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m23:51:55.818533 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.819322 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 23:51:55.791816 => 23:51:55.819229
[0m23:51:55.819533 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m23:51:55.844140 [info ] [Thread-1  ]: 16 of 29 OK created sql table model dim.dim_account ............................ [[32mOK[0m in 0.06s]
[0m23:51:55.844540 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m23:51:55.844765 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m23:51:55.845148 [info ] [Thread-1  ]: 17 of 29 START sql table model dim.dim_campaign ................................ [RUN]
[0m23:51:55.845657 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m23:51:55.845873 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m23:51:55.849094 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m23:51:55.849655 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 23:51:55.846010 => 23:51:55.849542
[0m23:51:55.849859 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m23:51:55.852502 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m23:51:55.852994 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:51:55.853186 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m23:51:55.853364 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.860331 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.860576 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:51:55.860851 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

select
    /* IDs */
    campaign_id,
    parentid as parent_campaign_id,
    ownerid as owner_id,
    createdbyid as created_by_id,
    lastmodifiedbyid as last_modified_by_id,
    campaignmemberrecordtypeid as campaign_member_record_type_id,

    /* Dates */
    startdate as start_date,
    enddate as end_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,

    /* Dimensions */
    name as campaign_name,
    type as campaign_type,
    status,
    description,
    isactive as is_active,

    /* Metrics */
    expectedrevenue as expected_revenue,
    budgetedcost as budgeted_cost,
    actualcost as actual_cost,
    expectedresponse as expected_response,
    numbersent as number_sent,
    numberofleads as number_of_leads,
    numberofconvertedleads as number_of_converted_leads,
    numberofcontacts as number_of_contacts,
    numberofresponses as number_of_responses,
    numberofopportunities as number_of_opportunities,
    numberofwonopportunities as number_of_won_opportunities,
    amountallopportunities as amount_all_opportunities,
    amountwonopportunities as amount_won_opportunities,
    hierarchynumberofleads as hierarchy_number_of_leads,
    hierarchynumberofconvertedleads as hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts as hierarchy_number_of_contacts,
    hierarchynumberofresponses as hierarchy_number_of_responses,
    hierarchynumberofopportunities as hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities as hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities as hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities as hierarchy_amount_won_opportunities,
    hierarchynumbersent as hierarchy_number_sent,
    hierarchyexpectedrevenue as hierarchy_expected_revenue,
    hierarchybudgetedcost as hierarchy_budgeted_cost,
    hierarchyactualcost as hierarchy_actual_cost

from "dbt"."staging"."stg_salesforce__campaign"
where isdeleted = false
    );
  
  
[0m23:51:55.863664 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.865936 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:51:55.866287 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign" rename to "dim_campaign__dbt_backup"
[0m23:51:55.866749 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.868709 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:51:55.868931 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m23:51:55.869283 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.870232 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m23:51:55.870419 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:51:55.870589 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m23:51:55.871652 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.873537 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:51:55.873787 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."dim"."dim_campaign__dbt_backup" cascade
[0m23:51:55.874302 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.875173 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 23:51:55.849989 => 23:51:55.875072
[0m23:51:55.875376 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m23:51:55.892212 [info ] [Thread-1  ]: 17 of 29 OK created sql table model dim.dim_campaign ........................... [[32mOK[0m in 0.05s]
[0m23:51:55.892629 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m23:51:55.892873 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m23:51:55.893138 [info ] [Thread-1  ]: 18 of 29 START sql table model dim.dim_case_status ............................. [RUN]
[0m23:51:55.893495 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m23:51:55.893687 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m23:51:55.895833 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m23:51:55.896384 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 23:51:55.893820 => 23:51:55.896258
[0m23:51:55.896605 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m23:51:55.900167 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m23:51:55.900665 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:51:55.900863 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m23:51:55.901042 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.907713 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.907995 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:51:55.908204 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."dim"."dim_case_status__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct status as status_name, status as status_description  -- Adjust as needed, typically a description field should be separate
        from "dbt"."staging"."stg_salesforce__case_history_2"
    )

select
    row_number() over (order by status_name) as status_id,  -- Surrogate Key
    status_name,
    status_description
from source
    );
  
  
[0m23:51:55.909888 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.912200 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:51:55.912429 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m23:51:55.912839 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.914745 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:51:55.914953 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m23:51:55.915268 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.916266 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m23:51:55.916477 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:51:55.916651 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m23:51:55.917241 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.918788 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:51:55.918982 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."dim"."dim_case_status__dbt_backup" cascade
[0m23:51:55.919342 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.920037 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 23:51:55.896744 => 23:51:55.919940
[0m23:51:55.920236 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m23:51:55.934587 [info ] [Thread-1  ]: 18 of 29 OK created sql table model dim.dim_case_status ........................ [[32mOK[0m in 0.04s]
[0m23:51:55.934980 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m23:51:55.935205 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m23:51:55.935578 [info ] [Thread-1  ]: 19 of 29 START sql table model dim.dim_contact ................................. [RUN]
[0m23:51:55.936070 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m23:51:55.936278 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m23:51:55.938684 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m23:51:55.939197 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 23:51:55.936414 => 23:51:55.939101
[0m23:51:55.939393 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m23:51:55.942997 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m23:51:55.943475 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:51:55.943664 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m23:51:55.943836 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:55.950892 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.951185 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:51:55.951438 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

select
    /* IDs */
    contact_id,
    masterrecordid as master_record_id,
    accountid as account_id,
    reportstoid as reports_to_id,
    ownerid as owner_id,
    jigsawcontactid as jigsaw_contact_id,
    individualid as individual_id,

    /* Dates */
    birthdate as birth_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    lastcurequestdate as last_cu_request_date,
    lastcuupdatedate as last_cu_update_date,
    emailbounceddate as email_bounced_date,

    /* Dimensions */
    salutation,
    firstname as first_name,
    lastname as last_name,
    otherstreet as other_street,
    othercity as other_city,
    otherstate as other_state,
    otherpostalcode as other_postal_code,
    othercountry as other_country,
    otherlatitude as other_latitude,
    otherlongitude as other_longitude,
    othergeocodeaccuracy as other_geocode_accuracy,
    mailingstreet as mailing_street,
    mailingcity as mailing_city,
    mailingstate as mailing_state,
    mailingpostalcode as mailing_postal_code,
    mailingcountry as mailing_country,
    mailinglatitude as mailing_latitude,
    mailinglongitude as mailing_longitude,
    mailinggeocodeaccuracy as mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone as mobile_phone,
    homephone as home_phone,
    otherphone as other_phone,
    assistantphone as assistant_phone,
    email,
    title,
    department,
    assistantname as assistant_name,
    leadsource as lead_source,
    description,
    pronouns,
    genderidentity as gender_identity,
    cleanstatus as clean_status,
    emailbouncedreason as email_bounced_reason,
    level__c as level,
    languages__c as languages,

    /* Metrics */
    hasoptedoutofemail as has_opted_out_of_email,
    hasoptedoutoffax as has_opted_out_of_fax,
    donotcall as do_not_call

from "dbt"."staging"."stg_salesforce__contact"
where isdeleted = false
    );
  
  
[0m23:51:55.955299 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.957376 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:51:55.957588 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact" rename to "dim_contact__dbt_backup"
[0m23:51:55.957955 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.959740 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:51:55.959949 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m23:51:55.960417 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.961659 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m23:51:55.961899 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:51:55.962116 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m23:51:55.963484 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.965288 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:51:55.965502 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m23:51:55.966076 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:55.966813 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 23:51:55.939521 => 23:51:55.966720
[0m23:51:55.967020 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m23:51:55.989472 [info ] [Thread-1  ]: 19 of 29 OK created sql table model dim.dim_contact ............................ [[32mOK[0m in 0.05s]
[0m23:51:55.989927 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m23:51:55.990177 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m23:51:55.990452 [info ] [Thread-1  ]: 20 of 29 START sql table model dim.dim_lead .................................... [RUN]
[0m23:51:55.990807 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m23:51:55.991005 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m23:51:55.993260 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m23:51:55.993916 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 23:51:55.991139 => 23:51:55.993730
[0m23:51:55.994171 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m23:51:55.996875 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m23:51:55.997354 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:51:55.997552 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m23:51:55.997725 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:56.004630 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.004919 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:51:56.005173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

select
    lead_id,
    masterrecordid as master_record_id,
    salutation as lead_salutation,
    firstname as first_name,
    lastname as last_name,
    title as lead_title,
    company as lead_company,
    street as lead_street,
    city as lead_city,
    state as lead_state,
    postalcode as lead_postal_code,
    country as lead_country,
    latitude as lead_latitude,
    longitude as lead_longitude,
    geocodeaccuracy as geocode_accuracy,
    phone as lead_phone,
    mobilephone as lead_mobile_phone,
    fax as lead_fax,
    email as lead_email,
    website as lead_website,
    description as lead_description,
    leadsource as lead_source,
    status as lead_status,
    industry as lead_industry,
    rating as lead_rating,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownerid as owner_id,
    hasoptedoutofemail as has_opted_out_of_email,
    isconverted as is_converted,
    converteddate as converted_date,
    convertedaccountid as converted_account_id,
    convertedcontactid as converted_contact_id,
    convertedopportunityid as converted_opportunity_id,
    isunreadbyowner as is_unread_by_owner,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    donotcall as do_not_call,
    hasoptedoutoffax as has_opted_out_of_fax,
    lasttransferdate as last_transfer_date,
    jigsaw as jigsaw_id,
    jigsawcontactid as jigsaw_contact_id,
    cleanstatus as clean_status,
    companydunsnumber as company_duns_number,
    dandbcompanyid as dandb_company_id,
    emailbouncedreason as email_bounced_reason,
    emailbounceddate as email_bounced_date,
    individualid as individual_id,
    pronouns as lead_pronouns,
    genderidentity as gender_identity,
    siccode__c as sic_code,
    productinterest__c as product_interest,
    primary__c as is_primary,
    currentgenerators__c as current_generators,
    numberoflocations__c as number_of_locations
from "dbt"."staging"."stg_salesforce__lead"
where isdeleted = false  -- Exclude deleted leads
order by last_name, first_name
    );
  
  
[0m23:51:56.009324 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.012225 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:51:56.012437 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead" rename to "dim_lead__dbt_backup"
[0m23:51:56.012815 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.014841 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:51:56.015168 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m23:51:56.015628 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.016723 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m23:51:56.016946 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:51:56.017134 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m23:51:56.018539 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.020199 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:51:56.020463 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m23:51:56.021094 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.021922 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 23:51:55.994308 => 23:51:56.021825
[0m23:51:56.022154 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m23:51:56.043497 [info ] [Thread-1  ]: 20 of 29 OK created sql table model dim.dim_lead ............................... [[32mOK[0m in 0.05s]
[0m23:51:56.043916 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m23:51:56.044161 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m23:51:56.044429 [info ] [Thread-1  ]: 21 of 29 START sql table model dim.dim_opportunity ............................. [RUN]
[0m23:51:56.044783 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m23:51:56.044977 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m23:51:56.047261 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m23:51:56.047837 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 23:51:56.045106 => 23:51:56.047719
[0m23:51:56.048067 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m23:51:56.050900 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m23:51:56.051450 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:51:56.051653 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m23:51:56.051831 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:56.058505 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.058810 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:51:56.059081 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m23:51:56.062511 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.064780 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:51:56.065016 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m23:51:56.065436 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.068091 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:51:56.068324 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m23:51:56.068683 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.069646 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m23:51:56.069839 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:51:56.070014 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m23:51:56.071167 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.072736 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:51:56.072936 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m23:51:56.073417 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.074119 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 23:51:56.048200 => 23:51:56.074028
[0m23:51:56.074320 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m23:51:56.094205 [info ] [Thread-1  ]: 21 of 29 OK created sql table model dim.dim_opportunity ........................ [[32mOK[0m in 0.05s]
[0m23:51:56.094699 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m23:51:56.094991 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:51:56.095358 [info ] [Thread-1  ]: 22 of 29 START sql table model dim.dim_opportunity_stage ....................... [RUN]
[0m23:51:56.095792 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m23:51:56.096065 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:51:56.098306 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:51:56.098833 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 23:51:56.096222 => 23:51:56.098722
[0m23:51:56.099042 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:51:56.101946 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:51:56.102425 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:51:56.102621 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m23:51:56.102798 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:56.109337 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.109571 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:51:56.109766 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct stagename as stage_name, stagesortorder as stage_sort_order
        from "dbt"."staging"."stg_salesforce__opportunity"
    )

select
    row_number() over (order by stage_sort_order) as stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
from source
    );
  
  
[0m23:51:56.112283 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.114537 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:51:56.114765 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m23:51:56.115123 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.117973 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:51:56.118234 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m23:51:56.118562 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.119586 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m23:51:56.119776 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:51:56.119949 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m23:51:56.120545 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.122402 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:51:56.122645 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m23:51:56.123120 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.123971 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 23:51:56.099175 => 23:51:56.123868
[0m23:51:56.124187 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m23:51:56.139618 [info ] [Thread-1  ]: 22 of 29 OK created sql table model dim.dim_opportunity_stage .................. [[32mOK[0m in 0.04s]
[0m23:51:56.140043 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:51:56.140287 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m23:51:56.140562 [info ] [Thread-1  ]: 23 of 29 START sql table model dim.dim_pricebook ............................... [RUN]
[0m23:51:56.140923 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m23:51:56.141112 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m23:51:56.143318 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m23:51:56.143806 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 23:51:56.141242 => 23:51:56.143709
[0m23:51:56.143998 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m23:51:56.146723 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m23:51:56.147260 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:51:56.147462 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m23:51:56.147636 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:56.154758 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.155106 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:51:56.155459 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

select
    pricebook_entry_id,
    pricebook2id as pricebook_id,
    product2id as product_id,
    unitprice as unit_price,
    isactive as is_active,
    usestandardprice as use_standard_price,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__pricebook_entry"
where isdeleted = false  -- Exclude deleted entries
order by pricebook_entry_id
    );
  
  
[0m23:51:56.157048 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.159290 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:51:56.159531 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook" rename to "dim_pricebook__dbt_backup"
[0m23:51:56.159862 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.161845 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:51:56.162186 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m23:51:56.162604 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.163718 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m23:51:56.163924 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:51:56.164102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m23:51:56.164813 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.167352 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:51:56.167596 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m23:51:56.168066 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.168896 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 23:51:56.144126 => 23:51:56.168802
[0m23:51:56.169110 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m23:51:56.185207 [info ] [Thread-1  ]: 23 of 29 OK created sql table model dim.dim_pricebook .......................... [[32mOK[0m in 0.04s]
[0m23:51:56.185632 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m23:51:56.185881 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m23:51:56.186151 [info ] [Thread-1  ]: 24 of 29 START sql table model dim.dim_product ................................. [RUN]
[0m23:51:56.186541 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m23:51:56.186740 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m23:51:56.188997 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m23:51:56.189463 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 23:51:56.186879 => 23:51:56.189364
[0m23:51:56.189656 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m23:51:56.192367 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m23:51:56.192961 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:51:56.193188 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m23:51:56.193367 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:56.201262 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.201569 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:51:56.201797 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    productcode as product_code,
    description as product_description,
    isactive as is_active,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    family as product_family,
    externaldatasourceid as external_datasource_id,
    externalid as external_id,
    displayurl as display_url,
    quantityunitofmeasure as quantity_unit_of_measure,
    stockkeepingunit as stock_keeping_unit,
    type as product_type,
    productclass as product_class,
    sourceproductid as source_product_id,
    sellerid as seller_id
from "dbt"."staging"."stg_salesforce__product_2"
where isdeleted = false
-- isactive =1 ?
order by product_name
    );
  
  
[0m23:51:56.203907 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.206086 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:51:56.206318 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product" rename to "dim_product__dbt_backup"
[0m23:51:56.206949 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.209187 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:51:56.209469 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m23:51:56.209863 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.210935 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m23:51:56.211151 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:51:56.211331 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m23:51:56.212140 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.213807 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:51:56.214006 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m23:51:56.214536 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.215456 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 23:51:56.189790 => 23:51:56.215361
[0m23:51:56.215693 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m23:51:56.232788 [info ] [Thread-1  ]: 24 of 29 OK created sql table model dim.dim_product ............................ [[32mOK[0m in 0.05s]
[0m23:51:56.233206 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m23:51:56.233432 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m23:51:56.233797 [info ] [Thread-1  ]: 25 of 29 START sql table model dim.dim_record_type ............................. [RUN]
[0m23:51:56.234320 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m23:51:56.234564 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m23:51:56.237777 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m23:51:56.238351 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 23:51:56.234718 => 23:51:56.238222
[0m23:51:56.238565 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m23:51:56.241103 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m23:51:56.241556 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:51:56.241742 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m23:51:56.241914 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:56.248979 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.249268 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:51:56.249476 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

select
    record_type_id,
    name as record_type_name,
    modulenamespace as module_namespace,
    description as record_type_description,
    businessprocessid as business_process_id,
    sobjecttype as sobject_type,
    isactive as is_active,
    createdbyid as created_by_id,
    createddate as created_date,
    lastmodifiedbyid as last_modified_by_id,
    lastmodifieddate as last_modified_date,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__record_type"
where isdeleted = false
order by record_type_name
    );
  
  
[0m23:51:56.250987 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.253334 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:51:56.253666 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m23:51:56.254071 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.256065 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:51:56.256292 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m23:51:56.256588 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.257549 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m23:51:56.257734 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:51:56.258075 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m23:51:56.258974 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.260874 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:51:56.261097 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m23:51:56.261530 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.262377 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 23:51:56.238692 => 23:51:56.262278
[0m23:51:56.262598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m23:51:56.277961 [info ] [Thread-1  ]: 25 of 29 OK created sql table model dim.dim_record_type ........................ [[32mOK[0m in 0.04s]
[0m23:51:56.278345 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m23:51:56.278571 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m23:51:56.278939 [info ] [Thread-1  ]: 26 of 29 START sql table model dim.dim_solution ................................ [RUN]
[0m23:51:56.279436 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m23:51:56.279649 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m23:51:56.282639 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m23:51:56.283141 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 23:51:56.279785 => 23:51:56.283037
[0m23:51:56.283341 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m23:51:56.285905 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m23:51:56.286393 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:51:56.286598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m23:51:56.286780 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:56.293498 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.293727 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:51:56.293933 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solutionnumber as solution_number,
    solutionname as solution_name,
    ispublished as is_published,
    ispublishedinpublickb as is_published_in_public_kb,
    status as solution_status,
    isreviewed as is_reviewed,
    solutionnote as solution_note,
    caseid as case_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    timesused as times_used,
    ishtml as is_html
from "dbt"."staging"."stg_salesforce__solution"
where isdeleted = false
order by solution_name
    );
  
  
[0m23:51:56.295685 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.297968 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:51:56.298216 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m23:51:56.298650 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.300510 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:51:56.300720 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m23:51:56.301019 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.301979 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m23:51:56.302165 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:51:56.302335 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m23:51:56.303235 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.304784 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:51:56.304980 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m23:51:56.305398 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.306104 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 23:51:56.283471 => 23:51:56.306008
[0m23:51:56.306323 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m23:51:56.325053 [info ] [Thread-1  ]: 26 of 29 OK created sql table model dim.dim_solution ........................... [[32mOK[0m in 0.05s]
[0m23:51:56.325517 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m23:51:56.325846 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m23:51:56.326241 [info ] [Thread-1  ]: 27 of 29 START sql table model dim.dim_user .................................... [RUN]
[0m23:51:56.326694 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m23:51:56.326897 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m23:51:56.329335 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m23:51:56.329848 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 23:51:56.327032 => 23:51:56.329751
[0m23:51:56.330041 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m23:51:56.333459 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m23:51:56.333904 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:51:56.334099 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m23:51:56.334274 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:56.341223 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.341511 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:51:56.341747 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m23:51:56.345724 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.348018 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:51:56.348274 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user" rename to "dim_user__dbt_backup"
[0m23:51:56.348623 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.350364 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:51:56.350558 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m23:51:56.350859 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.351777 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m23:51:56.351973 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:51:56.352142 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m23:51:56.353093 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.355106 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:51:56.355344 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m23:51:56.355843 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.356636 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 23:51:56.330172 => 23:51:56.356541
[0m23:51:56.356846 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m23:51:56.375830 [info ] [Thread-1  ]: 27 of 29 OK created sql table model dim.dim_user ............................... [[32mOK[0m in 0.05s]
[0m23:51:56.376255 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m23:51:56.376506 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_lead_conversion
[0m23:51:56.376860 [info ] [Thread-1  ]: 28 of 29 START sql view model fact.fact_lead_conversion ........................ [RUN]
[0m23:51:56.377260 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_lead_conversion)
[0m23:51:56.377474 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_lead_conversion
[0m23:51:56.379735 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:51:56.380236 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (compile): 23:51:56.377613 => 23:51:56.380139
[0m23:51:56.380432 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_lead_conversion
[0m23:51:56.384419 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:51:56.384916 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:51:56.385106 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: BEGIN
[0m23:51:56.385276 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:56.392696 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.393001 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:51:56.393234 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */

  
  create view "dbt"."fact"."fact_lead_conversion__dbt_tmp" as (
    -- fact_lead_conversion.sql

with lead_data as (
    select 
        lead_id,
        lead_salutation,
        first_name,
        last_name,
        lead_company,
        lead_source,
        lead_status,
        is_converted,
        converted_date,
        converted_account_id,
        converted_contact_id,
        converted_opportunity_id,
        created_date as lead_created_date,
        last_modified_date as lead_last_modified_date
    from 
        "dbt"."dim"."dim_lead"
    where 
        is_converted = true
),

account_data as (
    select 
        account_id,
        account_name
    from 
        "dbt"."dim"."dim_account"
),

opportunity_data as (
    select 
        opportunity_id,
        opportunity_name,
        opportunity_amount
    from 
        "dbt"."dim"."dim_opportunity"
)

select 
    l.lead_id,
    l.first_name,
    l.last_name,
    l.lead_company,
    l.lead_source,
    l.lead_status,
    l.converted_date,
    a.account_name as converted_account_name,
    o.opportunity_name as converted_opportunity_name,
    o.opportunity_amount as converted_opportunity_amount,
    l.lead_created_date,
    l.lead_last_modified_date
from 
    lead_data l
left join 
    account_data a on l.converted_account_id = a.account_id
left join 
    opportunity_data o on l.converted_opportunity_id = o.opportunity_id
  );

[0m23:51:56.393884 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.396061 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:51:56.396279 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter view "dbt"."fact"."fact_lead_conversion" rename to "fact_lead_conversion__dbt_backup"
[0m23:51:56.396580 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.398552 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:51:56.398878 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter view "dbt"."fact"."fact_lead_conversion__dbt_tmp" rename to "fact_lead_conversion"
[0m23:51:56.399320 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.400281 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m23:51:56.400479 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:51:56.400655 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m23:51:56.401190 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.402806 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:51:56.403073 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
drop view if exists "dbt"."fact"."fact_lead_conversion__dbt_backup" cascade
[0m23:51:56.403479 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:51:56.404210 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (execute): 23:51:56.380560 => 23:51:56.404115
[0m23:51:56.404406 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: Close
[0m23:51:56.419349 [info ] [Thread-1  ]: 28 of 29 OK created sql view model fact.fact_lead_conversion ................... [[32mOK[0m in 0.04s]
[0m23:51:56.419856 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_lead_conversion
[0m23:51:56.420132 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m23:51:56.420462 [info ] [Thread-1  ]: 29 of 29 START sql incremental model fact.fact_opportunity ..................... [RUN]
[0m23:51:56.420878 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_lead_conversion, now model.elastic_dbt_interview.fact_opportunity)
[0m23:51:56.421081 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m23:51:56.427238 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m23:51:56.427864 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 23:51:56.421213 => 23:51:56.427746
[0m23:51:56.428081 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m23:51:56.446270 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:51:56.446618 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821235156443756"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        a.account_id as account_id,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
    on o.opportunity_id = a.opportunity_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m23:51:56.446836 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:51:56.454116 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 23:51:56.428221 => 23:51:56.453948
[0m23:51:56.454417 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m23:51:56.460040 [debug] [Thread-1  ]: Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "a" does not have a column named "opportunity_id"
  LINE 28:     on o.opportunity_id = a.opportunity_id
                                     ^
[0m23:51:56.460557 [error] [Thread-1  ]: 29 of 29 ERROR creating sql incremental model fact.fact_opportunity ............ [[31mERROR[0m in 0.04s]
[0m23:51:56.460905 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m23:51:56.461777 [debug] [MainThread]: Using duckdb connection "master"
[0m23:51:56.462014 [debug] [MainThread]: On master: BEGIN
[0m23:51:56.462176 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:51:56.468836 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:51:56.469104 [debug] [MainThread]: On master: COMMIT
[0m23:51:56.469261 [debug] [MainThread]: Using duckdb connection "master"
[0m23:51:56.469425 [debug] [MainThread]: On master: COMMIT
[0m23:51:56.469619 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:51:56.469783 [debug] [MainThread]: On master: Close
[0m23:51:56.472024 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:51:56.472238 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m23:51:56.472475 [info ] [MainThread]: 
[0m23:51:56.472653 [info ] [MainThread]: Finished running 13 table models, 15 view models, 1 incremental model in 0 hours 0 minutes and 1.77 seconds (1.77s).
[0m23:51:56.474573 [debug] [MainThread]: Command end result
[0m23:51:56.482237 [info ] [MainThread]: 
[0m23:51:56.482500 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:51:56.482661 [info ] [MainThread]: 
[0m23:51:56.482823 [error] [MainThread]:   Runtime Error in model fact_opportunity (models/facts/fact_opportunity.sql)
  Binder Error: Table "a" does not have a column named "opportunity_id"
  LINE 28:     on o.opportunity_id = a.opportunity_id
                                     ^
[0m23:51:56.482998 [info ] [MainThread]: 
[0m23:51:56.483167 [info ] [MainThread]: Done. PASS=28 WARN=0 ERROR=1 SKIP=0 TOTAL=29
[0m23:51:56.483506 [debug] [MainThread]: Command `dbt run` failed at 23:51:56.483458 after 2.00 seconds
[0m23:51:56.483702 [debug] [MainThread]: Flushing usage events


============================== 23:52:12.076093 | 71b240a3-e11f-412d-ac5a-27c8a67e3705 ==============================
[0m23:52:12.076093 [info ] [MainThread]: Running with dbt=1.6.18
[0m23:52:12.078571 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m23:52:12.078805 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m23:52:12.153783 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m23:52:12.171895 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m23:52:12.209321 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:52:12.209853 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_opportunity.sql
[0m23:52:12.265528 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
[0m23:52:12.278157 [info ] [MainThread]: Found 30 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m23:52:12.279821 [info ] [MainThread]: 
[0m23:52:12.280251 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m23:52:12.281665 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m23:52:12.288663 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:52:12.288917 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:52:12.289085 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:12.297991 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.298929 [debug] [ThreadPool]: On list_dbt: Close
[0m23:52:12.302565 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:52:12.302901 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:52:12.303166 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:12.311242 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.312147 [debug] [ThreadPool]: On list_dbt: Close
[0m23:52:12.315943 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m23:52:12.316290 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m23:52:12.316491 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:12.324065 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.324878 [debug] [ThreadPool]: On list_dbt: Close
[0m23:52:12.327401 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_dim)
[0m23:52:12.327840 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m23:52:12.354474 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m23:52:12.354767 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m23:52:12.354942 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:12.362589 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.362896 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m23:52:12.363077 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m23:52:12.363333 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.363843 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m23:52:12.364034 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m23:52:12.364189 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m23:52:12.364406 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.364588 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m23:52:12.366937 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now create_dbt_staging)
[0m23:52:12.367473 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m23:52:12.369378 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m23:52:12.369641 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m23:52:12.369814 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:12.376525 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.376785 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m23:52:12.376950 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m23:52:12.377185 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.377699 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m23:52:12.377873 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m23:52:12.378024 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m23:52:12.378238 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.378397 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m23:52:12.380840 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now create_dbt_fact)
[0m23:52:12.381329 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m23:52:12.382899 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:52:12.383095 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m23:52:12.383253 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:12.390311 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.390595 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:52:12.390769 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m23:52:12.391028 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.391549 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:52:12.391719 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m23:52:12.391865 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m23:52:12.392080 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.392231 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m23:52:12.395428 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_dim)
[0m23:52:12.399040 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:52:12.399253 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m23:52:12.399410 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:12.406853 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.407120 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m23:52:12.407289 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m23:52:12.423359 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.426968 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m23:52:12.427375 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m23:52:12.427543 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m23:52:12.429757 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_staging)
[0m23:52:12.432053 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:52:12.432233 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m23:52:12.432383 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:12.439219 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.439491 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m23:52:12.439662 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m23:52:12.457733 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.458661 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m23:52:12.458899 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m23:52:12.459070 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m23:52:12.461387 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_main)
[0m23:52:12.464428 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:52:12.464604 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m23:52:12.464749 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:12.471189 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.471432 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m23:52:12.471601 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m23:52:12.487181 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.490662 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m23:52:12.490920 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m23:52:12.491079 [debug] [ThreadPool]: On list_dbt_main: Close
[0m23:52:12.493441 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_fact)
[0m23:52:12.495748 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:52:12.495923 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m23:52:12.496069 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:52:12.502689 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.502944 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m23:52:12.503116 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m23:52:12.518428 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m23:52:12.521721 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m23:52:12.521943 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m23:52:12.522100 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m23:52:12.524654 [debug] [MainThread]: Using duckdb connection "master"
[0m23:52:12.524861 [debug] [MainThread]: On master: BEGIN
[0m23:52:12.525018 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:52:12.531373 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:52:12.531652 [debug] [MainThread]: On master: COMMIT
[0m23:52:12.531840 [debug] [MainThread]: Using duckdb connection "master"
[0m23:52:12.531988 [debug] [MainThread]: On master: COMMIT
[0m23:52:12.532210 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:52:12.532365 [debug] [MainThread]: On master: Close
[0m23:52:12.534291 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:52:12.534572 [info ] [MainThread]: 
[0m23:52:12.535696 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m23:52:12.536014 [info ] [Thread-1  ]: 1 of 29 START sql table model dim.dim_date ..................................... [RUN]
[0m23:52:12.536428 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_fact, now model.elastic_dbt_interview.dim_date)
[0m23:52:12.536772 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m23:52:12.563383 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:52:12.563703 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m23:52:12.563883 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:12.570983 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.571208 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:52:12.571396 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m23:52:12.571780 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.614950 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m23:52:12.615645 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 23:52:12.536907 => 23:52:12.615533
[0m23:52:12.615866 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m23:52:12.634075 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m23:52:12.634563 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:52:12.634963 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m23:52:12.682598 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.686600 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:52:12.686833 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m23:52:12.687251 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.689504 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:52:12.689703 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m23:52:12.690010 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.700314 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m23:52:12.700528 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:52:12.700708 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m23:52:12.704002 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.707046 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m23:52:12.707296 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m23:52:12.707829 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.708563 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 23:52:12.616005 => 23:52:12.708471
[0m23:52:12.708762 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m23:52:12.735947 [info ] [Thread-1  ]: 1 of 29 OK created sql table model dim.dim_date ................................ [[32mOK[0m in 0.20s]
[0m23:52:12.736393 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m23:52:12.736629 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m23:52:12.737005 [info ] [Thread-1  ]: 2 of 29 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m23:52:12.737541 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.stg_salesforce__account)
[0m23:52:12.737752 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m23:52:12.739993 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:52:12.740529 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 23:52:12.737888 => 23:52:12.740417
[0m23:52:12.740729 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m23:52:12.751712 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:52:12.752357 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:52:12.752576 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m23:52:12.752760 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:12.759869 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.760189 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:52:12.760431 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m23:52:12.761277 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.764095 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:52:12.764333 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m23:52:12.764652 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.766451 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:52:12.766664 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m23:52:12.766953 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.768220 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m23:52:12.768450 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:52:12.768647 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m23:52:12.769313 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.771096 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m23:52:12.771342 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m23:52:12.771726 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.772460 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 23:52:12.740859 => 23:52:12.772364
[0m23:52:12.772663 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m23:52:12.787591 [info ] [Thread-1  ]: 2 of 29 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.05s]
[0m23:52:12.787974 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m23:52:12.788196 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:52:12.788653 [info ] [Thread-1  ]: 3 of 29 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m23:52:12.789182 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m23:52:12.789408 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:52:12.791509 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:52:12.792001 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 23:52:12.789552 => 23:52:12.791889
[0m23:52:12.792210 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:52:12.794990 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:52:12.795553 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:52:12.795791 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m23:52:12.795966 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:12.803191 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.803509 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:52:12.803751 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m23:52:12.804514 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.807014 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:52:12.807298 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m23:52:12.807643 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.810479 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:52:12.810799 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m23:52:12.811156 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.812110 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m23:52:12.812317 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:52:12.812496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m23:52:12.813091 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.814837 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m23:52:12.815043 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m23:52:12.815402 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.816143 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 23:52:12.792356 => 23:52:12.816017
[0m23:52:12.816378 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m23:52:12.831184 [info ] [Thread-1  ]: 3 of 29 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.04s]
[0m23:52:12.831614 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m23:52:12.831853 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m23:52:12.832299 [info ] [Thread-1  ]: 4 of 29 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m23:52:12.832782 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m23:52:12.833009 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m23:52:12.835069 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:52:12.835571 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 23:52:12.833151 => 23:52:12.835461
[0m23:52:12.835775 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m23:52:12.838503 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:52:12.838986 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:52:12.839194 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m23:52:12.839381 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:12.846220 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.846498 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:52:12.846722 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m23:52:12.847415 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.849627 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:52:12.849835 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m23:52:12.850138 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.851852 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:52:12.852046 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m23:52:12.852328 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.853170 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m23:52:12.853353 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:52:12.853523 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m23:52:12.854171 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.856844 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m23:52:12.857072 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m23:52:12.857509 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.858390 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 23:52:12.835907 => 23:52:12.858290
[0m23:52:12.858610 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m23:52:12.874022 [info ] [Thread-1  ]: 4 of 29 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.04s]
[0m23:52:12.874446 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m23:52:12.874697 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:52:12.874980 [info ] [Thread-1  ]: 5 of 29 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m23:52:12.875341 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m23:52:12.875531 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:52:12.877507 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:52:12.878027 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 23:52:12.875664 => 23:52:12.877927
[0m23:52:12.878226 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:52:12.880933 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:52:12.881399 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:52:12.881598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m23:52:12.881786 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:12.888476 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.888761 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:52:12.888964 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m23:52:12.889409 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.891628 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:52:12.891840 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m23:52:12.892150 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.893872 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:52:12.894087 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m23:52:12.894371 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.895421 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m23:52:12.895756 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:52:12.895957 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m23:52:12.896593 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.899100 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m23:52:12.899344 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m23:52:12.899780 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.900617 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 23:52:12.878350 => 23:52:12.900518
[0m23:52:12.900837 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m23:52:12.914643 [info ] [Thread-1  ]: 5 of 29 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.04s]
[0m23:52:12.915151 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m23:52:12.915419 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:52:12.915783 [info ] [Thread-1  ]: 6 of 29 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m23:52:12.916226 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m23:52:12.916450 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:52:12.918574 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:52:12.919109 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 23:52:12.916591 => 23:52:12.919008
[0m23:52:12.919315 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:52:12.922134 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:52:12.922621 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:52:12.922818 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m23:52:12.923017 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:12.930013 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.930349 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:52:12.930625 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m23:52:12.931638 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.933882 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:52:12.934112 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m23:52:12.934439 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.936311 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:52:12.936620 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m23:52:12.937008 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.937978 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m23:52:12.938177 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:52:12.938368 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m23:52:12.939016 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.940777 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m23:52:12.940990 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m23:52:12.941361 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.942094 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 23:52:12.919447 => 23:52:12.942000
[0m23:52:12.942295 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m23:52:12.957266 [info ] [Thread-1  ]: 6 of 29 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.04s]
[0m23:52:12.957706 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m23:52:12.957957 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:52:12.958273 [info ] [Thread-1  ]: 7 of 29 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m23:52:12.958644 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m23:52:12.958840 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:52:12.961647 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:52:12.962156 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 23:52:12.958967 => 23:52:12.962049
[0m23:52:12.962349 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:52:12.965159 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:52:12.965637 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:52:12.965834 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m23:52:12.966008 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:12.972797 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.973120 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:52:12.973464 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m23:52:12.974581 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.976774 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:52:12.977005 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m23:52:12.977317 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.979011 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:52:12.979247 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m23:52:12.979529 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.980625 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m23:52:12.980932 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:52:12.981146 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m23:52:12.982066 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.983863 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m23:52:12.984093 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m23:52:12.984496 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:12.985269 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 23:52:12.962475 => 23:52:12.985176
[0m23:52:12.985479 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m23:52:13.000178 [info ] [Thread-1  ]: 7 of 29 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.04s]
[0m23:52:13.000577 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m23:52:13.000811 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:52:13.001241 [info ] [Thread-1  ]: 8 of 29 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m23:52:13.001737 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m23:52:13.001943 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:52:13.004778 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:52:13.005283 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 23:52:13.002086 => 23:52:13.005171
[0m23:52:13.005472 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:52:13.008254 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:52:13.008716 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:52:13.008912 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m23:52:13.009090 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.016276 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.016585 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:52:13.016825 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m23:52:13.017571 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.019835 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:52:13.020045 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m23:52:13.020348 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.022448 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:52:13.022775 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m23:52:13.023175 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.024169 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m23:52:13.024368 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:52:13.024547 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m23:52:13.025176 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.026883 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m23:52:13.027100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m23:52:13.027514 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.028296 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 23:52:13.005599 => 23:52:13.028193
[0m23:52:13.028512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m23:52:13.043191 [info ] [Thread-1  ]: 8 of 29 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.04s]
[0m23:52:13.043612 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m23:52:13.043866 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:52:13.044212 [info ] [Thread-1  ]: 9 of 29 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m23:52:13.044580 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m23:52:13.044776 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:52:13.046895 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:52:13.047410 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 23:52:13.044905 => 23:52:13.047311
[0m23:52:13.047604 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:52:13.050819 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:52:13.051189 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:52:13.051377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m23:52:13.051556 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.058623 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.058890 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:52:13.059099 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m23:52:13.059668 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.061849 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:52:13.062069 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m23:52:13.062384 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.064104 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:52:13.064418 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m23:52:13.065047 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.066100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m23:52:13.066328 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:52:13.066519 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m23:52:13.067147 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.069085 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m23:52:13.069315 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m23:52:13.069744 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.070538 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 23:52:13.047730 => 23:52:13.070434
[0m23:52:13.070746 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m23:52:13.084655 [info ] [Thread-1  ]: 9 of 29 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.04s]
[0m23:52:13.085116 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m23:52:13.085363 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:52:13.085660 [info ] [Thread-1  ]: 10 of 29 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m23:52:13.086053 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m23:52:13.086259 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:52:13.088276 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:52:13.088721 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 23:52:13.086395 => 23:52:13.088617
[0m23:52:13.088927 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:52:13.092558 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:52:13.093038 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:52:13.093230 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m23:52:13.093409 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.100129 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.100345 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:52:13.100549 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m23:52:13.101007 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.103138 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:52:13.103342 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m23:52:13.103640 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.105339 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:52:13.105556 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m23:52:13.105841 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.106677 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m23:52:13.106864 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:52:13.107043 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m23:52:13.107737 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.109791 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m23:52:13.110015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m23:52:13.110455 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.111310 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 23:52:13.089055 => 23:52:13.111203
[0m23:52:13.111535 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m23:52:13.125303 [info ] [Thread-1  ]: 10 of 29 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.04s]
[0m23:52:13.125716 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m23:52:13.125969 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:52:13.126291 [info ] [Thread-1  ]: 11 of 29 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m23:52:13.126667 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m23:52:13.126856 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:52:13.128838 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:52:13.129314 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 23:52:13.126989 => 23:52:13.129215
[0m23:52:13.129506 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:52:13.132205 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:52:13.132685 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:52:13.132883 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m23:52:13.133069 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.140034 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.140399 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:52:13.140849 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m23:52:13.141500 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.144441 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:52:13.144668 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m23:52:13.144985 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.173631 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:52:13.173989 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m23:52:13.174443 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.175544 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m23:52:13.175856 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:52:13.176051 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m23:52:13.176760 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.178461 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m23:52:13.178690 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m23:52:13.179093 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.179927 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 23:52:13.129632 => 23:52:13.179824
[0m23:52:13.180146 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m23:52:13.194763 [info ] [Thread-1  ]: 11 of 29 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.07s]
[0m23:52:13.195186 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m23:52:13.195429 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:52:13.195774 [info ] [Thread-1  ]: 12 of 29 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m23:52:13.196189 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m23:52:13.196402 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:52:13.198592 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:52:13.199107 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 23:52:13.196536 => 23:52:13.199005
[0m23:52:13.199305 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:52:13.202083 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:52:13.202529 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:52:13.202723 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m23:52:13.202906 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.209609 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.209901 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:52:13.210107 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m23:52:13.210576 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.212760 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:52:13.212974 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m23:52:13.213281 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.215818 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:52:13.216076 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m23:52:13.216600 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.217642 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m23:52:13.217858 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:52:13.218041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m23:52:13.218606 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.220340 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m23:52:13.220565 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m23:52:13.220920 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.221681 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 23:52:13.199433 => 23:52:13.221547
[0m23:52:13.221960 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m23:52:13.236111 [info ] [Thread-1  ]: 12 of 29 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.04s]
[0m23:52:13.236524 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m23:52:13.236769 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:52:13.237091 [info ] [Thread-1  ]: 13 of 29 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m23:52:13.237452 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m23:52:13.237644 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:52:13.239636 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:52:13.240110 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 23:52:13.237776 => 23:52:13.240009
[0m23:52:13.240308 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:52:13.243006 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:52:13.243507 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:52:13.243822 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m23:52:13.244038 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.250987 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.251250 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:52:13.251459 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m23:52:13.252035 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.254235 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:52:13.254464 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m23:52:13.254773 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.256492 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:52:13.256689 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m23:52:13.257087 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.259259 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m23:52:13.259507 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:52:13.259684 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m23:52:13.260270 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.262082 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m23:52:13.262309 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m23:52:13.262733 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.263494 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 23:52:13.240441 => 23:52:13.263402
[0m23:52:13.263701 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m23:52:13.277652 [info ] [Thread-1  ]: 13 of 29 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.04s]
[0m23:52:13.278105 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m23:52:13.278359 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m23:52:13.278721 [info ] [Thread-1  ]: 14 of 29 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m23:52:13.279120 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m23:52:13.279322 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m23:52:13.281510 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:52:13.281981 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 23:52:13.279475 => 23:52:13.281880
[0m23:52:13.282196 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m23:52:13.285006 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:52:13.285514 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:52:13.285701 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m23:52:13.285874 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.292785 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.293055 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:52:13.293317 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m23:52:13.294299 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.296543 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:52:13.296752 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m23:52:13.297059 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.298761 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:52:13.298957 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m23:52:13.299243 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.300066 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m23:52:13.300252 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:52:13.300420 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m23:52:13.301223 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.304053 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m23:52:13.304314 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m23:52:13.304722 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.305558 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 23:52:13.282339 => 23:52:13.305462
[0m23:52:13.305771 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m23:52:13.319757 [info ] [Thread-1  ]: 14 of 29 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.04s]
[0m23:52:13.320157 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m23:52:13.320411 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:52:13.320690 [info ] [Thread-1  ]: 15 of 29 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m23:52:13.321054 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m23:52:13.321249 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:52:13.323284 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:52:13.323760 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 23:52:13.321375 => 23:52:13.323660
[0m23:52:13.323951 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:52:13.326686 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:52:13.327147 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:52:13.327349 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m23:52:13.327678 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.334608 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.334977 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:52:13.335209 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m23:52:13.335775 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.338040 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:52:13.338259 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m23:52:13.338576 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.340285 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:52:13.340485 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m23:52:13.340798 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.341661 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m23:52:13.341846 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:52:13.342017 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m23:52:13.342610 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.344562 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m23:52:13.344803 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m23:52:13.345188 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.346110 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 23:52:13.324076 => 23:52:13.346004
[0m23:52:13.346339 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m23:52:13.360759 [info ] [Thread-1  ]: 15 of 29 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.04s]
[0m23:52:13.361204 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m23:52:13.361451 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m23:52:13.361774 [info ] [Thread-1  ]: 16 of 29 START sql table model dim.dim_account ................................. [RUN]
[0m23:52:13.362188 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m23:52:13.362404 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m23:52:13.365590 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m23:52:13.366112 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 23:52:13.362542 => 23:52:13.365994
[0m23:52:13.366327 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m23:52:13.369104 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m23:52:13.369558 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:52:13.369748 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m23:52:13.369936 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.376794 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.377052 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:52:13.377310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

select
    account_id,
    masterrecordid as master_record_id,
    name as account_name,
    type as account_type,
    parentid as parent_account_id,
    billingstreet as billing_street,
    billingcity as billing_city,
    billingstate as billing_state,
    billingpostalcode as billing_postal_code,
    billingcountry as billing_country,
    billinglatitude as billing_latitude,
    billinglongitude as billing_longitude,
    billinggeocodeaccuracy as billing_geocode_accuracy,
    shippingstreet as shipping_street,
    shippingcity as shipping_city,
    shippingstate as shipping_state,
    shippingpostalcode as shipping_postal_code,
    shippingcountry as shipping_country,
    shippinglatitude as shipping_latitude,
    shippinglongitude as shipping_longitude,
    shippinggeocodeaccuracy as shipping_geocode_accuracy,
    phone as account_phone,
    fax as account_fax,
    accountnumber as account_number,
    website as account_website,
    sic as sic_code,
    industry as account_industry,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownership as account_ownership,
    tickersymbol as ticker_symbol,
    description as account_description,
    rating as account_rating,
    site as account_site,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    jigsaw as jigsaw_id,
    jigsawcompanyid as jigsaw_company_id,
    cleanstatus as clean_status,
    accountsource as account_source,
    dunsnumber as duns_number,
    tradestyle as trade_style,
    naicscode as naics_code,
    naicsdesc as naics_description,
    yearstarted as year_started,
    sicdesc as sic_description,
    dandbcompanyid as dandb_company_id,
    operatinghoursid as operating_hours_id,
    customerpriority__c as customer_priority,
    sla__c as sla,
    active__c as is_active,
    numberoflocations__c as number_of_locations,
    upsellopportunity__c as upsell_opportunity,
    slaserialnumber__c as sla_serial_number,
    slaexpirationdate__c as sla_expiration_date
from "dbt"."staging"."stg_salesforce__account"
where isdeleted = false  -- Exclude deleted accounts
order by account_name
    );
  
  
[0m23:52:13.381481 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.383847 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:52:13.384103 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account" rename to "dim_account__dbt_backup"
[0m23:52:13.384523 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.386330 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:52:13.386532 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m23:52:13.386872 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.387825 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m23:52:13.388019 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:52:13.388198 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m23:52:13.389542 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.391222 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m23:52:13.391446 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m23:52:13.391973 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.392728 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 23:52:13.366464 => 23:52:13.392634
[0m23:52:13.392941 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m23:52:13.419891 [info ] [Thread-1  ]: 16 of 29 OK created sql table model dim.dim_account ............................ [[32mOK[0m in 0.06s]
[0m23:52:13.420267 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m23:52:13.420497 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m23:52:13.420896 [info ] [Thread-1  ]: 17 of 29 START sql table model dim.dim_campaign ................................ [RUN]
[0m23:52:13.421343 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m23:52:13.421557 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m23:52:13.424799 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m23:52:13.425315 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 23:52:13.421692 => 23:52:13.425210
[0m23:52:13.425516 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m23:52:13.428170 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m23:52:13.428657 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:52:13.428853 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m23:52:13.429027 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.436100 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.436371 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:52:13.436620 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

select
    /* IDs */
    campaign_id,
    parentid as parent_campaign_id,
    ownerid as owner_id,
    createdbyid as created_by_id,
    lastmodifiedbyid as last_modified_by_id,
    campaignmemberrecordtypeid as campaign_member_record_type_id,

    /* Dates */
    startdate as start_date,
    enddate as end_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,

    /* Dimensions */
    name as campaign_name,
    type as campaign_type,
    status,
    description,
    isactive as is_active,

    /* Metrics */
    expectedrevenue as expected_revenue,
    budgetedcost as budgeted_cost,
    actualcost as actual_cost,
    expectedresponse as expected_response,
    numbersent as number_sent,
    numberofleads as number_of_leads,
    numberofconvertedleads as number_of_converted_leads,
    numberofcontacts as number_of_contacts,
    numberofresponses as number_of_responses,
    numberofopportunities as number_of_opportunities,
    numberofwonopportunities as number_of_won_opportunities,
    amountallopportunities as amount_all_opportunities,
    amountwonopportunities as amount_won_opportunities,
    hierarchynumberofleads as hierarchy_number_of_leads,
    hierarchynumberofconvertedleads as hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts as hierarchy_number_of_contacts,
    hierarchynumberofresponses as hierarchy_number_of_responses,
    hierarchynumberofopportunities as hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities as hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities as hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities as hierarchy_amount_won_opportunities,
    hierarchynumbersent as hierarchy_number_sent,
    hierarchyexpectedrevenue as hierarchy_expected_revenue,
    hierarchybudgetedcost as hierarchy_budgeted_cost,
    hierarchyactualcost as hierarchy_actual_cost

from "dbt"."staging"."stg_salesforce__campaign"
where isdeleted = false
    );
  
  
[0m23:52:13.439403 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.441693 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:52:13.441988 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign" rename to "dim_campaign__dbt_backup"
[0m23:52:13.442427 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.444355 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:52:13.444557 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m23:52:13.444876 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.445873 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m23:52:13.446060 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:52:13.446238 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m23:52:13.447259 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.449036 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m23:52:13.449284 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."dim"."dim_campaign__dbt_backup" cascade
[0m23:52:13.449820 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.450713 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 23:52:13.425646 => 23:52:13.450615
[0m23:52:13.450932 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m23:52:13.467781 [info ] [Thread-1  ]: 17 of 29 OK created sql table model dim.dim_campaign ........................... [[32mOK[0m in 0.05s]
[0m23:52:13.468214 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m23:52:13.468462 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m23:52:13.468742 [info ] [Thread-1  ]: 18 of 29 START sql table model dim.dim_case_status ............................. [RUN]
[0m23:52:13.469110 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m23:52:13.469304 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m23:52:13.471546 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m23:52:13.472038 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 23:52:13.469437 => 23:52:13.471940
[0m23:52:13.472231 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m23:52:13.475923 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m23:52:13.476417 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:52:13.476609 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m23:52:13.476782 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.484009 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.484304 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:52:13.484506 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."dim"."dim_case_status__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct status as status_name, status as status_description  -- Adjust as needed, typically a description field should be separate
        from "dbt"."staging"."stg_salesforce__case_history_2"
    )

select
    row_number() over (order by status_name) as status_id,  -- Surrogate Key
    status_name,
    status_description
from source
    );
  
  
[0m23:52:13.486098 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.488340 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:52:13.488604 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m23:52:13.488999 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.490861 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:52:13.491100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m23:52:13.491394 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.492346 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m23:52:13.492545 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:52:13.492716 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m23:52:13.493349 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.494963 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m23:52:13.495166 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."dim"."dim_case_status__dbt_backup" cascade
[0m23:52:13.495543 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.496260 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 23:52:13.472356 => 23:52:13.496166
[0m23:52:13.496463 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m23:52:13.511773 [info ] [Thread-1  ]: 18 of 29 OK created sql table model dim.dim_case_status ........................ [[32mOK[0m in 0.04s]
[0m23:52:13.512189 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m23:52:13.512428 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m23:52:13.512695 [info ] [Thread-1  ]: 19 of 29 START sql table model dim.dim_contact ................................. [RUN]
[0m23:52:13.513043 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m23:52:13.513239 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m23:52:13.515500 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m23:52:13.516044 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 23:52:13.513369 => 23:52:13.515939
[0m23:52:13.516241 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m23:52:13.518880 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m23:52:13.519408 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:52:13.519624 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m23:52:13.519812 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.526635 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.526927 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:52:13.527178 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

select
    /* IDs */
    contact_id,
    masterrecordid as master_record_id,
    accountid as account_id,
    reportstoid as reports_to_id,
    ownerid as owner_id,
    jigsawcontactid as jigsaw_contact_id,
    individualid as individual_id,

    /* Dates */
    birthdate as birth_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    lastcurequestdate as last_cu_request_date,
    lastcuupdatedate as last_cu_update_date,
    emailbounceddate as email_bounced_date,

    /* Dimensions */
    salutation,
    firstname as first_name,
    lastname as last_name,
    otherstreet as other_street,
    othercity as other_city,
    otherstate as other_state,
    otherpostalcode as other_postal_code,
    othercountry as other_country,
    otherlatitude as other_latitude,
    otherlongitude as other_longitude,
    othergeocodeaccuracy as other_geocode_accuracy,
    mailingstreet as mailing_street,
    mailingcity as mailing_city,
    mailingstate as mailing_state,
    mailingpostalcode as mailing_postal_code,
    mailingcountry as mailing_country,
    mailinglatitude as mailing_latitude,
    mailinglongitude as mailing_longitude,
    mailinggeocodeaccuracy as mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone as mobile_phone,
    homephone as home_phone,
    otherphone as other_phone,
    assistantphone as assistant_phone,
    email,
    title,
    department,
    assistantname as assistant_name,
    leadsource as lead_source,
    description,
    pronouns,
    genderidentity as gender_identity,
    cleanstatus as clean_status,
    emailbouncedreason as email_bounced_reason,
    level__c as level,
    languages__c as languages,

    /* Metrics */
    hasoptedoutofemail as has_opted_out_of_email,
    hasoptedoutoffax as has_opted_out_of_fax,
    donotcall as do_not_call

from "dbt"."staging"."stg_salesforce__contact"
where isdeleted = false
    );
  
  
[0m23:52:13.530841 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.534535 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:52:13.534852 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact" rename to "dim_contact__dbt_backup"
[0m23:52:13.535290 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.537102 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:52:13.537312 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m23:52:13.537647 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.538649 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m23:52:13.538846 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:52:13.539019 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m23:52:13.540385 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.542037 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m23:52:13.542245 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m23:52:13.542792 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.543515 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 23:52:13.516368 => 23:52:13.543423
[0m23:52:13.543715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m23:52:13.568696 [info ] [Thread-1  ]: 19 of 29 OK created sql table model dim.dim_contact ............................ [[32mOK[0m in 0.06s]
[0m23:52:13.569271 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m23:52:13.569534 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m23:52:13.569888 [info ] [Thread-1  ]: 20 of 29 START sql table model dim.dim_lead .................................... [RUN]
[0m23:52:13.570315 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m23:52:13.570522 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m23:52:13.573038 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m23:52:13.573573 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 23:52:13.570659 => 23:52:13.573461
[0m23:52:13.573787 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m23:52:13.576458 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m23:52:13.576958 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:52:13.577153 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m23:52:13.577345 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.584428 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.584694 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:52:13.584946 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

select
    lead_id,
    masterrecordid as master_record_id,
    salutation as lead_salutation,
    firstname as first_name,
    lastname as last_name,
    title as lead_title,
    company as lead_company,
    street as lead_street,
    city as lead_city,
    state as lead_state,
    postalcode as lead_postal_code,
    country as lead_country,
    latitude as lead_latitude,
    longitude as lead_longitude,
    geocodeaccuracy as geocode_accuracy,
    phone as lead_phone,
    mobilephone as lead_mobile_phone,
    fax as lead_fax,
    email as lead_email,
    website as lead_website,
    description as lead_description,
    leadsource as lead_source,
    status as lead_status,
    industry as lead_industry,
    rating as lead_rating,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownerid as owner_id,
    hasoptedoutofemail as has_opted_out_of_email,
    isconverted as is_converted,
    converteddate as converted_date,
    convertedaccountid as converted_account_id,
    convertedcontactid as converted_contact_id,
    convertedopportunityid as converted_opportunity_id,
    isunreadbyowner as is_unread_by_owner,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    donotcall as do_not_call,
    hasoptedoutoffax as has_opted_out_of_fax,
    lasttransferdate as last_transfer_date,
    jigsaw as jigsaw_id,
    jigsawcontactid as jigsaw_contact_id,
    cleanstatus as clean_status,
    companydunsnumber as company_duns_number,
    dandbcompanyid as dandb_company_id,
    emailbouncedreason as email_bounced_reason,
    emailbounceddate as email_bounced_date,
    individualid as individual_id,
    pronouns as lead_pronouns,
    genderidentity as gender_identity,
    siccode__c as sic_code,
    productinterest__c as product_interest,
    primary__c as is_primary,
    currentgenerators__c as current_generators,
    numberoflocations__c as number_of_locations
from "dbt"."staging"."stg_salesforce__lead"
where isdeleted = false  -- Exclude deleted leads
order by last_name, first_name
    );
  
  
[0m23:52:13.589347 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.592625 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:52:13.592915 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead" rename to "dim_lead__dbt_backup"
[0m23:52:13.593324 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.595149 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:52:13.595346 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m23:52:13.595652 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.596594 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m23:52:13.596777 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:52:13.596956 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m23:52:13.598634 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.600595 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m23:52:13.600818 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m23:52:13.601367 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.602144 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 23:52:13.573923 => 23:52:13.602044
[0m23:52:13.602390 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m23:52:13.623860 [info ] [Thread-1  ]: 20 of 29 OK created sql table model dim.dim_lead ............................... [[32mOK[0m in 0.05s]
[0m23:52:13.624312 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m23:52:13.624568 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m23:52:13.624866 [info ] [Thread-1  ]: 21 of 29 START sql table model dim.dim_opportunity ............................. [RUN]
[0m23:52:13.625228 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m23:52:13.625419 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m23:52:13.627749 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m23:52:13.628247 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 23:52:13.625546 => 23:52:13.628141
[0m23:52:13.628452 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m23:52:13.631109 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m23:52:13.631606 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:52:13.631882 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m23:52:13.632137 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.639282 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.639610 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:52:13.639883 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m23:52:13.643292 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.645572 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:52:13.645816 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m23:52:13.646227 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.649012 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:52:13.649278 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m23:52:13.649633 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.650772 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m23:52:13.651029 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:52:13.651219 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m23:52:13.652395 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.654048 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m23:52:13.654257 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m23:52:13.654741 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.655537 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 23:52:13.628601 => 23:52:13.655443
[0m23:52:13.655757 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m23:52:13.675336 [info ] [Thread-1  ]: 21 of 29 OK created sql table model dim.dim_opportunity ........................ [[32mOK[0m in 0.05s]
[0m23:52:13.675768 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m23:52:13.676020 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:52:13.676384 [info ] [Thread-1  ]: 22 of 29 START sql table model dim.dim_opportunity_stage ....................... [RUN]
[0m23:52:13.676792 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m23:52:13.676993 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:52:13.679267 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:52:13.679785 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 23:52:13.677125 => 23:52:13.679682
[0m23:52:13.679983 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:52:13.682666 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:52:13.683121 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:52:13.683314 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m23:52:13.683485 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.690256 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.690625 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:52:13.690861 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct stagename as stage_name, stagesortorder as stage_sort_order
        from "dbt"."staging"."stg_salesforce__opportunity"
    )

select
    row_number() over (order by stage_sort_order) as stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
from source
    );
  
  
[0m23:52:13.693301 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.695648 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:52:13.695902 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m23:52:13.696287 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.699243 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:52:13.699496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m23:52:13.699819 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.700969 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m23:52:13.701246 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:52:13.701438 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m23:52:13.702116 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.703866 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m23:52:13.704067 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m23:52:13.704441 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.705188 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 23:52:13.680114 => 23:52:13.705088
[0m23:52:13.705391 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m23:52:13.720817 [info ] [Thread-1  ]: 22 of 29 OK created sql table model dim.dim_opportunity_stage .................. [[32mOK[0m in 0.04s]
[0m23:52:13.721250 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m23:52:13.721497 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m23:52:13.721785 [info ] [Thread-1  ]: 23 of 29 START sql table model dim.dim_pricebook ............................... [RUN]
[0m23:52:13.722172 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m23:52:13.722372 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m23:52:13.724606 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m23:52:13.725104 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 23:52:13.722504 => 23:52:13.725003
[0m23:52:13.725315 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m23:52:13.727751 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m23:52:13.728112 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:52:13.728311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m23:52:13.728637 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.735390 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.735687 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:52:13.735893 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

select
    pricebook_entry_id,
    pricebook2id as pricebook_id,
    product2id as product_id,
    unitprice as unit_price,
    isactive as is_active,
    usestandardprice as use_standard_price,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__pricebook_entry"
where isdeleted = false  -- Exclude deleted entries
order by pricebook_entry_id
    );
  
  
[0m23:52:13.737338 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.739637 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:52:13.739923 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook" rename to "dim_pricebook__dbt_backup"
[0m23:52:13.740344 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.742173 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:52:13.742404 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m23:52:13.742708 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.743743 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m23:52:13.743938 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:52:13.744108 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m23:52:13.744813 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.747022 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m23:52:13.747228 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m23:52:13.747661 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.748401 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 23:52:13.725447 => 23:52:13.748313
[0m23:52:13.748650 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m23:52:13.764932 [info ] [Thread-1  ]: 23 of 29 OK created sql table model dim.dim_pricebook .......................... [[32mOK[0m in 0.04s]
[0m23:52:13.765353 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m23:52:13.765606 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m23:52:13.765886 [info ] [Thread-1  ]: 24 of 29 START sql table model dim.dim_product ................................. [RUN]
[0m23:52:13.766270 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m23:52:13.766457 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m23:52:13.768634 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m23:52:13.769098 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 23:52:13.766588 => 23:52:13.768997
[0m23:52:13.769293 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m23:52:13.771660 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m23:52:13.772002 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:52:13.772197 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m23:52:13.772386 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.780241 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.780546 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:52:13.780764 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    productcode as product_code,
    description as product_description,
    isactive as is_active,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    family as product_family,
    externaldatasourceid as external_datasource_id,
    externalid as external_id,
    displayurl as display_url,
    quantityunitofmeasure as quantity_unit_of_measure,
    stockkeepingunit as stock_keeping_unit,
    type as product_type,
    productclass as product_class,
    sourceproductid as source_product_id,
    sellerid as seller_id
from "dbt"."staging"."stg_salesforce__product_2"
where isdeleted = false
-- isactive =1 ?
order by product_name
    );
  
  
[0m23:52:13.782760 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.785028 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:52:13.785285 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product" rename to "dim_product__dbt_backup"
[0m23:52:13.785680 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.787406 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:52:13.787608 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m23:52:13.787907 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.788824 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m23:52:13.789007 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:52:13.789176 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m23:52:13.789910 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.791833 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m23:52:13.792073 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m23:52:13.792617 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.793539 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 23:52:13.769423 => 23:52:13.793434
[0m23:52:13.793757 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m23:52:13.810077 [info ] [Thread-1  ]: 24 of 29 OK created sql table model dim.dim_product ............................ [[32mOK[0m in 0.04s]
[0m23:52:13.810493 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m23:52:13.810738 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m23:52:13.811001 [info ] [Thread-1  ]: 25 of 29 START sql table model dim.dim_record_type ............................. [RUN]
[0m23:52:13.811406 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m23:52:13.811694 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m23:52:13.814679 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m23:52:13.815160 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 23:52:13.811832 => 23:52:13.815061
[0m23:52:13.815349 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m23:52:13.842134 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m23:52:13.842876 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:52:13.843154 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m23:52:13.843337 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.852666 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.852964 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:52:13.853181 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

select
    record_type_id,
    name as record_type_name,
    modulenamespace as module_namespace,
    description as record_type_description,
    businessprocessid as business_process_id,
    sobjecttype as sobject_type,
    isactive as is_active,
    createdbyid as created_by_id,
    createddate as created_date,
    lastmodifiedbyid as last_modified_by_id,
    lastmodifieddate as last_modified_date,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__record_type"
where isdeleted = false
order by record_type_name
    );
  
  
[0m23:52:13.854758 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.857056 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:52:13.857301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m23:52:13.857675 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.859396 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:52:13.859604 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m23:52:13.859911 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.860863 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m23:52:13.861059 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:52:13.861230 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m23:52:13.861908 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.863422 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m23:52:13.863619 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m23:52:13.863992 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.864671 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 23:52:13.815475 => 23:52:13.864580
[0m23:52:13.864897 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m23:52:13.881571 [info ] [Thread-1  ]: 25 of 29 OK created sql table model dim.dim_record_type ........................ [[32mOK[0m in 0.07s]
[0m23:52:13.882025 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m23:52:13.882273 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m23:52:13.882549 [info ] [Thread-1  ]: 26 of 29 START sql table model dim.dim_solution ................................ [RUN]
[0m23:52:13.882911 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m23:52:13.883112 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m23:52:13.885967 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m23:52:13.886438 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 23:52:13.883244 => 23:52:13.886340
[0m23:52:13.886657 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m23:52:13.889246 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m23:52:13.890006 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:52:13.890296 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m23:52:13.890563 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.897335 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.897663 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:52:13.897957 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solutionnumber as solution_number,
    solutionname as solution_name,
    ispublished as is_published,
    ispublishedinpublickb as is_published_in_public_kb,
    status as solution_status,
    isreviewed as is_reviewed,
    solutionnote as solution_note,
    caseid as case_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    timesused as times_used,
    ishtml as is_html
from "dbt"."staging"."stg_salesforce__solution"
where isdeleted = false
order by solution_name
    );
  
  
[0m23:52:13.899932 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.902091 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:52:13.902315 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m23:52:13.902679 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.904461 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:52:13.904651 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m23:52:13.904940 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.905859 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m23:52:13.906045 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:52:13.906293 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m23:52:13.907069 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.908896 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m23:52:13.909116 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m23:52:13.909525 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.910350 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 23:52:13.886794 => 23:52:13.910256
[0m23:52:13.910556 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m23:52:13.928004 [info ] [Thread-1  ]: 26 of 29 OK created sql table model dim.dim_solution ........................... [[32mOK[0m in 0.05s]
[0m23:52:13.928399 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m23:52:13.928630 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m23:52:13.928996 [info ] [Thread-1  ]: 27 of 29 START sql table model dim.dim_user .................................... [RUN]
[0m23:52:13.929479 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m23:52:13.929685 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m23:52:13.932067 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m23:52:13.932557 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 23:52:13.929819 => 23:52:13.932451
[0m23:52:13.932758 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m23:52:13.936316 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m23:52:13.936881 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:52:13.937091 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m23:52:13.937282 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.944376 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.944680 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:52:13.944908 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m23:52:13.948689 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.951010 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:52:13.951259 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user" rename to "dim_user__dbt_backup"
[0m23:52:13.951666 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.953416 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:52:13.953626 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m23:52:13.953938 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.954856 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m23:52:13.955056 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:52:13.955226 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m23:52:13.956124 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.958084 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m23:52:13.958334 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m23:52:13.958849 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.959705 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 23:52:13.932887 => 23:52:13.959612
[0m23:52:13.959919 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m23:52:13.977931 [info ] [Thread-1  ]: 27 of 29 OK created sql table model dim.dim_user ............................... [[32mOK[0m in 0.05s]
[0m23:52:13.978362 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m23:52:13.978637 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_lead_conversion
[0m23:52:13.978999 [info ] [Thread-1  ]: 28 of 29 START sql view model fact.fact_lead_conversion ........................ [RUN]
[0m23:52:13.979408 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_lead_conversion)
[0m23:52:13.979628 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_lead_conversion
[0m23:52:13.981991 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:52:13.982460 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (compile): 23:52:13.979776 => 23:52:13.982361
[0m23:52:13.982655 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_lead_conversion
[0m23:52:13.986016 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:52:13.986459 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:52:13.986667 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: BEGIN
[0m23:52:13.986842 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:13.993558 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.993844 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:52:13.994075 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */

  
  create view "dbt"."fact"."fact_lead_conversion__dbt_tmp" as (
    -- fact_lead_conversion.sql

with lead_data as (
    select 
        lead_id,
        lead_salutation,
        first_name,
        last_name,
        lead_company,
        lead_source,
        lead_status,
        is_converted,
        converted_date,
        converted_account_id,
        converted_contact_id,
        converted_opportunity_id,
        created_date as lead_created_date,
        last_modified_date as lead_last_modified_date
    from 
        "dbt"."dim"."dim_lead"
    where 
        is_converted = true
),

account_data as (
    select 
        account_id,
        account_name
    from 
        "dbt"."dim"."dim_account"
),

opportunity_data as (
    select 
        opportunity_id,
        opportunity_name,
        opportunity_amount
    from 
        "dbt"."dim"."dim_opportunity"
)

select 
    l.lead_id,
    l.first_name,
    l.last_name,
    l.lead_company,
    l.lead_source,
    l.lead_status,
    l.converted_date,
    a.account_name as converted_account_name,
    o.opportunity_name as converted_opportunity_name,
    o.opportunity_amount as converted_opportunity_amount,
    l.lead_created_date,
    l.lead_last_modified_date
from 
    lead_data l
left join 
    account_data a on l.converted_account_id = a.account_id
left join 
    opportunity_data o on l.converted_opportunity_id = o.opportunity_id
  );

[0m23:52:13.994692 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.996814 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:52:13.997021 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter view "dbt"."fact"."fact_lead_conversion" rename to "fact_lead_conversion__dbt_backup"
[0m23:52:13.997312 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:13.999025 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:52:13.999241 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter view "dbt"."fact"."fact_lead_conversion__dbt_tmp" rename to "fact_lead_conversion"
[0m23:52:13.999524 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:14.000555 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m23:52:14.000854 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:52:14.001043 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m23:52:14.001672 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:14.003459 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m23:52:14.003665 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
drop view if exists "dbt"."fact"."fact_lead_conversion__dbt_backup" cascade
[0m23:52:14.004040 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:14.004852 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (execute): 23:52:13.982783 => 23:52:14.004750
[0m23:52:14.005061 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: Close
[0m23:52:14.019045 [info ] [Thread-1  ]: 28 of 29 OK created sql view model fact.fact_lead_conversion ................... [[32mOK[0m in 0.04s]
[0m23:52:14.019440 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_lead_conversion
[0m23:52:14.019666 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m23:52:14.020049 [info ] [Thread-1  ]: 29 of 29 START sql incremental model fact.fact_opportunity ..................... [RUN]
[0m23:52:14.020539 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_lead_conversion, now model.elastic_dbt_interview.fact_opportunity)
[0m23:52:14.020740 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m23:52:14.024693 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m23:52:14.025177 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 23:52:14.020871 => 23:52:14.025068
[0m23:52:14.025399 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m23:52:14.081415 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:52:14.081844 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240821235214041107"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        a.account_id,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m23:52:14.082095 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:52:14.092375 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:14.097844 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:52:14.098173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m23:52:14.098541 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:14.098738 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:52:14.098940 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240821235214041107'
      
      
      order by ordinal_position

  
[0m23:52:14.127422 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:14.131228 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:52:14.131577 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m23:52:14.145882 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:14.150732 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:52:14.150986 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m23:52:14.165434 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:14.174470 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m23:52:14.175042 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:52:14.175279 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240821235214041107"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240821235214041107"
    )
  
[0m23:52:14.176342 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:14.177402 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m23:52:14.177631 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m23:52:14.177812 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m23:52:14.178121 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m23:52:14.178602 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 23:52:14.025689 => 23:52:14.178497
[0m23:52:14.178812 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m23:52:14.181568 [info ] [Thread-1  ]: 29 of 29 OK created sql incremental model fact.fact_opportunity ................ [[32mOK[0m in 0.16s]
[0m23:52:14.181966 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m23:52:14.182795 [debug] [MainThread]: Using duckdb connection "master"
[0m23:52:14.182983 [debug] [MainThread]: On master: BEGIN
[0m23:52:14.183139 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:52:14.190151 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:52:14.190422 [debug] [MainThread]: On master: COMMIT
[0m23:52:14.190581 [debug] [MainThread]: Using duckdb connection "master"
[0m23:52:14.190755 [debug] [MainThread]: On master: COMMIT
[0m23:52:14.190953 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m23:52:14.191120 [debug] [MainThread]: On master: Close
[0m23:52:14.193543 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:52:14.193827 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m23:52:14.194085 [info ] [MainThread]: 
[0m23:52:14.194273 [info ] [MainThread]: Finished running 13 table models, 15 view models, 1 incremental model in 0 hours 0 minutes and 1.91 seconds (1.91s).
[0m23:52:14.196455 [debug] [MainThread]: Command end result
[0m23:52:14.205208 [info ] [MainThread]: 
[0m23:52:14.205481 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:52:14.205830 [info ] [MainThread]: 
[0m23:52:14.206046 [info ] [MainThread]: Done. PASS=29 WARN=0 ERROR=0 SKIP=0 TOTAL=29
[0m23:52:14.206423 [debug] [MainThread]: Command `dbt run` succeeded at 23:52:14.206369 after 2.15 seconds
[0m23:52:14.206608 [debug] [MainThread]: Flushing usage events


============================== 11:40:28.290471 | f25159e5-4136-4995-95fc-0638d3e6f5ee ==============================
[0m11:40:28.290471 [info ] [MainThread]: Running with dbt=1.6.18
[0m11:40:28.293500 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m11:40:28.293908 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m11:40:28.378226 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m11:40:28.398996 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m11:40:28.450742 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:40:28.451247 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_lead_conversion.sql
[0m11:40:28.479506 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m11:40:28.494211 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m11:40:28.495987 [info ] [MainThread]: 
[0m11:40:28.496470 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:40:28.497798 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m11:40:28.504767 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:40:28.505067 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:40:28.505258 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:28.512654 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:40:28.513018 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:40:28.513331 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:28.518023 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:40:28.518260 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:40:28.518422 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:28.521561 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:40:28.521824 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m11:40:28.522007 [info ] [MainThread]: 
[0m11:40:28.522222 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m11:40:28.522513 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 40475) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m11:40:28.523038 [debug] [MainThread]: Command `dbt run` failed at 11:40:28.522980 after 0.26 seconds
[0m11:40:28.523250 [debug] [MainThread]: Flushing usage events


============================== 11:40:40.435956 | 95b8e7ad-9c8d-448f-963e-2d4021327eb5 ==============================
[0m11:40:40.435956 [info ] [MainThread]: Running with dbt=1.6.18
[0m11:40:40.439773 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt run', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m11:40:40.440140 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m11:40:40.517880 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m11:40:40.537411 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m11:40:40.585348 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:40:40.585648 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:40:40.586572 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m11:40:40.600395 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m11:40:40.602231 [info ] [MainThread]: 
[0m11:40:40.602704 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:40:40.604128 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m11:40:40.611454 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:40:40.611725 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:40:40.611904 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:40.619712 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:40:40.620023 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:40:40.620197 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:40.624923 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:40:40.625172 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:40:40.625335 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:40.628386 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:40:40.628675 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m11:40:40.628872 [info ] [MainThread]: 
[0m11:40:40.629098 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m11:40:40.629385 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 40475) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m11:40:40.629787 [debug] [MainThread]: Command `dbt run` failed at 11:40:40.629735 after 0.23 seconds
[0m11:40:40.629989 [debug] [MainThread]: Flushing usage events


============================== 11:40:47.945433 | f526bcf5-92b6-4c09-a356-43b0298f6b3b ==============================
[0m11:40:47.945433 [info ] [MainThread]: Running with dbt=1.6.18
[0m11:40:47.947671 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'False'}
[0m11:40:47.947907 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m11:40:48.021988 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m11:40:48.040596 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m11:40:48.076740 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:40:48.077020 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:40:48.077942 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
[0m11:40:48.092341 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m11:40:48.094040 [info ] [MainThread]: 
[0m11:40:48.094484 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:40:48.095876 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m11:40:48.102920 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:40:48.103179 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:40:48.103359 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:48.118724 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.119753 [debug] [ThreadPool]: On list_dbt: Close
[0m11:40:48.123238 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:40:48.123487 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:40:48.123659 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:48.131410 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.132245 [debug] [ThreadPool]: On list_dbt: Close
[0m11:40:48.135625 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:40:48.135851 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:40:48.136036 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:48.142974 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.143735 [debug] [ThreadPool]: On list_dbt: Close
[0m11:40:48.145878 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_fact)
[0m11:40:48.146347 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m11:40:48.149238 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:40:48.149432 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m11:40:48.149589 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:48.156132 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.156392 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:40:48.156559 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m11:40:48.156817 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.157332 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m11:40:48.157496 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:40:48.157640 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m11:40:48.157852 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.158006 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m11:40:48.160040 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now create_dbt_staging)
[0m11:40:48.160447 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m11:40:48.161921 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:40:48.162093 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m11:40:48.162234 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:48.168431 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.168672 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:40:48.168831 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m11:40:48.169057 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.169538 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m11:40:48.169698 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:40:48.169841 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m11:40:48.170047 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.170205 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m11:40:48.172107 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now create_dbt_dim)
[0m11:40:48.172495 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m11:40:48.174564 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:40:48.174736 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m11:40:48.174880 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:48.181502 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.181823 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:40:48.182003 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m11:40:48.182294 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.182810 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m11:40:48.182979 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:40:48.183142 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m11:40:48.183381 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.183547 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m11:40:48.186688 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_fact)
[0m11:40:48.189990 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m11:40:48.190292 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m11:40:48.190546 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:48.197769 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.198034 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m11:40:48.198206 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m11:40:48.214880 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.218513 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m11:40:48.220042 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m11:40:48.220221 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m11:40:48.222809 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m11:40:48.224738 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m11:40:48.224932 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m11:40:48.225083 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:48.232296 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.232554 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m11:40:48.232724 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m11:40:48.248451 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.251924 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m11:40:48.252176 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m11:40:48.252335 [debug] [ThreadPool]: On list_dbt_main: Close
[0m11:40:48.254840 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m11:40:48.256431 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m11:40:48.256631 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m11:40:48.256788 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:48.264336 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.264569 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m11:40:48.264739 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m11:40:48.282572 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.283504 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m11:40:48.283746 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m11:40:48.283901 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m11:40:48.286247 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m11:40:48.288630 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m11:40:48.288802 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m11:40:48.288947 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:48.295261 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.295506 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m11:40:48.295675 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m11:40:48.310924 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:40:48.314390 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m11:40:48.314619 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m11:40:48.314777 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m11:40:48.318287 [debug] [MainThread]: Using duckdb connection "master"
[0m11:40:48.318470 [debug] [MainThread]: On master: BEGIN
[0m11:40:48.318617 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:40:48.325462 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:40:48.325716 [debug] [MainThread]: On master: COMMIT
[0m11:40:48.325874 [debug] [MainThread]: Using duckdb connection "master"
[0m11:40:48.326024 [debug] [MainThread]: On master: COMMIT
[0m11:40:48.326219 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:40:48.326380 [debug] [MainThread]: On master: Close
[0m11:40:48.328238 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:40:48.328518 [info ] [MainThread]: 
[0m11:40:48.331059 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m11:40:48.331384 [info ] [Thread-1  ]: 1 of 30 START sql table model dim.dim_date ..................................... [RUN]
[0m11:40:48.331780 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_date)
[0m11:40:48.331975 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m11:40:48.358166 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:40:48.358471 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m11:40:48.358659 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.365310 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.365620 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:40:48.365821 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m11:40:48.366186 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.408012 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m11:40:48.409465 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 11:40:48.332111 => 11:40:48.409327
[0m11:40:48.409708 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m11:40:48.454414 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m11:40:48.455549 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:40:48.456035 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m11:40:48.503265 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.507337 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:40:48.507588 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m11:40:48.508017 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.509866 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:40:48.510102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m11:40:48.510458 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.520875 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m11:40:48.521099 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:40:48.521281 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m11:40:48.525996 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.528994 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:40:48.529216 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m11:40:48.529734 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.530540 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 11:40:48.409858 => 11:40:48.530444
[0m11:40:48.530749 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m11:40:48.567100 [info ] [Thread-1  ]: 1 of 30 OK created sql table model dim.dim_date ................................ [[32mOK[0m in 0.24s]
[0m11:40:48.567523 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m11:40:48.567775 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m11:40:48.568053 [info ] [Thread-1  ]: 2 of 30 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m11:40:48.568409 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.stg_salesforce__account)
[0m11:40:48.568607 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m11:40:48.571322 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:40:48.571830 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 11:40:48.568739 => 11:40:48.571722
[0m11:40:48.572026 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m11:40:48.583251 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:40:48.584069 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:40:48.584388 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m11:40:48.584593 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.591749 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.592122 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:40:48.592392 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m11:40:48.593245 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.595578 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:40:48.595801 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m11:40:48.596168 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.598114 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:40:48.598432 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m11:40:48.598832 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.599806 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m11:40:48.600034 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:40:48.600223 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m11:40:48.601213 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.603110 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:40:48.603365 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m11:40:48.603755 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.604592 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 11:40:48.572156 => 11:40:48.604493
[0m11:40:48.604815 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m11:40:48.619073 [info ] [Thread-1  ]: 2 of 30 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.05s]
[0m11:40:48.619500 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m11:40:48.619759 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:40:48.620110 [info ] [Thread-1  ]: 3 of 30 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m11:40:48.620501 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m11:40:48.620699 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:40:48.623602 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:40:48.624103 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 11:40:48.620836 => 11:40:48.624001
[0m11:40:48.624299 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:40:48.627207 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:40:48.627716 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:40:48.627909 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m11:40:48.628088 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.634900 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.635173 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:40:48.635401 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m11:40:48.636165 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.638335 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:40:48.638545 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m11:40:48.638845 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.640761 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:40:48.641084 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m11:40:48.641479 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.642498 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m11:40:48.642702 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:40:48.642884 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m11:40:48.643517 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.645340 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:40:48.645542 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m11:40:48.645900 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.646661 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 11:40:48.624428 => 11:40:48.646567
[0m11:40:48.646864 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m11:40:48.661081 [info ] [Thread-1  ]: 3 of 30 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.04s]
[0m11:40:48.661508 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:40:48.661764 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m11:40:48.662072 [info ] [Thread-1  ]: 4 of 30 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m11:40:48.662604 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m11:40:48.662826 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m11:40:48.664970 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:40:48.665441 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 11:40:48.662972 => 11:40:48.665335
[0m11:40:48.665648 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m11:40:48.669632 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:40:48.670164 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:40:48.670394 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m11:40:48.670588 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.677609 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.677914 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:40:48.678153 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m11:40:48.678879 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.681249 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:40:48.681487 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m11:40:48.681835 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.683826 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:40:48.684090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m11:40:48.684499 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.685552 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m11:40:48.685795 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:40:48.685990 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m11:40:48.686573 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.688295 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:40:48.688523 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m11:40:48.689017 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.689989 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 11:40:48.665777 => 11:40:48.689884
[0m11:40:48.690228 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m11:40:48.710673 [info ] [Thread-1  ]: 4 of 30 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.05s]
[0m11:40:48.711145 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m11:40:48.711406 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:40:48.711661 [info ] [Thread-1  ]: 5 of 30 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m11:40:48.712366 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m11:40:48.712771 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:40:48.715166 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:40:48.715747 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 11:40:48.713013 => 11:40:48.715633
[0m11:40:48.715976 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:40:48.719813 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:40:48.720693 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:40:48.721009 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m11:40:48.721211 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.728077 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.728374 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:40:48.728580 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m11:40:48.729015 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.731225 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:40:48.731542 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m11:40:48.732110 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.734227 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:40:48.734460 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m11:40:48.734791 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.735692 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m11:40:48.735885 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:40:48.736083 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m11:40:48.736654 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.738282 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:40:48.738495 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m11:40:48.738866 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.739703 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 11:40:48.716120 => 11:40:48.739603
[0m11:40:48.739948 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m11:40:48.753495 [info ] [Thread-1  ]: 5 of 30 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.04s]
[0m11:40:48.753894 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:40:48.754127 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:40:48.754541 [info ] [Thread-1  ]: 6 of 30 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m11:40:48.755042 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m11:40:48.755255 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:40:48.757383 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:40:48.757842 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 11:40:48.755391 => 11:40:48.757740
[0m11:40:48.758039 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:40:48.760701 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:40:48.761167 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:40:48.761364 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m11:40:48.761544 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.768207 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.768479 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:40:48.768712 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m11:40:48.769591 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.772404 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:40:48.772635 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m11:40:48.773006 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.774856 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:40:48.775090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m11:40:48.775406 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.776250 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m11:40:48.776440 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:40:48.776770 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m11:40:48.777496 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.779313 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:40:48.779539 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m11:40:48.779923 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.780704 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 11:40:48.758170 => 11:40:48.780610
[0m11:40:48.780909 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m11:40:48.794788 [info ] [Thread-1  ]: 6 of 30 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.04s]
[0m11:40:48.795198 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:40:48.795429 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:40:48.795872 [info ] [Thread-1  ]: 7 of 30 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m11:40:48.796411 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m11:40:48.796629 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:40:48.798751 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:40:48.799206 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 11:40:48.796769 => 11:40:48.799101
[0m11:40:48.799408 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:40:48.802238 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:40:48.802751 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:40:48.802962 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m11:40:48.803141 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.810235 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.810565 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:40:48.810808 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    --

with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m11:40:48.811615 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.814001 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:40:48.814258 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m11:40:48.814602 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.817586 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:40:48.817895 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m11:40:48.818287 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.819252 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m11:40:48.819453 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:40:48.819631 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m11:40:48.820350 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.822015 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:40:48.822258 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m11:40:48.822673 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.823488 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 11:40:48.799545 => 11:40:48.823388
[0m11:40:48.823705 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m11:40:48.837524 [info ] [Thread-1  ]: 7 of 30 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.04s]
[0m11:40:48.837961 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:40:48.838224 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:40:48.838565 [info ] [Thread-1  ]: 8 of 30 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m11:40:48.838940 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m11:40:48.839134 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:40:48.841220 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:40:48.841726 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 11:40:48.839268 => 11:40:48.841629
[0m11:40:48.841928 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:40:48.844637 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:40:48.845133 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:40:48.845578 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m11:40:48.845821 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.852659 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.852955 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:40:48.853180 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m11:40:48.853899 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.856341 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:40:48.856648 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m11:40:48.857053 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.859661 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:40:48.859886 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m11:40:48.860199 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.861071 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m11:40:48.861273 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:40:48.861453 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m11:40:48.862103 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.863749 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:40:48.863963 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m11:40:48.864344 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.865099 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 11:40:48.842066 => 11:40:48.865004
[0m11:40:48.865311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m11:40:48.879377 [info ] [Thread-1  ]: 8 of 30 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.04s]
[0m11:40:48.879759 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:40:48.879988 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:40:48.880428 [info ] [Thread-1  ]: 9 of 30 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m11:40:48.880944 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m11:40:48.881169 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:40:48.883258 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:40:48.883706 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 11:40:48.881302 => 11:40:48.883599
[0m11:40:48.883906 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:40:48.886694 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:40:48.887175 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:40:48.887368 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m11:40:48.887546 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.894312 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.894596 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:40:48.894815 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m11:40:48.895360 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.897506 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:40:48.897729 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m11:40:48.898037 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.899727 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:40:48.899926 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m11:40:48.900212 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.901367 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m11:40:48.901626 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:40:48.901824 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m11:40:48.902467 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.904993 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:40:48.905208 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m11:40:48.905629 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.906426 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 11:40:48.884039 => 11:40:48.906326
[0m11:40:48.906636 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m11:40:48.920715 [info ] [Thread-1  ]: 9 of 30 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.04s]
[0m11:40:48.921123 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:40:48.921367 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:40:48.921627 [info ] [Thread-1  ]: 10 of 30 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m11:40:48.921990 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m11:40:48.922193 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:40:48.924161 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:40:48.924597 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 11:40:48.922325 => 11:40:48.924496
[0m11:40:48.924795 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:40:48.927573 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:40:48.928093 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:40:48.928307 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m11:40:48.928505 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.935585 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.935887 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:40:48.936105 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m11:40:48.936586 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.938826 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:40:48.939061 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m11:40:48.939391 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.941144 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:40:48.941362 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m11:40:48.941647 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.942570 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m11:40:48.942767 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:40:48.942953 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m11:40:48.943479 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.945707 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:40:48.945916 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m11:40:48.946279 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.947024 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 11:40:48.924923 => 11:40:48.946930
[0m11:40:48.947233 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m11:40:48.961443 [info ] [Thread-1  ]: 10 of 30 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.04s]
[0m11:40:48.961879 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:40:48.962141 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:40:48.962457 [info ] [Thread-1  ]: 11 of 30 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m11:40:48.962831 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m11:40:48.963026 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:40:48.965013 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:40:48.965706 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 11:40:48.963158 => 11:40:48.965607
[0m11:40:48.965912 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:40:48.968444 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:40:48.968814 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:40:48.969189 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m11:40:48.969421 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:48.976271 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.976583 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:40:48.976800 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m11:40:48.977396 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.979671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:40:48.979884 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m11:40:48.980199 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.981943 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:40:48.982242 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m11:40:48.982558 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.983412 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m11:40:48.983681 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:40:48.983856 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m11:40:48.984350 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.985823 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:40:48.986017 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m11:40:48.986362 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:48.987068 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 11:40:48.966064 => 11:40:48.986974
[0m11:40:48.987267 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m11:40:49.005352 [info ] [Thread-1  ]: 11 of 30 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.04s]
[0m11:40:49.005765 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:40:49.006019 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:40:49.006340 [info ] [Thread-1  ]: 12 of 30 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m11:40:49.006703 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m11:40:49.006890 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:40:49.009510 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:40:49.009956 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 11:40:49.007020 => 11:40:49.009859
[0m11:40:49.010173 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:40:49.013083 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:40:49.013584 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:40:49.013784 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m11:40:49.013963 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.020710 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.020999 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:40:49.021215 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m11:40:49.021684 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.024008 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:40:49.024280 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m11:40:49.024644 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.026599 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:40:49.026800 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m11:40:49.027096 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.027951 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m11:40:49.028140 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:40:49.028315 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m11:40:49.028887 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.030827 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:40:49.031050 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m11:40:49.031465 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.032294 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 11:40:49.010318 => 11:40:49.032199
[0m11:40:49.032532 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m11:40:49.046585 [info ] [Thread-1  ]: 12 of 30 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.04s]
[0m11:40:49.047001 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:40:49.047248 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:40:49.047565 [info ] [Thread-1  ]: 13 of 30 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m11:40:49.047930 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m11:40:49.048126 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:40:49.050121 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:40:49.050569 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 11:40:49.048266 => 11:40:49.050469
[0m11:40:49.050766 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:40:49.054555 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:40:49.055055 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:40:49.055256 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m11:40:49.055436 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.062318 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.062570 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:40:49.062782 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m11:40:49.063396 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.065778 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:40:49.066006 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m11:40:49.066394 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.068157 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:40:49.068377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m11:40:49.068677 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.069520 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m11:40:49.069707 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:40:49.069881 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m11:40:49.070406 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.072078 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:40:49.072287 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m11:40:49.072636 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.073327 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 11:40:49.050893 => 11:40:49.073235
[0m11:40:49.073531 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m11:40:49.087791 [info ] [Thread-1  ]: 13 of 30 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.04s]
[0m11:40:49.088196 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:40:49.088435 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m11:40:49.088734 [info ] [Thread-1  ]: 14 of 30 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m11:40:49.089172 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m11:40:49.089377 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m11:40:49.091486 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:40:49.091959 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 11:40:49.089513 => 11:40:49.091852
[0m11:40:49.092162 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m11:40:49.095630 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:40:49.096154 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:40:49.096379 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m11:40:49.096562 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.103187 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.103466 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:40:49.103715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m11:40:49.104684 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.106881 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:40:49.107090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m11:40:49.107387 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.109149 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:40:49.109454 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m11:40:49.109853 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.110863 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m11:40:49.111080 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:40:49.111264 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m11:40:49.111863 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.113674 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:40:49.113874 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m11:40:49.114235 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.115060 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 11:40:49.092294 => 11:40:49.114962
[0m11:40:49.115270 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m11:40:49.129506 [info ] [Thread-1  ]: 14 of 30 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.04s]
[0m11:40:49.129913 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m11:40:49.130159 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:40:49.130436 [info ] [Thread-1  ]: 15 of 30 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m11:40:49.130804 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m11:40:49.130993 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:40:49.133014 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:40:49.134875 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 11:40:49.131126 => 11:40:49.134766
[0m11:40:49.135096 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:40:49.137845 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:40:49.138356 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:40:49.138569 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m11:40:49.138758 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.145464 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.145751 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:40:49.145964 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m11:40:49.146481 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.149781 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:40:49.150064 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m11:40:49.150464 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.152457 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:40:49.152731 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m11:40:49.153052 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.153995 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m11:40:49.154200 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:40:49.154379 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m11:40:49.154947 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.156696 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:40:49.156915 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m11:40:49.157336 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.158212 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 11:40:49.135225 => 11:40:49.158111
[0m11:40:49.158431 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m11:40:49.172425 [info ] [Thread-1  ]: 15 of 30 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.04s]
[0m11:40:49.172855 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:40:49.173111 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m11:40:49.173449 [info ] [Thread-1  ]: 16 of 30 START sql table model dim.dim_account ................................. [RUN]
[0m11:40:49.173936 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m11:40:49.174171 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m11:40:49.176485 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m11:40:49.176937 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 11:40:49.174320 => 11:40:49.176835
[0m11:40:49.177129 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m11:40:49.179823 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m11:40:49.180331 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:40:49.180522 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m11:40:49.180692 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.187532 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.187837 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:40:49.188131 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

select
    account_id,
    masterrecordid as master_record_id,
    name as account_name,
    type as account_type,
    parentid as parent_account_id,
    billingstreet as billing_street,
    billingcity as billing_city,
    billingstate as billing_state,
    billingpostalcode as billing_postal_code,
    billingcountry as billing_country,
    billinglatitude as billing_latitude,
    billinglongitude as billing_longitude,
    billinggeocodeaccuracy as billing_geocode_accuracy,
    shippingstreet as shipping_street,
    shippingcity as shipping_city,
    shippingstate as shipping_state,
    shippingpostalcode as shipping_postal_code,
    shippingcountry as shipping_country,
    shippinglatitude as shipping_latitude,
    shippinglongitude as shipping_longitude,
    shippinggeocodeaccuracy as shipping_geocode_accuracy,
    phone as account_phone,
    fax as account_fax,
    accountnumber as account_number,
    website as account_website,
    sic as sic_code,
    industry as account_industry,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownership as account_ownership,
    tickersymbol as ticker_symbol,
    description as account_description,
    rating as account_rating,
    site as account_site,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    jigsaw as jigsaw_id,
    jigsawcompanyid as jigsaw_company_id,
    cleanstatus as clean_status,
    accountsource as account_source,
    dunsnumber as duns_number,
    tradestyle as trade_style,
    naicscode as naics_code,
    naicsdesc as naics_description,
    yearstarted as year_started,
    sicdesc as sic_description,
    dandbcompanyid as dandb_company_id,
    operatinghoursid as operating_hours_id,
    customerpriority__c as customer_priority,
    sla__c as sla,
    active__c as is_active,
    numberoflocations__c as number_of_locations,
    upsellopportunity__c as upsell_opportunity,
    slaserialnumber__c as sla_serial_number,
    slaexpirationdate__c as sla_expiration_date
from "dbt"."staging"."stg_salesforce__account"
where isdeleted = false  -- Exclude deleted accounts
order by account_name
    );
  
  
[0m11:40:49.192812 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.195835 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:40:49.196086 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account" rename to "dim_account__dbt_backup"
[0m11:40:49.196505 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.226075 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:40:49.226391 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m11:40:49.226842 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.227902 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m11:40:49.228106 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:40:49.228282 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m11:40:49.229690 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.231751 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:40:49.232041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m11:40:49.232713 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.233501 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 11:40:49.177263 => 11:40:49.233407
[0m11:40:49.233706 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m11:40:49.256237 [info ] [Thread-1  ]: 16 of 30 OK created sql table model dim.dim_account ............................ [[32mOK[0m in 0.08s]
[0m11:40:49.256659 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m11:40:49.256909 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m11:40:49.257252 [info ] [Thread-1  ]: 17 of 30 START sql table model dim.dim_campaign ................................ [RUN]
[0m11:40:49.257662 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m11:40:49.257872 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m11:40:49.260111 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m11:40:49.260566 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 11:40:49.258013 => 11:40:49.260470
[0m11:40:49.260779 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m11:40:49.263293 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m11:40:49.263753 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:40:49.263952 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m11:40:49.264125 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.271112 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.271410 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:40:49.271681 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

select
    /* IDs */
    campaign_id,
    parentid as parent_campaign_id,
    ownerid as owner_id,
    createdbyid as created_by_id,
    lastmodifiedbyid as last_modified_by_id,
    campaignmemberrecordtypeid as campaign_member_record_type_id,

    /* Dates */
    startdate as start_date,
    enddate as end_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,

    /* Dimensions */
    name as campaign_name,
    type as campaign_type,
    status,
    description,
    isactive as is_active,

    /* Metrics */
    expectedrevenue as expected_revenue,
    budgetedcost as budgeted_cost,
    actualcost as actual_cost,
    expectedresponse as expected_response,
    numbersent as number_sent,
    numberofleads as number_of_leads,
    numberofconvertedleads as number_of_converted_leads,
    numberofcontacts as number_of_contacts,
    numberofresponses as number_of_responses,
    numberofopportunities as number_of_opportunities,
    numberofwonopportunities as number_of_won_opportunities,
    amountallopportunities as amount_all_opportunities,
    amountwonopportunities as amount_won_opportunities,
    hierarchynumberofleads as hierarchy_number_of_leads,
    hierarchynumberofconvertedleads as hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts as hierarchy_number_of_contacts,
    hierarchynumberofresponses as hierarchy_number_of_responses,
    hierarchynumberofopportunities as hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities as hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities as hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities as hierarchy_amount_won_opportunities,
    hierarchynumbersent as hierarchy_number_sent,
    hierarchyexpectedrevenue as hierarchy_expected_revenue,
    hierarchybudgetedcost as hierarchy_budgeted_cost,
    hierarchyactualcost as hierarchy_actual_cost

from "dbt"."staging"."stg_salesforce__campaign"
where isdeleted = false
    );
  
  
[0m11:40:49.274857 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.277124 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:40:49.277352 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign" rename to "dim_campaign__dbt_backup"
[0m11:40:49.277880 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.280745 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:40:49.280979 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m11:40:49.281354 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.282302 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m11:40:49.282544 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:40:49.282732 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m11:40:49.283791 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.285706 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:40:49.285938 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."dim"."dim_campaign__dbt_backup" cascade
[0m11:40:49.286481 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.287349 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 11:40:49.260916 => 11:40:49.287256
[0m11:40:49.287555 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m11:40:49.305027 [info ] [Thread-1  ]: 17 of 30 OK created sql table model dim.dim_campaign ........................... [[32mOK[0m in 0.05s]
[0m11:40:49.305571 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m11:40:49.305835 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m11:40:49.306132 [info ] [Thread-1  ]: 18 of 30 START sql table model dim.dim_case_status ............................. [RUN]
[0m11:40:49.306540 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m11:40:49.306740 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m11:40:49.308878 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m11:40:49.309372 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 11:40:49.306874 => 11:40:49.309272
[0m11:40:49.309568 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m11:40:49.312174 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m11:40:49.312662 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:40:49.313228 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m11:40:49.313489 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.320557 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.320830 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:40:49.321030 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."dim"."dim_case_status__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct status as status_name, status as status_description 
        from "dbt"."staging"."stg_salesforce__case_history_2"
    )

select
    row_number() over (order by status_name) as status_id, 
    status_description
from source
    );
  
  
[0m11:40:49.322447 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.324725 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:40:49.324977 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m11:40:49.325318 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.327229 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:40:49.327534 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m11:40:49.327944 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.328987 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m11:40:49.329185 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:40:49.329361 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m11:40:49.329923 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.332660 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:40:49.332902 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."dim"."dim_case_status__dbt_backup" cascade
[0m11:40:49.333307 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.334099 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 11:40:49.309699 => 11:40:49.334001
[0m11:40:49.334310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m11:40:49.349162 [info ] [Thread-1  ]: 18 of 30 OK created sql table model dim.dim_case_status ........................ [[32mOK[0m in 0.04s]
[0m11:40:49.349537 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m11:40:49.349761 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m11:40:49.350163 [info ] [Thread-1  ]: 19 of 30 START sql table model dim.dim_contact ................................. [RUN]
[0m11:40:49.350656 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m11:40:49.350860 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m11:40:49.353211 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m11:40:49.353725 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 11:40:49.350993 => 11:40:49.353617
[0m11:40:49.353931 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m11:40:49.356616 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m11:40:49.357097 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:40:49.357300 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m11:40:49.357475 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.364169 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.364437 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:40:49.364689 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

select
    /* IDs */
    contact_id,
    masterrecordid as master_record_id,
    accountid as account_id,
    reportstoid as reports_to_id,
    ownerid as owner_id,
    jigsawcontactid as jigsaw_contact_id,
    individualid as individual_id,

    /* Dates */
    birthdate as birth_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    lastcurequestdate as last_cu_request_date,
    lastcuupdatedate as last_cu_update_date,
    emailbounceddate as email_bounced_date,

    /* Dimensions */
    salutation,
    firstname as first_name,
    lastname as last_name,
    otherstreet as other_street,
    othercity as other_city,
    otherstate as other_state,
    otherpostalcode as other_postal_code,
    othercountry as other_country,
    otherlatitude as other_latitude,
    otherlongitude as other_longitude,
    othergeocodeaccuracy as other_geocode_accuracy,
    mailingstreet as mailing_street,
    mailingcity as mailing_city,
    mailingstate as mailing_state,
    mailingpostalcode as mailing_postal_code,
    mailingcountry as mailing_country,
    mailinglatitude as mailing_latitude,
    mailinglongitude as mailing_longitude,
    mailinggeocodeaccuracy as mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone as mobile_phone,
    homephone as home_phone,
    otherphone as other_phone,
    assistantphone as assistant_phone,
    email,
    title,
    department,
    assistantname as assistant_name,
    leadsource as lead_source,
    description,
    pronouns,
    genderidentity as gender_identity,
    cleanstatus as clean_status,
    emailbouncedreason as email_bounced_reason,
    level__c as level,
    languages__c as languages,

    /* Metrics */
    hasoptedoutofemail as has_opted_out_of_email,
    hasoptedoutoffax as has_opted_out_of_fax,
    donotcall as do_not_call

from "dbt"."staging"."stg_salesforce__contact"
where isdeleted = false
    );
  
  
[0m11:40:49.368403 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.370667 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:40:49.370911 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact" rename to "dim_contact__dbt_backup"
[0m11:40:49.371321 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.373118 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:40:49.373332 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m11:40:49.373669 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.374611 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m11:40:49.374814 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:40:49.374985 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m11:40:49.376217 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.378522 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:40:49.378727 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m11:40:49.379224 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.379912 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 11:40:49.354062 => 11:40:49.379822
[0m11:40:49.380113 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m11:40:49.408641 [info ] [Thread-1  ]: 19 of 30 OK created sql table model dim.dim_contact ............................ [[32mOK[0m in 0.06s]
[0m11:40:49.409009 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m11:40:49.409233 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m11:40:49.409624 [info ] [Thread-1  ]: 20 of 30 START sql table model dim.dim_lead .................................... [RUN]
[0m11:40:49.410145 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m11:40:49.410371 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m11:40:49.412778 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m11:40:49.413286 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 11:40:49.410508 => 11:40:49.413181
[0m11:40:49.413487 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m11:40:49.416240 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m11:40:49.416725 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:40:49.416944 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m11:40:49.417132 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.423786 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.424040 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:40:49.424297 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

select
    lead_id,
    masterrecordid as master_record_id,
    salutation as lead_salutation,
    firstname as first_name,
    lastname as last_name,
    title as lead_title,
    company as lead_company,
    street as lead_street,
    city as lead_city,
    state as lead_state,
    postalcode as lead_postal_code,
    country as lead_country,
    latitude as lead_latitude,
    longitude as lead_longitude,
    geocodeaccuracy as geocode_accuracy,
    phone as lead_phone,
    mobilephone as lead_mobile_phone,
    fax as lead_fax,
    email as lead_email,
    website as lead_website,
    description as lead_description,
    leadsource as lead_source,
    status as lead_status,
    industry as lead_industry,
    rating as lead_rating,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownerid as owner_id,
    hasoptedoutofemail as has_opted_out_of_email,
    isconverted as is_converted,
    converteddate as converted_date,
    convertedaccountid as converted_account_id,
    convertedcontactid as converted_contact_id,
    convertedopportunityid as converted_opportunity_id,
    isunreadbyowner as is_unread_by_owner,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    donotcall as do_not_call,
    hasoptedoutoffax as has_opted_out_of_fax,
    lasttransferdate as last_transfer_date,
    jigsaw as jigsaw_id,
    jigsawcontactid as jigsaw_contact_id,
    cleanstatus as clean_status,
    companydunsnumber as company_duns_number,
    dandbcompanyid as dandb_company_id,
    emailbouncedreason as email_bounced_reason,
    emailbounceddate as email_bounced_date,
    individualid as individual_id,
    pronouns as lead_pronouns,
    genderidentity as gender_identity,
    siccode__c as sic_code,
    productinterest__c as product_interest,
    primary__c as is_primary,
    currentgenerators__c as current_generators,
    numberoflocations__c as number_of_locations
from "dbt"."staging"."stg_salesforce__lead"
where isdeleted = false  -- Exclude deleted leads
order by last_name, first_name
    );
  
  
[0m11:40:49.428475 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.430679 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:40:49.430916 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead" rename to "dim_lead__dbt_backup"
[0m11:40:49.431322 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.433228 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:40:49.433470 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m11:40:49.433820 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.434830 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m11:40:49.435022 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:40:49.435196 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m11:40:49.436693 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.438288 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:40:49.438484 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m11:40:49.438991 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.439741 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 11:40:49.413619 => 11:40:49.439648
[0m11:40:49.439957 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m11:40:49.465334 [info ] [Thread-1  ]: 20 of 30 OK created sql table model dim.dim_lead ............................... [[32mOK[0m in 0.06s]
[0m11:40:49.465762 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m11:40:49.466013 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m11:40:49.466290 [info ] [Thread-1  ]: 21 of 30 START sql table model dim.dim_opportunity ............................. [RUN]
[0m11:40:49.466656 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m11:40:49.466846 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m11:40:49.470028 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m11:40:49.470512 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 11:40:49.466977 => 11:40:49.470408
[0m11:40:49.470730 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m11:40:49.473603 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m11:40:49.474370 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:40:49.474641 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m11:40:49.474820 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.481743 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.482094 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:40:49.482412 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m11:40:49.485843 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.488133 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:40:49.488377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m11:40:49.488802 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.490761 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:40:49.491037 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m11:40:49.491413 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.492527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m11:40:49.492737 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:40:49.492921 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m11:40:49.494024 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.495923 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:40:49.496208 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m11:40:49.496794 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.497625 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 11:40:49.470867 => 11:40:49.497526
[0m11:40:49.497853 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m11:40:49.518964 [info ] [Thread-1  ]: 21 of 30 OK created sql table model dim.dim_opportunity ........................ [[32mOK[0m in 0.05s]
[0m11:40:49.519420 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m11:40:49.519675 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:40:49.519990 [info ] [Thread-1  ]: 22 of 30 START sql table model dim.dim_opportunity_stage ....................... [RUN]
[0m11:40:49.520459 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m11:40:49.520670 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:40:49.523698 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:40:49.524219 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 11:40:49.520821 => 11:40:49.524108
[0m11:40:49.524542 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:40:49.527420 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:40:49.527965 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:40:49.528172 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m11:40:49.528353 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.535247 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.535531 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:40:49.535738 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct stagename as stage_name, stagesortorder as stage_sort_order
        from "dbt"."staging"."stg_salesforce__opportunity"
    )

select
    row_number() over (order by stage_sort_order) as stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
from source
    );
  
  
[0m11:40:49.538491 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.540737 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:40:49.540983 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m11:40:49.541378 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.543248 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:40:49.543489 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m11:40:49.543822 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.544795 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m11:40:49.545003 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:40:49.545193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m11:40:49.545851 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.547480 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:40:49.547685 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m11:40:49.548093 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.548878 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 11:40:49.524790 => 11:40:49.548780
[0m11:40:49.549086 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m11:40:49.564522 [info ] [Thread-1  ]: 22 of 30 OK created sql table model dim.dim_opportunity_stage .................. [[32mOK[0m in 0.04s]
[0m11:40:49.564929 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:40:49.565178 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m11:40:49.565458 [info ] [Thread-1  ]: 23 of 30 START sql table model dim.dim_pricebook ............................... [RUN]
[0m11:40:49.565832 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m11:40:49.566029 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m11:40:49.568204 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m11:40:49.568689 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 11:40:49.566169 => 11:40:49.568590
[0m11:40:49.568884 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m11:40:49.572204 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m11:40:49.572709 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:40:49.572923 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m11:40:49.573104 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.580005 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.580316 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:40:49.580541 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

select
    pricebook_entry_id,
    pricebook2id as pricebook_id,
    product2id as product_id,
    unitprice as unit_price,
    isactive as is_active,
    usestandardprice as use_standard_price,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__pricebook_entry"
where isdeleted = false  -- Exclude deleted entries
order by pricebook_entry_id
    );
  
  
[0m11:40:49.581974 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.584254 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:40:49.584521 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook" rename to "dim_pricebook__dbt_backup"
[0m11:40:49.584894 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.586745 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:40:49.586948 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m11:40:49.587245 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.588181 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m11:40:49.588369 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:40:49.588540 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m11:40:49.589335 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.591208 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:40:49.591434 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m11:40:49.591888 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.592767 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 11:40:49.569009 => 11:40:49.592666
[0m11:40:49.592990 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m11:40:49.609526 [info ] [Thread-1  ]: 23 of 30 OK created sql table model dim.dim_pricebook .......................... [[32mOK[0m in 0.04s]
[0m11:40:49.609966 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m11:40:49.610213 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m11:40:49.610484 [info ] [Thread-1  ]: 24 of 30 START sql table model dim.dim_product ................................. [RUN]
[0m11:40:49.610856 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m11:40:49.611050 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m11:40:49.613315 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m11:40:49.613946 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 11:40:49.611182 => 11:40:49.613818
[0m11:40:49.614180 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m11:40:49.616904 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m11:40:49.617430 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:40:49.617641 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m11:40:49.617823 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.625091 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.625408 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:40:49.625634 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    productcode as product_code,
    description as product_description,
    isactive as is_active,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    family as product_family,
    externaldatasourceid as external_datasource_id,
    externalid as external_id,
    displayurl as display_url,
    quantityunitofmeasure as quantity_unit_of_measure,
    stockkeepingunit as stock_keeping_unit,
    type as product_type,
    productclass as product_class,
    sourceproductid as source_product_id,
    sellerid as seller_id
from "dbt"."staging"."stg_salesforce__product_2"
where isdeleted = false
-- isactive =1 ?
order by product_name
    );
  
  
[0m11:40:49.627576 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.630677 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:40:49.630915 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product" rename to "dim_product__dbt_backup"
[0m11:40:49.631328 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.633311 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:40:49.633537 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m11:40:49.633846 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.634947 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m11:40:49.635145 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:40:49.635324 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m11:40:49.636193 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.638054 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:40:49.638271 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m11:40:49.638758 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.639547 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 11:40:49.614321 => 11:40:49.639458
[0m11:40:49.639754 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m11:40:49.657535 [info ] [Thread-1  ]: 24 of 30 OK created sql table model dim.dim_product ............................ [[32mOK[0m in 0.05s]
[0m11:40:49.657966 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m11:40:49.658213 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m11:40:49.658483 [info ] [Thread-1  ]: 25 of 30 START sql table model dim.dim_record_type ............................. [RUN]
[0m11:40:49.658842 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m11:40:49.659056 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m11:40:49.661259 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m11:40:49.661725 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 11:40:49.659207 => 11:40:49.661626
[0m11:40:49.661920 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m11:40:49.664551 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m11:40:49.665157 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:40:49.665432 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m11:40:49.665619 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.672366 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.672637 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:40:49.672849 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

select
    record_type_id,
    name as record_type_name,
    modulenamespace as module_namespace,
    description as record_type_description,
    businessprocessid as business_process_id,
    sobjecttype as sobject_type,
    isactive as is_active,
    createdbyid as created_by_id,
    createddate as created_date,
    lastmodifiedbyid as last_modified_by_id,
    lastmodifieddate as last_modified_date,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__record_type"
where isdeleted = false
order by record_type_name
    );
  
  
[0m11:40:49.674802 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.677862 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:40:49.678142 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m11:40:49.678534 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.680295 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:40:49.680492 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m11:40:49.680787 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.681720 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m11:40:49.681910 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:40:49.682161 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m11:40:49.682961 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.684681 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:40:49.684896 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m11:40:49.685349 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.686275 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 11:40:49.662053 => 11:40:49.686177
[0m11:40:49.686497 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m11:40:49.702723 [info ] [Thread-1  ]: 25 of 30 OK created sql table model dim.dim_record_type ........................ [[32mOK[0m in 0.04s]
[0m11:40:49.703098 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m11:40:49.703336 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m11:40:49.703692 [info ] [Thread-1  ]: 26 of 30 START sql table model dim.dim_solution ................................ [RUN]
[0m11:40:49.704171 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m11:40:49.704398 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m11:40:49.706639 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m11:40:49.707109 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 11:40:49.704534 => 11:40:49.707006
[0m11:40:49.707303 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m11:40:49.710074 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m11:40:49.710577 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:40:49.710772 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m11:40:49.710948 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.717607 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.717861 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:40:49.718067 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solutionnumber as solution_number,
    solutionname as solution_name,
    ispublished as is_published,
    ispublishedinpublickb as is_published_in_public_kb,
    status as solution_status,
    isreviewed as is_reviewed,
    solutionnote as solution_note,
    caseid as case_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    timesused as times_used,
    ishtml as is_html
from "dbt"."staging"."stg_salesforce__solution"
where isdeleted = false
order by solution_name
    );
  
  
[0m11:40:49.719788 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.722003 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:40:49.722308 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m11:40:49.722786 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.725793 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:40:49.726076 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m11:40:49.726477 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.727545 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m11:40:49.727742 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:40:49.727921 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m11:40:49.728750 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.730382 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:40:49.730585 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m11:40:49.731011 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.731777 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 11:40:49.707430 => 11:40:49.731682
[0m11:40:49.732003 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m11:40:49.749832 [info ] [Thread-1  ]: 26 of 30 OK created sql table model dim.dim_solution ........................... [[32mOK[0m in 0.05s]
[0m11:40:49.750271 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m11:40:49.750514 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m11:40:49.750777 [info ] [Thread-1  ]: 27 of 30 START sql table model dim.dim_user .................................... [RUN]
[0m11:40:49.751138 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m11:40:49.751331 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m11:40:49.753762 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m11:40:49.754603 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 11:40:49.751468 => 11:40:49.754461
[0m11:40:49.754845 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m11:40:49.757503 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m11:40:49.757938 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:40:49.758127 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m11:40:49.758299 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.765349 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.765636 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:40:49.765861 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m11:40:49.770050 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.772411 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:40:49.772697 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user" rename to "dim_user__dbt_backup"
[0m11:40:49.773109 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.775072 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:40:49.775290 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m11:40:49.775605 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.777361 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m11:40:49.777561 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:40:49.777732 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m11:40:49.778732 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.780672 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:40:49.780895 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m11:40:49.781413 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.782237 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 11:40:49.754975 => 11:40:49.782111
[0m11:40:49.782475 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m11:40:49.801381 [info ] [Thread-1  ]: 27 of 30 OK created sql table model dim.dim_user ............................... [[32mOK[0m in 0.05s]
[0m11:40:49.801819 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m11:40:49.802066 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m11:40:49.802425 [info ] [Thread-1  ]: 28 of 30 START sql table model fact.fact_campaign_performance .................. [RUN]
[0m11:40:49.802808 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_campaign_performance)
[0m11:40:49.803003 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m11:40:49.805347 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:40:49.806185 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 11:40:49.803134 => 11:40:49.806078
[0m11:40:49.806394 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m11:40:49.809005 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:40:49.809522 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:40:49.809723 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: BEGIN
[0m11:40:49.809913 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.816493 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.816782 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:40:49.817001 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */

  
    
    

    create  table
      "dbt"."fact"."fact_campaign_performance__dbt_tmp"
  
    as (
      

WITH campaign_performance AS (
    SELECT
        c.campaign_id,
        c.created_at,
        c.last_modified_at,
        c.start_date,
        c.end_date,
        c.owner_id as user_id,
        o.opportunity_id,
        c.expected_revenue,
        c.budgeted_cost,
        c.actual_cost,
        c.number_sent,
        c.number_of_leads,
        c.number_of_converted_leads,
        c.number_of_contacts
    FROM "dbt"."dim"."dim_campaign" c
    LEFT JOIN "dbt"."dim"."dim_opportunity" o ON c.campaign_id = o.campaign_id
)

SELECT
    cp.campaign_id,
    cp.user_id,
    cp.opportunity_id,
    cp.start_date,
    cp.end_date,
    cp.expected_revenue,
    cp.budgeted_cost,
    cp.actual_cost,
    cp.number_sent,
    cp.number_of_leads,
    cp.number_of_converted_leads,
    cp.number_of_contacts
FROM campaign_performance cp


--need to be improved - No of leads closed based a particular campaign
    );
  
  
[0m11:40:49.818915 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.821289 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:40:49.821544 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
alter table "dbt"."fact"."fact_campaign_performance" rename to "fact_campaign_performance__dbt_backup"
[0m11:40:49.821890 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.823722 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:40:49.823929 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
alter table "dbt"."fact"."fact_campaign_performance__dbt_tmp" rename to "fact_campaign_performance"
[0m11:40:49.824232 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.825163 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: COMMIT
[0m11:40:49.825365 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:40:49.825536 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: COMMIT
[0m11:40:49.826269 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.828943 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:40:49.829177 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
drop table if exists "dbt"."fact"."fact_campaign_performance__dbt_backup" cascade
[0m11:40:49.829646 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.830425 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 11:40:49.806529 => 11:40:49.830333
[0m11:40:49.830631 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: Close
[0m11:40:49.846192 [info ] [Thread-1  ]: 28 of 30 OK created sql table model fact.fact_campaign_performance ............. [[32mOK[0m in 0.04s]
[0m11:40:49.846601 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m11:40:49.846838 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_lead_conversion
[0m11:40:49.847101 [info ] [Thread-1  ]: 29 of 30 START sql view model fact.fact_lead_conversion ........................ [RUN]
[0m11:40:49.847456 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_lead_conversion)
[0m11:40:49.847643 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_lead_conversion
[0m11:40:49.850033 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:40:49.850537 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (compile): 11:40:49.847773 => 11:40:49.850436
[0m11:40:49.850736 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_lead_conversion
[0m11:40:49.853424 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:40:49.853974 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:40:49.854265 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: BEGIN
[0m11:40:49.854449 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.861875 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.862229 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:40:49.862571 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */

  
  create view "dbt"."fact"."fact_lead_conversion__dbt_tmp" as (
    -- fact_lead_conversion.sql

with lead_data as (
    select 
        lead_id,
        lead_salutation,
        first_name,
        last_name,
        lead_company,
        lead_source,
        lead_status,
        is_converted,
        converted_date,
        converted_account_id,
        converted_contact_id,
        converted_opportunity_id,
        owner_id,  -- Assuming this is the user ID
        created_date as lead_created_date,
        last_modified_date as lead_last_modified_date
    from 
        "dbt"."dim"."dim_lead"
    where 
        is_converted = true
),

account_data as (
    select 
        account_id,
        account_name
    from 
        "dbt"."dim"."dim_account"
),

opportunity_data as (
    select 
        opportunity_id,
        opportunity_name,
        opportunity_amount
    from 
        "dbt"."dim"."dim_opportunity"
),

user_data as (
    select 
        user_id,
        user_name,
        user_email
    from 
        "dbt"."dim"."dim_user"
)

select 
    l.lead_id,
    l.first_name,
    l.last_name,
    l.lead_company,
    l.lead_source,
    l.lead_status,
    l.converted_date,
    a.account_name as converted_account_name,
    o.opportunity_name as converted_opportunity_name,
    o.opportunity_amount as converted_opportunity_amount,
    u.user_name as lead_owner_name,
    u.user_email as lead_owner_email,
    l.lead_created_date,
    l.lead_last_modified_date
from 
    lead_data l
left join 
    account_data a on l.converted_account_id = a.account_id
left join 
    opportunity_data o on l.converted_opportunity_id = o.opportunity_id
left join 
    user_data u on l.owner_id = u.user_id
  );

[0m11:40:49.863718 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (execute): 11:40:49.850866 => 11:40:49.863599
[0m11:40:49.863931 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: ROLLBACK
[0m11:40:49.867889 [debug] [Thread-1  ]: Failed to rollback 'model.elastic_dbt_interview.fact_lead_conversion'
[0m11:40:49.868147 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: Close
[0m11:40:49.870529 [debug] [Thread-1  ]: Runtime Error in model fact_lead_conversion (models/facts/fact_lead_conversion.sql)
  Binder Error: Referenced column "user_email" not found in FROM clause!
  Candidate bindings: "l.owner_id"
  LINE 51:         user_email
                   ^
[0m11:40:49.870942 [error] [Thread-1  ]: 29 of 30 ERROR creating sql view model fact.fact_lead_conversion ............... [[31mERROR[0m in 0.02s]
[0m11:40:49.871285 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_lead_conversion
[0m11:40:49.871526 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m11:40:49.871806 [info ] [Thread-1  ]: 30 of 30 START sql incremental model fact.fact_opportunity ..................... [RUN]
[0m11:40:49.872207 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_lead_conversion, now model.elastic_dbt_interview.fact_opportunity)
[0m11:40:49.872407 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m11:40:49.878226 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m11:40:49.878703 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 11:40:49.872540 => 11:40:49.878578
[0m11:40:49.878916 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m11:40:49.896502 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:40:49.896824 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240822114049893451"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        a.account_id,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m11:40:49.897047 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:49.906628 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.911014 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:40:49.911244 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m11:40:49.911502 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.911685 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:40:49.911875 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240822114049893451'
      
      
      order by ordinal_position

  
[0m11:40:49.939827 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.943508 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:40:49.943759 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m11:40:49.957409 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.962005 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:40:49.962269 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m11:40:49.977431 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.986215 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m11:40:49.986766 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:40:49.986985 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240822114049893451"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240822114049893451"
    )
  
[0m11:40:49.988588 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.989631 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m11:40:49.989861 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:40:49.990046 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m11:40:49.990341 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:40:49.990798 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 11:40:49.879049 => 11:40:49.990704
[0m11:40:49.991002 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m11:40:49.994219 [info ] [Thread-1  ]: 30 of 30 OK created sql incremental model fact.fact_opportunity ................ [[32mOK[0m in 0.12s]
[0m11:40:49.994574 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m11:40:49.995402 [debug] [MainThread]: Using duckdb connection "master"
[0m11:40:49.995578 [debug] [MainThread]: On master: BEGIN
[0m11:40:49.995721 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:40:50.002370 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:40:50.002756 [debug] [MainThread]: On master: COMMIT
[0m11:40:50.002925 [debug] [MainThread]: Using duckdb connection "master"
[0m11:40:50.003076 [debug] [MainThread]: On master: COMMIT
[0m11:40:50.003276 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:40:50.003433 [debug] [MainThread]: On master: Close
[0m11:40:50.005524 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:40:50.005712 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m11:40:50.005948 [info ] [MainThread]: 
[0m11:40:50.006131 [info ] [MainThread]: Finished running 14 table models, 15 view models, 1 incremental model in 0 hours 0 minutes and 1.91 seconds (1.91s).
[0m11:40:50.008104 [debug] [MainThread]: Command end result
[0m11:40:50.016262 [info ] [MainThread]: 
[0m11:40:50.016558 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:40:50.016734 [info ] [MainThread]: 
[0m11:40:50.016912 [error] [MainThread]:   Runtime Error in model fact_lead_conversion (models/facts/fact_lead_conversion.sql)
  Binder Error: Referenced column "user_email" not found in FROM clause!
  Candidate bindings: "l.owner_id"
  LINE 51:         user_email
                   ^
[0m11:40:50.017103 [info ] [MainThread]: 
[0m11:40:50.017281 [info ] [MainThread]: Done. PASS=29 WARN=0 ERROR=1 SKIP=0 TOTAL=30
[0m11:40:50.017634 [debug] [MainThread]: Command `dbt run` failed at 11:40:50.017585 after 2.09 seconds
[0m11:40:50.017841 [debug] [MainThread]: Flushing usage events


============================== 11:41:25.397753 | f7110c99-0115-47aa-9b87-94f5bfdaa248 ==============================
[0m11:41:25.397753 [info ] [MainThread]: Running with dbt=1.6.18
[0m11:41:25.400792 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'False'}
[0m11:41:25.401046 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m11:41:25.483970 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m11:41:25.504400 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m11:41:25.555845 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:41:25.556352 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_lead_conversion.sql
[0m11:41:25.584574 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m11:41:25.598541 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m11:41:25.600425 [info ] [MainThread]: 
[0m11:41:25.600947 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:41:25.602418 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m11:41:25.609431 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:41:25.609714 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:41:25.609892 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:25.625576 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.626603 [debug] [ThreadPool]: On list_dbt: Close
[0m11:41:25.630109 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:41:25.630428 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:41:25.630641 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:41:25.638371 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.639208 [debug] [ThreadPool]: On list_dbt: Close
[0m11:41:25.642644 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:41:25.642920 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:41:25.643101 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:41:25.649949 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.650805 [debug] [ThreadPool]: On list_dbt: Close
[0m11:41:25.652877 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m11:41:25.653284 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m11:41:25.656775 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:41:25.656978 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m11:41:25.657133 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:41:25.664538 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.664796 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:41:25.664960 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m11:41:25.665201 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.665716 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m11:41:25.665879 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:41:25.666022 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m11:41:25.666230 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.666386 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m11:41:25.668379 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now create_dbt_fact)
[0m11:41:25.668733 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m11:41:25.670250 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:41:25.670432 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m11:41:25.670578 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:41:25.677534 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.677834 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:41:25.678012 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m11:41:25.678406 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.678923 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m11:41:25.679123 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:41:25.679299 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m11:41:25.679529 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.679706 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m11:41:25.681989 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now create_dbt_dim)
[0m11:41:25.682491 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m11:41:25.683960 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:41:25.684140 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m11:41:25.684292 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:41:25.691278 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.691559 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:41:25.691731 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m11:41:25.691971 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.692508 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m11:41:25.692681 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:41:25.692835 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m11:41:25.693056 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.693223 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m11:41:25.697183 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_staging)
[0m11:41:25.700877 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m11:41:25.701102 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m11:41:25.701259 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:41:25.708465 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.708693 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m11:41:25.708876 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m11:41:25.727253 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.728228 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m11:41:25.729585 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m11:41:25.729775 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m11:41:25.732330 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m11:41:25.734778 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m11:41:25.734956 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m11:41:25.735104 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:41:25.741963 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.742214 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m11:41:25.742383 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m11:41:25.757758 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.761265 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m11:41:25.761515 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m11:41:25.761671 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m11:41:25.764150 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m11:41:25.765967 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m11:41:25.766143 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m11:41:25.766289 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:41:25.772798 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.773056 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m11:41:25.773223 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m11:41:25.788539 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.791964 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m11:41:25.792191 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m11:41:25.792343 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m11:41:25.795298 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_main)
[0m11:41:25.821004 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m11:41:25.821313 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m11:41:25.821468 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:41:25.828822 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.829064 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m11:41:25.829231 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m11:41:25.844611 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:41:25.848002 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m11:41:25.848236 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m11:41:25.848389 [debug] [ThreadPool]: On list_dbt_main: Close
[0m11:41:25.851737 [debug] [MainThread]: Using duckdb connection "master"
[0m11:41:25.851914 [debug] [MainThread]: On master: BEGIN
[0m11:41:25.852057 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:41:25.858518 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:41:25.858796 [debug] [MainThread]: On master: COMMIT
[0m11:41:25.858955 [debug] [MainThread]: Using duckdb connection "master"
[0m11:41:25.859107 [debug] [MainThread]: On master: COMMIT
[0m11:41:25.859303 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:41:25.859460 [debug] [MainThread]: On master: Close
[0m11:41:25.861021 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:41:25.861230 [info ] [MainThread]: 
[0m11:41:25.863582 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m11:41:25.863934 [info ] [Thread-1  ]: 1 of 30 START sql table model dim.dim_date ..................................... [RUN]
[0m11:41:25.864310 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_date)
[0m11:41:25.864503 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m11:41:25.890874 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:41:25.891202 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m11:41:25.891390 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:25.898204 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:25.898528 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:41:25.898808 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m11:41:25.899255 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:25.940101 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m11:41:25.941018 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 11:41:25.864638 => 11:41:25.940900
[0m11:41:25.941234 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m11:41:25.959426 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m11:41:25.959916 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:41:25.960315 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m11:41:26.008527 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.012691 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:41:26.012947 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m11:41:26.013417 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.015343 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:41:26.015577 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m11:41:26.016084 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.028233 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m11:41:26.028593 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:41:26.028811 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m11:41:26.031875 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.035080 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:41:26.035335 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m11:41:26.035978 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.036787 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 11:41:25.941371 => 11:41:26.036692
[0m11:41:26.036991 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m11:41:26.067145 [info ] [Thread-1  ]: 1 of 30 OK created sql table model dim.dim_date ................................ [[32mOK[0m in 0.20s]
[0m11:41:26.067654 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m11:41:26.067924 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m11:41:26.068229 [info ] [Thread-1  ]: 2 of 30 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m11:41:26.068619 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.stg_salesforce__account)
[0m11:41:26.068831 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m11:41:26.071146 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:41:26.071739 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 11:41:26.068973 => 11:41:26.071623
[0m11:41:26.071959 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m11:41:26.085017 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:41:26.085687 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:41:26.085890 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m11:41:26.086081 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.093150 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.093461 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:41:26.093703 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m11:41:26.094570 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.096965 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:41:26.097192 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m11:41:26.097514 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.100009 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:41:26.100218 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m11:41:26.100518 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.101385 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m11:41:26.101580 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:41:26.101753 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m11:41:26.102442 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.104067 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:41:26.104282 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m11:41:26.104689 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.105630 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 11:41:26.072101 => 11:41:26.105450
[0m11:41:26.105935 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m11:41:26.120358 [info ] [Thread-1  ]: 2 of 30 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.05s]
[0m11:41:26.120828 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m11:41:26.121083 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:41:26.121433 [info ] [Thread-1  ]: 3 of 30 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m11:41:26.121826 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m11:41:26.122026 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:41:26.124108 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:41:26.124623 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 11:41:26.122159 => 11:41:26.124513
[0m11:41:26.124822 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:41:26.127779 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:41:26.128338 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:41:26.128554 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m11:41:26.128740 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.135896 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.136247 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:41:26.136499 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m11:41:26.137301 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.139727 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:41:26.139969 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m11:41:26.140311 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.142087 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:41:26.142297 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m11:41:26.142593 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.143545 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m11:41:26.143734 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:41:26.143910 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m11:41:26.144492 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.146655 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:41:26.146908 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m11:41:26.147269 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.147963 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 11:41:26.124956 => 11:41:26.147869
[0m11:41:26.148164 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m11:41:26.163540 [info ] [Thread-1  ]: 3 of 30 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.04s]
[0m11:41:26.163955 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:41:26.164194 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m11:41:26.164523 [info ] [Thread-1  ]: 4 of 30 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m11:41:26.164875 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m11:41:26.165064 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m11:41:26.167056 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:41:26.167540 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 11:41:26.165194 => 11:41:26.167443
[0m11:41:26.167737 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m11:41:26.170185 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:41:26.170789 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:41:26.171053 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m11:41:26.171247 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.178056 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.178375 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:41:26.178603 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m11:41:26.179317 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.181544 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:41:26.181758 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m11:41:26.182076 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.183778 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:41:26.183991 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m11:41:26.184270 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.185350 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m11:41:26.185691 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:41:26.185882 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m11:41:26.186554 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.188316 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:41:26.188527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m11:41:26.188939 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.190483 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 11:41:26.167867 => 11:41:26.190378
[0m11:41:26.190699 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m11:41:26.206242 [info ] [Thread-1  ]: 4 of 30 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.04s]
[0m11:41:26.206653 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m11:41:26.206892 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:41:26.207164 [info ] [Thread-1  ]: 5 of 30 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m11:41:26.207519 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m11:41:26.207707 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:41:26.209850 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:41:26.210458 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 11:41:26.207842 => 11:41:26.210355
[0m11:41:26.210656 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:41:26.213323 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:41:26.213782 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:41:26.213982 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m11:41:26.214161 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.221070 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.221348 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:41:26.221552 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m11:41:26.221984 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.224161 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:41:26.224376 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m11:41:26.224686 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.226383 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:41:26.226603 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m11:41:26.226882 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.227723 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m11:41:26.227909 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:41:26.228079 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m11:41:26.228776 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.230721 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:41:26.230955 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m11:41:26.231355 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.232204 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 11:41:26.210796 => 11:41:26.232105
[0m11:41:26.232413 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m11:41:26.246128 [info ] [Thread-1  ]: 5 of 30 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.04s]
[0m11:41:26.246556 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:41:26.246804 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:41:26.247151 [info ] [Thread-1  ]: 6 of 30 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m11:41:26.247562 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m11:41:26.247772 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:41:26.250541 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:41:26.251050 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 11:41:26.247906 => 11:41:26.250949
[0m11:41:26.251251 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:41:26.254035 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:41:26.254531 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:41:26.254752 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m11:41:26.254941 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.261715 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.261992 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:41:26.262228 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m11:41:26.263026 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.265199 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:41:26.265407 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m11:41:26.265709 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.267575 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:41:26.267884 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m11:41:26.268281 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.269326 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m11:41:26.269536 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:41:26.269711 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m11:41:26.270355 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.272079 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:41:26.272276 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m11:41:26.272642 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.273354 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 11:41:26.251386 => 11:41:26.273262
[0m11:41:26.273553 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m11:41:26.287720 [info ] [Thread-1  ]: 6 of 30 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.04s]
[0m11:41:26.288177 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:41:26.288421 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:41:26.288741 [info ] [Thread-1  ]: 7 of 30 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m11:41:26.289119 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m11:41:26.289307 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:41:26.291409 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:41:26.291870 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 11:41:26.289435 => 11:41:26.291769
[0m11:41:26.292062 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:41:26.295847 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:41:26.296476 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:41:26.296710 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m11:41:26.296890 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.303831 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.304153 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:41:26.304414 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    --

with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m11:41:26.305360 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.307811 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:41:26.308071 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m11:41:26.308444 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.310218 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:41:26.310433 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m11:41:26.310743 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.311593 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m11:41:26.311784 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:41:26.311957 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m11:41:26.312608 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.314541 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:41:26.314780 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m11:41:26.315227 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.316073 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 11:41:26.292187 => 11:41:26.315965
[0m11:41:26.316307 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m11:41:26.330685 [info ] [Thread-1  ]: 7 of 30 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.04s]
[0m11:41:26.331148 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:41:26.331401 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:41:26.331753 [info ] [Thread-1  ]: 8 of 30 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m11:41:26.332140 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m11:41:26.332355 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:41:26.334437 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:41:26.334897 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 11:41:26.332494 => 11:41:26.334795
[0m11:41:26.335104 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:41:26.339067 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:41:26.339656 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:41:26.339880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m11:41:26.340061 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.347140 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.347477 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:41:26.347741 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m11:41:26.348558 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.350930 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:41:26.351191 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m11:41:26.351558 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.353786 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:41:26.354088 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m11:41:26.354495 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.355469 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m11:41:26.355677 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:41:26.355856 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m11:41:26.356542 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.358272 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:41:26.358494 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m11:41:26.358935 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.359831 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 11:41:26.335236 => 11:41:26.359728
[0m11:41:26.360048 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m11:41:26.373840 [info ] [Thread-1  ]: 8 of 30 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.04s]
[0m11:41:26.374273 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:41:26.374520 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:41:26.374865 [info ] [Thread-1  ]: 9 of 30 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m11:41:26.375242 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m11:41:26.375445 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:41:26.377431 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:41:26.377893 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 11:41:26.375583 => 11:41:26.377794
[0m11:41:26.378087 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:41:26.380800 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:41:26.381195 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:41:26.381407 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m11:41:26.381598 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.388599 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.388886 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:41:26.389121 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m11:41:26.389725 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.392941 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:41:26.393263 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m11:41:26.393654 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.395590 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:41:26.395800 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m11:41:26.396110 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.397009 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m11:41:26.397214 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:41:26.397391 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m11:41:26.397918 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.399653 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:41:26.399879 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m11:41:26.400263 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.401060 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 11:41:26.378214 => 11:41:26.400960
[0m11:41:26.401271 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m11:41:26.415325 [info ] [Thread-1  ]: 9 of 30 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.04s]
[0m11:41:26.415771 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:41:26.416013 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:41:26.416286 [info ] [Thread-1  ]: 10 of 30 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m11:41:26.416650 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m11:41:26.416847 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:41:26.418877 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:41:26.419320 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 11:41:26.416980 => 11:41:26.419219
[0m11:41:26.419512 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:41:26.422235 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:41:26.422752 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:41:26.422966 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m11:41:26.423159 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.429993 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.430275 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:41:26.430489 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m11:41:26.430960 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.433841 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:41:26.434064 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m11:41:26.434380 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.436076 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:41:26.436287 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m11:41:26.436825 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.437872 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m11:41:26.438097 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:41:26.438286 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m11:41:26.438895 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.440531 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:41:26.440738 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m11:41:26.441147 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.442020 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 11:41:26.419642 => 11:41:26.441920
[0m11:41:26.442263 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m11:41:26.456285 [info ] [Thread-1  ]: 10 of 30 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.04s]
[0m11:41:26.456678 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:41:26.456907 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:41:26.457329 [info ] [Thread-1  ]: 11 of 30 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m11:41:26.457846 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m11:41:26.458056 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:41:26.460155 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:41:26.460611 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 11:41:26.458193 => 11:41:26.460506
[0m11:41:26.460812 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:41:26.463575 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:41:26.464101 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:41:26.464313 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m11:41:26.464684 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.471296 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.471582 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:41:26.471794 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m11:41:26.472334 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.474552 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:41:26.474907 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m11:41:26.475315 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.478114 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:41:26.478351 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m11:41:26.478661 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.479640 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m11:41:26.479853 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:41:26.480036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m11:41:26.480589 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.482120 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:41:26.482319 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m11:41:26.482689 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.483409 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 11:41:26.460940 => 11:41:26.483314
[0m11:41:26.483610 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m11:41:26.497911 [info ] [Thread-1  ]: 11 of 30 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.04s]
[0m11:41:26.498351 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:41:26.498594 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:41:26.498911 [info ] [Thread-1  ]: 12 of 30 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m11:41:26.499368 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m11:41:26.499578 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:41:26.501587 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:41:26.502016 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 11:41:26.499716 => 11:41:26.501916
[0m11:41:26.502209 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:41:26.504833 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:41:26.505272 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:41:26.505458 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m11:41:26.505632 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.512252 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.512546 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:41:26.512752 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m11:41:26.513228 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.515484 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:41:26.515692 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m11:41:26.515992 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.518348 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:41:26.518555 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m11:41:26.518847 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.519681 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m11:41:26.519879 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:41:26.520252 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m11:41:26.520861 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.549794 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:41:26.550126 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m11:41:26.550619 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.551410 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 11:41:26.502335 => 11:41:26.551318
[0m11:41:26.551612 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m11:41:26.565327 [info ] [Thread-1  ]: 12 of 30 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.07s]
[0m11:41:26.565738 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:41:26.565991 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:41:26.566309 [info ] [Thread-1  ]: 13 of 30 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m11:41:26.566712 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m11:41:26.566922 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:41:26.568952 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:41:26.569398 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 11:41:26.567061 => 11:41:26.569301
[0m11:41:26.569590 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:41:26.572398 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:41:26.572898 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:41:26.573107 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m11:41:26.573288 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.579869 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.580137 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:41:26.580341 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m11:41:26.580854 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.583028 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:41:26.583242 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m11:41:26.583551 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.585262 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:41:26.585462 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m11:41:26.585746 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.586767 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m11:41:26.587073 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:41:26.587265 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m11:41:26.587898 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.590531 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:41:26.590774 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m11:41:26.591177 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.592008 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 11:41:26.569717 => 11:41:26.591909
[0m11:41:26.592219 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m11:41:26.605968 [info ] [Thread-1  ]: 13 of 30 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.04s]
[0m11:41:26.606384 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:41:26.606623 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m11:41:26.606943 [info ] [Thread-1  ]: 14 of 30 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m11:41:26.607299 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m11:41:26.607491 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m11:41:26.609665 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:41:26.610115 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 11:41:26.607624 => 11:41:26.610015
[0m11:41:26.610313 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m11:41:26.613034 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:41:26.613466 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:41:26.613662 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m11:41:26.613847 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.620533 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.620818 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:41:26.621070 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m11:41:26.622053 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.624263 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:41:26.624476 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m11:41:26.624780 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.626481 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:41:26.626677 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m11:41:26.627037 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.628270 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m11:41:26.628491 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:41:26.628679 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m11:41:26.629426 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.631146 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:41:26.631354 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m11:41:26.631725 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.632435 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 11:41:26.610448 => 11:41:26.632345
[0m11:41:26.632651 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m11:41:26.646451 [info ] [Thread-1  ]: 14 of 30 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.04s]
[0m11:41:26.646815 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m11:41:26.647035 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:41:26.647426 [info ] [Thread-1  ]: 15 of 30 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m11:41:26.647924 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m11:41:26.648139 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:41:26.651105 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:41:26.651574 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 11:41:26.648270 => 11:41:26.651461
[0m11:41:26.651785 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:41:26.654645 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:41:26.655124 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:41:26.655313 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m11:41:26.655487 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.662216 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.662490 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:41:26.662698 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m11:41:26.663217 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.665389 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:41:26.665605 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m11:41:26.665900 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.667768 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:41:26.668067 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m11:41:26.668462 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.669544 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m11:41:26.669771 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:41:26.669958 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m11:41:26.670533 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.672272 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:41:26.672472 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m11:41:26.672833 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.673565 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 11:41:26.651945 => 11:41:26.673464
[0m11:41:26.673767 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m11:41:26.687716 [info ] [Thread-1  ]: 15 of 30 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.04s]
[0m11:41:26.688128 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:41:26.688375 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m11:41:26.688721 [info ] [Thread-1  ]: 16 of 30 START sql table model dim.dim_account ................................. [RUN]
[0m11:41:26.689118 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m11:41:26.689314 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m11:41:26.691637 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m11:41:26.692064 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 11:41:26.689448 => 11:41:26.691972
[0m11:41:26.692256 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m11:41:26.695957 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m11:41:26.696466 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:41:26.696658 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m11:41:26.696828 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.703373 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.703668 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:41:26.703932 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

select
    account_id,
    masterrecordid as master_record_id,
    name as account_name,
    type as account_type,
    parentid as parent_account_id,
    billingstreet as billing_street,
    billingcity as billing_city,
    billingstate as billing_state,
    billingpostalcode as billing_postal_code,
    billingcountry as billing_country,
    billinglatitude as billing_latitude,
    billinglongitude as billing_longitude,
    billinggeocodeaccuracy as billing_geocode_accuracy,
    shippingstreet as shipping_street,
    shippingcity as shipping_city,
    shippingstate as shipping_state,
    shippingpostalcode as shipping_postal_code,
    shippingcountry as shipping_country,
    shippinglatitude as shipping_latitude,
    shippinglongitude as shipping_longitude,
    shippinggeocodeaccuracy as shipping_geocode_accuracy,
    phone as account_phone,
    fax as account_fax,
    accountnumber as account_number,
    website as account_website,
    sic as sic_code,
    industry as account_industry,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownership as account_ownership,
    tickersymbol as ticker_symbol,
    description as account_description,
    rating as account_rating,
    site as account_site,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    jigsaw as jigsaw_id,
    jigsawcompanyid as jigsaw_company_id,
    cleanstatus as clean_status,
    accountsource as account_source,
    dunsnumber as duns_number,
    tradestyle as trade_style,
    naicscode as naics_code,
    naicsdesc as naics_description,
    yearstarted as year_started,
    sicdesc as sic_description,
    dandbcompanyid as dandb_company_id,
    operatinghoursid as operating_hours_id,
    customerpriority__c as customer_priority,
    sla__c as sla,
    active__c as is_active,
    numberoflocations__c as number_of_locations,
    upsellopportunity__c as upsell_opportunity,
    slaserialnumber__c as sla_serial_number,
    slaexpirationdate__c as sla_expiration_date
from "dbt"."staging"."stg_salesforce__account"
where isdeleted = false  -- Exclude deleted accounts
order by account_name
    );
  
  
[0m11:41:26.708302 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.710607 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:41:26.710858 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account" rename to "dim_account__dbt_backup"
[0m11:41:26.711283 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.713025 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:41:26.713223 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m11:41:26.713547 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.714492 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m11:41:26.714678 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:41:26.714850 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m11:41:26.716355 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.717904 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:41:26.718102 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m11:41:26.718590 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.719473 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 11:41:26.692383 => 11:41:26.719367
[0m11:41:26.719726 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m11:41:26.741300 [info ] [Thread-1  ]: 16 of 30 OK created sql table model dim.dim_account ............................ [[32mOK[0m in 0.05s]
[0m11:41:26.741735 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m11:41:26.742002 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m11:41:26.742280 [info ] [Thread-1  ]: 17 of 30 START sql table model dim.dim_campaign ................................ [RUN]
[0m11:41:26.742637 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m11:41:26.742843 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m11:41:26.745102 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m11:41:26.745553 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 11:41:26.742977 => 11:41:26.745453
[0m11:41:26.745748 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m11:41:26.749336 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m11:41:26.749947 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:41:26.750151 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m11:41:26.750326 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.757813 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.758118 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:41:26.758384 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

select
    /* IDs */
    campaign_id,
    parentid as parent_campaign_id,
    ownerid as owner_id,
    createdbyid as created_by_id,
    lastmodifiedbyid as last_modified_by_id,
    campaignmemberrecordtypeid as campaign_member_record_type_id,

    /* Dates */
    startdate as start_date,
    enddate as end_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,

    /* Dimensions */
    name as campaign_name,
    type as campaign_type,
    status,
    description,
    isactive as is_active,

    /* Metrics */
    expectedrevenue as expected_revenue,
    budgetedcost as budgeted_cost,
    actualcost as actual_cost,
    expectedresponse as expected_response,
    numbersent as number_sent,
    numberofleads as number_of_leads,
    numberofconvertedleads as number_of_converted_leads,
    numberofcontacts as number_of_contacts,
    numberofresponses as number_of_responses,
    numberofopportunities as number_of_opportunities,
    numberofwonopportunities as number_of_won_opportunities,
    amountallopportunities as amount_all_opportunities,
    amountwonopportunities as amount_won_opportunities,
    hierarchynumberofleads as hierarchy_number_of_leads,
    hierarchynumberofconvertedleads as hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts as hierarchy_number_of_contacts,
    hierarchynumberofresponses as hierarchy_number_of_responses,
    hierarchynumberofopportunities as hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities as hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities as hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities as hierarchy_amount_won_opportunities,
    hierarchynumbersent as hierarchy_number_sent,
    hierarchyexpectedrevenue as hierarchy_expected_revenue,
    hierarchybudgetedcost as hierarchy_budgeted_cost,
    hierarchyactualcost as hierarchy_actual_cost

from "dbt"."staging"."stg_salesforce__campaign"
where isdeleted = false
    );
  
  
[0m11:41:26.761555 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.763843 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:41:26.764073 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign" rename to "dim_campaign__dbt_backup"
[0m11:41:26.764497 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.766224 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:41:26.766421 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m11:41:26.766740 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.767701 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m11:41:26.767890 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:41:26.768062 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m11:41:26.769177 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.770852 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:41:26.771048 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."dim"."dim_campaign__dbt_backup" cascade
[0m11:41:26.771533 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.772315 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 11:41:26.745878 => 11:41:26.772220
[0m11:41:26.772522 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m11:41:26.789072 [info ] [Thread-1  ]: 17 of 30 OK created sql table model dim.dim_campaign ........................... [[32mOK[0m in 0.05s]
[0m11:41:26.789498 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m11:41:26.789742 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m11:41:26.790006 [info ] [Thread-1  ]: 18 of 30 START sql table model dim.dim_case_status ............................. [RUN]
[0m11:41:26.790372 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m11:41:26.790564 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m11:41:26.792681 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m11:41:26.793147 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 11:41:26.790694 => 11:41:26.793048
[0m11:41:26.793342 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m11:41:26.796015 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m11:41:26.796500 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:41:26.796694 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m11:41:26.796866 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.803798 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.804121 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:41:26.804337 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."dim"."dim_case_status__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct status as status_name, status as status_description 
        from "dbt"."staging"."stg_salesforce__case_history_2"
    )

select
    row_number() over (order by status_name) as status_id, 
    status_description
from source
    );
  
  
[0m11:41:26.805873 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.808924 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:41:26.809205 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m11:41:26.809715 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.811681 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:41:26.811908 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m11:41:26.812210 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.813358 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m11:41:26.813640 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:41:26.813833 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m11:41:26.814437 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.816179 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:41:26.817337 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."dim"."dim_case_status__dbt_backup" cascade
[0m11:41:26.817767 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.818552 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 11:41:26.793480 => 11:41:26.818452
[0m11:41:26.818760 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m11:41:26.838674 [info ] [Thread-1  ]: 18 of 30 OK created sql table model dim.dim_case_status ........................ [[32mOK[0m in 0.05s]
[0m11:41:26.839143 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m11:41:26.839415 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m11:41:26.839717 [info ] [Thread-1  ]: 19 of 30 START sql table model dim.dim_contact ................................. [RUN]
[0m11:41:26.840126 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m11:41:26.840347 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m11:41:26.842672 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m11:41:26.843279 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 11:41:26.840490 => 11:41:26.843183
[0m11:41:26.843482 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m11:41:26.846188 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m11:41:26.846661 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:41:26.846860 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m11:41:26.847036 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.853542 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.853828 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:41:26.854078 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

select
    /* IDs */
    contact_id,
    masterrecordid as master_record_id,
    accountid as account_id,
    reportstoid as reports_to_id,
    ownerid as owner_id,
    jigsawcontactid as jigsaw_contact_id,
    individualid as individual_id,

    /* Dates */
    birthdate as birth_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    lastcurequestdate as last_cu_request_date,
    lastcuupdatedate as last_cu_update_date,
    emailbounceddate as email_bounced_date,

    /* Dimensions */
    salutation,
    firstname as first_name,
    lastname as last_name,
    otherstreet as other_street,
    othercity as other_city,
    otherstate as other_state,
    otherpostalcode as other_postal_code,
    othercountry as other_country,
    otherlatitude as other_latitude,
    otherlongitude as other_longitude,
    othergeocodeaccuracy as other_geocode_accuracy,
    mailingstreet as mailing_street,
    mailingcity as mailing_city,
    mailingstate as mailing_state,
    mailingpostalcode as mailing_postal_code,
    mailingcountry as mailing_country,
    mailinglatitude as mailing_latitude,
    mailinglongitude as mailing_longitude,
    mailinggeocodeaccuracy as mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone as mobile_phone,
    homephone as home_phone,
    otherphone as other_phone,
    assistantphone as assistant_phone,
    email,
    title,
    department,
    assistantname as assistant_name,
    leadsource as lead_source,
    description,
    pronouns,
    genderidentity as gender_identity,
    cleanstatus as clean_status,
    emailbouncedreason as email_bounced_reason,
    level__c as level,
    languages__c as languages,

    /* Metrics */
    hasoptedoutofemail as has_opted_out_of_email,
    hasoptedoutoffax as has_opted_out_of_fax,
    donotcall as do_not_call

from "dbt"."staging"."stg_salesforce__contact"
where isdeleted = false
    );
  
  
[0m11:41:26.858166 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.861232 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:41:26.861481 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact" rename to "dim_contact__dbt_backup"
[0m11:41:26.861887 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.863655 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:41:26.863865 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m11:41:26.864205 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.865180 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m11:41:26.865372 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:41:26.865551 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m11:41:26.866819 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.868380 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:41:26.868598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m11:41:26.869158 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.869884 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 11:41:26.843613 => 11:41:26.869793
[0m11:41:26.870089 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m11:41:26.908722 [info ] [Thread-1  ]: 19 of 30 OK created sql table model dim.dim_contact ............................ [[32mOK[0m in 0.07s]
[0m11:41:26.910197 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m11:41:26.910666 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m11:41:26.910957 [info ] [Thread-1  ]: 20 of 30 START sql table model dim.dim_lead .................................... [RUN]
[0m11:41:26.911335 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m11:41:26.911537 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m11:41:26.913852 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m11:41:26.914938 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 11:41:26.911674 => 11:41:26.914835
[0m11:41:26.915300 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m11:41:26.917947 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m11:41:26.918683 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:41:26.918992 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m11:41:26.919244 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.926089 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.926380 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:41:26.926633 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

select
    lead_id,
    masterrecordid as master_record_id,
    salutation as lead_salutation,
    firstname as first_name,
    lastname as last_name,
    title as lead_title,
    company as lead_company,
    street as lead_street,
    city as lead_city,
    state as lead_state,
    postalcode as lead_postal_code,
    country as lead_country,
    latitude as lead_latitude,
    longitude as lead_longitude,
    geocodeaccuracy as geocode_accuracy,
    phone as lead_phone,
    mobilephone as lead_mobile_phone,
    fax as lead_fax,
    email as lead_email,
    website as lead_website,
    description as lead_description,
    leadsource as lead_source,
    status as lead_status,
    industry as lead_industry,
    rating as lead_rating,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownerid as owner_id,
    hasoptedoutofemail as has_opted_out_of_email,
    isconverted as is_converted,
    converteddate as converted_date,
    convertedaccountid as converted_account_id,
    convertedcontactid as converted_contact_id,
    convertedopportunityid as converted_opportunity_id,
    isunreadbyowner as is_unread_by_owner,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    donotcall as do_not_call,
    hasoptedoutoffax as has_opted_out_of_fax,
    lasttransferdate as last_transfer_date,
    jigsaw as jigsaw_id,
    jigsawcontactid as jigsaw_contact_id,
    cleanstatus as clean_status,
    companydunsnumber as company_duns_number,
    dandbcompanyid as dandb_company_id,
    emailbouncedreason as email_bounced_reason,
    emailbounceddate as email_bounced_date,
    individualid as individual_id,
    pronouns as lead_pronouns,
    genderidentity as gender_identity,
    siccode__c as sic_code,
    productinterest__c as product_interest,
    primary__c as is_primary,
    currentgenerators__c as current_generators,
    numberoflocations__c as number_of_locations
from "dbt"."staging"."stg_salesforce__lead"
where isdeleted = false  -- Exclude deleted leads
order by last_name, first_name
    );
  
  
[0m11:41:26.931049 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.933297 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:41:26.933527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead" rename to "dim_lead__dbt_backup"
[0m11:41:26.933914 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.936443 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:41:26.936653 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m11:41:26.936976 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.937959 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m11:41:26.938147 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:41:26.938323 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m11:41:26.939853 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.941432 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:41:26.941633 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m11:41:26.942149 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.942917 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 11:41:26.915446 => 11:41:26.942813
[0m11:41:26.943125 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m11:41:26.969651 [info ] [Thread-1  ]: 20 of 30 OK created sql table model dim.dim_lead ............................... [[32mOK[0m in 0.06s]
[0m11:41:26.970060 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m11:41:26.970288 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m11:41:26.970668 [info ] [Thread-1  ]: 21 of 30 START sql table model dim.dim_opportunity ............................. [RUN]
[0m11:41:26.971156 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m11:41:26.971369 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m11:41:26.973712 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m11:41:26.974994 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 11:41:26.971509 => 11:41:26.974894
[0m11:41:26.975200 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m11:41:26.978024 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m11:41:26.978810 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:41:26.979065 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m11:41:26.979318 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:26.986202 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.986496 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:41:26.986744 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m11:41:26.990250 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.992438 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:41:26.992964 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m11:41:26.993443 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.995561 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:41:26.995800 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m11:41:26.996176 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:26.998041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m11:41:26.998253 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:41:26.998432 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m11:41:26.999652 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.001571 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:41:27.001805 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m11:41:27.002331 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.003200 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 11:41:26.975340 => 11:41:27.003105
[0m11:41:27.003421 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m11:41:27.023989 [info ] [Thread-1  ]: 21 of 30 OK created sql table model dim.dim_opportunity ........................ [[32mOK[0m in 0.05s]
[0m11:41:27.024382 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m11:41:27.024609 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:41:27.025031 [info ] [Thread-1  ]: 22 of 30 START sql table model dim.dim_opportunity_stage ....................... [RUN]
[0m11:41:27.025524 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m11:41:27.025739 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:41:27.028019 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:41:27.028677 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 11:41:27.025888 => 11:41:27.028551
[0m11:41:27.028912 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:41:27.031710 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:41:27.032209 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:41:27.032411 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m11:41:27.032588 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:27.039602 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.039887 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:41:27.040099 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct stagename as stage_name, stagesortorder as stage_sort_order
        from "dbt"."staging"."stg_salesforce__opportunity"
    )

select
    row_number() over (order by stage_sort_order) as stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
from source
    );
  
  
[0m11:41:27.042782 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.044930 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:41:27.045167 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m11:41:27.045501 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.047196 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:41:27.047400 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m11:41:27.047685 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.048620 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m11:41:27.048811 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:41:27.048992 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m11:41:27.049557 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.051756 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:41:27.051961 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m11:41:27.052375 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.053333 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 11:41:27.029055 => 11:41:27.053231
[0m11:41:27.053573 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m11:41:27.068561 [info ] [Thread-1  ]: 22 of 30 OK created sql table model dim.dim_opportunity_stage .................. [[32mOK[0m in 0.04s]
[0m11:41:27.068931 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:41:27.069156 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m11:41:27.069549 [info ] [Thread-1  ]: 23 of 30 START sql table model dim.dim_pricebook ............................... [RUN]
[0m11:41:27.070048 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m11:41:27.070272 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m11:41:27.072582 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m11:41:27.073059 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 11:41:27.070416 => 11:41:27.072957
[0m11:41:27.073269 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m11:41:27.076190 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m11:41:27.076818 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:41:27.077040 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m11:41:27.077218 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:27.084070 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.084350 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:41:27.084560 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

select
    pricebook_entry_id,
    pricebook2id as pricebook_id,
    product2id as product_id,
    unitprice as unit_price,
    isactive as is_active,
    usestandardprice as use_standard_price,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__pricebook_entry"
where isdeleted = false  -- Exclude deleted entries
order by pricebook_entry_id
    );
  
  
[0m11:41:27.086032 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.088386 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:41:27.088648 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook" rename to "dim_pricebook__dbt_backup"
[0m11:41:27.089037 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.090884 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:41:27.091096 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m11:41:27.091468 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.092477 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m11:41:27.092671 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:41:27.092849 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m11:41:27.093627 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.095263 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:41:27.095479 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m11:41:27.095939 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.096748 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 11:41:27.073406 => 11:41:27.096651
[0m11:41:27.096963 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m11:41:27.113100 [info ] [Thread-1  ]: 23 of 30 OK created sql table model dim.dim_pricebook .......................... [[32mOK[0m in 0.04s]
[0m11:41:27.113496 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m11:41:27.113721 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m11:41:27.114092 [info ] [Thread-1  ]: 24 of 30 START sql table model dim.dim_product ................................. [RUN]
[0m11:41:27.114555 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m11:41:27.114768 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m11:41:27.117926 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m11:41:27.118412 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 11:41:27.114903 => 11:41:27.118306
[0m11:41:27.118618 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m11:41:27.121517 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m11:41:27.122234 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:41:27.122498 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m11:41:27.122691 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:27.129667 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.129955 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:41:27.130183 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    productcode as product_code,
    description as product_description,
    isactive as is_active,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    family as product_family,
    externaldatasourceid as external_datasource_id,
    externalid as external_id,
    displayurl as display_url,
    quantityunitofmeasure as quantity_unit_of_measure,
    stockkeepingunit as stock_keeping_unit,
    type as product_type,
    productclass as product_class,
    sourceproductid as source_product_id,
    sellerid as seller_id
from "dbt"."staging"."stg_salesforce__product_2"
where isdeleted = false
-- isactive =1 ?
order by product_name
    );
  
  
[0m11:41:27.132093 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.134439 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:41:27.134701 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product" rename to "dim_product__dbt_backup"
[0m11:41:27.135102 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.137078 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:41:27.137310 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m11:41:27.137676 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.139042 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m11:41:27.139288 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:41:27.139489 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m11:41:27.140295 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.142104 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:41:27.142329 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m11:41:27.142758 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.143551 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 11:41:27.118761 => 11:41:27.143452
[0m11:41:27.143768 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m11:41:27.161405 [info ] [Thread-1  ]: 24 of 30 OK created sql table model dim.dim_product ............................ [[32mOK[0m in 0.05s]
[0m11:41:27.161836 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m11:41:27.162083 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m11:41:27.162399 [info ] [Thread-1  ]: 25 of 30 START sql table model dim.dim_record_type ............................. [RUN]
[0m11:41:27.162814 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m11:41:27.163172 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m11:41:27.166201 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m11:41:27.166659 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 11:41:27.163329 => 11:41:27.166560
[0m11:41:27.166852 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m11:41:27.169945 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m11:41:27.170542 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:41:27.170774 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m11:41:27.170953 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:27.177799 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.178084 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:41:27.178296 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

select
    record_type_id,
    name as record_type_name,
    modulenamespace as module_namespace,
    description as record_type_description,
    businessprocessid as business_process_id,
    sobjecttype as sobject_type,
    isactive as is_active,
    createdbyid as created_by_id,
    createddate as created_date,
    lastmodifiedbyid as last_modified_by_id,
    lastmodifieddate as last_modified_date,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__record_type"
where isdeleted = false
order by record_type_name
    );
  
  
[0m11:41:27.180127 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.182527 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:41:27.182808 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m11:41:27.183250 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.185240 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:41:27.185448 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m11:41:27.185754 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.186693 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m11:41:27.186880 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:41:27.187052 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m11:41:27.187738 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.189631 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:41:27.189850 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m11:41:27.190254 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.191182 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 11:41:27.166982 => 11:41:27.191080
[0m11:41:27.191414 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m11:41:27.207816 [info ] [Thread-1  ]: 25 of 30 OK created sql table model dim.dim_record_type ........................ [[32mOK[0m in 0.04s]
[0m11:41:27.208242 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m11:41:27.208481 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m11:41:27.208715 [info ] [Thread-1  ]: 26 of 30 START sql table model dim.dim_solution ................................ [RUN]
[0m11:41:27.209114 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m11:41:27.209326 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m11:41:27.211612 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m11:41:27.212095 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 11:41:27.209465 => 11:41:27.211995
[0m11:41:27.212290 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m11:41:27.216305 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m11:41:27.216909 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:41:27.217105 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m11:41:27.217292 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:27.224191 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.224488 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:41:27.224711 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solutionnumber as solution_number,
    solutionname as solution_name,
    ispublished as is_published,
    ispublishedinpublickb as is_published_in_public_kb,
    status as solution_status,
    isreviewed as is_reviewed,
    solutionnote as solution_note,
    caseid as case_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    timesused as times_used,
    ishtml as is_html
from "dbt"."staging"."stg_salesforce__solution"
where isdeleted = false
order by solution_name
    );
  
  
[0m11:41:27.226472 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.228495 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:41:27.228709 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m11:41:27.229044 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.231359 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:41:27.231593 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m11:41:27.231983 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.233014 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m11:41:27.233275 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:41:27.233460 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m11:41:27.234277 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.236159 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:41:27.236425 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m11:41:27.236894 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.237698 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 11:41:27.212421 => 11:41:27.237601
[0m11:41:27.237932 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m11:41:27.261536 [info ] [Thread-1  ]: 26 of 30 OK created sql table model dim.dim_solution ........................... [[32mOK[0m in 0.05s]
[0m11:41:27.261975 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m11:41:27.262226 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m11:41:27.262501 [info ] [Thread-1  ]: 27 of 30 START sql table model dim.dim_user .................................... [RUN]
[0m11:41:27.262879 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m11:41:27.263097 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m11:41:27.265473 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m11:41:27.265994 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 11:41:27.263251 => 11:41:27.265895
[0m11:41:27.266191 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m11:41:27.268852 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m11:41:27.269492 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:41:27.269768 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m11:41:27.269978 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:27.277262 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.277511 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:41:27.277733 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m11:41:27.297823 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.320717 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:41:27.321046 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user" rename to "dim_user__dbt_backup"
[0m11:41:27.321564 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.323536 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:41:27.323799 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m11:41:27.324205 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.325819 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m11:41:27.326136 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:41:27.326345 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m11:41:27.327538 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.329674 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:41:27.329968 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m11:41:27.330613 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.331515 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 11:41:27.266326 => 11:41:27.331415
[0m11:41:27.331750 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m11:41:27.350515 [info ] [Thread-1  ]: 27 of 30 OK created sql table model dim.dim_user ............................... [[32mOK[0m in 0.09s]
[0m11:41:27.350908 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m11:41:27.351137 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m11:41:27.351588 [info ] [Thread-1  ]: 28 of 30 START sql table model fact.fact_campaign_performance .................. [RUN]
[0m11:41:27.352125 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_campaign_performance)
[0m11:41:27.352353 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m11:41:27.354741 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:41:27.355512 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 11:41:27.352495 => 11:41:27.355403
[0m11:41:27.355718 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m11:41:27.358755 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:41:27.359289 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:41:27.359567 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: BEGIN
[0m11:41:27.359761 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:27.366448 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.366756 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:41:27.366997 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */

  
    
    

    create  table
      "dbt"."fact"."fact_campaign_performance__dbt_tmp"
  
    as (
      

WITH campaign_performance AS (
    SELECT
        c.campaign_id,
        c.created_at,
        c.last_modified_at,
        c.start_date,
        c.end_date,
        c.owner_id as user_id,
        o.opportunity_id,
        c.expected_revenue,
        c.budgeted_cost,
        c.actual_cost,
        c.number_sent,
        c.number_of_leads,
        c.number_of_converted_leads,
        c.number_of_contacts
    FROM "dbt"."dim"."dim_campaign" c
    LEFT JOIN "dbt"."dim"."dim_opportunity" o ON c.campaign_id = o.campaign_id
)

SELECT
    cp.campaign_id,
    cp.user_id,
    cp.opportunity_id,
    cp.start_date,
    cp.end_date,
    cp.expected_revenue,
    cp.budgeted_cost,
    cp.actual_cost,
    cp.number_sent,
    cp.number_of_leads,
    cp.number_of_converted_leads,
    cp.number_of_contacts
FROM campaign_performance cp


--need to be improved - No of leads closed based a particular campaign
    );
  
  
[0m11:41:27.368760 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.371822 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:41:27.372116 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
alter table "dbt"."fact"."fact_campaign_performance" rename to "fact_campaign_performance__dbt_backup"
[0m11:41:27.372521 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.374351 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:41:27.374557 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
alter table "dbt"."fact"."fact_campaign_performance__dbt_tmp" rename to "fact_campaign_performance"
[0m11:41:27.374945 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.376213 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: COMMIT
[0m11:41:27.376452 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:41:27.376657 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: COMMIT
[0m11:41:27.377342 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.379228 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:41:27.379462 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
drop table if exists "dbt"."fact"."fact_campaign_performance__dbt_backup" cascade
[0m11:41:27.379956 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.380900 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 11:41:27.355853 => 11:41:27.380751
[0m11:41:27.381172 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: Close
[0m11:41:27.397503 [info ] [Thread-1  ]: 28 of 30 OK created sql table model fact.fact_campaign_performance ............. [[32mOK[0m in 0.05s]
[0m11:41:27.397963 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m11:41:27.398214 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_lead_conversion
[0m11:41:27.398516 [info ] [Thread-1  ]: 29 of 30 START sql view model fact.fact_lead_conversion ........................ [RUN]
[0m11:41:27.398896 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_lead_conversion)
[0m11:41:27.399109 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_lead_conversion
[0m11:41:27.401633 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:41:27.402142 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (compile): 11:41:27.399248 => 11:41:27.402036
[0m11:41:27.402423 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_lead_conversion
[0m11:41:27.405436 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:41:27.405996 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:41:27.406208 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: BEGIN
[0m11:41:27.406406 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:27.413469 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.413797 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:41:27.414062 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */

  
  create view "dbt"."fact"."fact_lead_conversion__dbt_tmp" as (
    -- fact_lead_conversion.sql

with lead_data as (
    select 
        lead_id,
        lead_salutation,
        first_name,
        last_name,
        lead_company,
        lead_source,
        lead_status,
        is_converted,
        converted_date,
        converted_account_id,
        converted_contact_id,
        converted_opportunity_id,
        owner_id,  -- Assuming this is the user ID
        created_date as lead_created_date,
        last_modified_date as lead_last_modified_date
    from 
        "dbt"."dim"."dim_lead"
    where 
        is_converted = true
),

account_data as (
    select 
        account_id,
        account_name
    from 
        "dbt"."dim"."dim_account"
),

opportunity_data as (
    select 
        opportunity_id,
        opportunity_name,
        opportunity_amount
    from 
        "dbt"."dim"."dim_opportunity"
),

user_data as (
    select 
        user_id,
        user_name,
        email as user_email
    from 
        "dbt"."dim"."dim_user"
)

select 
    l.lead_id,
    l.first_name,
    l.last_name,
    l.lead_company,
    l.lead_source,
    l.lead_status,
    l.converted_date,
    a.account_name as converted_account_name,
    o.opportunity_name as converted_opportunity_name,
    o.opportunity_amount as converted_opportunity_amount,
    u.user_name as lead_owner_name,
    u.user_email as lead_owner_email,
    l.lead_created_date,
    l.lead_last_modified_date
from 
    lead_data l
left join 
    account_data a on l.converted_account_id = a.account_id
left join 
    opportunity_data o on l.converted_opportunity_id = o.opportunity_id
left join 
    user_data u on l.owner_id = u.user_id
  );

[0m11:41:27.414818 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.416989 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:41:27.417205 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter view "dbt"."fact"."fact_lead_conversion" rename to "fact_lead_conversion__dbt_backup"
[0m11:41:27.417618 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.448874 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:41:27.449268 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter view "dbt"."fact"."fact_lead_conversion__dbt_tmp" rename to "fact_lead_conversion"
[0m11:41:27.449700 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.450657 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m11:41:27.450869 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:41:27.451068 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m11:41:27.451836 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.453691 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:41:27.453921 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
drop view if exists "dbt"."fact"."fact_lead_conversion__dbt_backup" cascade
[0m11:41:27.454400 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.455319 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (execute): 11:41:27.402582 => 11:41:27.455215
[0m11:41:27.455552 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: Close
[0m11:41:27.470455 [info ] [Thread-1  ]: 29 of 30 OK created sql view model fact.fact_lead_conversion ................... [[32mOK[0m in 0.07s]
[0m11:41:27.470847 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_lead_conversion
[0m11:41:27.471076 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m11:41:27.471432 [info ] [Thread-1  ]: 30 of 30 START sql incremental model fact.fact_opportunity ..................... [RUN]
[0m11:41:27.471927 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_lead_conversion, now model.elastic_dbt_interview.fact_opportunity)
[0m11:41:27.472149 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m11:41:27.478122 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m11:41:27.479020 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 11:41:27.472285 => 11:41:27.478873
[0m11:41:27.479256 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m11:41:27.496234 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:41:27.496597 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240822114127493727"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        a.account_id,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m11:41:27.496832 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:27.506371 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.510723 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:41:27.510969 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m11:41:27.511262 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.511445 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:41:27.511634 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240822114127493727'
      
      
      order by ordinal_position

  
[0m11:41:27.539372 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.543638 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:41:27.543956 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m11:41:27.558065 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.563023 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:41:27.563303 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m11:41:27.577958 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.586504 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m11:41:27.587137 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:41:27.587374 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240822114127493727"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240822114127493727"
    )
  
[0m11:41:27.588717 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.589615 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m11:41:27.589812 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:41:27.589993 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m11:41:27.590263 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:41:27.590705 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 11:41:27.479408 => 11:41:27.590616
[0m11:41:27.590899 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m11:41:27.593468 [info ] [Thread-1  ]: 30 of 30 OK created sql incremental model fact.fact_opportunity ................ [[32mOK[0m in 0.12s]
[0m11:41:27.593847 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m11:41:27.594632 [debug] [MainThread]: Using duckdb connection "master"
[0m11:41:27.594816 [debug] [MainThread]: On master: BEGIN
[0m11:41:27.594964 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:41:27.602149 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:41:27.602440 [debug] [MainThread]: On master: COMMIT
[0m11:41:27.602606 [debug] [MainThread]: Using duckdb connection "master"
[0m11:41:27.602757 [debug] [MainThread]: On master: COMMIT
[0m11:41:27.602959 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:41:27.603116 [debug] [MainThread]: On master: Close
[0m11:41:27.605185 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:41:27.605443 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m11:41:27.605722 [info ] [MainThread]: 
[0m11:41:27.605928 [info ] [MainThread]: Finished running 14 table models, 15 view models, 1 incremental model in 0 hours 0 minutes and 2.00 seconds (2.00s).
[0m11:41:27.607957 [debug] [MainThread]: Command end result
[0m11:41:27.616934 [info ] [MainThread]: 
[0m11:41:27.617225 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:41:27.617422 [info ] [MainThread]: 
[0m11:41:27.617605 [info ] [MainThread]: Done. PASS=30 WARN=0 ERROR=0 SKIP=0 TOTAL=30
[0m11:41:27.617965 [debug] [MainThread]: Command `dbt run` succeeded at 11:41:27.617915 after 2.24 seconds
[0m11:41:27.618160 [debug] [MainThread]: Flushing usage events


============================== 11:42:14.260014 | c5008e2b-9e88-4104-a96d-b04b8b04a3ce ==============================
[0m11:42:14.260014 [info ] [MainThread]: Running with dbt=1.6.18
[0m11:42:14.263726 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'False'}
[0m11:42:14.264025 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m11:42:14.345762 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m11:42:14.366365 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m11:42:14.418108 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:42:14.418629 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_lead_conversion.sql
[0m11:42:14.447108 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.marts
[0m11:42:14.463891 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m11:42:14.465767 [info ] [MainThread]: 
[0m11:42:14.466242 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:42:14.467711 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m11:42:14.474412 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:42:14.474665 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:42:14.474846 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:14.482816 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:42:14.483147 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:42:14.483526 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:14.488333 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:42:14.488525 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:42:14.488681 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:14.491880 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:42:14.492199 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m11:42:14.492398 [info ] [MainThread]: 
[0m11:42:14.492614 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m11:42:14.492902 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 48383) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m11:42:14.493344 [debug] [MainThread]: Command `dbt run` failed at 11:42:14.493283 after 0.26 seconds
[0m11:42:14.493555 [debug] [MainThread]: Flushing usage events


============================== 11:42:21.672705 | ebef74a7-7138-4c11-97b9-0e4f9e65008a ==============================
[0m11:42:21.672705 [info ] [MainThread]: Running with dbt=1.6.18
[0m11:42:21.674982 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'False'}
[0m11:42:21.675229 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m11:42:21.748467 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m11:42:21.765945 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m11:42:21.801616 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:42:21.801923 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:42:21.802864 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m11:42:21.816420 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m11:42:21.818091 [info ] [MainThread]: 
[0m11:42:21.818521 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:42:21.819922 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m11:42:21.827602 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:42:21.827923 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:42:21.828100 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:21.843447 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.844441 [debug] [ThreadPool]: On list_dbt: Close
[0m11:42:21.847751 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:42:21.848005 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:42:21.848185 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:42:21.855728 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.856507 [debug] [ThreadPool]: On list_dbt: Close
[0m11:42:21.860230 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:42:21.860527 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:42:21.860740 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:42:21.868119 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.868905 [debug] [ThreadPool]: On list_dbt: Close
[0m11:42:21.870818 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m11:42:21.871189 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m11:42:21.874277 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:42:21.874485 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m11:42:21.874640 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:42:21.881862 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.882102 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:42:21.882265 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m11:42:21.882501 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.882969 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m11:42:21.883131 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:42:21.883276 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m11:42:21.883479 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.883637 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m11:42:21.885659 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now create_dbt_fact)
[0m11:42:21.886045 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m11:42:21.887468 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:42:21.887630 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m11:42:21.887770 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:42:21.894258 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.894532 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:42:21.894691 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m11:42:21.894919 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.895400 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m11:42:21.895563 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:42:21.895709 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m11:42:21.895925 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.896084 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m11:42:21.898288 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now create_dbt_dim)
[0m11:42:21.898819 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m11:42:21.901064 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:42:21.901279 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m11:42:21.901439 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:42:21.908539 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.908803 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:42:21.908985 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m11:42:21.909218 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.909712 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m11:42:21.909900 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:42:21.910058 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m11:42:21.910291 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.910458 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m11:42:21.913413 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now list_dbt_main)
[0m11:42:21.916726 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m11:42:21.916957 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m11:42:21.917142 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:42:21.924787 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.925418 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m11:42:21.925622 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m11:42:21.944080 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.947978 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m11:42:21.949361 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m11:42:21.949562 [debug] [ThreadPool]: On list_dbt_main: Close
[0m11:42:21.952034 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_main, now list_dbt_staging)
[0m11:42:21.954921 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m11:42:21.955135 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m11:42:21.955296 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:42:21.962894 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.963198 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m11:42:21.963383 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m11:42:21.982498 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.983505 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m11:42:21.983738 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m11:42:21.983896 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m11:42:21.986266 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_fact)
[0m11:42:21.989033 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m11:42:21.989261 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m11:42:21.989422 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:42:21.995925 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:21.996218 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m11:42:21.996394 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m11:42:22.012900 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:22.016489 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m11:42:22.016758 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m11:42:22.016928 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m11:42:22.019408 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_dim)
[0m11:42:22.021343 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m11:42:22.021530 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m11:42:22.021685 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:42:22.028635 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:22.028917 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m11:42:22.029091 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m11:42:22.045008 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:42:22.048614 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m11:42:22.048846 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m11:42:22.049005 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m11:42:22.052563 [debug] [MainThread]: Using duckdb connection "master"
[0m11:42:22.052769 [debug] [MainThread]: On master: BEGIN
[0m11:42:22.052926 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:42:22.059481 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:42:22.059737 [debug] [MainThread]: On master: COMMIT
[0m11:42:22.059899 [debug] [MainThread]: Using duckdb connection "master"
[0m11:42:22.060052 [debug] [MainThread]: On master: COMMIT
[0m11:42:22.060252 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:42:22.060415 [debug] [MainThread]: On master: Close
[0m11:42:22.062387 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:42:22.062680 [info ] [MainThread]: 
[0m11:42:22.065289 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m11:42:22.065641 [info ] [Thread-1  ]: 1 of 30 START sql table model dim.dim_date ..................................... [RUN]
[0m11:42:22.066061 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_dim, now model.elastic_dbt_interview.dim_date)
[0m11:42:22.066260 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m11:42:22.093611 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:42:22.093958 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m11:42:22.094145 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.100903 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.101187 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:42:22.101384 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m11:42:22.101744 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.142305 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m11:42:22.145053 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 11:42:22.066409 => 11:42:22.144914
[0m11:42:22.145291 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m11:42:22.186886 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m11:42:22.187762 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:42:22.188183 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m11:42:22.235737 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.239779 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:42:22.240015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m11:42:22.240459 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.242217 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:42:22.242412 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m11:42:22.242726 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.253229 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m11:42:22.253488 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:42:22.253674 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m11:42:22.256923 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.260020 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:42:22.260243 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m11:42:22.260758 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.261500 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 11:42:22.145446 => 11:42:22.261408
[0m11:42:22.261701 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m11:42:22.298104 [info ] [Thread-1  ]: 1 of 30 OK created sql table model dim.dim_date ................................ [[32mOK[0m in 0.23s]
[0m11:42:22.298527 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m11:42:22.298770 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m11:42:22.299049 [info ] [Thread-1  ]: 2 of 30 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m11:42:22.299398 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.stg_salesforce__account)
[0m11:42:22.299596 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m11:42:22.302261 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:42:22.302716 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 11:42:22.299726 => 11:42:22.302615
[0m11:42:22.302915 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m11:42:22.313650 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:42:22.314524 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:42:22.314825 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m11:42:22.315021 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.321980 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.322308 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:42:22.322553 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m11:42:22.323404 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.325753 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:42:22.325986 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m11:42:22.326325 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.328107 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:42:22.328307 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m11:42:22.328598 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.329565 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m11:42:22.329779 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:42:22.329960 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m11:42:22.330634 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.332337 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:42:22.332608 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m11:42:22.333017 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.333762 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 11:42:22.303049 => 11:42:22.333669
[0m11:42:22.333974 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m11:42:22.347532 [info ] [Thread-1  ]: 2 of 30 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.05s]
[0m11:42:22.347931 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m11:42:22.348183 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:42:22.348478 [info ] [Thread-1  ]: 3 of 30 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m11:42:22.348912 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m11:42:22.349132 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:42:22.351929 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:42:22.352414 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 11:42:22.349274 => 11:42:22.352314
[0m11:42:22.352614 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:42:22.355295 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:42:22.355871 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:42:22.356085 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m11:42:22.356265 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.363062 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.363342 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:42:22.363578 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m11:42:22.364342 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.366562 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:42:22.366775 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m11:42:22.367082 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.368818 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:42:22.369027 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m11:42:22.369321 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.370445 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m11:42:22.370696 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:42:22.370882 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m11:42:22.371628 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.373472 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:42:22.373667 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m11:42:22.374006 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.374754 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 11:42:22.352744 => 11:42:22.374659
[0m11:42:22.374960 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m11:42:22.389160 [info ] [Thread-1  ]: 3 of 30 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.04s]
[0m11:42:22.389539 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:42:22.389765 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m11:42:22.390172 [info ] [Thread-1  ]: 4 of 30 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m11:42:22.390673 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m11:42:22.390883 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m11:42:22.392951 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:42:22.393450 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 11:42:22.391015 => 11:42:22.393343
[0m11:42:22.393652 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m11:42:22.397384 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:42:22.398168 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:42:22.398370 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m11:42:22.398564 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.405223 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.405495 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:42:22.405717 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m11:42:22.406440 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.408855 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:42:22.409209 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m11:42:22.409619 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.411644 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:42:22.411863 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m11:42:22.412172 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.413070 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m11:42:22.413263 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:42:22.413439 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m11:42:22.414013 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.415602 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:42:22.415807 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m11:42:22.416179 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.416888 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 11:42:22.393781 => 11:42:22.416798
[0m11:42:22.417090 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m11:42:22.431071 [info ] [Thread-1  ]: 4 of 30 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.04s]
[0m11:42:22.431498 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m11:42:22.431730 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:42:22.432104 [info ] [Thread-1  ]: 5 of 30 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m11:42:22.432598 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m11:42:22.432810 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:42:22.434838 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:42:22.435295 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 11:42:22.432949 => 11:42:22.435191
[0m11:42:22.435495 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:42:22.439093 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:42:22.440096 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:42:22.440314 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m11:42:22.440493 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.447065 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.447365 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:42:22.447572 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m11:42:22.448010 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.450352 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:42:22.450647 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m11:42:22.451068 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.453091 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:42:22.453331 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m11:42:22.453639 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.454508 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m11:42:22.454695 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:42:22.454868 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m11:42:22.455410 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.456996 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:42:22.457193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m11:42:22.457546 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.458313 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 11:42:22.435624 => 11:42:22.458215
[0m11:42:22.458531 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m11:42:22.472511 [info ] [Thread-1  ]: 5 of 30 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.04s]
[0m11:42:22.472919 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:42:22.473172 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:42:22.473514 [info ] [Thread-1  ]: 6 of 30 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m11:42:22.473896 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m11:42:22.474091 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:42:22.476141 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:42:22.476576 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 11:42:22.474224 => 11:42:22.476477
[0m11:42:22.476767 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:42:22.479463 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:42:22.480009 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:42:22.480226 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m11:42:22.480409 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.487299 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.487612 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:42:22.487857 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m11:42:22.488795 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.492134 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:42:22.492423 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m11:42:22.492818 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.494932 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:42:22.495226 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m11:42:22.495591 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.496536 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m11:42:22.496740 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:42:22.496914 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m11:42:22.497546 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.499443 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:42:22.499657 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m11:42:22.500051 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.500861 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 11:42:22.476891 => 11:42:22.500762
[0m11:42:22.501077 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m11:42:22.515233 [info ] [Thread-1  ]: 6 of 30 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.04s]
[0m11:42:22.515640 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:42:22.515885 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:42:22.516206 [info ] [Thread-1  ]: 7 of 30 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m11:42:22.516560 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m11:42:22.516748 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:42:22.518850 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:42:22.519529 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 11:42:22.516879 => 11:42:22.519432
[0m11:42:22.519729 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:42:22.522396 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:42:22.523267 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:42:22.523470 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m11:42:22.523644 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.530128 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.530420 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:42:22.530662 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    --

with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m11:42:22.531453 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.533827 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:42:22.534130 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m11:42:22.534532 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.537422 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:42:22.537640 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m11:42:22.537971 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.538836 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m11:42:22.539094 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:42:22.539265 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m11:42:22.539958 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.541497 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:42:22.541693 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m11:42:22.542044 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.542745 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 11:42:22.519861 => 11:42:22.542650
[0m11:42:22.542949 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m11:42:22.557418 [info ] [Thread-1  ]: 7 of 30 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.04s]
[0m11:42:22.557859 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:42:22.558114 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:42:22.558466 [info ] [Thread-1  ]: 8 of 30 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m11:42:22.558883 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m11:42:22.559110 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:42:22.561157 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:42:22.561653 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 11:42:22.559256 => 11:42:22.561551
[0m11:42:22.561855 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:42:22.564488 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:42:22.564968 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:42:22.565172 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m11:42:22.565356 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.572054 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.572348 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:42:22.572574 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m11:42:22.573301 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.575481 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:42:22.575690 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m11:42:22.575989 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.578373 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:42:22.578789 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m11:42:22.579311 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.580332 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m11:42:22.580546 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:42:22.580727 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m11:42:22.581386 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.583209 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:42:22.583422 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m11:42:22.583778 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.584551 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 11:42:22.561985 => 11:42:22.584453
[0m11:42:22.584770 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m11:42:22.599579 [info ] [Thread-1  ]: 8 of 30 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.04s]
[0m11:42:22.599995 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:42:22.600253 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:42:22.600597 [info ] [Thread-1  ]: 9 of 30 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m11:42:22.600950 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m11:42:22.601147 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:42:22.603215 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:42:22.604073 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 11:42:22.601285 => 11:42:22.603963
[0m11:42:22.604284 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:42:22.607121 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:42:22.607661 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:42:22.607866 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m11:42:22.608044 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.614548 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.614847 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:42:22.615066 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m11:42:22.615612 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.618055 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:42:22.618331 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m11:42:22.618716 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.620609 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:42:22.620833 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m11:42:22.621166 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.622039 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m11:42:22.622242 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:42:22.622422 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m11:42:22.622962 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.625199 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:42:22.625408 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m11:42:22.625772 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.626466 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 11:42:22.604419 => 11:42:22.626368
[0m11:42:22.626670 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m11:42:22.640891 [info ] [Thread-1  ]: 9 of 30 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.04s]
[0m11:42:22.641297 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:42:22.641542 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:42:22.641816 [info ] [Thread-1  ]: 10 of 30 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m11:42:22.642172 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m11:42:22.642364 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:42:22.644340 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:42:22.644777 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 11:42:22.642498 => 11:42:22.644679
[0m11:42:22.644970 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:42:22.647733 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:42:22.648244 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:42:22.648458 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m11:42:22.648844 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.655926 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.656253 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:42:22.656483 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m11:42:22.656960 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.659342 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:42:22.659613 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m11:42:22.659959 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.661959 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:42:22.662260 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m11:42:22.662623 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.663590 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m11:42:22.663793 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:42:22.663977 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m11:42:22.664531 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.666955 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:42:22.667174 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m11:42:22.667553 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.668296 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 11:42:22.645099 => 11:42:22.668204
[0m11:42:22.668497 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m11:42:22.682916 [info ] [Thread-1  ]: 10 of 30 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.04s]
[0m11:42:22.683306 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:42:22.683532 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:42:22.683958 [info ] [Thread-1  ]: 11 of 30 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m11:42:22.684441 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m11:42:22.684647 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:42:22.686788 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:42:22.687469 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 11:42:22.684784 => 11:42:22.687363
[0m11:42:22.687683 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:42:22.690328 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:42:22.690972 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:42:22.691278 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m11:42:22.691470 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.698674 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.699007 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:42:22.699267 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m11:42:22.699938 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.702268 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:42:22.702553 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m11:42:22.702933 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.704843 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:42:22.705054 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m11:42:22.705347 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.706222 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m11:42:22.706480 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:42:22.706653 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m11:42:22.707189 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.708697 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:42:22.708907 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m11:42:22.709299 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.710067 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 11:42:22.687827 => 11:42:22.709967
[0m11:42:22.710281 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m11:42:22.724237 [info ] [Thread-1  ]: 11 of 30 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.04s]
[0m11:42:22.724665 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:42:22.724907 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:42:22.725225 [info ] [Thread-1  ]: 12 of 30 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m11:42:22.725587 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m11:42:22.725777 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:42:22.728461 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:42:22.729117 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 11:42:22.725911 => 11:42:22.729003
[0m11:42:22.729329 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:42:22.731967 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:42:22.732714 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:42:22.733041 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m11:42:22.733279 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.740182 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.740429 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:42:22.740634 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m11:42:22.741083 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.743428 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:42:22.743757 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m11:42:22.744169 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.746396 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:42:22.746615 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m11:42:22.746920 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.747798 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m11:42:22.747988 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:42:22.748163 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m11:42:22.748686 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.750405 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:42:22.750638 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m11:42:22.750995 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.751712 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 11:42:22.729459 => 11:42:22.751621
[0m11:42:22.751910 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m11:42:22.766278 [info ] [Thread-1  ]: 12 of 30 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.04s]
[0m11:42:22.766695 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:42:22.766938 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:42:22.767283 [info ] [Thread-1  ]: 13 of 30 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m11:42:22.767671 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m11:42:22.767875 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:42:22.769986 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:42:22.770454 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 11:42:22.768012 => 11:42:22.770358
[0m11:42:22.770649 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:42:22.774083 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:42:22.774542 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:42:22.774734 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m11:42:22.774911 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.781639 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.781924 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:42:22.782129 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m11:42:22.782615 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.784747 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:42:22.784954 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m11:42:22.785253 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.786946 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:42:22.787164 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m11:42:22.787634 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.788735 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m11:42:22.789040 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:42:22.789273 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m11:42:22.789954 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.791807 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:42:22.792060 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m11:42:22.792473 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.793348 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 11:42:22.770776 => 11:42:22.793253
[0m11:42:22.793565 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m11:42:22.808087 [info ] [Thread-1  ]: 13 of 30 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.04s]
[0m11:42:22.808528 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:42:22.808790 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m11:42:22.809124 [info ] [Thread-1  ]: 14 of 30 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m11:42:22.809502 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m11:42:22.809700 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m11:42:22.811871 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:42:22.812333 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 11:42:22.809869 => 11:42:22.812233
[0m11:42:22.812523 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m11:42:22.815855 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:42:22.817088 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:42:22.817345 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m11:42:22.817549 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.824650 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.824938 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:42:22.825187 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m11:42:22.826148 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.828369 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:42:22.828596 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m11:42:22.828901 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.830790 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:42:22.830999 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m11:42:22.831303 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.832168 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m11:42:22.832357 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:42:22.832527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m11:42:22.833196 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.835109 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:42:22.835328 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m11:42:22.835756 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.836716 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 11:42:22.812652 => 11:42:22.836615
[0m11:42:22.836952 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m11:42:22.851004 [info ] [Thread-1  ]: 14 of 30 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.04s]
[0m11:42:22.851397 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m11:42:22.851623 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:42:22.851996 [info ] [Thread-1  ]: 15 of 30 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m11:42:22.852492 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m11:42:22.852701 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:42:22.854778 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:42:22.856128 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 11:42:22.852838 => 11:42:22.856023
[0m11:42:22.856330 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:42:22.858970 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:42:22.859468 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:42:22.859659 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m11:42:22.859837 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.866301 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.866594 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:42:22.866800 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m11:42:22.867318 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.870787 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:42:22.871044 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m11:42:22.871402 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.873207 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:42:22.873419 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m11:42:22.873712 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.874551 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m11:42:22.874736 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:42:22.874907 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m11:42:22.875493 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.877426 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:42:22.877639 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m11:42:22.878057 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.878976 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 11:42:22.856457 => 11:42:22.878870
[0m11:42:22.879230 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m11:42:22.893424 [info ] [Thread-1  ]: 15 of 30 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.04s]
[0m11:42:22.893815 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:42:22.894043 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m11:42:22.894463 [info ] [Thread-1  ]: 16 of 30 START sql table model dim.dim_account ................................. [RUN]
[0m11:42:22.894939 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m11:42:22.895149 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m11:42:22.897469 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m11:42:22.898134 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 11:42:22.895290 => 11:42:22.898031
[0m11:42:22.898332 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m11:42:22.900967 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m11:42:22.901782 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:42:22.902091 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m11:42:22.902317 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.908965 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.909246 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:42:22.909515 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

select
    account_id,
    masterrecordid as master_record_id,
    name as account_name,
    type as account_type,
    parentid as parent_account_id,
    billingstreet as billing_street,
    billingcity as billing_city,
    billingstate as billing_state,
    billingpostalcode as billing_postal_code,
    billingcountry as billing_country,
    billinglatitude as billing_latitude,
    billinglongitude as billing_longitude,
    billinggeocodeaccuracy as billing_geocode_accuracy,
    shippingstreet as shipping_street,
    shippingcity as shipping_city,
    shippingstate as shipping_state,
    shippingpostalcode as shipping_postal_code,
    shippingcountry as shipping_country,
    shippinglatitude as shipping_latitude,
    shippinglongitude as shipping_longitude,
    shippinggeocodeaccuracy as shipping_geocode_accuracy,
    phone as account_phone,
    fax as account_fax,
    accountnumber as account_number,
    website as account_website,
    sic as sic_code,
    industry as account_industry,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownership as account_ownership,
    tickersymbol as ticker_symbol,
    description as account_description,
    rating as account_rating,
    site as account_site,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    jigsaw as jigsaw_id,
    jigsawcompanyid as jigsaw_company_id,
    cleanstatus as clean_status,
    accountsource as account_source,
    dunsnumber as duns_number,
    tradestyle as trade_style,
    naicscode as naics_code,
    naicsdesc as naics_description,
    yearstarted as year_started,
    sicdesc as sic_description,
    dandbcompanyid as dandb_company_id,
    operatinghoursid as operating_hours_id,
    customerpriority__c as customer_priority,
    sla__c as sla,
    active__c as is_active,
    numberoflocations__c as number_of_locations,
    upsellopportunity__c as upsell_opportunity,
    slaserialnumber__c as sla_serial_number,
    slaexpirationdate__c as sla_expiration_date
from "dbt"."staging"."stg_salesforce__account"
where isdeleted = false  -- Exclude deleted accounts
order by account_name
    );
  
  
[0m11:42:22.913969 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.916743 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:42:22.916962 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account" rename to "dim_account__dbt_backup"
[0m11:42:22.917327 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.945368 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:42:22.945723 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m11:42:22.946186 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.947264 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m11:42:22.947459 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:42:22.947637 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m11:42:22.949056 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.950613 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:42:22.950812 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m11:42:22.951335 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.952056 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 11:42:22.898460 => 11:42:22.951959
[0m11:42:22.952281 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m11:42:22.975252 [info ] [Thread-1  ]: 16 of 30 OK created sql table model dim.dim_account ............................ [[32mOK[0m in 0.08s]
[0m11:42:22.975631 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m11:42:22.975854 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m11:42:22.976221 [info ] [Thread-1  ]: 17 of 30 START sql table model dim.dim_campaign ................................ [RUN]
[0m11:42:22.976682 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m11:42:22.976891 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m11:42:22.979242 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m11:42:22.979739 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 11:42:22.977030 => 11:42:22.979633
[0m11:42:22.979950 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m11:42:22.982656 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m11:42:22.983377 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:42:22.983620 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m11:42:22.983814 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:22.991097 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.991397 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:42:22.991653 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

select
    /* IDs */
    campaign_id,
    parentid as parent_campaign_id,
    ownerid as owner_id,
    createdbyid as created_by_id,
    lastmodifiedbyid as last_modified_by_id,
    campaignmemberrecordtypeid as campaign_member_record_type_id,

    /* Dates */
    startdate as start_date,
    enddate as end_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,

    /* Dimensions */
    name as campaign_name,
    type as campaign_type,
    status,
    description,
    isactive as is_active,

    /* Metrics */
    expectedrevenue as expected_revenue,
    budgetedcost as budgeted_cost,
    actualcost as actual_cost,
    expectedresponse as expected_response,
    numbersent as number_sent,
    numberofleads as number_of_leads,
    numberofconvertedleads as number_of_converted_leads,
    numberofcontacts as number_of_contacts,
    numberofresponses as number_of_responses,
    numberofopportunities as number_of_opportunities,
    numberofwonopportunities as number_of_won_opportunities,
    amountallopportunities as amount_all_opportunities,
    amountwonopportunities as amount_won_opportunities,
    hierarchynumberofleads as hierarchy_number_of_leads,
    hierarchynumberofconvertedleads as hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts as hierarchy_number_of_contacts,
    hierarchynumberofresponses as hierarchy_number_of_responses,
    hierarchynumberofopportunities as hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities as hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities as hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities as hierarchy_amount_won_opportunities,
    hierarchynumbersent as hierarchy_number_sent,
    hierarchyexpectedrevenue as hierarchy_expected_revenue,
    hierarchybudgetedcost as hierarchy_budgeted_cost,
    hierarchyactualcost as hierarchy_actual_cost

from "dbt"."staging"."stg_salesforce__campaign"
where isdeleted = false
    );
  
  
[0m11:42:22.994739 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:22.997067 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:42:22.997327 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign" rename to "dim_campaign__dbt_backup"
[0m11:42:22.997747 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.000756 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:42:23.001018 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m11:42:23.001393 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.002411 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m11:42:23.002611 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:42:23.002787 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m11:42:23.003858 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.005600 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:42:23.005807 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."dim"."dim_campaign__dbt_backup" cascade
[0m11:42:23.006321 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.007178 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 11:42:22.980088 => 11:42:23.007087
[0m11:42:23.007388 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m11:42:23.024475 [info ] [Thread-1  ]: 17 of 30 OK created sql table model dim.dim_campaign ........................... [[32mOK[0m in 0.05s]
[0m11:42:23.024919 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m11:42:23.025165 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m11:42:23.025453 [info ] [Thread-1  ]: 18 of 30 START sql table model dim.dim_case_status ............................. [RUN]
[0m11:42:23.025815 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m11:42:23.026011 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m11:42:23.028192 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m11:42:23.028677 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 11:42:23.026149 => 11:42:23.028573
[0m11:42:23.028881 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m11:42:23.031694 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m11:42:23.032193 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:42:23.032545 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m11:42:23.032762 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.039768 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.040074 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:42:23.040299 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."dim"."dim_case_status__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct status as status_name, status as status_description 
        from "dbt"."staging"."stg_salesforce__case_history_2"
    )

select
    row_number() over (order by status_name) as status_id, 
    status_description
from source
    );
  
  
[0m11:42:23.042320 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.044540 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:42:23.044770 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m11:42:23.045149 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.046921 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:42:23.047133 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m11:42:23.047469 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.048470 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m11:42:23.048694 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:42:23.048885 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m11:42:23.049552 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.052213 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:42:23.052435 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."dim"."dim_case_status__dbt_backup" cascade
[0m11:42:23.052889 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.053680 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 11:42:23.029043 => 11:42:23.053586
[0m11:42:23.053889 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m11:42:23.068733 [info ] [Thread-1  ]: 18 of 30 OK created sql table model dim.dim_case_status ........................ [[32mOK[0m in 0.04s]
[0m11:42:23.069207 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m11:42:23.069466 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m11:42:23.069739 [info ] [Thread-1  ]: 19 of 30 START sql table model dim.dim_contact ................................. [RUN]
[0m11:42:23.070107 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m11:42:23.070314 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m11:42:23.072636 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m11:42:23.073197 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 11:42:23.070447 => 11:42:23.073081
[0m11:42:23.073412 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m11:42:23.075918 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m11:42:23.076341 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:42:23.076548 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m11:42:23.076746 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.083901 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.084207 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:42:23.084460 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

select
    /* IDs */
    contact_id,
    masterrecordid as master_record_id,
    accountid as account_id,
    reportstoid as reports_to_id,
    ownerid as owner_id,
    jigsawcontactid as jigsaw_contact_id,
    individualid as individual_id,

    /* Dates */
    birthdate as birth_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    lastcurequestdate as last_cu_request_date,
    lastcuupdatedate as last_cu_update_date,
    emailbounceddate as email_bounced_date,

    /* Dimensions */
    salutation,
    firstname as first_name,
    lastname as last_name,
    otherstreet as other_street,
    othercity as other_city,
    otherstate as other_state,
    otherpostalcode as other_postal_code,
    othercountry as other_country,
    otherlatitude as other_latitude,
    otherlongitude as other_longitude,
    othergeocodeaccuracy as other_geocode_accuracy,
    mailingstreet as mailing_street,
    mailingcity as mailing_city,
    mailingstate as mailing_state,
    mailingpostalcode as mailing_postal_code,
    mailingcountry as mailing_country,
    mailinglatitude as mailing_latitude,
    mailinglongitude as mailing_longitude,
    mailinggeocodeaccuracy as mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone as mobile_phone,
    homephone as home_phone,
    otherphone as other_phone,
    assistantphone as assistant_phone,
    email,
    title,
    department,
    assistantname as assistant_name,
    leadsource as lead_source,
    description,
    pronouns,
    genderidentity as gender_identity,
    cleanstatus as clean_status,
    emailbouncedreason as email_bounced_reason,
    level__c as level,
    languages__c as languages,

    /* Metrics */
    hasoptedoutofemail as has_opted_out_of_email,
    hasoptedoutoffax as has_opted_out_of_fax,
    donotcall as do_not_call

from "dbt"."staging"."stg_salesforce__contact"
where isdeleted = false
    );
  
  
[0m11:42:23.088395 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.090838 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:42:23.091107 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact" rename to "dim_contact__dbt_backup"
[0m11:42:23.091515 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.093439 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:42:23.093669 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m11:42:23.094029 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.094980 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m11:42:23.095179 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:42:23.095350 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m11:42:23.096935 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.099709 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:42:23.099939 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m11:42:23.100497 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.101388 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 11:42:23.073550 => 11:42:23.101292
[0m11:42:23.101605 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m11:42:23.123230 [info ] [Thread-1  ]: 19 of 30 OK created sql table model dim.dim_contact ............................ [[32mOK[0m in 0.05s]
[0m11:42:23.123735 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m11:42:23.123999 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m11:42:23.124297 [info ] [Thread-1  ]: 20 of 30 START sql table model dim.dim_lead .................................... [RUN]
[0m11:42:23.124697 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m11:42:23.124898 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m11:42:23.127337 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m11:42:23.127891 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 11:42:23.125039 => 11:42:23.127760
[0m11:42:23.128099 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m11:42:23.131091 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m11:42:23.131587 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:42:23.131788 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m11:42:23.131965 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.138811 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.139152 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:42:23.139465 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

select
    lead_id,
    masterrecordid as master_record_id,
    salutation as lead_salutation,
    firstname as first_name,
    lastname as last_name,
    title as lead_title,
    company as lead_company,
    street as lead_street,
    city as lead_city,
    state as lead_state,
    postalcode as lead_postal_code,
    country as lead_country,
    latitude as lead_latitude,
    longitude as lead_longitude,
    geocodeaccuracy as geocode_accuracy,
    phone as lead_phone,
    mobilephone as lead_mobile_phone,
    fax as lead_fax,
    email as lead_email,
    website as lead_website,
    description as lead_description,
    leadsource as lead_source,
    status as lead_status,
    industry as lead_industry,
    rating as lead_rating,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownerid as owner_id,
    hasoptedoutofemail as has_opted_out_of_email,
    isconverted as is_converted,
    converteddate as converted_date,
    convertedaccountid as converted_account_id,
    convertedcontactid as converted_contact_id,
    convertedopportunityid as converted_opportunity_id,
    isunreadbyowner as is_unread_by_owner,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    donotcall as do_not_call,
    hasoptedoutoffax as has_opted_out_of_fax,
    lasttransferdate as last_transfer_date,
    jigsaw as jigsaw_id,
    jigsawcontactid as jigsaw_contact_id,
    cleanstatus as clean_status,
    companydunsnumber as company_duns_number,
    dandbcompanyid as dandb_company_id,
    emailbouncedreason as email_bounced_reason,
    emailbounceddate as email_bounced_date,
    individualid as individual_id,
    pronouns as lead_pronouns,
    genderidentity as gender_identity,
    siccode__c as sic_code,
    productinterest__c as product_interest,
    primary__c as is_primary,
    currentgenerators__c as current_generators,
    numberoflocations__c as number_of_locations
from "dbt"."staging"."stg_salesforce__lead"
where isdeleted = false  -- Exclude deleted leads
order by last_name, first_name
    );
  
  
[0m11:42:23.144296 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.146695 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:42:23.146959 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead" rename to "dim_lead__dbt_backup"
[0m11:42:23.147518 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.149557 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:42:23.149768 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m11:42:23.150090 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.151033 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m11:42:23.151231 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:42:23.151401 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m11:42:23.152947 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.154933 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:42:23.155162 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m11:42:23.155752 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.156595 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 11:42:23.128245 => 11:42:23.156495
[0m11:42:23.156831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m11:42:23.178967 [info ] [Thread-1  ]: 20 of 30 OK created sql table model dim.dim_lead ............................... [[32mOK[0m in 0.05s]
[0m11:42:23.179373 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m11:42:23.179601 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m11:42:23.179995 [info ] [Thread-1  ]: 21 of 30 START sql table model dim.dim_opportunity ............................. [RUN]
[0m11:42:23.180497 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m11:42:23.180713 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m11:42:23.184139 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m11:42:23.184649 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 11:42:23.180857 => 11:42:23.184548
[0m11:42:23.184848 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m11:42:23.187784 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m11:42:23.188710 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:42:23.188943 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m11:42:23.189150 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.195846 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.196127 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:42:23.196369 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m11:42:23.200002 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.202262 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:42:23.202501 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m11:42:23.202910 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.204657 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:42:23.204858 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m11:42:23.205179 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.206190 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m11:42:23.206379 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:42:23.206553 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m11:42:23.207638 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.209325 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:42:23.209560 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m11:42:23.210077 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.210785 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 11:42:23.184979 => 11:42:23.210696
[0m11:42:23.210982 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m11:42:23.237155 [info ] [Thread-1  ]: 21 of 30 OK created sql table model dim.dim_opportunity ........................ [[32mOK[0m in 0.06s]
[0m11:42:23.237578 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m11:42:23.237875 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:42:23.238220 [info ] [Thread-1  ]: 22 of 30 START sql table model dim.dim_opportunity_stage ....................... [RUN]
[0m11:42:23.238594 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m11:42:23.238800 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:42:23.241774 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:42:23.242244 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 11:42:23.238939 => 11:42:23.242141
[0m11:42:23.242443 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:42:23.245146 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:42:23.245654 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:42:23.245853 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m11:42:23.246045 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.253105 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.253376 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:42:23.253594 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct stagename as stage_name, stagesortorder as stage_sort_order
        from "dbt"."staging"."stg_salesforce__opportunity"
    )

select
    row_number() over (order by stage_sort_order) as stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
from source
    );
  
  
[0m11:42:23.255999 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.258246 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:42:23.258482 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m11:42:23.258858 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.260774 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:42:23.261005 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m11:42:23.261333 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.262291 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m11:42:23.262488 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:42:23.262667 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m11:42:23.263256 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.264860 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:42:23.265077 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m11:42:23.265450 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.266159 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 11:42:23.242574 => 11:42:23.266067
[0m11:42:23.266377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m11:42:23.285694 [info ] [Thread-1  ]: 22 of 30 OK created sql table model dim.dim_opportunity_stage .................. [[32mOK[0m in 0.05s]
[0m11:42:23.286119 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:42:23.286360 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m11:42:23.286625 [info ] [Thread-1  ]: 23 of 30 START sql table model dim.dim_pricebook ............................... [RUN]
[0m11:42:23.286989 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m11:42:23.287179 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m11:42:23.289300 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m11:42:23.289724 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 11:42:23.287309 => 11:42:23.289629
[0m11:42:23.289913 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m11:42:23.293090 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m11:42:23.293723 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:42:23.293945 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m11:42:23.294132 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.300857 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.301120 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:42:23.301332 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

select
    pricebook_entry_id,
    pricebook2id as pricebook_id,
    product2id as product_id,
    unitprice as unit_price,
    isactive as is_active,
    usestandardprice as use_standard_price,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__pricebook_entry"
where isdeleted = false  -- Exclude deleted entries
order by pricebook_entry_id
    );
  
  
[0m11:42:23.302789 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.305004 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:42:23.305233 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook" rename to "dim_pricebook__dbt_backup"
[0m11:42:23.305598 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.307340 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:42:23.307532 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m11:42:23.307823 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.308762 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m11:42:23.308950 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:42:23.309195 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m11:42:23.309910 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.311527 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:42:23.311735 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m11:42:23.312142 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.312895 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 11:42:23.290039 => 11:42:23.312804
[0m11:42:23.313124 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m11:42:23.328981 [info ] [Thread-1  ]: 23 of 30 OK created sql table model dim.dim_pricebook .......................... [[32mOK[0m in 0.04s]
[0m11:42:23.329392 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m11:42:23.329634 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m11:42:23.329920 [info ] [Thread-1  ]: 24 of 30 START sql table model dim.dim_product ................................. [RUN]
[0m11:42:23.330291 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m11:42:23.330490 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m11:42:23.332658 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m11:42:23.333134 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 11:42:23.330626 => 11:42:23.333037
[0m11:42:23.333326 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m11:42:23.335943 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m11:42:23.336808 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:42:23.337091 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m11:42:23.337277 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.344452 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.344787 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:42:23.345045 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    productcode as product_code,
    description as product_description,
    isactive as is_active,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    family as product_family,
    externaldatasourceid as external_datasource_id,
    externalid as external_id,
    displayurl as display_url,
    quantityunitofmeasure as quantity_unit_of_measure,
    stockkeepingunit as stock_keeping_unit,
    type as product_type,
    productclass as product_class,
    sourceproductid as source_product_id,
    sellerid as seller_id
from "dbt"."staging"."stg_salesforce__product_2"
where isdeleted = false
-- isactive =1 ?
order by product_name
    );
  
  
[0m11:42:23.347034 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.350153 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:42:23.350419 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product" rename to "dim_product__dbt_backup"
[0m11:42:23.350767 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.352680 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:42:23.352903 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m11:42:23.353230 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.354234 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m11:42:23.354421 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:42:23.354591 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m11:42:23.355518 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.357572 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:42:23.357825 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m11:42:23.358334 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.359204 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 11:42:23.333454 => 11:42:23.359084
[0m11:42:23.359442 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m11:42:23.376015 [info ] [Thread-1  ]: 24 of 30 OK created sql table model dim.dim_product ............................ [[32mOK[0m in 0.05s]
[0m11:42:23.376444 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m11:42:23.376695 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m11:42:23.376988 [info ] [Thread-1  ]: 25 of 30 START sql table model dim.dim_record_type ............................. [RUN]
[0m11:42:23.377393 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m11:42:23.377613 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m11:42:23.379864 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m11:42:23.380312 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 11:42:23.377756 => 11:42:23.380214
[0m11:42:23.380509 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m11:42:23.383175 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m11:42:23.383660 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:42:23.383851 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m11:42:23.384024 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.390817 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.391069 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:42:23.391272 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

select
    record_type_id,
    name as record_type_name,
    modulenamespace as module_namespace,
    description as record_type_description,
    businessprocessid as business_process_id,
    sobjecttype as sobject_type,
    isactive as is_active,
    createdbyid as created_by_id,
    createddate as created_date,
    lastmodifiedbyid as last_modified_by_id,
    lastmodifieddate as last_modified_date,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__record_type"
where isdeleted = false
order by record_type_name
    );
  
  
[0m11:42:23.392785 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.395798 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:42:23.396034 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m11:42:23.396427 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.398196 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:42:23.398395 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m11:42:23.398699 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.399830 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m11:42:23.400060 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:42:23.400237 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m11:42:23.400943 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.402718 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:42:23.402928 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m11:42:23.403395 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.404206 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 11:42:23.380641 => 11:42:23.404109
[0m11:42:23.404416 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m11:42:23.421259 [info ] [Thread-1  ]: 25 of 30 OK created sql table model dim.dim_record_type ........................ [[32mOK[0m in 0.04s]
[0m11:42:23.421696 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m11:42:23.421936 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m11:42:23.422211 [info ] [Thread-1  ]: 26 of 30 START sql table model dim.dim_solution ................................ [RUN]
[0m11:42:23.422566 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m11:42:23.422753 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m11:42:23.424913 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m11:42:23.425609 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 11:42:23.422881 => 11:42:23.425511
[0m11:42:23.425804 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m11:42:23.428499 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m11:42:23.428988 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:42:23.429193 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m11:42:23.429366 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.436139 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.436431 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:42:23.436641 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solutionnumber as solution_number,
    solutionname as solution_name,
    ispublished as is_published,
    ispublishedinpublickb as is_published_in_public_kb,
    status as solution_status,
    isreviewed as is_reviewed,
    solutionnote as solution_note,
    caseid as case_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    timesused as times_used,
    ishtml as is_html
from "dbt"."staging"."stg_salesforce__solution"
where isdeleted = false
order by solution_name
    );
  
  
[0m11:42:23.438468 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.440890 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:42:23.441171 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m11:42:23.441542 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.444170 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:42:23.444388 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m11:42:23.444727 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.445652 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m11:42:23.445840 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:42:23.446014 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m11:42:23.446866 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.448783 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:42:23.449015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m11:42:23.449565 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.450404 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 11:42:23.425933 => 11:42:23.450306
[0m11:42:23.450619 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m11:42:23.468131 [info ] [Thread-1  ]: 26 of 30 OK created sql table model dim.dim_solution ........................... [[32mOK[0m in 0.05s]
[0m11:42:23.468559 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m11:42:23.468831 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m11:42:23.469125 [info ] [Thread-1  ]: 27 of 30 START sql table model dim.dim_user .................................... [RUN]
[0m11:42:23.469522 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m11:42:23.469716 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m11:42:23.472070 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m11:42:23.472730 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 11:42:23.469853 => 11:42:23.472614
[0m11:42:23.472945 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m11:42:23.475632 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m11:42:23.476052 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:42:23.476238 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m11:42:23.476403 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.482898 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.483186 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:42:23.483417 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m11:42:23.487656 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.489981 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:42:23.490244 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user" rename to "dim_user__dbt_backup"
[0m11:42:23.490590 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.492444 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:42:23.492647 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m11:42:23.492954 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.494704 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m11:42:23.494903 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:42:23.495073 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m11:42:23.496024 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.497823 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:42:23.498028 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m11:42:23.498536 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.499333 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 11:42:23.473081 => 11:42:23.499235
[0m11:42:23.499570 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m11:42:23.518492 [info ] [Thread-1  ]: 27 of 30 OK created sql table model dim.dim_user ............................... [[32mOK[0m in 0.05s]
[0m11:42:23.518948 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m11:42:23.519221 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m11:42:23.519585 [info ] [Thread-1  ]: 28 of 30 START sql table model fact.fact_campaign_performance .................. [RUN]
[0m11:42:23.519966 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_campaign_performance)
[0m11:42:23.520166 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m11:42:23.522526 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:42:23.523529 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 11:42:23.520334 => 11:42:23.523424
[0m11:42:23.523734 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m11:42:23.526262 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:42:23.527074 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:42:23.527412 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: BEGIN
[0m11:42:23.527719 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.534119 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.534394 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:42:23.534622 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */

  
    
    

    create  table
      "dbt"."fact"."fact_campaign_performance__dbt_tmp"
  
    as (
      

WITH campaign_performance AS (
    SELECT
        c.campaign_id,
        c.created_at,
        c.last_modified_at,
        c.start_date,
        c.end_date,
        c.owner_id as user_id,
        o.opportunity_id,
        c.expected_revenue,
        c.budgeted_cost,
        c.actual_cost,
        c.number_sent,
        c.number_of_leads,
        c.number_of_converted_leads,
        c.number_of_contacts
    FROM "dbt"."dim"."dim_campaign" c
    LEFT JOIN "dbt"."dim"."dim_opportunity" o ON c.campaign_id = o.campaign_id
)

SELECT
    cp.campaign_id,
    cp.user_id,
    cp.opportunity_id,
    cp.start_date,
    cp.end_date,
    cp.expected_revenue,
    cp.budgeted_cost,
    cp.actual_cost,
    cp.number_sent,
    cp.number_of_leads,
    cp.number_of_converted_leads,
    cp.number_of_contacts
FROM campaign_performance cp


--need to be improved - No of leads closed based a particular campaign
    );
  
  
[0m11:42:23.536332 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.538558 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:42:23.538795 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
alter table "dbt"."fact"."fact_campaign_performance" rename to "fact_campaign_performance__dbt_backup"
[0m11:42:23.539215 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.541021 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:42:23.541231 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
alter table "dbt"."fact"."fact_campaign_performance__dbt_tmp" rename to "fact_campaign_performance"
[0m11:42:23.541542 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.542472 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: COMMIT
[0m11:42:23.542658 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:42:23.542831 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: COMMIT
[0m11:42:23.543532 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.545760 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:42:23.545959 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
drop table if exists "dbt"."fact"."fact_campaign_performance__dbt_backup" cascade
[0m11:42:23.546348 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.547032 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 11:42:23.523863 => 11:42:23.546942
[0m11:42:23.547225 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: Close
[0m11:42:23.562906 [info ] [Thread-1  ]: 28 of 30 OK created sql table model fact.fact_campaign_performance ............. [[32mOK[0m in 0.04s]
[0m11:42:23.563315 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m11:42:23.563569 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_lead_conversion
[0m11:42:23.563866 [info ] [Thread-1  ]: 29 of 30 START sql table model fact.fact_lead_conversion ....................... [RUN]
[0m11:42:23.564250 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_lead_conversion)
[0m11:42:23.564451 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_lead_conversion
[0m11:42:23.566976 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:42:23.567446 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (compile): 11:42:23.564588 => 11:42:23.567347
[0m11:42:23.567643 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_lead_conversion
[0m11:42:23.570202 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:42:23.570669 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:42:23.570933 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: BEGIN
[0m11:42:23.571126 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.578134 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.578448 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:42:23.578695 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */

  
    
    

    create  table
      "dbt"."fact"."fact_lead_conversion__dbt_tmp"
  
    as (
      

with lead_data as (
    select 
        lead_id,
        lead_salutation,
        first_name,
        last_name,
        lead_company,
        lead_source,
        lead_status,
        is_converted,
        converted_date,
        converted_account_id,
        converted_contact_id,
        converted_opportunity_id,
        owner_id,  -- Assuming this is the user ID
        created_date as lead_created_date,
        last_modified_date as lead_last_modified_date
    from 
        "dbt"."dim"."dim_lead"
    where 
        is_converted = true
),

account_data as (
    select 
        account_id,
        account_name
    from 
        "dbt"."dim"."dim_account"
),

opportunity_data as (
    select 
        opportunity_id,
        opportunity_name,
        opportunity_amount
    from 
        "dbt"."dim"."dim_opportunity"
),

user_data as (
    select 
        user_id,
        user_name,
        email as user_email
    from 
        "dbt"."dim"."dim_user"
)

select 
    l.lead_id,
    l.first_name,
    l.last_name,
    l.lead_company,
    l.lead_source,
    l.lead_status,
    l.converted_date,
    a.account_name as converted_account_name,
    o.opportunity_name as converted_opportunity_name,
    o.opportunity_amount as converted_opportunity_amount,
    u.user_name as lead_owner_name,
    u.user_email as lead_owner_email,
    l.lead_created_date,
    l.lead_last_modified_date
from 
    lead_data l
left join 
    account_data a on l.converted_account_id = a.account_id
left join 
    opportunity_data o on l.converted_opportunity_id = o.opportunity_id
left join 
    user_data u on l.owner_id = u.user_id
    );
  
  
[0m11:42:23.580862 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.582987 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:42:23.583208 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter view "dbt"."fact"."fact_lead_conversion" rename to "fact_lead_conversion__dbt_backup"
[0m11:42:23.583534 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.585242 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:42:23.585443 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter table "dbt"."fact"."fact_lead_conversion__dbt_tmp" rename to "fact_lead_conversion"
[0m11:42:23.585746 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.586663 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m11:42:23.586845 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:42:23.587015 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m11:42:23.587539 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.589094 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:42:23.589482 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
drop view if exists "dbt"."fact"."fact_lead_conversion__dbt_backup" cascade
[0m11:42:23.589923 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.590721 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (execute): 11:42:23.567771 => 11:42:23.590627
[0m11:42:23.590936 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: Close
[0m11:42:23.604876 [info ] [Thread-1  ]: 29 of 30 OK created sql table model fact.fact_lead_conversion .................. [[32mOK[0m in 0.04s]
[0m11:42:23.605283 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_lead_conversion
[0m11:42:23.605524 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m11:42:23.605797 [info ] [Thread-1  ]: 30 of 30 START sql incremental model fact.fact_opportunity ..................... [RUN]
[0m11:42:23.606142 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_lead_conversion, now model.elastic_dbt_interview.fact_opportunity)
[0m11:42:23.606336 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m11:42:23.613246 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m11:42:23.614111 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 11:42:23.606468 => 11:42:23.613944
[0m11:42:23.614402 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m11:42:23.632519 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:42:23.632872 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240822114223629890"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        a.account_id,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m11:42:23.633095 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:23.642537 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.647020 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:42:23.647242 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m11:42:23.647518 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.647696 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:42:23.647880 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240822114223629890'
      
      
      order by ordinal_position

  
[0m11:42:23.675043 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.678352 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:42:23.678591 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m11:42:23.692994 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.698042 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:42:23.698308 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m11:42:23.713442 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.722239 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m11:42:23.723666 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:42:23.723974 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240822114223629890"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240822114223629890"
    )
  
[0m11:42:23.725495 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.726615 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m11:42:23.726855 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:42:23.727038 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m11:42:23.727369 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:42:23.727819 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 11:42:23.614548 => 11:42:23.727726
[0m11:42:23.728017 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m11:42:23.730782 [info ] [Thread-1  ]: 30 of 30 OK created sql incremental model fact.fact_opportunity ................ [[32mOK[0m in 0.12s]
[0m11:42:23.731159 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m11:42:23.732103 [debug] [MainThread]: Using duckdb connection "master"
[0m11:42:23.732313 [debug] [MainThread]: On master: BEGIN
[0m11:42:23.732472 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:42:23.739868 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:42:23.740271 [debug] [MainThread]: On master: COMMIT
[0m11:42:23.740471 [debug] [MainThread]: Using duckdb connection "master"
[0m11:42:23.740841 [debug] [MainThread]: On master: COMMIT
[0m11:42:23.741285 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:42:23.741472 [debug] [MainThread]: On master: Close
[0m11:42:23.744311 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:42:23.744515 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m11:42:23.744770 [info ] [MainThread]: 
[0m11:42:23.744968 [info ] [MainThread]: Finished running 15 table models, 14 view models, 1 incremental model in 0 hours 0 minutes and 1.93 seconds (1.93s).
[0m11:42:23.746961 [debug] [MainThread]: Command end result
[0m11:42:23.755505 [info ] [MainThread]: 
[0m11:42:23.755897 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:42:23.756185 [info ] [MainThread]: 
[0m11:42:23.756441 [info ] [MainThread]: Done. PASS=30 WARN=0 ERROR=0 SKIP=0 TOTAL=30
[0m11:42:23.756796 [debug] [MainThread]: Command `dbt run` succeeded at 11:42:23.756744 after 2.10 seconds
[0m11:42:23.756994 [debug] [MainThread]: Flushing usage events


============================== 11:43:46.781845 | a28c2913-41f3-4443-82f9-16f41c0af417 ==============================
[0m11:43:46.781845 [info ] [MainThread]: Running with dbt=1.6.18
[0m11:43:46.785280 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'False'}
[0m11:43:46.785549 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m11:43:46.871532 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m11:43:46.892039 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m11:43:46.943896 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:43:46.944429 [debug] [MainThread]: Partial parsing: updated file: elastic_dbt_interview://models/facts/fact_lead_conversion.sql
[0m11:43:46.973820 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.marts
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
[0m11:43:46.992275 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m11:43:46.994293 [info ] [MainThread]: 
[0m11:43:46.995191 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:43:46.996971 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m11:43:47.005781 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:43:47.006113 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:43:47.006309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:43:47.014526 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:43:47.015303 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:43:47.015731 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:43:47.020853 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:43:47.021132 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:43:47.021294 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:43:47.024451 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:43:47.024647 [debug] [MainThread]: Connection 'list_dbt' was properly closed.
[0m11:43:47.024816 [info ] [MainThread]: 
[0m11:43:47.025009 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m11:43:47.025266 [error] [MainThread]: Encountered an error:
Runtime Error
  IO Error: Could not set lock on file "/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/./dbt.duckdb": Conflicting lock is held in /Applications/DataGrip.app/Contents/jbr/Contents/Home/bin/java (PID 49761) by user sunjay.nair. See also https://duckdb.org/docs/connect/concurrency
[0m11:43:47.025650 [debug] [MainThread]: Command `dbt run` failed at 11:43:47.025602 after 0.27 seconds
[0m11:43:47.025841 [debug] [MainThread]: Flushing usage events


============================== 11:43:59.128255 | d9eede3e-e5b1-408f-a643-e76fc1d49988 ==============================
[0m11:43:59.128255 [info ] [MainThread]: Running with dbt=1.6.18
[0m11:43:59.131083 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/sunjay.nair/Documents/GitHub/dbt_sunjay/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'False'}
[0m11:43:59.131351 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m11:43:59.211099 [info ] [MainThread]: Registered adapter: duckdb=1.6.2
[0m11:43:59.230235 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m11:43:59.276313 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:43:59.276727 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:43:59.277735 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.elastic_dbt_interview.reporting
- models.elastic_dbt_interview.intermediate
- models.elastic_dbt_interview.marts
[0m11:43:59.292155 [info ] [MainThread]: Found 31 models, 1 seed, 2 tests, 14 sources, 0 exposures, 0 metrics, 604 macros, 0 groups, 0 semantic models
[0m11:43:59.293889 [info ] [MainThread]: 
[0m11:43:59.294345 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:43:59.295653 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_dbt'
[0m11:43:59.302909 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:43:59.303212 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:43:59.303392 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:43:59.312843 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.313755 [debug] [ThreadPool]: On list_dbt: Close
[0m11:43:59.317499 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:43:59.317834 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:43:59.318026 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:43:59.325195 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.325991 [debug] [ThreadPool]: On list_dbt: Close
[0m11:43:59.329195 [debug] [ThreadPool]: Using duckdb connection "list_dbt"
[0m11:43:59.329515 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"dbt"'
    
  
  
[0m11:43:59.329759 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:43:59.336840 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.337804 [debug] [ThreadPool]: On list_dbt: Close
[0m11:43:59.340228 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt, now create_dbt_staging)
[0m11:43:59.340674 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "staging"
"
[0m11:43:59.343544 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:43:59.343741 [debug] [ThreadPool]: On create_dbt_staging: BEGIN
[0m11:43:59.343893 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:43:59.351554 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.351872 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:43:59.352041 [debug] [ThreadPool]: On create_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_staging"} */
create schema if not exists "dbt"."staging"
[0m11:43:59.352293 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.352820 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m11:43:59.352993 [debug] [ThreadPool]: Using duckdb connection "create_dbt_staging"
[0m11:43:59.353140 [debug] [ThreadPool]: On create_dbt_staging: COMMIT
[0m11:43:59.353370 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.353534 [debug] [ThreadPool]: On create_dbt_staging: Close
[0m11:43:59.355747 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_staging, now create_dbt_dim)
[0m11:43:59.356245 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "dim"
"
[0m11:43:59.358039 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:43:59.358268 [debug] [ThreadPool]: On create_dbt_dim: BEGIN
[0m11:43:59.358427 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:43:59.365606 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.365881 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:43:59.366149 [debug] [ThreadPool]: On create_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_dim"} */
create schema if not exists "dbt"."dim"
[0m11:43:59.366546 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.367122 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m11:43:59.367305 [debug] [ThreadPool]: Using duckdb connection "create_dbt_dim"
[0m11:43:59.367461 [debug] [ThreadPool]: On create_dbt_dim: COMMIT
[0m11:43:59.367702 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.367874 [debug] [ThreadPool]: On create_dbt_dim: Close
[0m11:43:59.369851 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_dim, now create_dbt_fact)
[0m11:43:59.370349 [debug] [ThreadPool]: Creating schema "database: "dbt"
schema: "fact"
"
[0m11:43:59.372698 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:43:59.372912 [debug] [ThreadPool]: On create_dbt_fact: BEGIN
[0m11:43:59.373069 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:43:59.380302 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.380661 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:43:59.380833 [debug] [ThreadPool]: On create_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "create_dbt_fact"} */
create schema if not exists "dbt"."fact"
[0m11:43:59.381105 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.381620 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m11:43:59.381808 [debug] [ThreadPool]: Using duckdb connection "create_dbt_fact"
[0m11:43:59.381966 [debug] [ThreadPool]: On create_dbt_fact: COMMIT
[0m11:43:59.382197 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.382381 [debug] [ThreadPool]: On create_dbt_fact: Close
[0m11:43:59.385890 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_dbt_fact, now list_dbt_staging)
[0m11:43:59.389545 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m11:43:59.389948 [debug] [ThreadPool]: On list_dbt_staging: BEGIN
[0m11:43:59.390198 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:43:59.399167 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.399485 [debug] [ThreadPool]: Using duckdb connection "list_dbt_staging"
[0m11:43:59.399681 [debug] [ThreadPool]: On list_dbt_staging: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_staging"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'staging'
    and table_catalog = 'dbt'
  
[0m11:43:59.420996 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.422246 [debug] [ThreadPool]: On list_dbt_staging: ROLLBACK
[0m11:43:59.423596 [debug] [ThreadPool]: Failed to rollback 'list_dbt_staging'
[0m11:43:59.423857 [debug] [ThreadPool]: On list_dbt_staging: Close
[0m11:43:59.426566 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_staging, now list_dbt_dim)
[0m11:43:59.429296 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m11:43:59.429516 [debug] [ThreadPool]: On list_dbt_dim: BEGIN
[0m11:43:59.429671 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:43:59.436314 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.436598 [debug] [ThreadPool]: Using duckdb connection "list_dbt_dim"
[0m11:43:59.436779 [debug] [ThreadPool]: On list_dbt_dim: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_dim"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'dim'
    and table_catalog = 'dbt'
  
[0m11:43:59.453651 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.457552 [debug] [ThreadPool]: On list_dbt_dim: ROLLBACK
[0m11:43:59.457855 [debug] [ThreadPool]: Failed to rollback 'list_dbt_dim'
[0m11:43:59.458030 [debug] [ThreadPool]: On list_dbt_dim: Close
[0m11:43:59.460692 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_dim, now list_dbt_fact)
[0m11:43:59.463432 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m11:43:59.463669 [debug] [ThreadPool]: On list_dbt_fact: BEGIN
[0m11:43:59.463830 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:43:59.471348 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.471652 [debug] [ThreadPool]: Using duckdb connection "list_dbt_fact"
[0m11:43:59.471841 [debug] [ThreadPool]: On list_dbt_fact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_fact"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'fact'
    and table_catalog = 'dbt'
  
[0m11:43:59.488901 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.492820 [debug] [ThreadPool]: On list_dbt_fact: ROLLBACK
[0m11:43:59.493160 [debug] [ThreadPool]: Failed to rollback 'list_dbt_fact'
[0m11:43:59.493329 [debug] [ThreadPool]: On list_dbt_fact: Close
[0m11:43:59.496000 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_dbt_fact, now list_dbt_main)
[0m11:43:59.498259 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m11:43:59.498501 [debug] [ThreadPool]: On list_dbt_main: BEGIN
[0m11:43:59.498668 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:43:59.505477 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.505764 [debug] [ThreadPool]: Using duckdb connection "list_dbt_main"
[0m11:43:59.505936 [debug] [ThreadPool]: On list_dbt_main: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "connection_name": "list_dbt_main"} */
select
      'dbt' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'dbt'
  
[0m11:43:59.522078 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:43:59.525486 [debug] [ThreadPool]: On list_dbt_main: ROLLBACK
[0m11:43:59.525736 [debug] [ThreadPool]: Failed to rollback 'list_dbt_main'
[0m11:43:59.525893 [debug] [ThreadPool]: On list_dbt_main: Close
[0m11:43:59.529308 [debug] [MainThread]: Using duckdb connection "master"
[0m11:43:59.529593 [debug] [MainThread]: On master: BEGIN
[0m11:43:59.529759 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:43:59.537155 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:43:59.537373 [debug] [MainThread]: On master: COMMIT
[0m11:43:59.537529 [debug] [MainThread]: Using duckdb connection "master"
[0m11:43:59.537676 [debug] [MainThread]: On master: COMMIT
[0m11:43:59.537864 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:43:59.538021 [debug] [MainThread]: On master: Close
[0m11:43:59.539963 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:43:59.540290 [info ] [MainThread]: 
[0m11:43:59.542821 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_date
[0m11:43:59.543117 [info ] [Thread-1  ]: 1 of 30 START sql table model dim.dim_date ..................................... [RUN]
[0m11:43:59.543511 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_dbt_main, now model.elastic_dbt_interview.dim_date)
[0m11:43:59.543703 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_date
[0m11:43:59.570638 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:43:59.570998 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: BEGIN
[0m11:43:59.571185 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:43:59.577935 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.578211 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:43:59.578405 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */


        select 
        ((cast('2030-12-31' as TIMESTAMP))::date - (cast('2000-01-01' as TIMESTAMP))::date)
    
[0m11:43:59.578766 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.619599 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_date"
[0m11:43:59.621609 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (compile): 11:43:59.543845 => 11:43:59.621416
[0m11:43:59.621898 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_date
[0m11:43:59.665948 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_date"
[0m11:43:59.667167 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:43:59.667720 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */

  
    
    

    create  table
      "dbt"."dim"."dim_date__dbt_tmp"
  
    as (
      

WITH date_range AS (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
    
    

    )

    select *
    from unioned
    where generated_number <= 11322
    order by generated_number



),

all_periods as (

    select (
        

    cast('2000-01-01' as TIMESTAMP) + ((interval '1 day') * ((row_number() over (order by 1) - 1)))


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as TIMESTAMP)

)

select * from filtered



)
select
    cast(d.date_day as TIMESTAMP) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    d.date_day + ((interval '1 year') * (-1))

 as date) as prior_year_date_day,
        cast(

    d.date_day + ((interval '1 day') * (-364))

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    d.date_day + ((interval '1 day') * (-1))

 as date) as prior_date_day,
    cast(

    d.date_day + ((interval '1 day') * (1))

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    -- Sunday(1) to Saturday (7)
        cast(date_part('dow', d.date_day) + 1 as INT) as day_of_week,
    -- Monday(1) to Sunday (7)
        cast(date_part('isodow', d.date_day) as INT) as day_of_week_iso,
    dayname(d.date_day) as day_of_week_name,
    substr(dayname(d.date_day), 1, 3) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as week_end_date,
    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) as prior_year_week_start_date,
    cast(

    -- Sunday as week start date
cast(

    date_trunc('week', 

    d.prior_year_over_year_date_day + ((interval '1 day') * (1))

) + ((interval '1 day') * (-1))

 as date) + ((interval '1 day') * (6))

 as date) as prior_year_week_end_date,
    cast(ceil(dayofyear(d.date_day) / 7) as int) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    cast(date_trunc('week', d.date_day) as date) + ((interval '1 day') * (6))

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) + ((interval '1 day') * (6))

 as date) as prior_year_iso_week_end_date,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.date_day) as INT) as iso_week_of_year,

    cast(ceil(dayofyear(d.prior_year_over_year_date_day) / 7) as int) as prior_year_week_of_year,
    -- postgresql week is isoweek, the first week of a year containing January 4 of that year.
cast(date_part('week', d.prior_year_over_year_date_day) as INT) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as INT) as month_of_year,
    monthname(d.date_day)  as month_name,
    substr(monthname(d.date_day), 1, 3)  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    

    date_trunc('month', d.prior_year_date_day) + ((interval '1 month') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as INT) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(-- duckdb dateadd does not support quarter interval.
    cast(
        

    

    date_trunc('quarter', d.date_day) + ((interval '1 month') * (3))

 + ((interval '1 day') * (-1))


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as INT) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    

    date_trunc('year', d.date_day) + ((interval '1 year') * (1))

 + ((interval '1 day') * (-1))


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

SELECT 
    date_day,
    EXTRACT(YEAR FROM date_day) * 10000 + EXTRACT(MONTH FROM date_day) * 100 + EXTRACT(DAY FROM date_day) AS date_key,
    *
FROM date_range
ORDER BY date_day
    );
  
  
[0m11:43:59.715632 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.719820 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:43:59.720147 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date" rename to "dim_date__dbt_backup"
[0m11:43:59.720601 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.722865 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:43:59.723104 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
alter table "dbt"."dim"."dim_date__dbt_tmp" rename to "dim_date"
[0m11:43:59.723541 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.734837 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m11:43:59.735154 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:43:59.735376 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: COMMIT
[0m11:43:59.740384 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.743661 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_date"
[0m11:43:59.743887 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_date"} */
drop table if exists "dbt"."dim"."dim_date__dbt_backup" cascade
[0m11:43:59.744515 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.745393 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_date (execute): 11:43:59.622053 => 11:43:59.745296
[0m11:43:59.745627 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_date: Close
[0m11:43:59.786038 [info ] [Thread-1  ]: 1 of 30 OK created sql table model dim.dim_date ................................ [[32mOK[0m in 0.24s]
[0m11:43:59.786483 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_date
[0m11:43:59.786810 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__account
[0m11:43:59.787249 [info ] [Thread-1  ]: 2 of 30 START sql view model staging.stg_salesforce__account ................... [RUN]
[0m11:43:59.787719 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_date, now model.elastic_dbt_interview.stg_salesforce__account)
[0m11:43:59.787938 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__account
[0m11:43:59.790709 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:43:59.791604 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (compile): 11:43:59.788077 => 11:43:59.791456
[0m11:43:59.791870 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__account
[0m11:43:59.802230 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:43:59.803061 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:43:59.803348 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: BEGIN
[0m11:43:59.803552 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:43:59.810673 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.810936 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:43:59.811183 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */

  
  create view "dbt"."staging"."stg_salesforce__account__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."account"

),

renamed as (

    select
        id as account_id,
        isdeleted,
        masterrecordid,
        name,
        type,
        parentid,
        billingstreet,
        billingcity,
        billingstate,
        billingpostalcode,
        billingcountry,
        billinglatitude,
        billinglongitude,
        billinggeocodeaccuracy,
        shippingstreet,
        shippingcity,
        shippingstate,
        shippingpostalcode,
        shippingcountry,
        shippinglatitude,
        shippinglongitude,
        shippinggeocodeaccuracy,
        phone,
        fax,
        accountnumber,
        website,
        sic,
        industry,
        annualrevenue,
        numberofemployees,
        ownership,
        tickersymbol,
        description,
        rating,
        site,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        jigsaw,
        jigsawcompanyid,
        cleanstatus,
        accountsource,
        dunsnumber,
        tradestyle,
        naicscode,
        naicsdesc,
        yearstarted,
        sicdesc,
        dandbcompanyid,
        operatinghoursid,
        customerpriority__c,
        sla__c,
        active__c,
        numberoflocations__c,
        upsellopportunity__c,
        slaserialnumber__c,
        slaexpirationdate__c

    from source

)

select * from renamed
  );

[0m11:43:59.811992 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.814138 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:43:59.814350 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account" rename to "stg_salesforce__account__dbt_backup"
[0m11:43:59.814658 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.816390 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:43:59.816603 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
alter view "dbt"."staging"."stg_salesforce__account__dbt_tmp" rename to "stg_salesforce__account"
[0m11:43:59.816944 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.817789 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m11:43:59.817978 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:43:59.818149 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: COMMIT
[0m11:43:59.818846 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.820862 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__account"
[0m11:43:59.821082 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__account"} */
drop view if exists "dbt"."staging"."stg_salesforce__account__dbt_backup" cascade
[0m11:43:59.821494 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.822276 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__account (execute): 11:43:59.792016 => 11:43:59.822181
[0m11:43:59.822489 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__account: Close
[0m11:43:59.837222 [info ] [Thread-1  ]: 2 of 30 OK created sql view model staging.stg_salesforce__account .............. [[32mOK[0m in 0.05s]
[0m11:43:59.837645 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__account
[0m11:43:59.837887 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:43:59.838212 [info ] [Thread-1  ]: 3 of 30 START sql view model staging.stg_salesforce__campaign .................. [RUN]
[0m11:43:59.838569 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__account, now model.elastic_dbt_interview.stg_salesforce__campaign)
[0m11:43:59.838763 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:43:59.841593 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:43:59.842066 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (compile): 11:43:59.838892 => 11:43:59.841965
[0m11:43:59.842271 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:43:59.844897 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:43:59.845379 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:43:59.845575 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: BEGIN
[0m11:43:59.845755 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:43:59.852553 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.852827 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:43:59.853059 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */

  
  create view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."campaign"

),

renamed as (

    select
        id as campaign_id,
        isdeleted,
        name,
        parentid,
        type,
        status,
        startdate,
        enddate,
        expectedrevenue,
        budgetedcost,
        actualcost,
        expectedresponse,
        numbersent,
        isactive,
        description,
        numberofleads,
        numberofconvertedleads,
        numberofcontacts,
        numberofresponses,
        numberofopportunities,
        numberofwonopportunities,
        amountallopportunities,
        amountwonopportunities,
        hierarchynumberofleads,
        hierarchynumberofconvertedleads,
        hierarchynumberofcontacts,
        hierarchynumberofresponses,
        hierarchynumberofopportunities,
        hierarchynumberofwonopportunities,
        hierarchyamountallopportunities,
        hierarchyamountwonopportunities,
        hierarchynumbersent,
        hierarchyexpectedrevenue,
        hierarchybudgetedcost,
        hierarchyactualcost,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        campaignmemberrecordtypeid

    from source

)

select * from renamed
  );

[0m11:43:59.853823 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.856054 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:43:59.856271 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign" rename to "stg_salesforce__campaign__dbt_backup"
[0m11:43:59.856585 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.858324 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:43:59.858623 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
alter view "dbt"."staging"."stg_salesforce__campaign__dbt_tmp" rename to "stg_salesforce__campaign"
[0m11:43:59.859135 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.860151 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m11:43:59.860359 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:43:59.860538 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: COMMIT
[0m11:43:59.861207 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.863054 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__campaign"
[0m11:43:59.863294 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__campaign"} */
drop view if exists "dbt"."staging"."stg_salesforce__campaign__dbt_backup" cascade
[0m11:43:59.863680 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.864408 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__campaign (execute): 11:43:59.842404 => 11:43:59.864315
[0m11:43:59.864616 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__campaign: Close
[0m11:43:59.878547 [info ] [Thread-1  ]: 3 of 30 OK created sql view model staging.stg_salesforce__campaign ............. [[32mOK[0m in 0.04s]
[0m11:43:59.878934 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__campaign
[0m11:43:59.879159 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case
[0m11:43:59.879602 [info ] [Thread-1  ]: 4 of 30 START sql view model staging.stg_salesforce__case ...................... [RUN]
[0m11:43:59.880109 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__campaign, now model.elastic_dbt_interview.stg_salesforce__case)
[0m11:43:59.880331 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case
[0m11:43:59.882470 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:43:59.883178 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (compile): 11:43:59.880470 => 11:43:59.883067
[0m11:43:59.883380 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case
[0m11:43:59.886892 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:43:59.887734 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:43:59.888036 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: BEGIN
[0m11:43:59.888228 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:43:59.895020 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.895328 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:43:59.895558 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */

  
  create view "dbt"."staging"."stg_salesforce__case__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case"

),

renamed as (

    select
        id as case_id,
        isdeleted,
        masterrecordid,
        casenumber,
        contactid,
        accountid,
        assetid,
        productid,
        entitlementid,
        sourceid,
        businesshoursid,
        parentid,
        suppliedname,
        suppliedemail,
        suppliedphone,
        suppliedcompany,
        type,
        status,
        reason,
        origin,
        subject,
        priority,
        description,
        isclosed,
        closeddate,
        isescalated,
        ownerid,
        isclosedoncreate,
        slastartdate,
        slaexitdate,
        isstopped,
        stopstartdate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        servicecontractid,
        eventsprocesseddate,
        engineeringreqnumber__c,
        slaviolation__c,
        product__c,
        potentialliability__c

    from source

)

select * from renamed
  );

[0m11:43:59.896247 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.898721 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:43:59.899040 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case" rename to "stg_salesforce__case__dbt_backup"
[0m11:43:59.899464 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.901500 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:43:59.901719 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
alter view "dbt"."staging"."stg_salesforce__case__dbt_tmp" rename to "stg_salesforce__case"
[0m11:43:59.902035 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.902935 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m11:43:59.903130 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:43:59.903301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: COMMIT
[0m11:43:59.903873 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.905488 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case"
[0m11:43:59.905687 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case"} */
drop view if exists "dbt"."staging"."stg_salesforce__case__dbt_backup" cascade
[0m11:43:59.906031 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.906860 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case (execute): 11:43:59.883514 => 11:43:59.906761
[0m11:43:59.907108 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case: Close
[0m11:43:59.920794 [info ] [Thread-1  ]: 4 of 30 OK created sql view model staging.stg_salesforce__case ................. [[32mOK[0m in 0.04s]
[0m11:43:59.921208 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case
[0m11:43:59.921441 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:43:59.921816 [info ] [Thread-1  ]: 5 of 30 START sql view model staging.stg_salesforce__case_history_2 ............ [RUN]
[0m11:43:59.922294 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case, now model.elastic_dbt_interview.stg_salesforce__case_history_2)
[0m11:43:59.922498 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:43:59.924511 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:43:59.924958 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (compile): 11:43:59.922639 => 11:43:59.924863
[0m11:43:59.925150 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:43:59.928739 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:43:59.929685 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:43:59.929974 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: BEGIN
[0m11:43:59.930162 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:43:59.936865 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.937167 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:43:59.937377 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */

  
  create view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."case_history_2"

),

renamed as (

    select
        id as case_history_id,
        caseid,
        ownerid,
        status,
        previousupdate,
        lastmodifieddate,
        lastmodifiedbyid,
        isdeleted,
        systemmodstamp

    from source

)

select * from renamed
  );

[0m11:43:59.937816 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.940152 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:43:59.940457 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2" rename to "stg_salesforce__case_history_2__dbt_backup"
[0m11:43:59.940857 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.942892 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:43:59.943106 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
alter view "dbt"."staging"."stg_salesforce__case_history_2__dbt_tmp" rename to "stg_salesforce__case_history_2"
[0m11:43:59.943404 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.944313 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m11:43:59.944510 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:43:59.944688 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: COMMIT
[0m11:43:59.945213 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.946823 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__case_history_2"
[0m11:43:59.947025 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__case_history_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__case_history_2__dbt_backup" cascade
[0m11:43:59.947368 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.948072 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__case_history_2 (execute): 11:43:59.925275 => 11:43:59.947981
[0m11:43:59.948273 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__case_history_2: Close
[0m11:43:59.963010 [info ] [Thread-1  ]: 5 of 30 OK created sql view model staging.stg_salesforce__case_history_2 ....... [[32mOK[0m in 0.04s]
[0m11:43:59.963464 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__case_history_2
[0m11:43:59.963728 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:43:59.964082 [info ] [Thread-1  ]: 6 of 30 START sql view model staging.stg_salesforce__contact ................... [RUN]
[0m11:43:59.964488 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__case_history_2, now model.elastic_dbt_interview.stg_salesforce__contact)
[0m11:43:59.964689 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:43:59.966745 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:43:59.967347 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (compile): 11:43:59.964820 => 11:43:59.967247
[0m11:43:59.967548 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:43:59.970210 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:43:59.970642 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:43:59.970845 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: BEGIN
[0m11:43:59.971108 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:43:59.977707 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.977981 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:43:59.978210 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */

  
  create view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."contact"

),

renamed as (

    select
        id as contact_id,
        isdeleted,
        masterrecordid,
        accountid,
        salutation,
        firstname,
        lastname,
        otherstreet,
        othercity,
        otherstate,
        otherpostalcode,
        othercountry,
        otherlatitude,
        otherlongitude,
        othergeocodeaccuracy,
        mailingstreet,
        mailingcity,
        mailingstate,
        mailingpostalcode,
        mailingcountry,
        mailinglatitude,
        mailinglongitude,
        mailinggeocodeaccuracy,
        phone,
        fax,
        mobilephone,
        homephone,
        otherphone,
        assistantphone,
        reportstoid,
        email,
        title,
        department,
        assistantname,
        leadsource,
        birthdate,
        description,
        ownerid,
        hasoptedoutofemail,
        hasoptedoutoffax,
        donotcall,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        lastcurequestdate,
        lastcuupdatedate,
        emailbouncedreason,
        emailbounceddate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        individualid,
        pronouns,
        genderidentity,
        level__c,
        languages__c

    from source

)

select * from renamed
  );

[0m11:43:59.979104 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.982197 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:43:59.982469 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact" rename to "stg_salesforce__contact__dbt_backup"
[0m11:43:59.982874 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.984713 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:43:59.984913 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
alter view "dbt"."staging"."stg_salesforce__contact__dbt_tmp" rename to "stg_salesforce__contact"
[0m11:43:59.985214 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.986078 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m11:43:59.986269 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:43:59.986441 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: COMMIT
[0m11:43:59.987119 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.988651 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__contact"
[0m11:43:59.988849 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__contact"} */
drop view if exists "dbt"."staging"."stg_salesforce__contact__dbt_backup" cascade
[0m11:43:59.989220 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:43:59.989950 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__contact (execute): 11:43:59.967684 => 11:43:59.989857
[0m11:43:59.990156 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__contact: Close
[0m11:44:00.007313 [info ] [Thread-1  ]: 6 of 30 OK created sql view model staging.stg_salesforce__contact .............. [[32mOK[0m in 0.04s]
[0m11:44:00.007685 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__contact
[0m11:44:00.007906 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:44:00.008366 [info ] [Thread-1  ]: 7 of 30 START sql view model staging.stg_salesforce__lead ...................... [RUN]
[0m11:44:00.008872 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__contact, now model.elastic_dbt_interview.stg_salesforce__lead)
[0m11:44:00.009114 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:44:00.011260 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:44:00.011970 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (compile): 11:44:00.009255 => 11:44:00.011866
[0m11:44:00.012178 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:44:00.014910 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:44:00.015850 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:44:00.016049 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: BEGIN
[0m11:44:00.016221 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.022887 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.023178 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:44:00.023408 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */

  
  create view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" as (
    --

with source as (

    select * from "dbt"."raw"."lead"

),

renamed as (

    select
        id as lead_id,
        isdeleted,
        masterrecordid,
        salutation,
        firstname,
        lastname,
        title,
        company,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        phone,
        mobilephone,
        fax,
        email,
        website,
        description,
        leadsource,
        status,
        industry,
        rating,
        annualrevenue,
        numberofemployees,
        ownerid,
        hasoptedoutofemail,
        isconverted,
        converteddate,
        convertedaccountid,
        convertedcontactid,
        convertedopportunityid,
        isunreadbyowner,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        donotcall,
        hasoptedoutoffax,
        lasttransferdate,
        jigsaw,
        jigsawcontactid,
        cleanstatus,
        companydunsnumber,
        dandbcompanyid,
        emailbouncedreason,
        emailbounceddate,
        individualid,
        pronouns,
        genderidentity,
        siccode__c,
        productinterest__c,
        primary__c,
        currentgenerators__c,
        numberoflocations__c

    from source

)

select * from renamed
  );

[0m11:44:00.024190 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.026673 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:44:00.026967 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead" rename to "stg_salesforce__lead__dbt_backup"
[0m11:44:00.027377 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.030290 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:44:00.030520 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
alter view "dbt"."staging"."stg_salesforce__lead__dbt_tmp" rename to "stg_salesforce__lead"
[0m11:44:00.030843 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.031709 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m11:44:00.031899 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:44:00.032067 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: COMMIT
[0m11:44:00.032767 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.034334 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__lead"
[0m11:44:00.034540 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__lead"} */
drop view if exists "dbt"."staging"."stg_salesforce__lead__dbt_backup" cascade
[0m11:44:00.034902 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.035616 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__lead (execute): 11:44:00.012312 => 11:44:00.035522
[0m11:44:00.035820 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__lead: Close
[0m11:44:00.049616 [info ] [Thread-1  ]: 7 of 30 OK created sql view model staging.stg_salesforce__lead ................. [[32mOK[0m in 0.04s]
[0m11:44:00.050013 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__lead
[0m11:44:00.050264 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:44:00.050603 [info ] [Thread-1  ]: 8 of 30 START sql view model staging.stg_salesforce__opportunity ............... [RUN]
[0m11:44:00.050988 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__lead, now model.elastic_dbt_interview.stg_salesforce__opportunity)
[0m11:44:00.051189 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:44:00.053222 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:44:00.053741 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (compile): 11:44:00.051325 => 11:44:00.053627
[0m11:44:00.053942 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:44:00.056644 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:44:00.057193 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:44:00.057493 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: BEGIN
[0m11:44:00.057706 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.064626 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.064934 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:44:00.065168 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity"

),

renamed as (

    select
        id as opportunity_id,
        isdeleted,
        accountid,
        isprivate,
        name,
        description,
        stagename,
        stagesortorder,
        amount,
        probability,
        expectedrevenue,
        totalopportunityquantity,
        closedate,
        type,
        nextstep,
        leadsource,
        isclosed,
        iswon,
        forecastcategory,
        forecastcategoryname,
        campaignid,
        hasopportunitylineitem,
        pricebook2id,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        lastactivitydate,
        laststagechangedate,
        fiscalyear,
        fiscalquarter,
        contactid,
        primarypartneraccountid,
        contractid,
        lastamountchangedhistoryid,
        lastclosedatechangedhistoryid,
        deliveryinstallationstatus__c,
        trackingnumber__c,
        ordernumber__c,
        currentgenerators__c,
        maincompetitors__c

    from source

)

select * from renamed
  );

[0m11:44:00.065927 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.068350 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:44:00.068598 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity" rename to "stg_salesforce__opportunity__dbt_backup"
[0m11:44:00.068971 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.071488 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:44:00.071695 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
alter view "dbt"."staging"."stg_salesforce__opportunity__dbt_tmp" rename to "stg_salesforce__opportunity"
[0m11:44:00.071991 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.072820 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m11:44:00.073010 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:44:00.073182 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: COMMIT
[0m11:44:00.074043 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.075987 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity"
[0m11:44:00.076218 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity__dbt_backup" cascade
[0m11:44:00.076717 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.077719 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity (execute): 11:44:00.054075 => 11:44:00.077598
[0m11:44:00.077979 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity: Close
[0m11:44:00.092891 [info ] [Thread-1  ]: 8 of 30 OK created sql view model staging.stg_salesforce__opportunity .......... [[32mOK[0m in 0.04s]
[0m11:44:00.093313 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity
[0m11:44:00.093571 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:44:00.093913 [info ] [Thread-1  ]: 9 of 30 START sql view model staging.stg_salesforce__opportunity_history ....... [RUN]
[0m11:44:00.094281 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity, now model.elastic_dbt_interview.stg_salesforce__opportunity_history)
[0m11:44:00.094473 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:44:00.096527 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:44:00.097447 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (compile): 11:44:00.094611 => 11:44:00.097301
[0m11:44:00.097711 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:44:00.100525 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:44:00.101256 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:44:00.101535 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: BEGIN
[0m11:44:00.101722 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.119086 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.122507 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:44:00.122849 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */

  
  create view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."opportunity_history"

),

renamed as (

    select
        id as opportunity_history_id,
        opportunityid,
        createdbyid,
        createddate,
        createddateforinsert,
        stagename,
        amount,
        expectedrevenue,
        closedate,
        probability,
        fromforecastcategory,
        forecastcategory,
        prevforecastupdate,
        fromopportunitystagename,
        prevopportunitystageupdate,
        validthroughdate,
        systemmodstamp,
        isdeleted,
        prevamount,
        prevclosedate

    from source

)

select * from renamed
  );

[0m11:44:00.137829 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.144964 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:44:00.145205 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history" rename to "stg_salesforce__opportunity_history__dbt_backup"
[0m11:44:00.145620 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.148808 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:44:00.149672 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
alter view "dbt"."staging"."stg_salesforce__opportunity_history__dbt_tmp" rename to "stg_salesforce__opportunity_history"
[0m11:44:00.150588 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.151821 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m11:44:00.152124 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:44:00.152327 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: COMMIT
[0m11:44:00.153510 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.156418 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__opportunity_history"
[0m11:44:00.156708 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__opportunity_history"} */
drop view if exists "dbt"."staging"."stg_salesforce__opportunity_history__dbt_backup" cascade
[0m11:44:00.157226 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.158011 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__opportunity_history (execute): 11:44:00.097850 => 11:44:00.157917
[0m11:44:00.158213 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__opportunity_history: Close
[0m11:44:00.172113 [info ] [Thread-1  ]: 9 of 30 OK created sql view model staging.stg_salesforce__opportunity_history .. [[32mOK[0m in 0.08s]
[0m11:44:00.172537 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__opportunity_history
[0m11:44:00.172764 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:44:00.173155 [info ] [Thread-1  ]: 10 of 30 START sql view model staging.stg_salesforce__pricebook_entry .......... [RUN]
[0m11:44:00.173638 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__opportunity_history, now model.elastic_dbt_interview.stg_salesforce__pricebook_entry)
[0m11:44:00.173861 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:44:00.175937 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:44:00.176422 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (compile): 11:44:00.174000 => 11:44:00.176318
[0m11:44:00.176628 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:44:00.179449 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:44:00.179946 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:44:00.180161 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: BEGIN
[0m11:44:00.180344 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.187213 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.187468 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:44:00.187675 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */

  
  create view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."pricebook_entry"

),

renamed as (

    select
        id as pricebook_entry_id,
        pricebook2id,
        product2id,
        unitprice,
        isactive,
        usestandardprice,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        isdeleted,
        isarchived

    from source

)

select * from renamed
  );

[0m11:44:00.188190 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.190381 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:44:00.190591 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry" rename to "stg_salesforce__pricebook_entry__dbt_backup"
[0m11:44:00.190912 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.192623 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:44:00.192823 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
alter view "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_tmp" rename to "stg_salesforce__pricebook_entry"
[0m11:44:00.193189 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.194312 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m11:44:00.194541 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:44:00.194728 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: COMMIT
[0m11:44:00.195307 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.197814 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"
[0m11:44:00.198042 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__pricebook_entry"} */
drop view if exists "dbt"."staging"."stg_salesforce__pricebook_entry__dbt_backup" cascade
[0m11:44:00.198450 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.199259 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__pricebook_entry (execute): 11:44:00.176770 => 11:44:00.199162
[0m11:44:00.199494 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__pricebook_entry: Close
[0m11:44:00.213406 [info ] [Thread-1  ]: 10 of 30 OK created sql view model staging.stg_salesforce__pricebook_entry ..... [[32mOK[0m in 0.04s]
[0m11:44:00.213807 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__pricebook_entry
[0m11:44:00.214038 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:44:00.214495 [info ] [Thread-1  ]: 11 of 30 START sql view model staging.stg_salesforce__product_2 ................ [RUN]
[0m11:44:00.214992 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__pricebook_entry, now model.elastic_dbt_interview.stg_salesforce__product_2)
[0m11:44:00.215209 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:44:00.217314 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:44:00.219532 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (compile): 11:44:00.215343 => 11:44:00.219427
[0m11:44:00.219764 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:44:00.222450 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:44:00.223177 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:44:00.223533 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: BEGIN
[0m11:44:00.223754 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.230571 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.230864 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:44:00.231072 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */

  
  create view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."product_2"

),

renamed as (

    select
        id as product_id,
        name,
        productcode,
        description,
        isactive,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        family,
        externaldatasourceid,
        externalid,
        displayurl,
        quantityunitofmeasure,
        isdeleted,
        isarchived,
        stockkeepingunit,
        type,
        productclass,
        sourceproductid,
        sellerid

    from source

)

select * from renamed
  );

[0m11:44:00.231596 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.233746 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:44:00.233952 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2" rename to "stg_salesforce__product_2__dbt_backup"
[0m11:44:00.234243 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.235940 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:44:00.236155 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
alter view "dbt"."staging"."stg_salesforce__product_2__dbt_tmp" rename to "stg_salesforce__product_2"
[0m11:44:00.236435 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.237366 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m11:44:00.237653 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:44:00.237843 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: COMMIT
[0m11:44:00.238497 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.240409 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__product_2"
[0m11:44:00.240646 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__product_2"} */
drop view if exists "dbt"."staging"."stg_salesforce__product_2__dbt_backup" cascade
[0m11:44:00.241081 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.241884 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__product_2 (execute): 11:44:00.220037 => 11:44:00.241788
[0m11:44:00.242091 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__product_2: Close
[0m11:44:00.255876 [info ] [Thread-1  ]: 11 of 30 OK created sql view model staging.stg_salesforce__product_2 ........... [[32mOK[0m in 0.04s]
[0m11:44:00.256275 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__product_2
[0m11:44:00.256504 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:44:00.256962 [info ] [Thread-1  ]: 12 of 30 START sql view model staging.stg_salesforce__record_type .............. [RUN]
[0m11:44:00.257438 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__product_2, now model.elastic_dbt_interview.stg_salesforce__record_type)
[0m11:44:00.257672 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:44:00.260485 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:44:00.261309 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (compile): 11:44:00.257821 => 11:44:00.261087
[0m11:44:00.261565 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:44:00.264240 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:44:00.265066 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:44:00.265359 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: BEGIN
[0m11:44:00.265565 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.272824 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.273162 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:44:00.273378 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */

  
  create view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."record_type"

),

renamed as (

    select
        id as record_type_id,
        name,
        modulenamespace,
        description,
        businessprocessid,
        sobjecttype,
        isactive,
        createdbyid,
        createddate,
        lastmodifiedbyid,
        lastmodifieddate,
        systemmodstamp,
        isdeleted

    from source

)

select * from renamed
  );

[0m11:44:00.273866 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.276219 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:44:00.276590 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type" rename to "stg_salesforce__record_type__dbt_backup"
[0m11:44:00.277002 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.279132 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:44:00.279367 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
alter view "dbt"."staging"."stg_salesforce__record_type__dbt_tmp" rename to "stg_salesforce__record_type"
[0m11:44:00.279683 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.280577 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m11:44:00.280774 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:44:00.280962 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: COMMIT
[0m11:44:00.281595 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.283488 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__record_type"
[0m11:44:00.283751 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__record_type"} */
drop view if exists "dbt"."staging"."stg_salesforce__record_type__dbt_backup" cascade
[0m11:44:00.284207 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.285076 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__record_type (execute): 11:44:00.261713 => 11:44:00.284972
[0m11:44:00.285309 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__record_type: Close
[0m11:44:00.299814 [info ] [Thread-1  ]: 12 of 30 OK created sql view model staging.stg_salesforce__record_type ......... [[32mOK[0m in 0.04s]
[0m11:44:00.300277 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__record_type
[0m11:44:00.300529 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:44:00.300891 [info ] [Thread-1  ]: 13 of 30 START sql view model staging.stg_salesforce__solution ................. [RUN]
[0m11:44:00.301291 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__record_type, now model.elastic_dbt_interview.stg_salesforce__solution)
[0m11:44:00.301500 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:44:00.303474 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:44:00.309636 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (compile): 11:44:00.301635 => 11:44:00.309484
[0m11:44:00.309896 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:44:00.313681 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:44:00.314215 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:44:00.314433 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: BEGIN
[0m11:44:00.314622 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.321813 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.322116 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:44:00.322329 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */

  
  create view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."solution"

),

renamed as (

    select
        id as solution_id,
        isdeleted,
        solutionnumber,
        solutionname,
        ispublished,
        ispublishedinpublickb,
        status,
        isreviewed,
        solutionnote,
        caseid,
        ownerid,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        timesused,
        ishtml

    from source

)

select * from renamed
  );

[0m11:44:00.322834 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.325041 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:44:00.325402 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution" rename to "stg_salesforce__solution__dbt_backup"
[0m11:44:00.325853 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.327992 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:44:00.328211 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
alter view "dbt"."staging"."stg_salesforce__solution__dbt_tmp" rename to "stg_salesforce__solution"
[0m11:44:00.328522 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.329413 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m11:44:00.329605 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:44:00.329778 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: COMMIT
[0m11:44:00.330321 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.331908 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__solution"
[0m11:44:00.332133 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__solution"} */
drop view if exists "dbt"."staging"."stg_salesforce__solution__dbt_backup" cascade
[0m11:44:00.332547 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.333278 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__solution (execute): 11:44:00.310048 => 11:44:00.333182
[0m11:44:00.333493 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__solution: Close
[0m11:44:00.347821 [info ] [Thread-1  ]: 13 of 30 OK created sql view model staging.stg_salesforce__solution ............ [[32mOK[0m in 0.05s]
[0m11:44:00.348255 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__solution
[0m11:44:00.348518 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user
[0m11:44:00.348874 [info ] [Thread-1  ]: 14 of 30 START sql view model staging.stg_salesforce__user ..................... [RUN]
[0m11:44:00.349265 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__solution, now model.elastic_dbt_interview.stg_salesforce__user)
[0m11:44:00.349488 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user
[0m11:44:00.351754 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:44:00.354414 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (compile): 11:44:00.349855 => 11:44:00.354091
[0m11:44:00.354730 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user
[0m11:44:00.358174 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:44:00.359473 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:44:00.359746 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: BEGIN
[0m11:44:00.359930 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.366926 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.367244 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:44:00.367501 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */

  
  create view "dbt"."staging"."stg_salesforce__user__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user"

),

renamed as (

    select
        id as user_id,
        username,
        firstname,
        lastname,
        companyname,
        division,
        department,
        title,
        street,
        city,
        state,
        postalcode,
        country,
        latitude,
        longitude,
        geocodeaccuracy,
        email,
        senderemail,
        sendername,
        signature,
        stayintouchsubject,
        stayintouchsignature,
        stayintouchnote,
        phone,
        fax,
        mobilephone,
        alias,
        communitynickname,
        isactive,
        issystemcontrolled,
        timezonesidkey,
        userroleid,
        localesidkey,
        receivesinfoemails,
        receivesadmininfoemails,
        emailencodingkey,
        profileid,
        usertype,
        usersubtype,
        startday,
        endday,
        languagelocalekey,
        employeenumber,
        delegatedapproverid,
        managerid,
        lastlogindate,
        lastpasswordchangedate,
        createddate,
        createdbyid,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        numberoffailedlogins,
        suaccessexpirationdate,
        suorgadminexpirationdate,
        offlinetrialexpirationdate,
        wirelesstrialexpirationdate,
        offlinepdatrialexpirationdate,
        forecastenabled,
        contactid,
        accountid,
        callcenterid,
        extension,
        federationidentifier,
        aboutme,
        loginlimit,
        profilephotoid,
        digestfrequency,
        defaultgroupnotificationfrequency,
        jigsawimportlimitoverride,
        workspaceid,
        sharingtype,
        chatteradoptionstage,
        chatteradoptionstagemodifieddate,
        bannerphotoid,
        isprofilephotoactive,
        individualid,
        globalidentity

    from source

)

select * from renamed
  );

[0m11:44:00.368503 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.370872 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:44:00.371114 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user" rename to "stg_salesforce__user__dbt_backup"
[0m11:44:00.371438 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.373278 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:44:00.373480 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
alter view "dbt"."staging"."stg_salesforce__user__dbt_tmp" rename to "stg_salesforce__user"
[0m11:44:00.373778 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.374662 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m11:44:00.374854 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:44:00.375026 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: COMMIT
[0m11:44:00.375632 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.377526 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user"
[0m11:44:00.377800 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user"} */
drop view if exists "dbt"."staging"."stg_salesforce__user__dbt_backup" cascade
[0m11:44:00.378260 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.379186 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user (execute): 11:44:00.354886 => 11:44:00.379085
[0m11:44:00.379426 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user: Close
[0m11:44:00.393497 [info ] [Thread-1  ]: 14 of 30 OK created sql view model staging.stg_salesforce__user ................ [[32mOK[0m in 0.04s]
[0m11:44:00.393916 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user
[0m11:44:00.394166 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:44:00.394448 [info ] [Thread-1  ]: 15 of 30 START sql view model staging.stg_salesforce__user_role ................ [RUN]
[0m11:44:00.394812 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user, now model.elastic_dbt_interview.stg_salesforce__user_role)
[0m11:44:00.395011 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:44:00.397099 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:44:00.397726 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (compile): 11:44:00.395143 => 11:44:00.397623
[0m11:44:00.397927 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:44:00.400620 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:44:00.401057 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:44:00.401261 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: BEGIN
[0m11:44:00.401443 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.408302 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.408593 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:44:00.408801 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */

  
  create view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" as (
    with source as (

    select * from "dbt"."raw"."user_role"

),

renamed as (

    select
        id as user_role_id,
        name,
        parentroleid,
        rollupdescription,
        opportunityaccessforaccountowner,
        caseaccessforaccountowner,
        contactaccessforaccountowner,
        forecastuserid,
        mayforecastmanagershare,
        lastmodifieddate,
        lastmodifiedbyid,
        systemmodstamp,
        portalaccountid,
        portaltype,
        portalrole,
        portalaccountownerid

    from source

)

select * from renamed
  );

[0m11:44:00.409317 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.412497 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:44:00.412805 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role" rename to "stg_salesforce__user_role__dbt_backup"
[0m11:44:00.413225 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.415018 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:44:00.415224 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
alter view "dbt"."staging"."stg_salesforce__user_role__dbt_tmp" rename to "stg_salesforce__user_role"
[0m11:44:00.415522 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.416390 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m11:44:00.416580 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:44:00.416856 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: COMMIT
[0m11:44:00.417466 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.418995 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.stg_salesforce__user_role"
[0m11:44:00.419190 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.stg_salesforce__user_role"} */
drop view if exists "dbt"."staging"."stg_salesforce__user_role__dbt_backup" cascade
[0m11:44:00.419545 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.420283 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.stg_salesforce__user_role (execute): 11:44:00.398061 => 11:44:00.420189
[0m11:44:00.420481 [debug] [Thread-1  ]: On model.elastic_dbt_interview.stg_salesforce__user_role: Close
[0m11:44:00.434435 [info ] [Thread-1  ]: 15 of 30 OK created sql view model staging.stg_salesforce__user_role ........... [[32mOK[0m in 0.04s]
[0m11:44:00.434830 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.stg_salesforce__user_role
[0m11:44:00.435059 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_account
[0m11:44:00.435480 [info ] [Thread-1  ]: 16 of 30 START sql table model dim.dim_account ................................. [RUN]
[0m11:44:00.435979 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.stg_salesforce__user_role, now model.elastic_dbt_interview.dim_account)
[0m11:44:00.436191 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_account
[0m11:44:00.438508 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_account"
[0m11:44:00.438978 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (compile): 11:44:00.436328 => 11:44:00.438874
[0m11:44:00.439181 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_account
[0m11:44:00.441829 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_account"
[0m11:44:00.442336 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:44:00.442610 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: BEGIN
[0m11:44:00.442815 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.449776 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.450077 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:44:00.450340 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */

  
    
    

    create  table
      "dbt"."dim"."dim_account__dbt_tmp"
  
    as (
      

select
    account_id,
    masterrecordid as master_record_id,
    name as account_name,
    type as account_type,
    parentid as parent_account_id,
    billingstreet as billing_street,
    billingcity as billing_city,
    billingstate as billing_state,
    billingpostalcode as billing_postal_code,
    billingcountry as billing_country,
    billinglatitude as billing_latitude,
    billinglongitude as billing_longitude,
    billinggeocodeaccuracy as billing_geocode_accuracy,
    shippingstreet as shipping_street,
    shippingcity as shipping_city,
    shippingstate as shipping_state,
    shippingpostalcode as shipping_postal_code,
    shippingcountry as shipping_country,
    shippinglatitude as shipping_latitude,
    shippinglongitude as shipping_longitude,
    shippinggeocodeaccuracy as shipping_geocode_accuracy,
    phone as account_phone,
    fax as account_fax,
    accountnumber as account_number,
    website as account_website,
    sic as sic_code,
    industry as account_industry,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownership as account_ownership,
    tickersymbol as ticker_symbol,
    description as account_description,
    rating as account_rating,
    site as account_site,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    jigsaw as jigsaw_id,
    jigsawcompanyid as jigsaw_company_id,
    cleanstatus as clean_status,
    accountsource as account_source,
    dunsnumber as duns_number,
    tradestyle as trade_style,
    naicscode as naics_code,
    naicsdesc as naics_description,
    yearstarted as year_started,
    sicdesc as sic_description,
    dandbcompanyid as dandb_company_id,
    operatinghoursid as operating_hours_id,
    customerpriority__c as customer_priority,
    sla__c as sla,
    active__c as is_active,
    numberoflocations__c as number_of_locations,
    upsellopportunity__c as upsell_opportunity,
    slaserialnumber__c as sla_serial_number,
    slaexpirationdate__c as sla_expiration_date
from "dbt"."staging"."stg_salesforce__account"
where isdeleted = false  -- Exclude deleted accounts
order by account_name
    );
  
  
[0m11:44:00.454791 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.458121 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:44:00.458386 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account" rename to "dim_account__dbt_backup"
[0m11:44:00.458777 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.488835 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:44:00.489210 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
alter table "dbt"."dim"."dim_account__dbt_tmp" rename to "dim_account"
[0m11:44:00.489723 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.490791 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m11:44:00.490992 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:44:00.491169 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: COMMIT
[0m11:44:00.492490 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.494318 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_account"
[0m11:44:00.494588 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_account"} */
drop table if exists "dbt"."dim"."dim_account__dbt_backup" cascade
[0m11:44:00.495273 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.496232 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_account (execute): 11:44:00.439318 => 11:44:00.496132
[0m11:44:00.496455 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_account: Close
[0m11:44:00.519607 [info ] [Thread-1  ]: 16 of 30 OK created sql table model dim.dim_account ............................ [[32mOK[0m in 0.08s]
[0m11:44:00.520061 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_account
[0m11:44:00.520314 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_campaign
[0m11:44:00.520589 [info ] [Thread-1  ]: 17 of 30 START sql table model dim.dim_campaign ................................ [RUN]
[0m11:44:00.520948 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_account, now model.elastic_dbt_interview.dim_campaign)
[0m11:44:00.521147 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_campaign
[0m11:44:00.523446 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_campaign"
[0m11:44:00.523964 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (compile): 11:44:00.521284 => 11:44:00.523866
[0m11:44:00.524161 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_campaign
[0m11:44:00.526894 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_campaign"
[0m11:44:00.527461 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:44:00.527680 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: BEGIN
[0m11:44:00.527854 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.534908 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.535215 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:44:00.535476 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */

  
    
    

    create  table
      "dbt"."dim"."dim_campaign__dbt_tmp"
  
    as (
      

select
    /* IDs */
    campaign_id,
    parentid as parent_campaign_id,
    ownerid as owner_id,
    createdbyid as created_by_id,
    lastmodifiedbyid as last_modified_by_id,
    campaignmemberrecordtypeid as campaign_member_record_type_id,

    /* Dates */
    startdate as start_date,
    enddate as end_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,

    /* Dimensions */
    name as campaign_name,
    type as campaign_type,
    status,
    description,
    isactive as is_active,

    /* Metrics */
    expectedrevenue as expected_revenue,
    budgetedcost as budgeted_cost,
    actualcost as actual_cost,
    expectedresponse as expected_response,
    numbersent as number_sent,
    numberofleads as number_of_leads,
    numberofconvertedleads as number_of_converted_leads,
    numberofcontacts as number_of_contacts,
    numberofresponses as number_of_responses,
    numberofopportunities as number_of_opportunities,
    numberofwonopportunities as number_of_won_opportunities,
    amountallopportunities as amount_all_opportunities,
    amountwonopportunities as amount_won_opportunities,
    hierarchynumberofleads as hierarchy_number_of_leads,
    hierarchynumberofconvertedleads as hierarchy_number_of_converted_leads,
    hierarchynumberofcontacts as hierarchy_number_of_contacts,
    hierarchynumberofresponses as hierarchy_number_of_responses,
    hierarchynumberofopportunities as hierarchy_number_of_opportunities,
    hierarchynumberofwonopportunities as hierarchy_number_of_won_opportunities,
    hierarchyamountallopportunities as hierarchy_amount_all_opportunities,
    hierarchyamountwonopportunities as hierarchy_amount_won_opportunities,
    hierarchynumbersent as hierarchy_number_sent,
    hierarchyexpectedrevenue as hierarchy_expected_revenue,
    hierarchybudgetedcost as hierarchy_budgeted_cost,
    hierarchyactualcost as hierarchy_actual_cost

from "dbt"."staging"."stg_salesforce__campaign"
where isdeleted = false
    );
  
  
[0m11:44:00.538532 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.540805 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:44:00.541038 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign" rename to "dim_campaign__dbt_backup"
[0m11:44:00.541432 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.543899 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:44:00.544128 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
alter table "dbt"."dim"."dim_campaign__dbt_tmp" rename to "dim_campaign"
[0m11:44:00.544453 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.545406 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m11:44:00.545595 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:44:00.545772 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: COMMIT
[0m11:44:00.546884 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.548836 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_campaign"
[0m11:44:00.549057 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_campaign"} */
drop table if exists "dbt"."dim"."dim_campaign__dbt_backup" cascade
[0m11:44:00.549635 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.550655 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_campaign (execute): 11:44:00.524293 => 11:44:00.550555
[0m11:44:00.550879 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_campaign: Close
[0m11:44:00.568107 [info ] [Thread-1  ]: 17 of 30 OK created sql table model dim.dim_campaign ........................... [[32mOK[0m in 0.05s]
[0m11:44:00.568527 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_campaign
[0m11:44:00.568774 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_case_status
[0m11:44:00.569070 [info ] [Thread-1  ]: 18 of 30 START sql table model dim.dim_case_status ............................. [RUN]
[0m11:44:00.569541 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_campaign, now model.elastic_dbt_interview.dim_case_status)
[0m11:44:00.569851 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_case_status
[0m11:44:00.571948 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_case_status"
[0m11:44:00.572435 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (compile): 11:44:00.570020 => 11:44:00.572334
[0m11:44:00.572627 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_case_status
[0m11:44:00.575071 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_case_status"
[0m11:44:00.575524 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:44:00.575715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: BEGIN
[0m11:44:00.575889 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.583030 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.583311 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:44:00.583512 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */

  
    
    

    create  table
      "dbt"."dim"."dim_case_status__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct status as status_name, status as status_description 
        from "dbt"."staging"."stg_salesforce__case_history_2"
    )

select
    row_number() over (order by status_name) as status_id, 
    status_description
from source
    );
  
  
[0m11:44:00.584936 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.587269 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:44:00.587527 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status" rename to "dim_case_status__dbt_backup"
[0m11:44:00.587854 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.589735 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:44:00.589942 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
alter table "dbt"."dim"."dim_case_status__dbt_tmp" rename to "dim_case_status"
[0m11:44:00.590223 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.591173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m11:44:00.591365 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:44:00.591534 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: COMMIT
[0m11:44:00.592178 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.595048 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_case_status"
[0m11:44:00.595301 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_case_status"} */
drop table if exists "dbt"."dim"."dim_case_status__dbt_backup" cascade
[0m11:44:00.595754 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.596559 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_case_status (execute): 11:44:00.572759 => 11:44:00.596460
[0m11:44:00.596822 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_case_status: Close
[0m11:44:00.611563 [info ] [Thread-1  ]: 18 of 30 OK created sql table model dim.dim_case_status ........................ [[32mOK[0m in 0.04s]
[0m11:44:00.612000 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_case_status
[0m11:44:00.612245 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_contact
[0m11:44:00.612502 [info ] [Thread-1  ]: 19 of 30 START sql table model dim.dim_contact ................................. [RUN]
[0m11:44:00.612862 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_case_status, now model.elastic_dbt_interview.dim_contact)
[0m11:44:00.613052 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_contact
[0m11:44:00.615283 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_contact"
[0m11:44:00.615766 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (compile): 11:44:00.613179 => 11:44:00.615669
[0m11:44:00.615960 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_contact
[0m11:44:00.618639 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_contact"
[0m11:44:00.619149 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:44:00.619450 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: BEGIN
[0m11:44:00.619645 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.626837 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.627073 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:44:00.627325 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */

  
    
    

    create  table
      "dbt"."dim"."dim_contact__dbt_tmp"
  
    as (
      

select
    /* IDs */
    contact_id,
    masterrecordid as master_record_id,
    accountid as account_id,
    reportstoid as reports_to_id,
    ownerid as owner_id,
    jigsawcontactid as jigsaw_contact_id,
    individualid as individual_id,

    /* Dates */
    birthdate as birth_date,
    createddate as created_at,
    lastmodifieddate as last_modified_at,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    lastcurequestdate as last_cu_request_date,
    lastcuupdatedate as last_cu_update_date,
    emailbounceddate as email_bounced_date,

    /* Dimensions */
    salutation,
    firstname as first_name,
    lastname as last_name,
    otherstreet as other_street,
    othercity as other_city,
    otherstate as other_state,
    otherpostalcode as other_postal_code,
    othercountry as other_country,
    otherlatitude as other_latitude,
    otherlongitude as other_longitude,
    othergeocodeaccuracy as other_geocode_accuracy,
    mailingstreet as mailing_street,
    mailingcity as mailing_city,
    mailingstate as mailing_state,
    mailingpostalcode as mailing_postal_code,
    mailingcountry as mailing_country,
    mailinglatitude as mailing_latitude,
    mailinglongitude as mailing_longitude,
    mailinggeocodeaccuracy as mailing_geocode_accuracy,
    phone,
    fax,
    mobilephone as mobile_phone,
    homephone as home_phone,
    otherphone as other_phone,
    assistantphone as assistant_phone,
    email,
    title,
    department,
    assistantname as assistant_name,
    leadsource as lead_source,
    description,
    pronouns,
    genderidentity as gender_identity,
    cleanstatus as clean_status,
    emailbouncedreason as email_bounced_reason,
    level__c as level,
    languages__c as languages,

    /* Metrics */
    hasoptedoutofemail as has_opted_out_of_email,
    hasoptedoutoffax as has_opted_out_of_fax,
    donotcall as do_not_call

from "dbt"."staging"."stg_salesforce__contact"
where isdeleted = false
    );
  
  
[0m11:44:00.631113 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.633357 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:44:00.633586 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact" rename to "dim_contact__dbt_backup"
[0m11:44:00.634005 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.635754 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:44:00.635950 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
alter table "dbt"."dim"."dim_contact__dbt_tmp" rename to "dim_contact"
[0m11:44:00.636279 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.637344 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m11:44:00.637591 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:44:00.637778 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: COMMIT
[0m11:44:00.638983 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.641502 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_contact"
[0m11:44:00.641707 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_contact"} */
drop table if exists "dbt"."dim"."dim_contact__dbt_backup" cascade
[0m11:44:00.642228 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.643032 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_contact (execute): 11:44:00.616090 => 11:44:00.642933
[0m11:44:00.643248 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_contact: Close
[0m11:44:00.665883 [info ] [Thread-1  ]: 19 of 30 OK created sql table model dim.dim_contact ............................ [[32mOK[0m in 0.05s]
[0m11:44:00.666311 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_contact
[0m11:44:00.666563 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_lead
[0m11:44:00.666851 [info ] [Thread-1  ]: 20 of 30 START sql table model dim.dim_lead .................................... [RUN]
[0m11:44:00.667218 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_contact, now model.elastic_dbt_interview.dim_lead)
[0m11:44:00.667409 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_lead
[0m11:44:00.669675 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_lead"
[0m11:44:00.670418 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (compile): 11:44:00.667537 => 11:44:00.670309
[0m11:44:00.670626 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_lead
[0m11:44:00.673443 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_lead"
[0m11:44:00.674182 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:44:00.674485 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: BEGIN
[0m11:44:00.674731 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.682032 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.682324 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:44:00.682580 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */

  
    
    

    create  table
      "dbt"."dim"."dim_lead__dbt_tmp"
  
    as (
      

select
    lead_id,
    masterrecordid as master_record_id,
    salutation as lead_salutation,
    firstname as first_name,
    lastname as last_name,
    title as lead_title,
    company as lead_company,
    street as lead_street,
    city as lead_city,
    state as lead_state,
    postalcode as lead_postal_code,
    country as lead_country,
    latitude as lead_latitude,
    longitude as lead_longitude,
    geocodeaccuracy as geocode_accuracy,
    phone as lead_phone,
    mobilephone as lead_mobile_phone,
    fax as lead_fax,
    email as lead_email,
    website as lead_website,
    description as lead_description,
    leadsource as lead_source,
    status as lead_status,
    industry as lead_industry,
    rating as lead_rating,
    annualrevenue as annual_revenue,
    numberofemployees as number_of_employees,
    ownerid as owner_id,
    hasoptedoutofemail as has_opted_out_of_email,
    isconverted as is_converted,
    converteddate as converted_date,
    convertedaccountid as converted_account_id,
    convertedcontactid as converted_contact_id,
    convertedopportunityid as converted_opportunity_id,
    isunreadbyowner as is_unread_by_owner,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_mod_stamp,
    lastactivitydate as last_activity_date,
    donotcall as do_not_call,
    hasoptedoutoffax as has_opted_out_of_fax,
    lasttransferdate as last_transfer_date,
    jigsaw as jigsaw_id,
    jigsawcontactid as jigsaw_contact_id,
    cleanstatus as clean_status,
    companydunsnumber as company_duns_number,
    dandbcompanyid as dandb_company_id,
    emailbouncedreason as email_bounced_reason,
    emailbounceddate as email_bounced_date,
    individualid as individual_id,
    pronouns as lead_pronouns,
    genderidentity as gender_identity,
    siccode__c as sic_code,
    productinterest__c as product_interest,
    primary__c as is_primary,
    currentgenerators__c as current_generators,
    numberoflocations__c as number_of_locations
from "dbt"."staging"."stg_salesforce__lead"
where isdeleted = false  -- Exclude deleted leads
order by last_name, first_name
    );
  
  
[0m11:44:00.686647 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.688827 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:44:00.689044 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead" rename to "dim_lead__dbt_backup"
[0m11:44:00.689400 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.691119 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:44:00.691312 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
alter table "dbt"."dim"."dim_lead__dbt_tmp" rename to "dim_lead"
[0m11:44:00.691646 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.692569 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m11:44:00.692760 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:44:00.692931 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: COMMIT
[0m11:44:00.694484 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.696393 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_lead"
[0m11:44:00.696630 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_lead"} */
drop table if exists "dbt"."dim"."dim_lead__dbt_backup" cascade
[0m11:44:00.697241 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.698082 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_lead (execute): 11:44:00.670762 => 11:44:00.697989
[0m11:44:00.698304 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_lead: Close
[0m11:44:00.719730 [info ] [Thread-1  ]: 20 of 30 OK created sql table model dim.dim_lead ............................... [[32mOK[0m in 0.05s]
[0m11:44:00.720138 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_lead
[0m11:44:00.720375 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity
[0m11:44:00.720649 [info ] [Thread-1  ]: 21 of 30 START sql table model dim.dim_opportunity ............................. [RUN]
[0m11:44:00.721002 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_lead, now model.elastic_dbt_interview.dim_opportunity)
[0m11:44:00.721200 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity
[0m11:44:00.724440 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity"
[0m11:44:00.724976 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (compile): 11:44:00.721329 => 11:44:00.724879
[0m11:44:00.725166 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity
[0m11:44:00.727794 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity"
[0m11:44:00.728604 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:44:00.728900 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: BEGIN
[0m11:44:00.729097 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.735998 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.736314 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:44:00.736566 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity__dbt_tmp"
  
    as (
      

select
    opportunity_id,
    accountid as account_id,
    isprivate as is_private,
    name as opportunity_name,
    description as opportunity_description,
    stagename as stage_name,
    stagesortorder as stage_sort_order,
    amount as opportunity_amount,
    probability as opportunity_probability,
    expectedrevenue as expected_revenue,
    totalopportunityquantity as total_opportunity_quantity,
    closedate as close_date,
    type as opportunity_type,
    nextstep as next_step,
    leadsource as lead_source,
    isclosed as is_closed,
    iswon as is_won,
    forecastcategory as forecast_category,
    forecastcategoryname as forecast_category_name,
    campaignid as campaign_id,
    hasopportunitylineitem as has_opportunity_line_item,
    pricebook2id as pricebook_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    lastactivitydate as last_activity_date,
    laststagechangedate as last_stage_change_date,
    fiscalyear as fiscal_year,
    fiscalquarter as fiscal_quarter,
    contactid as contact_id,
    primarypartneraccountid as primary_partner_account_id,
    contractid as contract_id,
    lastamountchangedhistoryid as last_amount_changed_history_id,
    lastclosedatechangedhistoryid as last_close_date_changed_history_id,
    deliveryinstallationstatus__c as delivery_installation_status,
    trackingnumber__c as tracking_number,
    ordernumber__c as order_number,
    currentgenerators__c as current_generators,
    maincompetitors__c as main_competitors,
    isdeleted as is_deleted,
from "dbt"."staging"."stg_salesforce__opportunity"
where isdeleted = false
order by opportunity_name
    );
  
  
[0m11:44:00.740115 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.742202 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:44:00.742411 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity" rename to "dim_opportunity__dbt_backup"
[0m11:44:00.742755 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.744938 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:44:00.745229 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
alter table "dbt"."dim"."dim_opportunity__dbt_tmp" rename to "dim_opportunity"
[0m11:44:00.745641 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.746815 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m11:44:00.747097 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:44:00.747313 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: COMMIT
[0m11:44:00.748670 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.750386 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity"
[0m11:44:00.750600 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity"} */
drop table if exists "dbt"."dim"."dim_opportunity__dbt_backup" cascade
[0m11:44:00.751139 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.751865 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity (execute): 11:44:00.725296 => 11:44:00.751775
[0m11:44:00.752061 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity: Close
[0m11:44:00.771745 [info ] [Thread-1  ]: 21 of 30 OK created sql table model dim.dim_opportunity ........................ [[32mOK[0m in 0.05s]
[0m11:44:00.772166 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity
[0m11:44:00.772414 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:44:00.772767 [info ] [Thread-1  ]: 22 of 30 START sql table model dim.dim_opportunity_stage ....................... [RUN]
[0m11:44:00.773176 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity, now model.elastic_dbt_interview.dim_opportunity_stage)
[0m11:44:00.773393 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:44:00.776375 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:44:00.776932 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (compile): 11:44:00.773526 => 11:44:00.776826
[0m11:44:00.777196 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:44:00.779867 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:44:00.780756 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:44:00.781052 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: BEGIN
[0m11:44:00.781235 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.788382 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.788682 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:44:00.788889 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */

  
    
    

    create  table
      "dbt"."dim"."dim_opportunity_stage__dbt_tmp"
  
    as (
      

with
    source as (
        select distinct stagename as stage_name, stagesortorder as stage_sort_order
        from "dbt"."staging"."stg_salesforce__opportunity"
    )

select
    row_number() over (order by stage_sort_order) as stage_id,  -- Surrogate Key
    stage_name,
    stage_sort_order
from source
    );
  
  
[0m11:44:00.791209 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.793428 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:44:00.793662 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage" rename to "dim_opportunity_stage__dbt_backup"
[0m11:44:00.794001 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.795763 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:44:00.795966 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
alter table "dbt"."dim"."dim_opportunity_stage__dbt_tmp" rename to "dim_opportunity_stage"
[0m11:44:00.796257 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.797346 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m11:44:00.797607 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:44:00.797797 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: COMMIT
[0m11:44:00.798488 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.800252 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_opportunity_stage"
[0m11:44:00.800470 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_opportunity_stage"} */
drop table if exists "dbt"."dim"."dim_opportunity_stage__dbt_backup" cascade
[0m11:44:00.800879 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.801659 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_opportunity_stage (execute): 11:44:00.777392 => 11:44:00.801563
[0m11:44:00.801888 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_opportunity_stage: Close
[0m11:44:00.816877 [info ] [Thread-1  ]: 22 of 30 OK created sql table model dim.dim_opportunity_stage .................. [[32mOK[0m in 0.04s]
[0m11:44:00.817212 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_opportunity_stage
[0m11:44:00.817431 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_pricebook
[0m11:44:00.817758 [info ] [Thread-1  ]: 23 of 30 START sql table model dim.dim_pricebook ............................... [RUN]
[0m11:44:00.818210 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_opportunity_stage, now model.elastic_dbt_interview.dim_pricebook)
[0m11:44:00.818420 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_pricebook
[0m11:44:00.820658 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_pricebook"
[0m11:44:00.821185 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (compile): 11:44:00.818564 => 11:44:00.821079
[0m11:44:00.821385 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_pricebook
[0m11:44:00.824800 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_pricebook"
[0m11:44:00.825604 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:44:00.825790 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: BEGIN
[0m11:44:00.825976 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.832685 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.832969 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:44:00.833173 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */

  
    
    

    create  table
      "dbt"."dim"."dim_pricebook__dbt_tmp"
  
    as (
      

select
    pricebook_entry_id,
    pricebook2id as pricebook_id,
    product2id as product_id,
    unitprice as unit_price,
    isactive as is_active,
    usestandardprice as use_standard_price,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__pricebook_entry"
where isdeleted = false  -- Exclude deleted entries
order by pricebook_entry_id
    );
  
  
[0m11:44:00.835057 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.837309 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:44:00.837547 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook" rename to "dim_pricebook__dbt_backup"
[0m11:44:00.837949 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.839675 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:44:00.839873 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
alter table "dbt"."dim"."dim_pricebook__dbt_tmp" rename to "dim_pricebook"
[0m11:44:00.840163 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.841093 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m11:44:00.841299 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:44:00.841532 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: COMMIT
[0m11:44:00.842268 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.843828 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_pricebook"
[0m11:44:00.844020 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_pricebook"} */
drop table if exists "dbt"."dim"."dim_pricebook__dbt_backup" cascade
[0m11:44:00.844402 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.845158 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_pricebook (execute): 11:44:00.821517 => 11:44:00.845060
[0m11:44:00.845362 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_pricebook: Close
[0m11:44:00.861314 [info ] [Thread-1  ]: 23 of 30 OK created sql table model dim.dim_pricebook .......................... [[32mOK[0m in 0.04s]
[0m11:44:00.861729 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_pricebook
[0m11:44:00.861968 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_product
[0m11:44:00.862230 [info ] [Thread-1  ]: 24 of 30 START sql table model dim.dim_product ................................. [RUN]
[0m11:44:00.862591 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_pricebook, now model.elastic_dbt_interview.dim_product)
[0m11:44:00.862781 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_product
[0m11:44:00.864931 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_product"
[0m11:44:00.865383 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (compile): 11:44:00.862910 => 11:44:00.865286
[0m11:44:00.865575 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_product
[0m11:44:00.868190 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_product"
[0m11:44:00.868731 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:44:00.868939 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: BEGIN
[0m11:44:00.869165 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.875968 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.876211 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:44:00.876420 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */

  
    
    

    create  table
      "dbt"."dim"."dim_product__dbt_tmp"
  
    as (
      

select
    product_id,
    name as product_name,
    productcode as product_code,
    description as product_description,
    isactive as is_active,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    family as product_family,
    externaldatasourceid as external_datasource_id,
    externalid as external_id,
    displayurl as display_url,
    quantityunitofmeasure as quantity_unit_of_measure,
    stockkeepingunit as stock_keeping_unit,
    type as product_type,
    productclass as product_class,
    sourceproductid as source_product_id,
    sellerid as seller_id
from "dbt"."staging"."stg_salesforce__product_2"
where isdeleted = false
-- isactive =1 ?
order by product_name
    );
  
  
[0m11:44:00.878309 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.881205 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:44:00.881955 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product" rename to "dim_product__dbt_backup"
[0m11:44:00.882429 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.884563 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:44:00.884768 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
alter table "dbt"."dim"."dim_product__dbt_tmp" rename to "dim_product"
[0m11:44:00.885076 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.886126 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m11:44:00.886320 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:44:00.886491 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: COMMIT
[0m11:44:00.887370 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.888956 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_product"
[0m11:44:00.889159 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_product"} */
drop table if exists "dbt"."dim"."dim_product__dbt_backup" cascade
[0m11:44:00.889563 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.890263 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_product (execute): 11:44:00.865701 => 11:44:00.890174
[0m11:44:00.890457 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_product: Close
[0m11:44:00.906931 [info ] [Thread-1  ]: 24 of 30 OK created sql table model dim.dim_product ............................ [[32mOK[0m in 0.04s]
[0m11:44:00.907346 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_product
[0m11:44:00.907594 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_record_type
[0m11:44:00.907867 [info ] [Thread-1  ]: 25 of 30 START sql table model dim.dim_record_type ............................. [RUN]
[0m11:44:00.908223 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_product, now model.elastic_dbt_interview.dim_record_type)
[0m11:44:00.908417 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_record_type
[0m11:44:00.910604 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_record_type"
[0m11:44:00.911077 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (compile): 11:44:00.908550 => 11:44:00.910981
[0m11:44:00.911269 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_record_type
[0m11:44:00.913749 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_record_type"
[0m11:44:00.914197 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:44:00.914384 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: BEGIN
[0m11:44:00.914554 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.921540 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.921855 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:44:00.922068 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */

  
    
    

    create  table
      "dbt"."dim"."dim_record_type__dbt_tmp"
  
    as (
      

select
    record_type_id,
    name as record_type_name,
    modulenamespace as module_namespace,
    description as record_type_description,
    businessprocessid as business_process_id,
    sobjecttype as sobject_type,
    isactive as is_active,
    createdbyid as created_by_id,
    createddate as created_date,
    lastmodifiedbyid as last_modified_by_id,
    lastmodifieddate as last_modified_date,
    systemmodstamp as system_modstamp
from "dbt"."staging"."stg_salesforce__record_type"
where isdeleted = false
order by record_type_name
    );
  
  
[0m11:44:00.923421 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.926324 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:44:00.926557 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type" rename to "dim_record_type__dbt_backup"
[0m11:44:00.926975 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.928768 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:44:00.928985 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
alter table "dbt"."dim"."dim_record_type__dbt_tmp" rename to "dim_record_type"
[0m11:44:00.929297 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.930247 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m11:44:00.930441 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:44:00.930619 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: COMMIT
[0m11:44:00.931409 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.933022 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_record_type"
[0m11:44:00.933224 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_record_type"} */
drop table if exists "dbt"."dim"."dim_record_type__dbt_backup" cascade
[0m11:44:00.933619 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.934323 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_record_type (execute): 11:44:00.911400 => 11:44:00.934235
[0m11:44:00.934528 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_record_type: Close
[0m11:44:00.950801 [info ] [Thread-1  ]: 25 of 30 OK created sql table model dim.dim_record_type ........................ [[32mOK[0m in 0.04s]
[0m11:44:00.951219 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_record_type
[0m11:44:00.951462 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_solution
[0m11:44:00.951743 [info ] [Thread-1  ]: 26 of 30 START sql table model dim.dim_solution ................................ [RUN]
[0m11:44:00.952122 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_record_type, now model.elastic_dbt_interview.dim_solution)
[0m11:44:00.952319 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_solution
[0m11:44:00.954469 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_solution"
[0m11:44:00.955166 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (compile): 11:44:00.952453 => 11:44:00.955039
[0m11:44:00.955381 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_solution
[0m11:44:00.958207 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_solution"
[0m11:44:00.958797 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:44:00.959008 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: BEGIN
[0m11:44:00.959185 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:00.965938 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.966244 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:44:00.966464 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */

  
    
    

    create  table
      "dbt"."dim"."dim_solution__dbt_tmp"
  
    as (
      

select
    solution_id,
    solutionnumber as solution_number,
    solutionname as solution_name,
    ispublished as is_published,
    ispublishedinpublickb as is_published_in_public_kb,
    status as solution_status,
    isreviewed as is_reviewed,
    solutionnote as solution_note,
    caseid as case_id,
    ownerid as owner_id,
    createddate as created_date,
    createdbyid as created_by_id,
    lastmodifieddate as last_modified_date,
    lastmodifiedbyid as last_modified_by_id,
    systemmodstamp as system_modstamp,
    timesused as times_used,
    ishtml as is_html
from "dbt"."staging"."stg_salesforce__solution"
where isdeleted = false
order by solution_name
    );
  
  
[0m11:44:00.968221 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.970282 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:44:00.970496 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution" rename to "dim_solution__dbt_backup"
[0m11:44:00.970823 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.973296 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:44:00.973504 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
alter table "dbt"."dim"."dim_solution__dbt_tmp" rename to "dim_solution"
[0m11:44:00.973806 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.974723 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m11:44:00.974907 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:44:00.975078 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: COMMIT
[0m11:44:00.975906 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.977935 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_solution"
[0m11:44:00.978203 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_solution"} */
drop table if exists "dbt"."dim"."dim_solution__dbt_backup" cascade
[0m11:44:00.978723 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:00.979581 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_solution (execute): 11:44:00.955515 => 11:44:00.979481
[0m11:44:00.979814 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_solution: Close
[0m11:44:00.996575 [info ] [Thread-1  ]: 26 of 30 OK created sql table model dim.dim_solution ........................... [[32mOK[0m in 0.04s]
[0m11:44:00.996972 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_solution
[0m11:44:00.997200 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.dim_user
[0m11:44:00.997579 [info ] [Thread-1  ]: 27 of 30 START sql table model dim.dim_user .................................... [RUN]
[0m11:44:00.998058 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_solution, now model.elastic_dbt_interview.dim_user)
[0m11:44:00.998270 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.dim_user
[0m11:44:01.000600 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.dim_user"
[0m11:44:01.001411 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (compile): 11:44:00.998412 => 11:44:01.001298
[0m11:44:01.001621 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.dim_user
[0m11:44:01.004182 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.dim_user"
[0m11:44:01.005040 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:44:01.005332 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: BEGIN
[0m11:44:01.005583 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:01.012874 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.013162 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:44:01.013386 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */

  
    
    

    create  table
      "dbt"."dim"."dim_user__dbt_tmp"
  
    as (
      

select
    u.user_id,
    u.username as user_name,
    u.firstname as first_name,
    u.lastname as last_name,
    u.companyname as company_name,
    u.division as division,
    u.department as department,
    u.title as title,
    u.street as street,
    u.city as city,
    u.state as state,
    u.postalcode as postal_code,
    u.country as country,
    u.latitude as latitude,
    u.longitude as longitude,
    u.email as email,
    u.phone as phone,
    u.mobilephone as mobile_phone,
    u.alias as alias,
    u.isactive as is_active,
    u.timezonesidkey as timezone_sid_key,
    u.localesidkey as locale_sid_key,
    u.emailencodingkey as email_encoding_key,
    u.profileid as profile_id,
    u.usertype as user_type,
    u.usersubtype as user_subtype,
    u.lastlogindate as last_login_date,
    u.createddate as created_date,
    ur.name as role_name,
    ur.parentroleid as parent_role_id,
    ur.opportunityaccessforaccountowner as opportunity_access_for_account_owner,
    ur.caseaccessforaccountowner as case_access_for_account_owner,
    ur.contactaccessforaccountowner as contact_access_for_account_owner
from "dbt"."staging"."stg_salesforce__user" u
left join "dbt"."staging"."stg_salesforce__user_role" ur on u.userroleid = ur.user_role_id
where is_active = 1
    );
  
  
[0m11:44:01.017491 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.019778 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:44:01.020012 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user" rename to "dim_user__dbt_backup"
[0m11:44:01.020415 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.022257 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:44:01.022465 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
alter table "dbt"."dim"."dim_user__dbt_tmp" rename to "dim_user"
[0m11:44:01.022789 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.024607 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m11:44:01.024822 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:44:01.024992 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: COMMIT
[0m11:44:01.025955 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.027730 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.dim_user"
[0m11:44:01.028005 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.dim_user"} */
drop table if exists "dbt"."dim"."dim_user__dbt_backup" cascade
[0m11:44:01.028506 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.029355 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.dim_user (execute): 11:44:01.001755 => 11:44:01.029256
[0m11:44:01.029567 [debug] [Thread-1  ]: On model.elastic_dbt_interview.dim_user: Close
[0m11:44:01.047731 [info ] [Thread-1  ]: 27 of 30 OK created sql table model dim.dim_user ............................... [[32mOK[0m in 0.05s]
[0m11:44:01.048159 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.dim_user
[0m11:44:01.048412 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_campaign_performance
[0m11:44:01.048786 [info ] [Thread-1  ]: 28 of 30 START sql table model fact.fact_campaign_performance .................. [RUN]
[0m11:44:01.049185 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.dim_user, now model.elastic_dbt_interview.fact_campaign_performance)
[0m11:44:01.049385 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_campaign_performance
[0m11:44:01.051768 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:44:01.052721 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (compile): 11:44:01.049517 => 11:44:01.052604
[0m11:44:01.052934 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_campaign_performance
[0m11:44:01.055565 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:44:01.056189 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:44:01.056402 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: BEGIN
[0m11:44:01.056574 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:01.063149 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.063434 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:44:01.063654 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */

  
    
    

    create  table
      "dbt"."fact"."fact_campaign_performance__dbt_tmp"
  
    as (
      

WITH campaign_performance AS (
    SELECT
        c.campaign_id,
        c.created_at,
        c.last_modified_at,
        c.start_date,
        c.end_date,
        c.owner_id as user_id,
        o.opportunity_id,
        c.expected_revenue,
        c.budgeted_cost,
        c.actual_cost,
        c.number_sent,
        c.number_of_leads,
        c.number_of_converted_leads,
        c.number_of_contacts
    FROM "dbt"."dim"."dim_campaign" c
    LEFT JOIN "dbt"."dim"."dim_opportunity" o ON c.campaign_id = o.campaign_id
)

SELECT
    cp.campaign_id,
    cp.user_id,
    cp.opportunity_id,
    cp.start_date,
    cp.end_date,
    cp.expected_revenue,
    cp.budgeted_cost,
    cp.actual_cost,
    cp.number_sent,
    cp.number_of_leads,
    cp.number_of_converted_leads,
    cp.number_of_contacts
FROM campaign_performance cp


--need to be improved - No of leads closed based a particular campaign
    );
  
  
[0m11:44:01.065360 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.067672 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:44:01.067940 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
alter table "dbt"."fact"."fact_campaign_performance" rename to "fact_campaign_performance__dbt_backup"
[0m11:44:01.068320 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.070066 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:44:01.070268 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
alter table "dbt"."fact"."fact_campaign_performance__dbt_tmp" rename to "fact_campaign_performance"
[0m11:44:01.070561 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.071492 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: COMMIT
[0m11:44:01.071682 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:44:01.071854 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: COMMIT
[0m11:44:01.072525 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.074882 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_campaign_performance"
[0m11:44:01.075100 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_campaign_performance"} */
drop table if exists "dbt"."fact"."fact_campaign_performance__dbt_backup" cascade
[0m11:44:01.075502 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.076210 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_campaign_performance (execute): 11:44:01.053071 => 11:44:01.076115
[0m11:44:01.076415 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_campaign_performance: Close
[0m11:44:01.092519 [info ] [Thread-1  ]: 28 of 30 OK created sql table model fact.fact_campaign_performance ............. [[32mOK[0m in 0.04s]
[0m11:44:01.092952 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_campaign_performance
[0m11:44:01.093205 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_lead_conversion
[0m11:44:01.093477 [info ] [Thread-1  ]: 29 of 30 START sql table model fact.fact_lead_conversion ....................... [RUN]
[0m11:44:01.093832 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_campaign_performance, now model.elastic_dbt_interview.fact_lead_conversion)
[0m11:44:01.094042 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_lead_conversion
[0m11:44:01.096618 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:44:01.097133 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (compile): 11:44:01.094175 => 11:44:01.097020
[0m11:44:01.097348 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_lead_conversion
[0m11:44:01.099979 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:44:01.100775 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:44:01.101046 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: BEGIN
[0m11:44:01.101277 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:01.108407 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.108680 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:44:01.108919 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */

  
    
    

    create  table
      "dbt"."fact"."fact_lead_conversion__dbt_tmp"
  
    as (
      

with lead_data as (
    select 
        lead_id,
        lead_salutation,
        first_name,
        last_name,
        lead_company,
        lead_source,
        lead_status,
        is_converted,
        converted_date,
        converted_account_id,
        converted_contact_id,
        converted_opportunity_id,
        owner_id,  -- Assuming this is the user ID
        created_date as lead_created_date,
        last_modified_date as lead_last_modified_date
    from 
        "dbt"."dim"."dim_lead"
    -- where is_converted = true
),

account_data as (
    select 
        account_id,
        account_name
    from 
        "dbt"."dim"."dim_account"
),

opportunity_data as (
    select 
        opportunity_id,
        opportunity_name,
        opportunity_amount
    from 
        "dbt"."dim"."dim_opportunity"
),

user_data as (
    select 
        user_id,
        user_name,
        email as user_email
    from 
        "dbt"."dim"."dim_user"
)

select 
    l.lead_id,
    l.first_name,
    l.last_name,
    l.lead_company,
    l.lead_source,
    l.lead_status,
    l.converted_date,
    a.account_name as converted_account_name,
    o.opportunity_name as converted_opportunity_name,
    o.opportunity_amount as converted_opportunity_amount,
    u.user_name as lead_owner_name,
    u.user_email as lead_owner_email,
    l.lead_created_date,
    l.lead_last_modified_date
from 
    lead_data l
left join 
    account_data a on l.converted_account_id = a.account_id
left join 
    opportunity_data o on l.converted_opportunity_id = o.opportunity_id
left join 
    user_data u on l.owner_id = u.user_id
    );
  
  
[0m11:44:01.111587 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.113621 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:44:01.113847 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter table "dbt"."fact"."fact_lead_conversion" rename to "fact_lead_conversion__dbt_backup"
[0m11:44:01.114199 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.115895 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:44:01.116094 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
alter table "dbt"."fact"."fact_lead_conversion__dbt_tmp" rename to "fact_lead_conversion"
[0m11:44:01.116388 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.117311 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m11:44:01.117497 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:44:01.117667 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: COMMIT
[0m11:44:01.118513 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.120480 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_lead_conversion"
[0m11:44:01.120715 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_lead_conversion"} */
drop table if exists "dbt"."fact"."fact_lead_conversion__dbt_backup" cascade
[0m11:44:01.121133 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.121953 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_lead_conversion (execute): 11:44:01.097481 => 11:44:01.121853
[0m11:44:01.122164 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_lead_conversion: Close
[0m11:44:01.138705 [info ] [Thread-1  ]: 29 of 30 OK created sql table model fact.fact_lead_conversion .................. [[32mOK[0m in 0.04s]
[0m11:44:01.139085 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_lead_conversion
[0m11:44:01.139306 [debug] [Thread-1  ]: Began running node model.elastic_dbt_interview.fact_opportunity
[0m11:44:01.139679 [info ] [Thread-1  ]: 30 of 30 START sql incremental model fact.fact_opportunity ..................... [RUN]
[0m11:44:01.140179 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.elastic_dbt_interview.fact_lead_conversion, now model.elastic_dbt_interview.fact_opportunity)
[0m11:44:01.140396 [debug] [Thread-1  ]: Began compiling node model.elastic_dbt_interview.fact_opportunity
[0m11:44:01.147266 [debug] [Thread-1  ]: Writing injected SQL for node "model.elastic_dbt_interview.fact_opportunity"
[0m11:44:01.147844 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (compile): 11:44:01.140538 => 11:44:01.147737
[0m11:44:01.148048 [debug] [Thread-1  ]: Began executing node model.elastic_dbt_interview.fact_opportunity
[0m11:44:01.165298 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:44:01.165649 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

    
  
    
    

    create temporary table
      "fact_opportunity__dbt_tmp20240822114401162776"
  
    as (
      

WITH latest_data AS (
    SELECT
        o.opportunity_id,
        o.opportunity_amount,
        o.opportunity_probability,
        o.close_date,
        o.created_date,
        o.last_modified_date,
        a.account_name,
        a.account_id,
        u.user_name AS owner_username,
        d.date_key AS close_date_key
    FROM "dbt"."dim"."dim_opportunity" o
    LEFT JOIN "dbt"."dim"."dim_account" a
        ON o.account_id = a.account_id
    LEFT JOIN "dbt"."dim"."dim_user" u
        ON o.owner_id = u.user_id
    LEFT JOIN "dbt"."dim"."dim_date" d
        ON o.close_date = d.date_day  -- Ensure the column name matches the one in dim_date

    WHERE o.is_deleted = FALSE
    
        AND o.last_modified_date > (SELECT MAX(last_modified_date) FROM "dbt"."fact"."fact_opportunity")
    
)

SELECT * FROM latest_data
    );
  
  
  
[0m11:44:01.165876 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:44:01.175198 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.179345 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:44:01.179581 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: BEGIN
[0m11:44:01.179853 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.180031 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:44:01.180225 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity__dbt_tmp20240822114401162776'
      
      
      order by ordinal_position

  
[0m11:44:01.207096 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.210345 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:44:01.210594 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m11:44:01.224403 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.229330 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:44:01.229568 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from system.information_schema.columns
      where table_name = 'fact_opportunity'
      
      and table_schema = 'fact'
      
      
      and table_catalog = 'dbt'
      
      order by ordinal_position

  
[0m11:44:01.243342 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.251719 [debug] [Thread-1  ]: Writing runtime sql for node "model.elastic_dbt_interview.fact_opportunity"
[0m11:44:01.252481 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:44:01.252739 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: /* {"app": "dbt", "dbt_version": "1.6.18", "profile_name": "default", "target_name": "dev", "node_id": "model.elastic_dbt_interview.fact_opportunity"} */

        
            delete from "dbt"."fact"."fact_opportunity"
            where (
                opportunity_id) in (
                select (opportunity_id)
                from "fact_opportunity__dbt_tmp20240822114401162776"
            );

        
    

    insert into "dbt"."fact"."fact_opportunity" ("opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key")
    (
        select "opportunity_id", "opportunity_amount", "opportunity_probability", "close_date", "created_date", "last_modified_date", "account_name", "owner_username", "close_date_key"
        from "fact_opportunity__dbt_tmp20240822114401162776"
    )
  
[0m11:44:01.254100 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.255000 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m11:44:01.255202 [debug] [Thread-1  ]: Using duckdb connection "model.elastic_dbt_interview.fact_opportunity"
[0m11:44:01.255376 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: COMMIT
[0m11:44:01.255655 [debug] [Thread-1  ]: SQL status: OK in 0.0 seconds
[0m11:44:01.256118 [debug] [Thread-1  ]: Timing info for model.elastic_dbt_interview.fact_opportunity (execute): 11:44:01.148180 => 11:44:01.256023
[0m11:44:01.256341 [debug] [Thread-1  ]: On model.elastic_dbt_interview.fact_opportunity: Close
[0m11:44:01.259316 [info ] [Thread-1  ]: 30 of 30 OK created sql incremental model fact.fact_opportunity ................ [[32mOK[0m in 0.12s]
[0m11:44:01.259692 [debug] [Thread-1  ]: Finished running node model.elastic_dbt_interview.fact_opportunity
[0m11:44:01.260457 [debug] [MainThread]: Using duckdb connection "master"
[0m11:44:01.260632 [debug] [MainThread]: On master: BEGIN
[0m11:44:01.260779 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:44:01.267840 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:44:01.268118 [debug] [MainThread]: On master: COMMIT
[0m11:44:01.268276 [debug] [MainThread]: Using duckdb connection "master"
[0m11:44:01.268422 [debug] [MainThread]: On master: COMMIT
[0m11:44:01.268618 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m11:44:01.268774 [debug] [MainThread]: On master: Close
[0m11:44:01.270943 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:44:01.271196 [debug] [MainThread]: Connection 'model.elastic_dbt_interview.fact_opportunity' was properly closed.
[0m11:44:01.271447 [info ] [MainThread]: 
[0m11:44:01.271668 [info ] [MainThread]: Finished running 15 table models, 14 view models, 1 incremental model in 0 hours 0 minutes and 1.98 seconds (1.98s).
[0m11:44:01.273890 [debug] [MainThread]: Command end result
[0m11:44:01.282200 [info ] [MainThread]: 
[0m11:44:01.282485 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:44:01.282681 [info ] [MainThread]: 
[0m11:44:01.282857 [info ] [MainThread]: Done. PASS=30 WARN=0 ERROR=0 SKIP=0 TOTAL=30
[0m11:44:01.283211 [debug] [MainThread]: Command `dbt run` succeeded at 11:44:01.283163 after 2.18 seconds
[0m11:44:01.283397 [debug] [MainThread]: Flushing usage events
